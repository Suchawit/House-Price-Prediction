{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from current directory\n",
    "df = pd.read_csv('./train.csv')\n",
    "#test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11200</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11924</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12968</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>91.0</td>\n",
       "      <td>10652</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10920</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11241</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>RL</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10791</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13695</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>COD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>101.0</td>\n",
       "      <td>14215</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>RM</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7449</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Bnk</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9742</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>120</td>\n",
       "      <td>RM</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4224</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8246</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>110.0</td>\n",
       "      <td>14230</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7200</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>98.0</td>\n",
       "      <td>11478</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>47.0</td>\n",
       "      <td>16321</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>RM</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6324</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>68500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1431</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21930</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR3</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>192140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1432</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4928</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>1433</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10800</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Grvl</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>64500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1434</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10261</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1435</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>17400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Low</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1436</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>COD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>1437</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1438</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12444</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>394617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1439</td>\n",
       "      <td>20</td>\n",
       "      <td>RM</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7407</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11584</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1441</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>79.0</td>\n",
       "      <td>11526</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Bnk</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1442</td>\n",
       "      <td>120</td>\n",
       "      <td>RM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4426</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1443</td>\n",
       "      <td>60</td>\n",
       "      <td>FV</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11003</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1444</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8854</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1445</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8500</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>1446</td>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>1447</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26142</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1448</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>1449</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11767</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdWo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>1450</td>\n",
       "      <td>180</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1533</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>92000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1451</td>\n",
       "      <td>90</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>1452</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9262</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>287090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1453</td>\n",
       "      <td>180</td>\n",
       "      <td>RM</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3675</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1454</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>90.0</td>\n",
       "      <td>17217</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>84500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>1455</td>\n",
       "      <td>20</td>\n",
       "      <td>FV</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "5        6          50       RL         85.0    14115   Pave   NaN      IR1   \n",
       "6        7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
       "7        8          60       RL          NaN    10382   Pave   NaN      IR1   \n",
       "8        9          50       RM         51.0     6120   Pave   NaN      Reg   \n",
       "9       10         190       RL         50.0     7420   Pave   NaN      Reg   \n",
       "10      11          20       RL         70.0    11200   Pave   NaN      Reg   \n",
       "11      12          60       RL         85.0    11924   Pave   NaN      IR1   \n",
       "12      13          20       RL          NaN    12968   Pave   NaN      IR2   \n",
       "13      14          20       RL         91.0    10652   Pave   NaN      IR1   \n",
       "14      15          20       RL          NaN    10920   Pave   NaN      IR1   \n",
       "15      16          45       RM         51.0     6120   Pave   NaN      Reg   \n",
       "16      17          20       RL          NaN    11241   Pave   NaN      IR1   \n",
       "17      18          90       RL         72.0    10791   Pave   NaN      Reg   \n",
       "18      19          20       RL         66.0    13695   Pave   NaN      Reg   \n",
       "19      20          20       RL         70.0     7560   Pave   NaN      Reg   \n",
       "20      21          60       RL        101.0    14215   Pave   NaN      IR1   \n",
       "21      22          45       RM         57.0     7449   Pave  Grvl      Reg   \n",
       "22      23          20       RL         75.0     9742   Pave   NaN      Reg   \n",
       "23      24         120       RM         44.0     4224   Pave   NaN      Reg   \n",
       "24      25          20       RL          NaN     8246   Pave   NaN      IR1   \n",
       "25      26          20       RL        110.0    14230   Pave   NaN      Reg   \n",
       "26      27          20       RL         60.0     7200   Pave   NaN      Reg   \n",
       "27      28          20       RL         98.0    11478   Pave   NaN      Reg   \n",
       "28      29          20       RL         47.0    16321   Pave   NaN      IR1   \n",
       "29      30          30       RM         60.0     6324   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1430  1431          60       RL         60.0    21930   Pave   NaN      IR3   \n",
       "1431  1432         120       RL          NaN     4928   Pave   NaN      IR1   \n",
       "1432  1433          30       RL         60.0    10800   Pave  Grvl      Reg   \n",
       "1433  1434          60       RL         93.0    10261   Pave   NaN      IR1   \n",
       "1434  1435          20       RL         80.0    17400   Pave   NaN      Reg   \n",
       "1435  1436          20       RL         80.0     8400   Pave   NaN      Reg   \n",
       "1436  1437          20       RL         60.0     9000   Pave   NaN      Reg   \n",
       "1437  1438          20       RL         96.0    12444   Pave   NaN      Reg   \n",
       "1438  1439          20       RM         90.0     7407   Pave   NaN      Reg   \n",
       "1439  1440          60       RL         80.0    11584   Pave   NaN      Reg   \n",
       "1440  1441          70       RL         79.0    11526   Pave   NaN      IR1   \n",
       "1441  1442         120       RM          NaN     4426   Pave   NaN      Reg   \n",
       "1442  1443          60       FV         85.0    11003   Pave   NaN      Reg   \n",
       "1443  1444          30       RL          NaN     8854   Pave   NaN      Reg   \n",
       "1444  1445          20       RL         63.0     8500   Pave   NaN      Reg   \n",
       "1445  1446          85       RL         70.0     8400   Pave   NaN      Reg   \n",
       "1446  1447          20       RL          NaN    26142   Pave   NaN      IR1   \n",
       "1447  1448          60       RL         80.0    10000   Pave   NaN      Reg   \n",
       "1448  1449          50       RL         70.0    11767   Pave   NaN      Reg   \n",
       "1449  1450         180       RM         21.0     1533   Pave   NaN      Reg   \n",
       "1450  1451          90       RL         60.0     9000   Pave   NaN      Reg   \n",
       "1451  1452          20       RL         78.0     9262   Pave   NaN      Reg   \n",
       "1452  1453         180       RM         35.0     3675   Pave   NaN      Reg   \n",
       "1453  1454          20       RL         90.0    17217   Pave   NaN      Reg   \n",
       "1454  1455          20       FV         62.0     7500   Pave  Pave      Reg   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "5            Lvl    AllPub  ...        0    NaN  MnPrv        Shed     700   \n",
       "6            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "7            Lvl    AllPub  ...        0    NaN    NaN        Shed     350   \n",
       "8            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "9            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "10           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "11           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "12           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "13           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "14           Lvl    AllPub  ...        0    NaN   GdWo         NaN       0   \n",
       "15           Lvl    AllPub  ...        0    NaN  GdPrv         NaN       0   \n",
       "16           Lvl    AllPub  ...        0    NaN    NaN        Shed     700   \n",
       "17           Lvl    AllPub  ...        0    NaN    NaN        Shed     500   \n",
       "18           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "19           Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "20           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "21           Bnk    AllPub  ...        0    NaN  GdPrv         NaN       0   \n",
       "22           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "23           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "24           Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "25           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "26           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "27           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "28           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "29           Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1430         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1431         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1432         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1433         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1434         Low    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1435         Lvl    AllPub  ...        0    NaN  GdPrv         NaN       0   \n",
       "1436         Lvl    AllPub  ...        0    NaN   GdWo         NaN       0   \n",
       "1437         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1438         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1439         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1440         Bnk    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1441         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1442         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1443         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1444         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1445         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1446         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1447         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1448         Lvl    AllPub  ...        0    NaN   GdWo         NaN       0   \n",
       "1449         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1450         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1451         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1452         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1453         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1454         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "5        10   2009        WD         Normal     143000  \n",
       "6         8   2007        WD         Normal     307000  \n",
       "7        11   2009        WD         Normal     200000  \n",
       "8         4   2008        WD        Abnorml     129900  \n",
       "9         1   2008        WD         Normal     118000  \n",
       "10        2   2008        WD         Normal     129500  \n",
       "11        7   2006       New        Partial     345000  \n",
       "12        9   2008        WD         Normal     144000  \n",
       "13        8   2007       New        Partial     279500  \n",
       "14        5   2008        WD         Normal     157000  \n",
       "15        7   2007        WD         Normal     132000  \n",
       "16        3   2010        WD         Normal     149000  \n",
       "17       10   2006        WD         Normal      90000  \n",
       "18        6   2008        WD         Normal     159000  \n",
       "19        5   2009       COD        Abnorml     139000  \n",
       "20       11   2006       New        Partial     325300  \n",
       "21        6   2007        WD         Normal     139400  \n",
       "22        9   2008        WD         Normal     230000  \n",
       "23        6   2007        WD         Normal     129900  \n",
       "24        5   2010        WD         Normal     154000  \n",
       "25        7   2009        WD         Normal     256300  \n",
       "26        5   2010        WD         Normal     134800  \n",
       "27        5   2010        WD         Normal     306000  \n",
       "28       12   2006        WD         Normal     207500  \n",
       "29        5   2008        WD         Normal      68500  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1430      7   2006        WD         Normal     192140  \n",
       "1431     10   2009        WD         Normal     143750  \n",
       "1432      8   2007        WD         Normal      64500  \n",
       "1433      5   2008        WD         Normal     186500  \n",
       "1434      5   2006        WD         Normal     160000  \n",
       "1435      7   2008       COD        Abnorml     174000  \n",
       "1436      5   2007        WD         Normal     120500  \n",
       "1437     11   2008       New        Partial     394617  \n",
       "1438      4   2010        WD         Normal     149700  \n",
       "1439     11   2007        WD         Normal     197000  \n",
       "1440      9   2008        WD         Normal     191000  \n",
       "1441      5   2008        WD         Normal     149300  \n",
       "1442      4   2009        WD         Normal     310000  \n",
       "1443      5   2009        WD         Normal     121000  \n",
       "1444     11   2007        WD         Normal     179600  \n",
       "1445      5   2007        WD         Normal     129000  \n",
       "1446      4   2010        WD         Normal     157900  \n",
       "1447     12   2007        WD         Normal     240000  \n",
       "1448      5   2007        WD         Normal     112000  \n",
       "1449      8   2006        WD        Abnorml      92000  \n",
       "1450      9   2009        WD         Normal     136000  \n",
       "1451      5   2009       New        Partial     287090  \n",
       "1452      5   2006        WD         Normal     145000  \n",
       "1453      7   2006        WD        Abnorml      84500  \n",
       "1454     10   2009        WD         Normal     185000  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PoolQC          1453\n",
       " MiscFeature     1406\n",
       " Alley           1369\n",
       " Fence           1179\n",
       " FireplaceQu      690\n",
       " LotFrontage      259\n",
       " GarageCond        81\n",
       " GarageType        81\n",
       " GarageYrBlt       81\n",
       " GarageFinish      81\n",
       " GarageQual        81\n",
       " BsmtExposure      38\n",
       " BsmtFinType2      38\n",
       " BsmtFinType1      37\n",
       " BsmtCond          37\n",
       " BsmtQual          37\n",
       " MasVnrArea         8\n",
       " MasVnrType         8\n",
       " Electrical         1\n",
       " Utilities          0\n",
       " YearRemodAdd       0\n",
       " MSSubClass         0\n",
       " Foundation         0\n",
       " ExterCond          0\n",
       " ExterQual          0\n",
       " Exterior2nd        0\n",
       " Exterior1st        0\n",
       " RoofMatl           0\n",
       " RoofStyle          0\n",
       " YearBuilt          0\n",
       " LotConfig          0\n",
       " OverallCond        0\n",
       " OverallQual        0\n",
       " dtype: int64, <matplotlib.axes._subplots.AxesSubplot at 0x1df049ce5f8>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the missing values and drop columns which have missing values more than 70%\n",
    "df.isnull().sum().sort_values(ascending=False)[0:33],sns.heatmap(df.isnull(),yticklabels=False, cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AllPub    1459\n",
       "NoSeWa       1\n",
       "Name: Utilities, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not much information\n",
    "df.Utilities.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values in these columns\n",
    "list1 = ['Alley','Utilities', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "for item in list1:\n",
    "    df.drop(columns=item, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of object and numerical type column\n",
    "def get_object_cols(df):\n",
    "    return list(df.select_dtypes(include='object').columns)\n",
    "\n",
    "def get_numerical_cols(df):\n",
    "    return list(df.select_dtypes(exclude='object').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MSZoning',\n",
       "  'Street',\n",
       "  'LotShape',\n",
       "  'LandContour',\n",
       "  'LotConfig',\n",
       "  'LandSlope',\n",
       "  'Neighborhood',\n",
       "  'Condition1',\n",
       "  'Condition2',\n",
       "  'BldgType',\n",
       "  'HouseStyle',\n",
       "  'RoofStyle',\n",
       "  'RoofMatl',\n",
       "  'Exterior1st',\n",
       "  'Exterior2nd',\n",
       "  'MasVnrType',\n",
       "  'ExterQual',\n",
       "  'ExterCond',\n",
       "  'Foundation',\n",
       "  'BsmtQual',\n",
       "  'BsmtCond',\n",
       "  'BsmtExposure',\n",
       "  'BsmtFinType1',\n",
       "  'BsmtFinType2',\n",
       "  'Heating',\n",
       "  'HeatingQC',\n",
       "  'CentralAir',\n",
       "  'Electrical',\n",
       "  'KitchenQual',\n",
       "  'Functional',\n",
       "  'FireplaceQu',\n",
       "  'GarageType',\n",
       "  'GarageFinish',\n",
       "  'GarageQual',\n",
       "  'GarageCond',\n",
       "  'PavedDrive',\n",
       "  'SaleType',\n",
       "  'SaleCondition'],\n",
       " 38)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the object type column\n",
    "object_cols_train = get_object_cols(df)\n",
    "object_cols_train, len(object_cols_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping object type to numerical\n",
    "## selecting columns for mapping dict\n",
    "\n",
    "dict={'Y' : 1, 'N' : 2, 'Ex': 1, 'Gd' : 2, 'TA' :3, 'Fa' : 4, 'Po' : 5,  \n",
    "     'GLQ' : 1, 'ALQ' : 2, 'BLQ' : 3, 'Rec' : 4, 'LwQ' : 5, 'Unf' : 6, 'NA' :7,\n",
    "     'Gd' : 1 , 'Av' :2, 'Mn' : 3, 'No' :4, 'Gtl' : 1, 'Mod' : 2, 'Sev' :3,\n",
    "      'Reg' : 1, 'IR1' :2, 'IR2' :3, 'IR3' :4}\n",
    "\n",
    "\n",
    "# 'RL':1, 'RM':2,'FV':3,'RH':4,'C (all)':5, 'Pave':1, 'Grvl':2,'Lvl':1,'Bnk':1,'HLS':2,'Low':3,'Inside':1, 'Corner':2,'CulDSac':3,\n",
    "# 'FR2':4, 'FR3':5, 'Y':1, 'N':2, 'P':3,'Norm':1, 'Feedr':2, 'PosN':3, 'Artery':4, 'RRAe':5, 'RRNn':6, 'RRAn':7, 'PosA':8,'RRNe':9 \n",
    "cols=['KitchenQual','LotShape','LandSlope','HeatingQC','FireplaceQu','ExterQual','ExterCond','BsmtQual',\n",
    "     'BsmtFinType2','BsmtFinType1','BsmtExposure','BsmtCond','CentralAir']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'RL' :1, 'RM' :2,'FV':3,'RH' :4,'C (all)' :5, 'Pave' :1, 'Grvl' :2,\n",
    "        'Lvl' :1,'Bnk' :1,'HLS':2,'Low' :3,'Inside' :1, 'Corner' :2,'CulDSac' :3,\n",
    "       'FR2' :4, 'FR3' :5, 'Y':1, 'N' :2, 'P':3,'Norm' :1, 'Feedr' :2, 'PosN' :3, 'Artery' :4,\n",
    "        'RRAe' :5, 'RRNn' :6, 'RRAn' :7, 'PosA':8,'RRNe' :9, 'TA': 1, 'Fa':2, 'Gd':3, 'Po':4, 'Ex':5}\n",
    "cols=['Street','GarageQual','MSZoning', 'LandContour', 'Condition1', 'Condition2', 'GarageCond']\n",
    "# \n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FR2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>FR2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11924</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12968</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>10652</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10920</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11241</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10791</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13695</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7560</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>COD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>14215</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7449</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9742</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8246</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>14230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>11478</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>16321</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6324</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>68500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1431</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21930</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>192140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1432</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4928</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>1433</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10800</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>64500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1434</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10261</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1435</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>17400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Inside</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1436</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>COD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>1437</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FR2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1438</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12444</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FR2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>394617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1439</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11584</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1441</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>79.0</td>\n",
       "      <td>11526</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1442</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4426</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>149300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1443</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1444</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8854</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1445</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FR2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>1446</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>1447</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26142</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CulDSac</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1448</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>1449</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11767</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>1450</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>92000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1451</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FR2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>1452</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9262</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>New</td>\n",
       "      <td>Partial</td>\n",
       "      <td>287090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1453</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3675</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1454</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>90.0</td>\n",
       "      <td>17217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>84500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>1455</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  \\\n",
       "0        1          60         1         65.0     8450       1         1   \n",
       "1        2          20         1         80.0     9600       1         1   \n",
       "2        3          60         1         68.0    11250       1         2   \n",
       "3        4          70         1         60.0     9550       1         2   \n",
       "4        5          60         1         84.0    14260       1         2   \n",
       "5        6          50         1         85.0    14115       1         2   \n",
       "6        7          20         1         75.0    10084       1         1   \n",
       "7        8          60         1          NaN    10382       1         2   \n",
       "8        9          50         2         51.0     6120       1         1   \n",
       "9       10         190         1         50.0     7420       1         1   \n",
       "10      11          20         1         70.0    11200       1         1   \n",
       "11      12          60         1         85.0    11924       1         2   \n",
       "12      13          20         1          NaN    12968       1         3   \n",
       "13      14          20         1         91.0    10652       1         2   \n",
       "14      15          20         1          NaN    10920       1         2   \n",
       "15      16          45         2         51.0     6120       1         1   \n",
       "16      17          20         1          NaN    11241       1         2   \n",
       "17      18          90         1         72.0    10791       1         1   \n",
       "18      19          20         1         66.0    13695       1         1   \n",
       "19      20          20         1         70.0     7560       1         1   \n",
       "20      21          60         1        101.0    14215       1         2   \n",
       "21      22          45         2         57.0     7449       1         1   \n",
       "22      23          20         1         75.0     9742       1         1   \n",
       "23      24         120         2         44.0     4224       1         1   \n",
       "24      25          20         1          NaN     8246       1         2   \n",
       "25      26          20         1        110.0    14230       1         1   \n",
       "26      27          20         1         60.0     7200       1         1   \n",
       "27      28          20         1         98.0    11478       1         1   \n",
       "28      29          20         1         47.0    16321       1         2   \n",
       "29      30          30         2         60.0     6324       1         2   \n",
       "...    ...         ...       ...          ...      ...     ...       ...   \n",
       "1430  1431          60         1         60.0    21930       1         4   \n",
       "1431  1432         120         1          NaN     4928       1         2   \n",
       "1432  1433          30         1         60.0    10800       1         1   \n",
       "1433  1434          60         1         93.0    10261       1         2   \n",
       "1434  1435          20         1         80.0    17400       1         1   \n",
       "1435  1436          20         1         80.0     8400       1         1   \n",
       "1436  1437          20         1         60.0     9000       1         1   \n",
       "1437  1438          20         1         96.0    12444       1         1   \n",
       "1438  1439          20         2         90.0     7407       1         1   \n",
       "1439  1440          60         1         80.0    11584       1         1   \n",
       "1440  1441          70         1         79.0    11526       1         2   \n",
       "1441  1442         120         2          NaN     4426       1         1   \n",
       "1442  1443          60         3         85.0    11003       1         1   \n",
       "1443  1444          30         1          NaN     8854       1         1   \n",
       "1444  1445          20         1         63.0     8500       1         1   \n",
       "1445  1446          85         1         70.0     8400       1         1   \n",
       "1446  1447          20         1          NaN    26142       1         2   \n",
       "1447  1448          60         1         80.0    10000       1         1   \n",
       "1448  1449          50         1         70.0    11767       1         1   \n",
       "1449  1450         180         2         21.0     1533       1         1   \n",
       "1450  1451          90         1         60.0     9000       1         1   \n",
       "1451  1452          20         1         78.0     9262       1         1   \n",
       "1452  1453         180         2         35.0     3675       1         1   \n",
       "1453  1454          20         1         90.0    17217       1         1   \n",
       "1454  1455          20         3         62.0     7500       1         1   \n",
       "1455  1456          60         1         62.0     7917       1         1   \n",
       "1456  1457          20         1         85.0    13175       1         1   \n",
       "1457  1458          70         1         66.0     9042       1         1   \n",
       "1458  1459          20         1         68.0     9717       1         1   \n",
       "1459  1460          20         1         75.0     9937       1         1   \n",
       "\n",
       "      LandContour LotConfig  LandSlope  ... EnclosedPorch  3SsnPorch  \\\n",
       "0               1    Inside          1  ...             0          0   \n",
       "1               1       FR2          1  ...             0          0   \n",
       "2               1    Inside          1  ...             0          0   \n",
       "3               1    Corner          1  ...           272          0   \n",
       "4               1       FR2          1  ...             0          0   \n",
       "5               1    Inside          1  ...             0        320   \n",
       "6               1    Inside          1  ...             0          0   \n",
       "7               1    Corner          1  ...           228          0   \n",
       "8               1    Inside          1  ...           205          0   \n",
       "9               1    Corner          1  ...             0          0   \n",
       "10              1    Inside          1  ...             0          0   \n",
       "11              1    Inside          1  ...             0          0   \n",
       "12              1    Inside          1  ...             0          0   \n",
       "13              1    Inside          1  ...             0          0   \n",
       "14              1    Corner          1  ...           176          0   \n",
       "15              1    Corner          1  ...             0          0   \n",
       "16              1   CulDSac          1  ...             0          0   \n",
       "17              1    Inside          1  ...             0          0   \n",
       "18              1    Inside          1  ...             0          0   \n",
       "19              1    Inside          1  ...             0          0   \n",
       "20              1    Corner          1  ...             0          0   \n",
       "21              1    Inside          1  ...           205          0   \n",
       "22              1    Inside          1  ...             0          0   \n",
       "23              1    Inside          1  ...             0          0   \n",
       "24              1    Inside          1  ...             0          0   \n",
       "25              1    Corner          1  ...             0          0   \n",
       "26              1    Corner          1  ...             0          0   \n",
       "27              1    Inside          1  ...             0          0   \n",
       "28              1   CulDSac          1  ...             0          0   \n",
       "29              1    Inside          1  ...            87          0   \n",
       "...           ...       ...        ...  ...           ...        ...   \n",
       "1430            1    Inside          1  ...             0          0   \n",
       "1431            1    Inside          1  ...             0          0   \n",
       "1432            1    Inside          1  ...             0          0   \n",
       "1433            1    Inside          1  ...             0          0   \n",
       "1434            3    Inside          2  ...             0          0   \n",
       "1435            1    Inside          1  ...             0          0   \n",
       "1436            1       FR2          1  ...             0          0   \n",
       "1437            1       FR2          1  ...             0        304   \n",
       "1438            1    Inside          1  ...           158          0   \n",
       "1439            1    Inside          1  ...           216          0   \n",
       "1440            1    Inside          2  ...             0          0   \n",
       "1441            1    Inside          1  ...             0          0   \n",
       "1442            1    Inside          1  ...             0          0   \n",
       "1443            1    Inside          1  ...             0          0   \n",
       "1444            1       FR2          1  ...             0          0   \n",
       "1445            1    Inside          1  ...           252          0   \n",
       "1446            1   CulDSac          1  ...             0          0   \n",
       "1447            1    Inside          1  ...             0          0   \n",
       "1448            1    Inside          1  ...             0          0   \n",
       "1449            1    Inside          1  ...             0          0   \n",
       "1450            1       FR2          1  ...             0          0   \n",
       "1451            1    Inside          1  ...             0          0   \n",
       "1452            1    Inside          1  ...             0          0   \n",
       "1453            1    Inside          1  ...             0          0   \n",
       "1454            1    Inside          1  ...             0          0   \n",
       "1455            1    Inside          1  ...             0          0   \n",
       "1456            1    Inside          1  ...             0          0   \n",
       "1457            1    Inside          1  ...             0          0   \n",
       "1458            1    Inside          1  ...           112          0   \n",
       "1459            1    Inside          1  ...             0          0   \n",
       "\n",
       "      ScreenPorch PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition  \\\n",
       "0               0        0       0       2    2008        WD         Normal   \n",
       "1               0        0       0       5    2007        WD         Normal   \n",
       "2               0        0       0       9    2008        WD         Normal   \n",
       "3               0        0       0       2    2006        WD        Abnorml   \n",
       "4               0        0       0      12    2008        WD         Normal   \n",
       "5               0        0     700      10    2009        WD         Normal   \n",
       "6               0        0       0       8    2007        WD         Normal   \n",
       "7               0        0     350      11    2009        WD         Normal   \n",
       "8               0        0       0       4    2008        WD        Abnorml   \n",
       "9               0        0       0       1    2008        WD         Normal   \n",
       "10              0        0       0       2    2008        WD         Normal   \n",
       "11              0        0       0       7    2006       New        Partial   \n",
       "12            176        0       0       9    2008        WD         Normal   \n",
       "13              0        0       0       8    2007       New        Partial   \n",
       "14              0        0       0       5    2008        WD         Normal   \n",
       "15              0        0       0       7    2007        WD         Normal   \n",
       "16              0        0     700       3    2010        WD         Normal   \n",
       "17              0        0     500      10    2006        WD         Normal   \n",
       "18              0        0       0       6    2008        WD         Normal   \n",
       "19              0        0       0       5    2009       COD        Abnorml   \n",
       "20              0        0       0      11    2006       New        Partial   \n",
       "21              0        0       0       6    2007        WD         Normal   \n",
       "22              0        0       0       9    2008        WD         Normal   \n",
       "23              0        0       0       6    2007        WD         Normal   \n",
       "24              0        0       0       5    2010        WD         Normal   \n",
       "25              0        0       0       7    2009        WD         Normal   \n",
       "26              0        0       0       5    2010        WD         Normal   \n",
       "27              0        0       0       5    2010        WD         Normal   \n",
       "28              0        0       0      12    2006        WD         Normal   \n",
       "29              0        0       0       5    2008        WD         Normal   \n",
       "...           ...      ...     ...     ...     ...       ...            ...   \n",
       "1430            0        0       0       7    2006        WD         Normal   \n",
       "1431            0        0       0      10    2009        WD         Normal   \n",
       "1432            0        0       0       8    2007        WD         Normal   \n",
       "1433            0        0       0       5    2008        WD         Normal   \n",
       "1434            0        0       0       5    2006        WD         Normal   \n",
       "1435            0        0       0       7    2008       COD        Abnorml   \n",
       "1436            0        0       0       5    2007        WD         Normal   \n",
       "1437            0        0       0      11    2008       New        Partial   \n",
       "1438            0        0       0       4    2010        WD         Normal   \n",
       "1439            0        0       0      11    2007        WD         Normal   \n",
       "1440            0        0       0       9    2008        WD         Normal   \n",
       "1441            0        0       0       5    2008        WD         Normal   \n",
       "1442            0        0       0       4    2009        WD         Normal   \n",
       "1443           40        0       0       5    2009        WD         Normal   \n",
       "1444            0        0       0      11    2007        WD         Normal   \n",
       "1445            0        0       0       5    2007        WD         Normal   \n",
       "1446            0        0       0       4    2010        WD         Normal   \n",
       "1447            0        0       0      12    2007        WD         Normal   \n",
       "1448            0        0       0       5    2007        WD         Normal   \n",
       "1449            0        0       0       8    2006        WD        Abnorml   \n",
       "1450            0        0       0       9    2009        WD         Normal   \n",
       "1451            0        0       0       5    2009       New        Partial   \n",
       "1452            0        0       0       5    2006        WD         Normal   \n",
       "1453            0        0       0       7    2006        WD        Abnorml   \n",
       "1454            0        0       0      10    2009        WD         Normal   \n",
       "1455            0        0       0       8    2007        WD         Normal   \n",
       "1456            0        0       0       2    2010        WD         Normal   \n",
       "1457            0        0    2500       5    2010        WD         Normal   \n",
       "1458            0        0       0       4    2010        WD         Normal   \n",
       "1459            0        0       0       6    2008        WD         Normal   \n",
       "\n",
       "     SalePrice  \n",
       "0       208500  \n",
       "1       181500  \n",
       "2       223500  \n",
       "3       140000  \n",
       "4       250000  \n",
       "5       143000  \n",
       "6       307000  \n",
       "7       200000  \n",
       "8       129900  \n",
       "9       118000  \n",
       "10      129500  \n",
       "11      345000  \n",
       "12      144000  \n",
       "13      279500  \n",
       "14      157000  \n",
       "15      132000  \n",
       "16      149000  \n",
       "17       90000  \n",
       "18      159000  \n",
       "19      139000  \n",
       "20      325300  \n",
       "21      139400  \n",
       "22      230000  \n",
       "23      129900  \n",
       "24      154000  \n",
       "25      256300  \n",
       "26      134800  \n",
       "27      306000  \n",
       "28      207500  \n",
       "29       68500  \n",
       "...        ...  \n",
       "1430    192140  \n",
       "1431    143750  \n",
       "1432     64500  \n",
       "1433    186500  \n",
       "1434    160000  \n",
       "1435    174000  \n",
       "1436    120500  \n",
       "1437    394617  \n",
       "1438    149700  \n",
       "1439    197000  \n",
       "1440    191000  \n",
       "1441    149300  \n",
       "1442    310000  \n",
       "1443    121000  \n",
       "1444    179600  \n",
       "1445    129000  \n",
       "1446    157900  \n",
       "1447    240000  \n",
       "1448    112000  \n",
       "1449     92000  \n",
       "1450    136000  \n",
       "1451    287090  \n",
       "1452    145000  \n",
       "1453     84500  \n",
       "1454    185000  \n",
       "1455    175000  \n",
       "1456    210000  \n",
       "1457    266500  \n",
       "1458    142125  \n",
       "1459    147500  \n",
       "\n",
       "[1460 rows x 76 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 76 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null int64\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null int64\n",
      "LotShape         1460 non-null int64\n",
      "LandContour      1460 non-null int64\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null int64\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null int64\n",
      "Condition2       1460 non-null int64\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null int64\n",
      "ExterCond        1460 non-null int64\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null float64\n",
      "BsmtCond         1423 non-null float64\n",
      "BsmtExposure     1422 non-null float64\n",
      "BsmtFinType1     1423 non-null float64\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null float64\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null int64\n",
      "CentralAir       1460 non-null int64\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null int64\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null float64\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null float64\n",
      "GarageCond       1379 non-null float64\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(11), int64(47), object(18)\n",
      "memory usage: 867.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain object type column and numerical\n",
    "object_cols_train = get_object_cols(df)\n",
    "# train numerical cols\n",
    "numerical_cols_train = get_numerical_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['LotConfig',\n",
       "  'Neighborhood',\n",
       "  'BldgType',\n",
       "  'HouseStyle',\n",
       "  'RoofStyle',\n",
       "  'RoofMatl',\n",
       "  'Exterior1st',\n",
       "  'Exterior2nd',\n",
       "  'MasVnrType',\n",
       "  'Foundation',\n",
       "  'Heating',\n",
       "  'Electrical',\n",
       "  'Functional',\n",
       "  'GarageType',\n",
       "  'GarageFinish',\n",
       "  'PavedDrive',\n",
       "  'SaleType',\n",
       "  'SaleCondition'],\n",
       " 18)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_cols_train,len(object_cols_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAmes      225\n",
       "CollgCr    150\n",
       "OldTown    113\n",
       "Edwards    100\n",
       "Somerst     86\n",
       "Gilbert     79\n",
       "NridgHt     77\n",
       "Sawyer      74\n",
       "NWAmes      73\n",
       "SawyerW     59\n",
       "BrkSide     58\n",
       "Crawfor     51\n",
       "Mitchel     49\n",
       "NoRidge     41\n",
       "Timber      38\n",
       "IDOTRR      37\n",
       "ClearCr     28\n",
       "SWISU       25\n",
       "StoneBr     25\n",
       "Blmngtn     17\n",
       "MeadowV     17\n",
       "BrDale      16\n",
       "Veenker     11\n",
       "NPkVill      9\n",
       "Blueste      2\n",
       "Name: Neighborhood, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Neighborhood.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create the dictionary to map the label for each column\n",
    "df.Neighborhood.unique()\n",
    "\n",
    "dict = {'CollgCr':1, 'Veenker':2, 'Crawfor':3, 'NoRidge':4, 'Mitchel':5, 'Somerst':6,\n",
    "       'NWAmes':7, 'OldTown':8, 'BrkSide':9, 'Sawyer':10, 'NridgHt':11, 'NAmes':12,\n",
    "       'SawyerW':13, 'IDOTRR':14, 'MeadowV':15, 'Edwards':16, 'Timber':17, 'Gilbert':18,\n",
    "       'StoneBr':19, 'ClearCr':20, 'NPkVill':21, 'Blmngtn':22, 'BrDale':23, 'SWISU':24,\n",
    "       'Blueste':25}\n",
    "\n",
    "cols=['Neighborhood']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.LotConfig.unique()\n",
    "dict={'Inside':1, 'FR2':2, 'Corner':3, 'CulDSac':4, 'FR3':5}\n",
    "cols=['LotConfig']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BldgType.unique()\n",
    "dict={'1Fam':1, '2fmCon':2, 'Duplex':3, 'TwnhsE':4, 'Twnhs':5}\n",
    "cols=['BldgType']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.HouseStyle.unique()\n",
    "dict={'2Story':1, '1Story':2, '1.5Fin':3, '1.5Unf':4, 'SFoyer':5, 'SLvl':6, '2.5Unf':7,\n",
    "       '2.5Fin':8}\n",
    "cols=['HouseStyle']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RoofStyle.unique()\n",
    "dict={'Gable':1, 'Hip':2, 'Gambrel':3, 'Mansard':4, 'Flat':5, 'Shed':6}\n",
    "cols=['RoofStyle']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RoofMatl.unique()\n",
    "dict={'CompShg':1, 'WdShngl':2, 'Metal':3, 'WdShake':4, 'Membran':5, 'Tar&Grv':6,\n",
    "       'Roll':7, 'ClyTile':8}\n",
    "cols=['RoofMatl']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Exterior1st.unique()\n",
    "dict={'VinylSd':1, 'MetalSd':2, 'Wd Sdng':3, 'HdBoard':4, 'BrkFace':5, 'WdShing':6,\n",
    "       'CemntBd':7, 'Plywood':8, 'AsbShng':9, 'Stucco':10, 'BrkComm':11, 'AsphShn':12,\n",
    "       'Stone':13, 'ImStucc':14, 'CBlock':15}\n",
    "cols=['Exterior1st']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Exterior2nd.unique()\n",
    "dict={'VinylSd':1, 'MetalSd':2, 'Wd Shng':3, 'HdBoard':4, 'Plywood':5, 'Wd Sdng':6,\n",
    "       'CmentBd':7, 'BrkFace':8, 'Stucco':9, 'AsbShng':10, 'Brk Cmn':11, 'ImStucc':12,\n",
    "       'AsphShn':13, 'Stone':14, 'Other':15, 'CBlock':16}\n",
    "cols=['Exterior2nd']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.MasVnrType.unique()\n",
    "dict={'BrkFace':1, 'None':2, 'Stone':3, 'BrkCmn':4}\n",
    "cols=['MasVnrType']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Foundation.unique()\n",
    "dict={'PConc':1, 'CBlock':2, 'BrkTil':3, 'Wood':4, 'Slab':5, 'Stone':6}\n",
    "cols=['Foundation']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Heating.unique()\n",
    "dict={'GasA':1, 'GasW':2, 'Grav':3, 'Wall':4, 'OthW':5, 'Floor':6}\n",
    "cols=['Heating']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Electrical.unique()\n",
    "dict={'SBrkr':1, 'FuseF':2, 'FuseA':3, 'FuseP':4, 'Mix':5}\n",
    "cols=['Electrical']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Functional.unique()\n",
    "dict={'Typ':1, 'Min1':2, 'Maj1':3, 'Min2':4, 'Mod':5, 'Maj2':6, 'Sev':7}\n",
    "cols=['Functional']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.GarageType.unique()\n",
    "dict={'Attchd':1, 'Detchd':2, 'BuiltIn':3, 'CarPort':4, 'Basment':5, '2Types':6}\n",
    "cols=['GarageType']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.GarageFinish.unique()\n",
    "dict={'RFn':1, 'Unf':2, 'Fin':3}\n",
    "cols=['GarageFinish']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PavedDrive.unique()\n",
    "dict={'Y':1, 'N':2, 'P':3}\n",
    "cols=['PavedDrive']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SaleType.unique()\n",
    "dict={'WD':1, 'New':2, 'COD':3, 'ConLD':4, 'ConLI':5, 'CWD':6, 'ConLw':7, 'Con':8, 'Oth':9}\n",
    "cols=['SaleType']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SaleCondition.unique()\n",
    "dict={'Normal':1, 'Abnorml':2, 'Partial':3, 'AdjLand':4, 'Alloca':5, 'Family':6}\n",
    "cols=['SaleCondition']\n",
    "for i in cols:\n",
    "    df[i]=df[i].map(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain object type column and numerical\n",
    "object_cols_train = get_object_cols(df)\n",
    "object_cols_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    690\n",
       "1.0    404\n",
       "3.0    313\n",
       "4.0     33\n",
       "5.0     20\n",
       "Name: FireplaceQu, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.FireplaceQu.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FireplaceQu      690\n",
       "LotFrontage      259\n",
       "GarageCond        81\n",
       "GarageType        81\n",
       "GarageYrBlt       81\n",
       "GarageFinish      81\n",
       "GarageQual        81\n",
       "BsmtExposure      38\n",
       "BsmtFinType2      38\n",
       "BsmtFinType1      37\n",
       "BsmtCond          37\n",
       "BsmtQual          37\n",
       "MasVnrType         8\n",
       "MasVnrArea         8\n",
       "Electrical         1\n",
       "YearBuilt          0\n",
       "Exterior2nd        0\n",
       "Exterior1st        0\n",
       "ExterQual          0\n",
       "ExterCond          0\n",
       "Foundation         0\n",
       "RoofMatl           0\n",
       "RoofStyle          0\n",
       "YearRemodAdd       0\n",
       "SalePrice          0\n",
       "OverallCond        0\n",
       "OverallQual        0\n",
       "HouseStyle         0\n",
       "BldgType           0\n",
       "Condition2         0\n",
       "                ... \n",
       "GarageArea         0\n",
       "PavedDrive         0\n",
       "WoodDeckSF         0\n",
       "OpenPorchSF        0\n",
       "3SsnPorch          0\n",
       "BsmtUnfSF          0\n",
       "ScreenPorch        0\n",
       "PoolArea           0\n",
       "MiscVal            0\n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "Functional         0\n",
       "TotRmsAbvGrd       0\n",
       "KitchenQual        0\n",
       "KitchenAbvGr       0\n",
       "BedroomAbvGr       0\n",
       "HalfBath           0\n",
       "FullBath           0\n",
       "BsmtHalfBath       0\n",
       "BsmtFullBath       0\n",
       "GrLivArea          0\n",
       "LowQualFinSF       0\n",
       "2ndFlrSF           0\n",
       "1stFlrSF           0\n",
       "CentralAir         0\n",
       "HeatingQC          0\n",
       "SaleCondition      0\n",
       "TotalBsmtSF        0\n",
       "Id                 0\n",
       "Length: 76, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NaN to mean\n",
    "for item in df.columns:\n",
    "    df[item].fillna((df[item].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y for training a model not including object column\n",
    "X=df.drop(['SalePrice'],axis=1)\n",
    "Y=df['SalePrice']\n",
    "#for item in object_cols_train:\n",
    "    #X.drop(columns=item, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>14115</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>10084</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10382</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>204</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>1</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>7420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>11200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>11924</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>12968</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>10652</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10920</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>213</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>11241</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>10791</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>13695</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>7560</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>14215</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>7449</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>9742</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>4224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>8246</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>14230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>7200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>11478</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>16321</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>6324</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1431</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>21930</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1432</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>4928</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>1433</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>10800</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1434</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>10261</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1435</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>17400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1436</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>1437</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>9000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1438</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>12444</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2008</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1439</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>7407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1440</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11584</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1441</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>11526</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1442</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>4426</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1443</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>11003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1444</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>8854</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1445</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>8500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>1446</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>8400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>1447</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>26142</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1448</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>1449</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>11767</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>1450</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1451</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>9000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>1452</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>9262</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1453</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>3675</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1454</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>17217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>1455</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7917</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>13175</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>9042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>9717</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>9937</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  \\\n",
       "0        1          60         1    65.000000     8450       1         1   \n",
       "1        2          20         1    80.000000     9600       1         1   \n",
       "2        3          60         1    68.000000    11250       1         2   \n",
       "3        4          70         1    60.000000     9550       1         2   \n",
       "4        5          60         1    84.000000    14260       1         2   \n",
       "5        6          50         1    85.000000    14115       1         2   \n",
       "6        7          20         1    75.000000    10084       1         1   \n",
       "7        8          60         1    70.049958    10382       1         2   \n",
       "8        9          50         2    51.000000     6120       1         1   \n",
       "9       10         190         1    50.000000     7420       1         1   \n",
       "10      11          20         1    70.000000    11200       1         1   \n",
       "11      12          60         1    85.000000    11924       1         2   \n",
       "12      13          20         1    70.049958    12968       1         3   \n",
       "13      14          20         1    91.000000    10652       1         2   \n",
       "14      15          20         1    70.049958    10920       1         2   \n",
       "15      16          45         2    51.000000     6120       1         1   \n",
       "16      17          20         1    70.049958    11241       1         2   \n",
       "17      18          90         1    72.000000    10791       1         1   \n",
       "18      19          20         1    66.000000    13695       1         1   \n",
       "19      20          20         1    70.000000     7560       1         1   \n",
       "20      21          60         1   101.000000    14215       1         2   \n",
       "21      22          45         2    57.000000     7449       1         1   \n",
       "22      23          20         1    75.000000     9742       1         1   \n",
       "23      24         120         2    44.000000     4224       1         1   \n",
       "24      25          20         1    70.049958     8246       1         2   \n",
       "25      26          20         1   110.000000    14230       1         1   \n",
       "26      27          20         1    60.000000     7200       1         1   \n",
       "27      28          20         1    98.000000    11478       1         1   \n",
       "28      29          20         1    47.000000    16321       1         2   \n",
       "29      30          30         2    60.000000     6324       1         2   \n",
       "...    ...         ...       ...          ...      ...     ...       ...   \n",
       "1430  1431          60         1    60.000000    21930       1         4   \n",
       "1431  1432         120         1    70.049958     4928       1         2   \n",
       "1432  1433          30         1    60.000000    10800       1         1   \n",
       "1433  1434          60         1    93.000000    10261       1         2   \n",
       "1434  1435          20         1    80.000000    17400       1         1   \n",
       "1435  1436          20         1    80.000000     8400       1         1   \n",
       "1436  1437          20         1    60.000000     9000       1         1   \n",
       "1437  1438          20         1    96.000000    12444       1         1   \n",
       "1438  1439          20         2    90.000000     7407       1         1   \n",
       "1439  1440          60         1    80.000000    11584       1         1   \n",
       "1440  1441          70         1    79.000000    11526       1         2   \n",
       "1441  1442         120         2    70.049958     4426       1         1   \n",
       "1442  1443          60         3    85.000000    11003       1         1   \n",
       "1443  1444          30         1    70.049958     8854       1         1   \n",
       "1444  1445          20         1    63.000000     8500       1         1   \n",
       "1445  1446          85         1    70.000000     8400       1         1   \n",
       "1446  1447          20         1    70.049958    26142       1         2   \n",
       "1447  1448          60         1    80.000000    10000       1         1   \n",
       "1448  1449          50         1    70.000000    11767       1         1   \n",
       "1449  1450         180         2    21.000000     1533       1         1   \n",
       "1450  1451          90         1    60.000000     9000       1         1   \n",
       "1451  1452          20         1    78.000000     9262       1         1   \n",
       "1452  1453         180         2    35.000000     3675       1         1   \n",
       "1453  1454          20         1    90.000000    17217       1         1   \n",
       "1454  1455          20         3    62.000000     7500       1         1   \n",
       "1455  1456          60         1    62.000000     7917       1         1   \n",
       "1456  1457          20         1    85.000000    13175       1         1   \n",
       "1457  1458          70         1    66.000000     9042       1         1   \n",
       "1458  1459          20         1    68.000000     9717       1         1   \n",
       "1459  1460          20         1    75.000000     9937       1         1   \n",
       "\n",
       "      LandContour  LotConfig  LandSlope  ...  OpenPorchSF  EnclosedPorch  \\\n",
       "0               1          1          1  ...           61              0   \n",
       "1               1          2          1  ...            0              0   \n",
       "2               1          1          1  ...           42              0   \n",
       "3               1          3          1  ...           35            272   \n",
       "4               1          2          1  ...           84              0   \n",
       "5               1          1          1  ...           30              0   \n",
       "6               1          1          1  ...           57              0   \n",
       "7               1          3          1  ...          204            228   \n",
       "8               1          1          1  ...            0            205   \n",
       "9               1          3          1  ...            4              0   \n",
       "10              1          1          1  ...            0              0   \n",
       "11              1          1          1  ...           21              0   \n",
       "12              1          1          1  ...            0              0   \n",
       "13              1          1          1  ...           33              0   \n",
       "14              1          3          1  ...          213            176   \n",
       "15              1          3          1  ...          112              0   \n",
       "16              1          4          1  ...            0              0   \n",
       "17              1          1          1  ...            0              0   \n",
       "18              1          1          1  ...          102              0   \n",
       "19              1          1          1  ...            0              0   \n",
       "20              1          3          1  ...          154              0   \n",
       "21              1          1          1  ...            0            205   \n",
       "22              1          1          1  ...          159              0   \n",
       "23              1          1          1  ...          110              0   \n",
       "24              1          1          1  ...           90              0   \n",
       "25              1          3          1  ...           56              0   \n",
       "26              1          3          1  ...           32              0   \n",
       "27              1          1          1  ...           50              0   \n",
       "28              1          4          1  ...          258              0   \n",
       "29              1          1          1  ...            0             87   \n",
       "...           ...        ...        ...  ...          ...            ...   \n",
       "1430            1          1          1  ...           40              0   \n",
       "1431            1          1          1  ...           60              0   \n",
       "1432            1          1          1  ...            0              0   \n",
       "1433            1          1          1  ...            0              0   \n",
       "1434            3          1          2  ...           41              0   \n",
       "1435            1          1          1  ...           36              0   \n",
       "1436            1          2          1  ...            0              0   \n",
       "1437            1          2          1  ...           66              0   \n",
       "1438            1          1          1  ...          158            158   \n",
       "1439            1          1          1  ...           88            216   \n",
       "1440            1          1          2  ...            0              0   \n",
       "1441            1          1          1  ...            0              0   \n",
       "1442            1          1          1  ...           52              0   \n",
       "1443            1          1          1  ...           98              0   \n",
       "1444            1          2          1  ...           60              0   \n",
       "1445            1          1          1  ...            0            252   \n",
       "1446            1          4          1  ...           39              0   \n",
       "1447            1          1          1  ...           65              0   \n",
       "1448            1          1          1  ...           24              0   \n",
       "1449            1          1          1  ...            0              0   \n",
       "1450            1          2          1  ...           45              0   \n",
       "1451            1          1          1  ...           36              0   \n",
       "1452            1          1          1  ...           28              0   \n",
       "1453            1          1          1  ...           56              0   \n",
       "1454            1          1          1  ...          113              0   \n",
       "1455            1          1          1  ...           40              0   \n",
       "1456            1          1          1  ...            0              0   \n",
       "1457            1          1          1  ...           60              0   \n",
       "1458            1          1          1  ...            0            112   \n",
       "1459            1          1          1  ...           68              0   \n",
       "\n",
       "      3SsnPorch  ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  SaleType  \\\n",
       "0             0            0         0        0       2    2008         1   \n",
       "1             0            0         0        0       5    2007         1   \n",
       "2             0            0         0        0       9    2008         1   \n",
       "3             0            0         0        0       2    2006         1   \n",
       "4             0            0         0        0      12    2008         1   \n",
       "5           320            0         0      700      10    2009         1   \n",
       "6             0            0         0        0       8    2007         1   \n",
       "7             0            0         0      350      11    2009         1   \n",
       "8             0            0         0        0       4    2008         1   \n",
       "9             0            0         0        0       1    2008         1   \n",
       "10            0            0         0        0       2    2008         1   \n",
       "11            0            0         0        0       7    2006         2   \n",
       "12            0          176         0        0       9    2008         1   \n",
       "13            0            0         0        0       8    2007         2   \n",
       "14            0            0         0        0       5    2008         1   \n",
       "15            0            0         0        0       7    2007         1   \n",
       "16            0            0         0      700       3    2010         1   \n",
       "17            0            0         0      500      10    2006         1   \n",
       "18            0            0         0        0       6    2008         1   \n",
       "19            0            0         0        0       5    2009         3   \n",
       "20            0            0         0        0      11    2006         2   \n",
       "21            0            0         0        0       6    2007         1   \n",
       "22            0            0         0        0       9    2008         1   \n",
       "23            0            0         0        0       6    2007         1   \n",
       "24            0            0         0        0       5    2010         1   \n",
       "25            0            0         0        0       7    2009         1   \n",
       "26            0            0         0        0       5    2010         1   \n",
       "27            0            0         0        0       5    2010         1   \n",
       "28            0            0         0        0      12    2006         1   \n",
       "29            0            0         0        0       5    2008         1   \n",
       "...         ...          ...       ...      ...     ...     ...       ...   \n",
       "1430          0            0         0        0       7    2006         1   \n",
       "1431          0            0         0        0      10    2009         1   \n",
       "1432          0            0         0        0       8    2007         1   \n",
       "1433          0            0         0        0       5    2008         1   \n",
       "1434          0            0         0        0       5    2006         1   \n",
       "1435          0            0         0        0       7    2008         3   \n",
       "1436          0            0         0        0       5    2007         1   \n",
       "1437        304            0         0        0      11    2008         2   \n",
       "1438          0            0         0        0       4    2010         1   \n",
       "1439          0            0         0        0      11    2007         1   \n",
       "1440          0            0         0        0       9    2008         1   \n",
       "1441          0            0         0        0       5    2008         1   \n",
       "1442          0            0         0        0       4    2009         1   \n",
       "1443          0           40         0        0       5    2009         1   \n",
       "1444          0            0         0        0      11    2007         1   \n",
       "1445          0            0         0        0       5    2007         1   \n",
       "1446          0            0         0        0       4    2010         1   \n",
       "1447          0            0         0        0      12    2007         1   \n",
       "1448          0            0         0        0       5    2007         1   \n",
       "1449          0            0         0        0       8    2006         1   \n",
       "1450          0            0         0        0       9    2009         1   \n",
       "1451          0            0         0        0       5    2009         2   \n",
       "1452          0            0         0        0       5    2006         1   \n",
       "1453          0            0         0        0       7    2006         1   \n",
       "1454          0            0         0        0      10    2009         1   \n",
       "1455          0            0         0        0       8    2007         1   \n",
       "1456          0            0         0        0       2    2010         1   \n",
       "1457          0            0         0     2500       5    2010         1   \n",
       "1458          0            0         0        0       4    2010         1   \n",
       "1459          0            0         0        0       6    2008         1   \n",
       "\n",
       "      SaleCondition  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 2  \n",
       "4                 1  \n",
       "5                 1  \n",
       "6                 1  \n",
       "7                 1  \n",
       "8                 2  \n",
       "9                 1  \n",
       "10                1  \n",
       "11                3  \n",
       "12                1  \n",
       "13                3  \n",
       "14                1  \n",
       "15                1  \n",
       "16                1  \n",
       "17                1  \n",
       "18                1  \n",
       "19                2  \n",
       "20                3  \n",
       "21                1  \n",
       "22                1  \n",
       "23                1  \n",
       "24                1  \n",
       "25                1  \n",
       "26                1  \n",
       "27                1  \n",
       "28                1  \n",
       "29                1  \n",
       "...             ...  \n",
       "1430              1  \n",
       "1431              1  \n",
       "1432              1  \n",
       "1433              1  \n",
       "1434              1  \n",
       "1435              2  \n",
       "1436              1  \n",
       "1437              3  \n",
       "1438              1  \n",
       "1439              1  \n",
       "1440              1  \n",
       "1441              1  \n",
       "1442              1  \n",
       "1443              1  \n",
       "1444              1  \n",
       "1445              1  \n",
       "1446              1  \n",
       "1447              1  \n",
       "1448              1  \n",
       "1449              2  \n",
       "1450              1  \n",
       "1451              3  \n",
       "1452              1  \n",
       "1453              2  \n",
       "1454              1  \n",
       "1455              1  \n",
       "1456              1  \n",
       "1457              1  \n",
       "1458              1  \n",
       "1459              1  \n",
       "\n",
       "[1460 rows x 75 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=None, subsample=1, verbosity=0)\n"
     ]
    }
   ],
   "source": [
    "# split data to train and test as 80% and 20%\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# Create a model\n",
    "\n",
    "xgbr = xgb.XGBRegressor(verbosity=0) \n",
    "print(xgbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbr.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = xgbr.score(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.9648151292579609\n"
     ]
    }
   ],
   "source": [
    "print(\"Training score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation score: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(xgbr, xtrain, ytrain,cv=10)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([141266.78 , 339928.7  , 124062.805, 152385.14 , 325916.6  ,\n",
       "        82814.195, 228171.39 , 143718.61 ,  82152.73 , 134925.7  ,\n",
       "       155228.02 , 125940.64 , 116206.49 , 202343.5  , 171119.38 ,\n",
       "       138163.9  , 195286.7  , 137797.83 , 112486.   , 213213.17 ,\n",
       "       161271.47 , 217943.38 , 173923.03 , 122646.78 , 200779.62 ,\n",
       "       165282.83 , 193429.   , 105406.4  , 175361.92 , 188020.4  ,\n",
       "       119858.63 , 247805.92 , 227479.66 , 117741.52 , 247967.81 ,\n",
       "       146802.36 , 130567.125, 207249.75 , 333621.   , 103307.15 ,\n",
       "       125428.22 , 247470.97 , 116469.2  , 367081.6  , 123693.98 ,\n",
       "       139344.73 , 119943.16 , 127987.06 , 422576.75 , 134394.53 ,\n",
       "       122936.65 , 196280.98 , 108808.56 , 378596.7  , 146872.38 ,\n",
       "       246463.66 , 197714.94 , 157522.33 , 141996.11 ,  98958.44 ,\n",
       "        72907.7  , 157028.44 , 301410.53 , 290121.44 , 283312.78 ,\n",
       "       224365.39 , 110699.22 , 342185.4  , 116987.98 , 156255.7  ,\n",
       "       130877.17 , 121017.02 , 114044.305,  87654.6  , 318354.56 ,\n",
       "       169003.19 , 320039.1  , 303021.88 , 139210.5  , 120678.016,\n",
       "       110524.43 ,  73960.305, 126224.414,  96225.05 , 147305.58 ,\n",
       "       130464.27 , 271579.6  , 195823.88 , 144669.86 , 178369.95 ,\n",
       "       145426.95 , 138019.5  , 129698.94 , 260002.1  , 111586.48 ,\n",
       "       172411.88 , 178225.8  , 167629.8  , 207221.42 , 244624.08 ,\n",
       "       183528.81 , 206999.   , 308714.4  , 136718.66 , 191216.06 ,\n",
       "       148153.98 , 150869.36 , 273705.34 , 138903.2  , 183362.23 ,\n",
       "        53522.44 , 122242.46 , 136862.58 , 123525.6  , 198655.19 ,\n",
       "       124416.87 , 107829.01 , 104120.68 , 148071.92 , 273820.75 ,\n",
       "       148299.7  , 145740.27 , 174996.67 , 192644.1  , 179351.42 ,\n",
       "       134388.08 , 233121.1  , 100597.234, 149774.08 , 184834.86 ,\n",
       "       195751.2  , 373146.2  , 194096.12 , 144815.66 ,  61863.645,\n",
       "       338596.88 , 414423.84 , 125949.37 , 224500.7  , 610827.8  ,\n",
       "       383689.9  , 134925.47 , 169438.62 , 159360.8  , 145936.61 ,\n",
       "       129475.29 , 225302.39 , 190890.33 , 120859.65 ,  63029.844,\n",
       "       108897.64 , 140844.97 , 234878.48 , 157064.81 ,  97470.836,\n",
       "       128417.99 , 150314.31 , 142344.14 ,  96659.27 , 133276.27 ,\n",
       "       217216.83 , 142112.42 , 317571.8  , 143944.62 , 110860.15 ,\n",
       "       100545.95 , 223510.19 , 351298.84 , 447651.78 , 232216.2  ,\n",
       "       378573.6  ,  96985.266, 116940.164, 157612.33 , 333955.94 ,\n",
       "       135670.34 , 130203.14 , 206355.75 , 119896.39 , 170500.52 ,\n",
       "       179523.8  , 101881.35 , 128701.54 , 145071.9  , 269505.3  ,\n",
       "       158144.78 , 303732.5  , 220716.45 , 183923.12 ,  87919.42 ,\n",
       "       110097.83 , 108936.14 , 137550.27 , 169350.98 , 171418.22 ,\n",
       "       172393.42 , 237382.6  ,  79097.44 , 203824.42 , 119968.87 ,\n",
       "       226229.56 , 184896.02 , 120011.4  , 356497.34 , 190044.56 ,\n",
       "       126426.07 , 252304.12 , 137029.12 , 155125.38 , 114874.89 ,\n",
       "       240934.95 , 134987.47 , 116924.95 , 148932.36 , 212910.64 ,\n",
       "       275528.88 , 180863.12 , 142061.77 , 116040.055, 140838.28 ,\n",
       "       142368.62 , 232044.25 , 197679.97 ,  94955.91 , 247806.22 ,\n",
       "       155105.73 ,  78179.32 , 105438.43 , 173270.22 , 103299.266,\n",
       "       111107.62 , 186093.45 , 129020.7  , 143303.97 , 236596.84 ,\n",
       "       127546.805, 194488.27 , 152648.38 , 240550.66 , 131048.18 ,\n",
       "       118251.31 , 235022.86 , 204039.6  , 418291.6  , 188518.33 ,\n",
       "       127443.43 , 146980.23 , 174568.47 , 145086.27 ,  99154.31 ,\n",
       "       175640.75 , 169381.33 , 126707.18 ,  95488.805, 140245.44 ,\n",
       "       140563.64 , 114036.38 , 107917.28 , 177167.61 , 276855.66 ,\n",
       "       299741.5  , 165102.1  , 132643.75 , 233196.67 , 272624.56 ,\n",
       "       208030.23 , 170500.28 , 141169.05 , 111405.46 , 167242.56 ,\n",
       "       403907.06 , 223628.88 , 222219.33 ,  91684.7  , 108035.195,\n",
       "       125473.4  , 154188.56 , 291444.06 , 213574.84 , 144578.   ,\n",
       "       201750.58 ,  97862.37 , 195088.4  , 108599.78 , 323889.2  ,\n",
       "       172210.47 , 217341.55 , 120874.47 , 239989.64 , 185337.06 ,\n",
       "       117067.555, 116548.19 ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict output\n",
    "y_pred=xgbr.predict(xtest)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892     154500\n",
       "1105    325000\n",
       "413     115000\n",
       "522     159000\n",
       "1036    315500\n",
       "614      75500\n",
       "218     311500\n",
       "1160    146000\n",
       "649      84500\n",
       "887     135500\n",
       "576     145000\n",
       "1252    130000\n",
       "1061     81000\n",
       "567     214000\n",
       "1108    181000\n",
       "1113    134500\n",
       "168     183500\n",
       "1102    135000\n",
       "1120    118400\n",
       "67      226000\n",
       "1040    155000\n",
       "453     210000\n",
       "670     173500\n",
       "1094    129000\n",
       "192     192000\n",
       "123     153900\n",
       "415     181134\n",
       "277     141000\n",
       "433     181000\n",
       "1317    208900\n",
       "         ...  \n",
       "233     128200\n",
       "426     275000\n",
       "196     311872\n",
       "1226    214000\n",
       "81      153500\n",
       "1368    144000\n",
       "1125    115000\n",
       "111     180000\n",
       "1243    465000\n",
       "744     180000\n",
       "937     253000\n",
       "344      85000\n",
       "1232    101800\n",
       "865     148500\n",
       "1088    137500\n",
       "350     318061\n",
       "588     143000\n",
       "1427    140000\n",
       "948     192500\n",
       "1449     92000\n",
       "989     197000\n",
       "677     109500\n",
       "478     297000\n",
       "1271    185750\n",
       "1410    230000\n",
       "479      89471\n",
       "1361    260000\n",
       "802     189000\n",
       "651     108000\n",
       "722     124500\n",
       "Name: SalePrice, Length: 292, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892     13233.218750\n",
       "1105   -14928.687500\n",
       "413     -9062.804688\n",
       "522      6614.859375\n",
       "1036   -10416.593750\n",
       "614     -7314.195312\n",
       "218     83328.609375\n",
       "1160     2281.390625\n",
       "649      2347.273438\n",
       "887       574.296875\n",
       "576    -10228.015625\n",
       "1252     4059.359375\n",
       "1061   -35206.492188\n",
       "567     11656.500000\n",
       "1108     9880.625000\n",
       "1113    -3663.906250\n",
       "168    -11786.703125\n",
       "1102    -2797.828125\n",
       "1120     5914.000000\n",
       "67      12786.828125\n",
       "1040    -6271.468750\n",
       "453     -7943.375000\n",
       "670      -423.031250\n",
       "1094     6353.218750\n",
       "192     -8779.625000\n",
       "123    -11382.828125\n",
       "415    -12295.000000\n",
       "277     35593.601562\n",
       "433      5638.078125\n",
       "1317    20879.593750\n",
       "            ...     \n",
       "233     -4443.750000\n",
       "426     41803.328125\n",
       "196     39247.437500\n",
       "1226     5969.765625\n",
       "81     -17000.281250\n",
       "1368     2830.953125\n",
       "1125     3594.539062\n",
       "111     12757.437500\n",
       "1243    61092.937500\n",
       "744    -43628.875000\n",
       "937     30780.671875\n",
       "344     -6684.703125\n",
       "1232    -6235.195312\n",
       "865     23026.601562\n",
       "1088   -16688.562500\n",
       "350     26616.937500\n",
       "588    -70574.843750\n",
       "1427    -4578.000000\n",
       "948     -9250.578125\n",
       "1449    -5862.367188\n",
       "989      1911.593750\n",
       "677       900.218750\n",
       "478    -26889.187500\n",
       "1271    13539.531250\n",
       "1410    12658.453125\n",
       "479    -31403.468750\n",
       "1361    20010.359375\n",
       "802      3662.937500\n",
       "651     -9067.554688\n",
       "722      7951.812500\n",
       "Name: SalePrice, Length: 292, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest-y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 17724.98678296233\n",
      "Mean Squared Error: 935230170.8312684\n",
      "Root Mean Squared Error: 30581.533166786594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "vytest= ytest.to_numpy()\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(vytest, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(vytest, y_pred))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(vytest, y_pred))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare actual and predicted values\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "axs[0].set_title('Actual value')\n",
    "axs[0].plot(vytest)\n",
    "axs[1].set_title('Predicted value')\n",
    "axs[1].plot(y_pred)\n",
    "axs[2].set_title('Compared values')\n",
    "axs[2].plot(vytest)\n",
    "axs[2].plot(y_pred)\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.45, gamma=0.045,\n",
      "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1.8, missing=None, n_estimators=2200,\n",
      "       n_jobs=1, nthread=-1, objective='reg:linear', random_state=6,\n",
      "       reg_alpha=0.45, reg_lambda=0.85, scale_pos_weight=1, seed=None,\n",
      "       silent=1, subsample=0.52, verbosity=0)\n"
     ]
    }
   ],
   "source": [
    "# XGB with regularization\n",
    "reg_xgb = xgb.XGBRegressor(colsample_bytree=0.45, gamma=0.045, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.8, n_estimators=2200,\n",
    "                             reg_alpha=0.45, reg_lambda=0.85,\n",
    "                             subsample=0.52, silent=1,\n",
    "                             random_state =6, nthread = -1,verbosity=0)\n",
    "print(reg_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.45, gamma=0.045,\n",
       "       importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1.8, missing=None, n_estimators=2200,\n",
       "       n_jobs=1, nthread=-1, objective='reg:linear', random_state=6,\n",
       "       reg_alpha=0.45, reg_lambda=0.85, scale_pos_weight=1, seed=None,\n",
       "       silent=1, subsample=0.52, verbosity=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg_xgb.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.998882254550263\n"
     ]
    }
   ],
   "source": [
    "score = reg_xgb.score(xtrain, ytrain)\n",
    "print(\"Training score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation score: 0.87\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(reg_xgb, xtrain, ytrain,cv=10)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17724.98678296233"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = mean_absolute_error(vytest, y_pred)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([141019.98 , 327876.5  , 106244.95 , 154908.9  , 323097.88 ,\n",
       "        72797.46 , 236475.52 , 148651.48 ,  79498.29 , 129504.71 ,\n",
       "       153005.78 , 122439.99 , 107608.11 , 204157.44 , 169214.89 ,\n",
       "       132708.62 , 193910.16 , 128101.7  , 115534.625, 210156.53 ,\n",
       "       162194.39 , 218209.19 , 174153.64 , 136143.9  , 203829.89 ,\n",
       "       165978.66 , 194887.55 , 103544.516, 175892.14 , 193367.72 ,\n",
       "       116972.766, 268052.53 , 231649.64 , 120210.63 , 268383.12 ,\n",
       "       147571.55 , 148090.12 , 207739.88 , 347676.62 , 108091.12 ,\n",
       "       122112.234, 231946.14 , 116900.086, 369718.53 , 122129.766,\n",
       "       128808.68 , 110567.95 , 124110.28 , 437549.3  , 142069.3  ,\n",
       "       119898.445, 214456.31 , 107455.82 , 334479.5  , 147935.31 ,\n",
       "       248450.36 , 204780.02 , 146068.2  , 137457.9  ,  88943.25 ,\n",
       "        66601.4  , 163328.23 , 294435.22 , 309080.62 , 302964.6  ,\n",
       "       228877.83 , 108613.34 , 333212.   , 115857.04 , 167904.16 ,\n",
       "       119798.375, 124690.78 , 111927.13 ,  83533.57 , 401665.97 ,\n",
       "       186504.03 , 312877.56 , 312459.66 , 144642.6  , 121629.06 ,\n",
       "       108126.14 ,  62633.26 , 109920.51 ,  92920.586, 153202.97 ,\n",
       "       149981.86 , 273867.2  , 202299.64 , 136952.33 , 164035.81 ,\n",
       "       149311.67 , 142397.06 , 127958.68 , 255143.25 , 124729.06 ,\n",
       "       172874.42 , 174162.92 , 180396.55 , 205936.05 , 251987.08 ,\n",
       "       170130.33 , 215956.72 , 297891.53 , 137570.77 , 193219.55 ,\n",
       "       168561.62 , 146890.44 , 276926.72 , 136994.05 , 181518.66 ,\n",
       "        52195.305, 115104.07 , 139851.81 , 138104.75 , 200474.4  ,\n",
       "       128626.07 , 107623.984, 101564.63 , 120758.68 , 265580.06 ,\n",
       "       136315.44 , 136805.77 , 175621.38 , 196846.22 , 164148.98 ,\n",
       "       136606.62 , 238499.22 ,  97363.43 , 140420.53 , 182103.11 ,\n",
       "       196016.48 , 365469.38 , 199362.4  , 114850.51 ,  57103.68 ,\n",
       "       339866.94 , 390407.34 , 135481.02 , 228290.11 , 630533.25 ,\n",
       "       338466.53 , 137099.4  , 166709.58 , 159716.55 , 127915.99 ,\n",
       "       127914.59 , 236479.89 , 196468.98 , 136532.02 ,  61484.86 ,\n",
       "       115755.695, 148410.67 , 269190.56 , 167121.38 ,  82100.49 ,\n",
       "       127085.11 , 151042.23 , 155270.45 ,  87486.86 , 131358.33 ,\n",
       "       192742.81 , 145892.28 , 355067.66 , 146741.62 , 111302.28 ,\n",
       "       103152.21 , 219558.56 , 331433.06 , 443590.56 , 247663.14 ,\n",
       "       357268.2  ,  91703.055, 111268.86 , 157534.69 , 302505.5  ,\n",
       "       131082.34 , 127192.29 , 210785.78 , 128746.664, 177124.03 ,\n",
       "       184365.95 ,  97137.2  , 125618.55 , 138012.66 , 262257.5  ,\n",
       "       145755.42 , 308840.8  , 227641.66 , 197536.3  ,  83709.89 ,\n",
       "       123224.   , 102033.41 , 145541.19 , 178684.58 , 190014.39 ,\n",
       "       176674.7  , 223663.55 ,  93106.94 , 194803.83 , 129843.29 ,\n",
       "       227405.42 , 198562.8  , 122520.34 , 339824.44 , 197047.08 ,\n",
       "       125314.03 , 243923.66 , 132867.36 , 156657.56 , 108924.54 ,\n",
       "       223810.69 , 140885.72 , 124052.28 , 167240.81 , 228275.3  ,\n",
       "       293433.4  , 170272.45 , 147956.78 , 132269.62 , 124894.9  ,\n",
       "       139438.8  , 229005.44 , 202425.89 ,  90368.516, 237504.45 ,\n",
       "       146081.12 ,  87784.47 , 101784.875, 165057.48 ,  95895.34 ,\n",
       "       110895.58 , 179002.48 , 111218.08 , 131943.66 , 209252.77 ,\n",
       "       136130.88 , 202951.69 , 150422.56 , 255171.2  , 120755.   ,\n",
       "       107542.04 , 256895.8  , 208205.89 , 439874.   , 196289.14 ,\n",
       "       131564.75 , 162689.3  , 168524.47 , 134404.16 , 100460.91 ,\n",
       "       174021.95 , 169860.81 , 138718.22 ,  84515.63 , 136537.2  ,\n",
       "       140536.14 , 108396.11 , 117584.31 , 175739.   , 276708.12 ,\n",
       "       284988.06 , 179343.11 , 137814.83 , 227170.97 , 314129.62 ,\n",
       "       200167.64 , 170939.86 , 136966.4  , 105770.23 , 175109.52 ,\n",
       "       409069.22 , 231440.6  , 235258.56 ,  83656.17 ,  96097.73 ,\n",
       "       134319.9  , 133982.64 , 278154.84 , 241842.34 , 147589.11 ,\n",
       "       193983.45 ,  88657.59 , 192511.88 , 109442.84 , 325415.25 ,\n",
       "       178573.44 , 222606.89 , 119695.62 , 247163.56 , 189182.25 ,\n",
       "       115142.59 , 121898.69 ], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict output\n",
    "y_pred=reg_xgb.predict(xtest)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892     154500\n",
       "1105    325000\n",
       "413     115000\n",
       "522     159000\n",
       "1036    315500\n",
       "614      75500\n",
       "218     311500\n",
       "1160    146000\n",
       "649      84500\n",
       "887     135500\n",
       "576     145000\n",
       "1252    130000\n",
       "1061     81000\n",
       "567     214000\n",
       "1108    181000\n",
       "1113    134500\n",
       "168     183500\n",
       "1102    135000\n",
       "1120    118400\n",
       "67      226000\n",
       "1040    155000\n",
       "453     210000\n",
       "670     173500\n",
       "1094    129000\n",
       "192     192000\n",
       "123     153900\n",
       "415     181134\n",
       "277     141000\n",
       "433     181000\n",
       "1317    208900\n",
       "         ...  \n",
       "233     128200\n",
       "426     275000\n",
       "196     311872\n",
       "1226    214000\n",
       "81      153500\n",
       "1368    144000\n",
       "1125    115000\n",
       "111     180000\n",
       "1243    465000\n",
       "744     180000\n",
       "937     253000\n",
       "344      85000\n",
       "1232    101800\n",
       "865     148500\n",
       "1088    137500\n",
       "350     318061\n",
       "588     143000\n",
       "1427    140000\n",
       "948     192500\n",
       "1449     92000\n",
       "989     197000\n",
       "677     109500\n",
       "478     297000\n",
       "1271    185750\n",
       "1410    230000\n",
       "479      89471\n",
       "1361    260000\n",
       "802     189000\n",
       "651     108000\n",
       "722     124500\n",
       "Name: SalePrice, Length: 292, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892     13480.015625\n",
       "1105    -2876.500000\n",
       "413      8755.046875\n",
       "522      4091.093750\n",
       "1036    -7597.875000\n",
       "614      2702.539062\n",
       "218     75024.484375\n",
       "1160    -2651.484375\n",
       "649      5001.710938\n",
       "887      5995.289062\n",
       "576     -8005.781250\n",
       "1252     7560.007812\n",
       "1061   -26608.109375\n",
       "567      9842.562500\n",
       "1108    11785.109375\n",
       "1113     1791.375000\n",
       "168    -10410.156250\n",
       "1102     6898.296875\n",
       "1120     2865.375000\n",
       "67      15843.468750\n",
       "1040    -7194.390625\n",
       "453     -8209.187500\n",
       "670      -653.640625\n",
       "1094    -7143.906250\n",
       "192    -11829.890625\n",
       "123    -12078.656250\n",
       "415    -13753.546875\n",
       "277     37455.484375\n",
       "433      5107.859375\n",
       "1317    15532.281250\n",
       "            ...     \n",
       "233     -9614.828125\n",
       "426     47829.031250\n",
       "196     -2257.625000\n",
       "1226    13832.359375\n",
       "81     -17439.859375\n",
       "1368     7033.593750\n",
       "1125     9229.773438\n",
       "111      4890.484375\n",
       "1243    55930.781250\n",
       "744    -51440.593750\n",
       "937     17741.437500\n",
       "344      1343.828125\n",
       "1232     5702.273438\n",
       "865     14180.093750\n",
       "1088     3517.359375\n",
       "350     39906.156250\n",
       "588    -98842.343750\n",
       "1427    -7589.109375\n",
       "948     -1483.453125\n",
       "1449     3342.406250\n",
       "989      4488.125000\n",
       "677        57.156250\n",
       "478    -28415.250000\n",
       "1271     7176.562500\n",
       "1410     7393.109375\n",
       "479    -30224.617188\n",
       "1361    12836.437500\n",
       "802      -182.250000\n",
       "651     -7142.593750\n",
       "722      2601.312500\n",
       "Name: SalePrice, Length: 292, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest-y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 15965.295925192637\n",
      "Mean Squared Error: 732608967.7486861\n",
      "Root Mean Squared Error: 27066.75022511358\n"
     ]
    }
   ],
   "source": [
    "vytest= ytest.to_numpy()\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(vytest, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(vytest, y_pred))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(vytest, y_pred))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare actual and predicted values\n",
    "fig, axs = plt.subplots(3)\n",
    "axs[0].set_title('Actual value')\n",
    "axs[0].plot(vytest)\n",
    "axs[1].set_title('Predicted value')\n",
    "axs[1].plot(y_pred)\n",
    "axs[2].set_title('Compared values')\n",
    "axs[2].plot(vytest)\n",
    "axs[2].plot(y_pred)\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1168, 75), (1168,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vxtrain= xtrain.to_numpy()\n",
    "vytrain= ytrain.to_numpy()\n",
    "vxtest= xtest.to_numpy()\n",
    "vytest= ytest.to_numpy()\n",
    "vxtrain.shape,vytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\non_n\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\non_n\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                2432      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,633\n",
      "Trainable params: 5,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create deep learning model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout\n",
    "from keras.layers import LSTM\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(32, kernel_initializer='normal',input_dim = vxtrain.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(32, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dropout(0.1))\n",
    "NN_model.add(Dense(32, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dropout(0.1))\n",
    "NN_model.add(Dense(32, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dropout(0.1))\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# Create Checkpoint to save the improved weight to computer with .hdf5\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\non_n\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/1500\n",
      "934/934 [==============================] - 1s 785us/step - loss: 112485.4631 - mean_absolute_error: 112485.4631 - val_loss: 41204.0521 - val_mean_absolute_error: 41204.0521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 41204.05206, saving model to Weights-001--41204.05206.hdf5\n",
      "Epoch 2/1500\n",
      "934/934 [==============================] - 0s 312us/step - loss: 50473.5942 - mean_absolute_error: 50473.5942 - val_loss: 38310.7327 - val_mean_absolute_error: 38310.7327\n",
      "\n",
      "Epoch 00002: val_loss improved from 41204.05206 to 38310.73270, saving model to Weights-002--38310.73270.hdf5\n",
      "Epoch 3/1500\n",
      "934/934 [==============================] - 0s 335us/step - loss: 48056.8676 - mean_absolute_error: 48056.8676 - val_loss: 36058.9719 - val_mean_absolute_error: 36058.9719\n",
      "\n",
      "Epoch 00003: val_loss improved from 38310.73270 to 36058.97188, saving model to Weights-003--36058.97188.hdf5\n",
      "Epoch 4/1500\n",
      "934/934 [==============================] - 0s 319us/step - loss: 45780.8313 - mean_absolute_error: 45780.8313 - val_loss: 32469.3384 - val_mean_absolute_error: 32469.3384\n",
      "\n",
      "Epoch 00004: val_loss improved from 36058.97188 to 32469.33841, saving model to Weights-004--32469.33841.hdf5\n",
      "Epoch 5/1500\n",
      "934/934 [==============================] - 0s 309us/step - loss: 41056.1969 - mean_absolute_error: 41056.1969 - val_loss: 29834.9657 - val_mean_absolute_error: 29834.9657\n",
      "\n",
      "Epoch 00005: val_loss improved from 32469.33841 to 29834.96568, saving model to Weights-005--29834.96568.hdf5\n",
      "Epoch 6/1500\n",
      "934/934 [==============================] - 0s 311us/step - loss: 41706.4594 - mean_absolute_error: 41706.4594 - val_loss: 30872.2180 - val_mean_absolute_error: 30872.2180\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 29834.96568\n",
      "Epoch 7/1500\n",
      "934/934 [==============================] - 0s 310us/step - loss: 38805.2083 - mean_absolute_error: 38805.2083 - val_loss: 26738.6034 - val_mean_absolute_error: 26738.6034\n",
      "\n",
      "Epoch 00007: val_loss improved from 29834.96568 to 26738.60335, saving model to Weights-007--26738.60335.hdf5\n",
      "Epoch 8/1500\n",
      "934/934 [==============================] - 0s 314us/step - loss: 37890.0263 - mean_absolute_error: 37890.0263 - val_loss: 25678.5837 - val_mean_absolute_error: 25678.5837\n",
      "\n",
      "Epoch 00008: val_loss improved from 26738.60335 to 25678.58365, saving model to Weights-008--25678.58365.hdf5\n",
      "Epoch 9/1500\n",
      "934/934 [==============================] - 0s 315us/step - loss: 36729.0069 - mean_absolute_error: 36729.0069 - val_loss: 24344.0110 - val_mean_absolute_error: 24344.0110\n",
      "\n",
      "Epoch 00009: val_loss improved from 25678.58365 to 24344.01103, saving model to Weights-009--24344.01103.hdf5\n",
      "Epoch 10/1500\n",
      "934/934 [==============================] - 0s 335us/step - loss: 36266.9888 - mean_absolute_error: 36266.9888 - val_loss: 26038.8993 - val_mean_absolute_error: 26038.8993\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 24344.01103\n",
      "Epoch 11/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 36221.7034 - mean_absolute_error: 36221.7034 - val_loss: 30244.8391 - val_mean_absolute_error: 30244.8391\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 24344.01103\n",
      "Epoch 12/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 37342.1085 - mean_absolute_error: 37342.1085 - val_loss: 23715.7945 - val_mean_absolute_error: 23715.7945\n",
      "\n",
      "Epoch 00012: val_loss improved from 24344.01103 to 23715.79455, saving model to Weights-012--23715.79455.hdf5\n",
      "Epoch 13/1500\n",
      "934/934 [==============================] - 0s 290us/step - loss: 36190.4576 - mean_absolute_error: 36190.4576 - val_loss: 26384.0842 - val_mean_absolute_error: 26384.0842\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 23715.79455\n",
      "Epoch 14/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 35537.7751 - mean_absolute_error: 35537.7751 - val_loss: 24540.4152 - val_mean_absolute_error: 24540.4152\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 23715.79455\n",
      "Epoch 15/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 34327.9177 - mean_absolute_error: 34327.9177 - val_loss: 25889.4937 - val_mean_absolute_error: 25889.4937\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 23715.79455\n",
      "Epoch 16/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 34724.6340 - mean_absolute_error: 34724.6340 - val_loss: 23353.4933 - val_mean_absolute_error: 23353.4933\n",
      "\n",
      "Epoch 00016: val_loss improved from 23715.79455 to 23353.49335, saving model to Weights-016--23353.49335.hdf5\n",
      "Epoch 17/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 34948.5483 - mean_absolute_error: 34948.5483 - val_loss: 23679.6553 - val_mean_absolute_error: 23679.6553\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 23353.49335\n",
      "Epoch 18/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 36554.5709 - mean_absolute_error: 36554.5709 - val_loss: 23004.7743 - val_mean_absolute_error: 23004.7743\n",
      "\n",
      "Epoch 00018: val_loss improved from 23353.49335 to 23004.77426, saving model to Weights-018--23004.77426.hdf5\n",
      "Epoch 19/1500\n",
      "934/934 [==============================] - 0s 300us/step - loss: 35528.5511 - mean_absolute_error: 35528.5511 - val_loss: 24285.0177 - val_mean_absolute_error: 24285.0177\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 23004.77426\n",
      "Epoch 20/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 35066.9736 - mean_absolute_error: 35066.9736 - val_loss: 24483.0122 - val_mean_absolute_error: 24483.0122\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 23004.77426\n",
      "Epoch 21/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 34939.7827 - mean_absolute_error: 34939.7827 - val_loss: 26081.6770 - val_mean_absolute_error: 26081.6770\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 23004.77426\n",
      "Epoch 22/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 34639.7955 - mean_absolute_error: 34639.7955 - val_loss: 23034.0099 - val_mean_absolute_error: 23034.0099\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 23004.77426\n",
      "Epoch 23/1500\n",
      "934/934 [==============================] - 0s 293us/step - loss: 34277.9175 - mean_absolute_error: 34277.9175 - val_loss: 24072.1237 - val_mean_absolute_error: 24072.1237\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 23004.77426\n",
      "Epoch 24/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 35093.5132 - mean_absolute_error: 35093.5132 - val_loss: 23496.9127 - val_mean_absolute_error: 23496.9127\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 23004.77426\n",
      "Epoch 25/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 34927.4945 - mean_absolute_error: 34927.4945 - val_loss: 23979.7728 - val_mean_absolute_error: 23979.7728\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23004.77426\n",
      "Epoch 26/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 35175.3438 - mean_absolute_error: 35175.3438 - val_loss: 24903.8072 - val_mean_absolute_error: 24903.8072\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 23004.77426\n",
      "Epoch 27/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 33614.2410 - mean_absolute_error: 33614.2410 - val_loss: 25473.6032 - val_mean_absolute_error: 25473.6032\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23004.77426\n",
      "Epoch 28/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 34317.4205 - mean_absolute_error: 34317.4205 - val_loss: 23856.1441 - val_mean_absolute_error: 23856.1441\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23004.77426\n",
      "Epoch 29/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 35249.9722 - mean_absolute_error: 35249.9722 - val_loss: 23243.2883 - val_mean_absolute_error: 23243.2883\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23004.77426\n",
      "Epoch 30/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 33377.0484 - mean_absolute_error: 33377.0484 - val_loss: 22525.9966 - val_mean_absolute_error: 22525.9966\n",
      "\n",
      "Epoch 00030: val_loss improved from 23004.77426 to 22525.99655, saving model to Weights-030--22525.99655.hdf5\n",
      "Epoch 31/1500\n",
      "934/934 [==============================] - 0s 308us/step - loss: 35483.7502 - mean_absolute_error: 35483.7502 - val_loss: 23664.4633 - val_mean_absolute_error: 23664.4633\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 22525.99655\n",
      "Epoch 32/1500\n",
      "934/934 [==============================] - 0s 312us/step - loss: 33496.2539 - mean_absolute_error: 33496.2539 - val_loss: 22335.0324 - val_mean_absolute_error: 22335.0324\n",
      "\n",
      "Epoch 00032: val_loss improved from 22525.99655 to 22335.03239, saving model to Weights-032--22335.03239.hdf5\n",
      "Epoch 33/1500\n",
      "934/934 [==============================] - 0s 316us/step - loss: 34180.1635 - mean_absolute_error: 34180.1635 - val_loss: 24612.2772 - val_mean_absolute_error: 24612.2772\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 22335.03239\n",
      "Epoch 34/1500\n",
      "934/934 [==============================] - 0s 327us/step - loss: 33078.5027 - mean_absolute_error: 33078.5027 - val_loss: 26194.9638 - val_mean_absolute_error: 26194.9638\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 22335.03239\n",
      "Epoch 35/1500\n",
      "934/934 [==============================] - 0s 314us/step - loss: 33373.0807 - mean_absolute_error: 33373.0807 - val_loss: 23542.5348 - val_mean_absolute_error: 23542.5348\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 22335.03239\n",
      "Epoch 36/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 33746.8346 - mean_absolute_error: 33746.8346 - val_loss: 24391.2966 - val_mean_absolute_error: 24391.2966\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 22335.03239\n",
      "Epoch 37/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 33306.8453 - mean_absolute_error: 33306.8453 - val_loss: 22288.1502 - val_mean_absolute_error: 22288.1502\n",
      "\n",
      "Epoch 00037: val_loss improved from 22335.03239 to 22288.15019, saving model to Weights-037--22288.15019.hdf5\n",
      "Epoch 38/1500\n",
      "934/934 [==============================] - 0s 331us/step - loss: 34389.8187 - mean_absolute_error: 34389.8187 - val_loss: 22165.3291 - val_mean_absolute_error: 22165.3291\n",
      "\n",
      "Epoch 00038: val_loss improved from 22288.15019 to 22165.32911, saving model to Weights-038--22165.32911.hdf5\n",
      "Epoch 39/1500\n",
      "934/934 [==============================] - 0s 326us/step - loss: 34582.9570 - mean_absolute_error: 34582.9570 - val_loss: 22089.5096 - val_mean_absolute_error: 22089.5096\n",
      "\n",
      "Epoch 00039: val_loss improved from 22165.32911 to 22089.50957, saving model to Weights-039--22089.50957.hdf5\n",
      "Epoch 40/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 34954.3551 - mean_absolute_error: 34954.3551 - val_loss: 21665.4380 - val_mean_absolute_error: 21665.4380\n",
      "\n",
      "Epoch 00040: val_loss improved from 22089.50957 to 21665.43804, saving model to Weights-040--21665.43804.hdf5\n",
      "Epoch 41/1500\n",
      "934/934 [==============================] - 0s 303us/step - loss: 33808.1544 - mean_absolute_error: 33808.1544 - val_loss: 21892.8465 - val_mean_absolute_error: 21892.8465\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 21665.43804\n",
      "Epoch 42/1500\n",
      "934/934 [==============================] - 0s 292us/step - loss: 33546.2929 - mean_absolute_error: 33546.2929 - val_loss: 25056.3415 - val_mean_absolute_error: 25056.3415\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 21665.43804\n",
      "Epoch 43/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 34968.8033 - mean_absolute_error: 34968.8033 - val_loss: 23402.6100 - val_mean_absolute_error: 23402.6100\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 21665.43804\n",
      "Epoch 44/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 34629.5489 - mean_absolute_error: 34629.5489 - val_loss: 26236.9607 - val_mean_absolute_error: 26236.9607\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 21665.43804\n",
      "Epoch 45/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 33191.1536 - mean_absolute_error: 33191.1536 - val_loss: 21733.3537 - val_mean_absolute_error: 21733.3537\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 21665.43804\n",
      "Epoch 46/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 33157.2944 - mean_absolute_error: 33157.2944 - val_loss: 23416.8098 - val_mean_absolute_error: 23416.8098\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 21665.43804\n",
      "Epoch 47/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 34127.3779 - mean_absolute_error: 34127.3779 - val_loss: 26283.1506 - val_mean_absolute_error: 26283.1506\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 21665.43804\n",
      "Epoch 48/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 32153.4696 - mean_absolute_error: 32153.4696 - val_loss: 22831.4899 - val_mean_absolute_error: 22831.4899\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 21665.43804\n",
      "Epoch 49/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 32929.7966 - mean_absolute_error: 32929.7966 - val_loss: 22007.2174 - val_mean_absolute_error: 22007.2174\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 21665.43804\n",
      "Epoch 50/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 33970.2284 - mean_absolute_error: 33970.2284 - val_loss: 21624.0891 - val_mean_absolute_error: 21624.0891\n",
      "\n",
      "Epoch 00050: val_loss improved from 21665.43804 to 21624.08913, saving model to Weights-050--21624.08913.hdf5\n",
      "Epoch 51/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 33868.5785 - mean_absolute_error: 33868.5785 - val_loss: 21699.6105 - val_mean_absolute_error: 21699.6105\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 21624.08913\n",
      "Epoch 52/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 33464.8583 - mean_absolute_error: 33464.8583 - val_loss: 22053.4983 - val_mean_absolute_error: 22053.4983\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21624.08913\n",
      "Epoch 53/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 32958.8711 - mean_absolute_error: 32958.8711 - val_loss: 24213.3115 - val_mean_absolute_error: 24213.3115\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21624.08913\n",
      "Epoch 54/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 34826.6048 - mean_absolute_error: 34826.6048 - val_loss: 21564.1822 - val_mean_absolute_error: 21564.1822\n",
      "\n",
      "Epoch 00054: val_loss improved from 21624.08913 to 21564.18220, saving model to Weights-054--21564.18220.hdf5\n",
      "Epoch 55/1500\n",
      "934/934 [==============================] - 0s 304us/step - loss: 33532.8059 - mean_absolute_error: 33532.8059 - val_loss: 26296.6635 - val_mean_absolute_error: 26296.6635\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21564.18220\n",
      "Epoch 56/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 33736.6722 - mean_absolute_error: 33736.6722 - val_loss: 21586.9425 - val_mean_absolute_error: 21586.9425\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21564.18220\n",
      "Epoch 57/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 33193.6450 - mean_absolute_error: 33193.6450 - val_loss: 22793.3038 - val_mean_absolute_error: 22793.3038\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21564.18220\n",
      "Epoch 58/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 33286.2566 - mean_absolute_error: 33286.2566 - val_loss: 21540.3415 - val_mean_absolute_error: 21540.3415\n",
      "\n",
      "Epoch 00058: val_loss improved from 21564.18220 to 21540.34149, saving model to Weights-058--21540.34149.hdf5\n",
      "Epoch 59/1500\n",
      "934/934 [==============================] - 0s 315us/step - loss: 34303.4469 - mean_absolute_error: 34303.4469 - val_loss: 22159.5413 - val_mean_absolute_error: 22159.5413\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 21540.34149\n",
      "Epoch 60/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 33298.5978 - mean_absolute_error: 33298.5978 - val_loss: 22719.8601 - val_mean_absolute_error: 22719.8601\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 21540.34149\n",
      "Epoch 61/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 32332.5251 - mean_absolute_error: 32332.5251 - val_loss: 21237.0538 - val_mean_absolute_error: 21237.0538\n",
      "\n",
      "Epoch 00061: val_loss improved from 21540.34149 to 21237.05381, saving model to Weights-061--21237.05381.hdf5\n",
      "Epoch 62/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 35093.0349 - mean_absolute_error: 35093.0349 - val_loss: 21843.1133 - val_mean_absolute_error: 21843.1133\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 21237.05381\n",
      "Epoch 63/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 264us/step - loss: 33638.3314 - mean_absolute_error: 33638.3314 - val_loss: 24932.4226 - val_mean_absolute_error: 24932.4226\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 21237.05381\n",
      "Epoch 64/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 32702.8184 - mean_absolute_error: 32702.8184 - val_loss: 21059.8593 - val_mean_absolute_error: 21059.8593\n",
      "\n",
      "Epoch 00064: val_loss improved from 21237.05381 to 21059.85932, saving model to Weights-064--21059.85932.hdf5\n",
      "Epoch 65/1500\n",
      "934/934 [==============================] - 0s 315us/step - loss: 33139.2296 - mean_absolute_error: 33139.2296 - val_loss: 21133.4551 - val_mean_absolute_error: 21133.4551\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 21059.85932\n",
      "Epoch 66/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 33990.9484 - mean_absolute_error: 33990.9484 - val_loss: 24506.7450 - val_mean_absolute_error: 24506.7450\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 21059.85932\n",
      "Epoch 67/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 34968.5403 - mean_absolute_error: 34968.5403 - val_loss: 21977.2624 - val_mean_absolute_error: 21977.2624\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 21059.85932\n",
      "Epoch 68/1500\n",
      "934/934 [==============================] - 0s 287us/step - loss: 32621.4343 - mean_absolute_error: 32621.4343 - val_loss: 25820.1826 - val_mean_absolute_error: 25820.1826\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 21059.85932\n",
      "Epoch 69/1500\n",
      "934/934 [==============================] - 0s 305us/step - loss: 33666.7773 - mean_absolute_error: 33666.7773 - val_loss: 21397.6553 - val_mean_absolute_error: 21397.6553\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 21059.85932\n",
      "Epoch 70/1500\n",
      "934/934 [==============================] - 0s 296us/step - loss: 33276.4820 - mean_absolute_error: 33276.4820 - val_loss: 21242.0059 - val_mean_absolute_error: 21242.0059\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 21059.85932\n",
      "Epoch 71/1500\n",
      "934/934 [==============================] - 0s 286us/step - loss: 32104.6884 - mean_absolute_error: 32104.6884 - val_loss: 21720.4595 - val_mean_absolute_error: 21720.4595\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 21059.85932\n",
      "Epoch 72/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 34230.1369 - mean_absolute_error: 34230.1369 - val_loss: 22022.7565 - val_mean_absolute_error: 22022.7565\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 21059.85932\n",
      "Epoch 73/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 33657.4435 - mean_absolute_error: 33657.4435 - val_loss: 21301.3063 - val_mean_absolute_error: 21301.3063\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 21059.85932\n",
      "Epoch 74/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 33636.4256 - mean_absolute_error: 33636.4256 - val_loss: 21465.3405 - val_mean_absolute_error: 21465.3405\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 21059.85932\n",
      "Epoch 75/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 33792.0787 - mean_absolute_error: 33792.0787 - val_loss: 23640.0908 - val_mean_absolute_error: 23640.0908\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 21059.85932\n",
      "Epoch 76/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 33005.4246 - mean_absolute_error: 33005.4246 - val_loss: 22068.6622 - val_mean_absolute_error: 22068.6622\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 21059.85932\n",
      "Epoch 77/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 33006.6615 - mean_absolute_error: 33006.6615 - val_loss: 22298.1570 - val_mean_absolute_error: 22298.1570\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 21059.85932\n",
      "Epoch 78/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 31943.1328 - mean_absolute_error: 31943.1328 - val_loss: 21099.6292 - val_mean_absolute_error: 21099.6292\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 21059.85932\n",
      "Epoch 79/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 33901.0189 - mean_absolute_error: 33901.0189 - val_loss: 21094.5830 - val_mean_absolute_error: 21094.5830\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 21059.85932\n",
      "Epoch 80/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 35104.2521 - mean_absolute_error: 35104.2521 - val_loss: 23081.1701 - val_mean_absolute_error: 23081.1701\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 21059.85932\n",
      "Epoch 81/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 32981.3728 - mean_absolute_error: 32981.3728 - val_loss: 22344.5717 - val_mean_absolute_error: 22344.5717\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 21059.85932\n",
      "Epoch 82/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 33214.0983 - mean_absolute_error: 33214.0983 - val_loss: 21667.0664 - val_mean_absolute_error: 21667.0664\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 21059.85932\n",
      "Epoch 83/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 34121.0694 - mean_absolute_error: 34121.0694 - val_loss: 21699.9383 - val_mean_absolute_error: 21699.9383\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 21059.85932\n",
      "Epoch 84/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 33323.5894 - mean_absolute_error: 33323.5894 - val_loss: 21458.3708 - val_mean_absolute_error: 21458.3708\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 21059.85932\n",
      "Epoch 85/1500\n",
      "934/934 [==============================] - 0s 281us/step - loss: 31107.7279 - mean_absolute_error: 31107.7279 - val_loss: 21606.9960 - val_mean_absolute_error: 21606.9960\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 21059.85932\n",
      "Epoch 86/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 32841.9798 - mean_absolute_error: 32841.9798 - val_loss: 22191.2274 - val_mean_absolute_error: 22191.2274\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 21059.85932\n",
      "Epoch 87/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 32973.7811 - mean_absolute_error: 32973.7811 - val_loss: 23137.3877 - val_mean_absolute_error: 23137.3877\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 21059.85932\n",
      "Epoch 88/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 34671.6830 - mean_absolute_error: 34671.6830 - val_loss: 21560.7117 - val_mean_absolute_error: 21560.7117\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 21059.85932\n",
      "Epoch 89/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 32902.0481 - mean_absolute_error: 32902.0481 - val_loss: 21249.8179 - val_mean_absolute_error: 21249.8179\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 21059.85932\n",
      "Epoch 90/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 34032.4245 - mean_absolute_error: 34032.4245 - val_loss: 29737.0256 - val_mean_absolute_error: 29737.0256\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 21059.85932\n",
      "Epoch 91/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 33622.7998 - mean_absolute_error: 33622.7998 - val_loss: 23147.7447 - val_mean_absolute_error: 23147.7447\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 21059.85932\n",
      "Epoch 92/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 32345.0637 - mean_absolute_error: 32345.0637 - val_loss: 20905.0962 - val_mean_absolute_error: 20905.0962\n",
      "\n",
      "Epoch 00092: val_loss improved from 21059.85932 to 20905.09616, saving model to Weights-092--20905.09616.hdf5\n",
      "Epoch 93/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 33757.2897 - mean_absolute_error: 33757.2897 - val_loss: 21232.0430 - val_mean_absolute_error: 21232.0430\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 20905.09616\n",
      "Epoch 94/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 34547.5521 - mean_absolute_error: 34547.5521 - val_loss: 21738.9625 - val_mean_absolute_error: 21738.9625\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 20905.09616\n",
      "Epoch 95/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31976.7554 - mean_absolute_error: 31976.7554 - val_loss: 21959.6233 - val_mean_absolute_error: 21959.6233\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 20905.09616\n",
      "Epoch 96/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 32067.5850 - mean_absolute_error: 32067.5850 - val_loss: 22784.6575 - val_mean_absolute_error: 22784.6575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00096: val_loss did not improve from 20905.09616\n",
      "Epoch 97/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 33975.6934 - mean_absolute_error: 33975.6934 - val_loss: 24382.0908 - val_mean_absolute_error: 24382.0908\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 20905.09616\n",
      "Epoch 98/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 34665.2408 - mean_absolute_error: 34665.2408 - val_loss: 23834.2527 - val_mean_absolute_error: 23834.2527\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 20905.09616\n",
      "Epoch 99/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 31856.3810 - mean_absolute_error: 31856.3810 - val_loss: 21105.8407 - val_mean_absolute_error: 21105.8407\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 20905.09616\n",
      "Epoch 100/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 32908.2103 - mean_absolute_error: 32908.2103 - val_loss: 20922.4554 - val_mean_absolute_error: 20922.4554\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 20905.09616\n",
      "Epoch 101/1500\n",
      "934/934 [==============================] - 0s 286us/step - loss: 33691.2853 - mean_absolute_error: 33691.2853 - val_loss: 20858.0264 - val_mean_absolute_error: 20858.0264\n",
      "\n",
      "Epoch 00101: val_loss improved from 20905.09616 to 20858.02643, saving model to Weights-101--20858.02643.hdf5\n",
      "Epoch 102/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 32440.4601 - mean_absolute_error: 32440.4601 - val_loss: 20973.9761 - val_mean_absolute_error: 20973.9761\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 20858.02643\n",
      "Epoch 103/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 32856.9113 - mean_absolute_error: 32856.9113 - val_loss: 21997.3399 - val_mean_absolute_error: 21997.3399\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 20858.02643\n",
      "Epoch 104/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 33224.5450 - mean_absolute_error: 33224.5450 - val_loss: 20890.0431 - val_mean_absolute_error: 20890.0431\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 20858.02643\n",
      "Epoch 105/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 33203.9234 - mean_absolute_error: 33203.9234 - val_loss: 22273.7779 - val_mean_absolute_error: 22273.7779\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 20858.02643\n",
      "Epoch 106/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 33523.9613 - mean_absolute_error: 33523.9613 - val_loss: 20922.0036 - val_mean_absolute_error: 20922.0036\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 20858.02643\n",
      "Epoch 107/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 33462.8184 - mean_absolute_error: 33462.8184 - val_loss: 21151.4974 - val_mean_absolute_error: 21151.4974\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 20858.02643\n",
      "Epoch 108/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 31927.0400 - mean_absolute_error: 31927.0400 - val_loss: 21088.4809 - val_mean_absolute_error: 21088.4809\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 20858.02643\n",
      "Epoch 109/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 34884.9770 - mean_absolute_error: 34884.9770 - val_loss: 21332.8043 - val_mean_absolute_error: 21332.8043\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 20858.02643\n",
      "Epoch 110/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 32499.6887 - mean_absolute_error: 32499.6887 - val_loss: 21079.3078 - val_mean_absolute_error: 21079.3078\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 20858.02643\n",
      "Epoch 111/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 32219.9873 - mean_absolute_error: 32219.9873 - val_loss: 21280.4532 - val_mean_absolute_error: 21280.4532\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 20858.02643\n",
      "Epoch 112/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 32387.9408 - mean_absolute_error: 32387.9408 - val_loss: 22791.9683 - val_mean_absolute_error: 22791.9683\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 20858.02643\n",
      "Epoch 113/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 34091.8372 - mean_absolute_error: 34091.8372 - val_loss: 29168.7805 - val_mean_absolute_error: 29168.7805\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 20858.02643\n",
      "Epoch 114/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 32518.7266 - mean_absolute_error: 32518.7266 - val_loss: 22845.6227 - val_mean_absolute_error: 22845.6227\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 20858.02643\n",
      "Epoch 115/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 33026.4728 - mean_absolute_error: 33026.4728 - val_loss: 21247.6183 - val_mean_absolute_error: 21247.6183\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 20858.02643\n",
      "Epoch 116/1500\n",
      "934/934 [==============================] - 0s 289us/step - loss: 32617.2253 - mean_absolute_error: 32617.2253 - val_loss: 24818.5283 - val_mean_absolute_error: 24818.5283\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 20858.02643\n",
      "Epoch 117/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 33063.6678 - mean_absolute_error: 33063.6678 - val_loss: 25023.4265 - val_mean_absolute_error: 25023.4265\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 20858.02643\n",
      "Epoch 118/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 31131.5435 - mean_absolute_error: 31131.5435 - val_loss: 21902.1311 - val_mean_absolute_error: 21902.1311\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 20858.02643\n",
      "Epoch 119/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 32478.4168 - mean_absolute_error: 32478.4168 - val_loss: 21359.1669 - val_mean_absolute_error: 21359.1669\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 20858.02643\n",
      "Epoch 120/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 32750.7745 - mean_absolute_error: 32750.7745 - val_loss: 21579.1658 - val_mean_absolute_error: 21579.1658\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 20858.02643\n",
      "Epoch 121/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 32312.8975 - mean_absolute_error: 32312.8975 - val_loss: 22816.4062 - val_mean_absolute_error: 22816.4062\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 20858.02643\n",
      "Epoch 122/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31945.8542 - mean_absolute_error: 31945.8542 - val_loss: 20949.3485 - val_mean_absolute_error: 20949.3485\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 20858.02643\n",
      "Epoch 123/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 32728.7398 - mean_absolute_error: 32728.7398 - val_loss: 20996.8389 - val_mean_absolute_error: 20996.8389\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 20858.02643\n",
      "Epoch 124/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 31895.6946 - mean_absolute_error: 31895.6946 - val_loss: 23184.0073 - val_mean_absolute_error: 23184.0073\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 20858.02643\n",
      "Epoch 125/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 31527.4838 - mean_absolute_error: 31527.4838 - val_loss: 22675.5853 - val_mean_absolute_error: 22675.5853\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 20858.02643\n",
      "Epoch 126/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 31603.0067 - mean_absolute_error: 31603.0067 - val_loss: 25231.4356 - val_mean_absolute_error: 25231.4356\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 20858.02643\n",
      "Epoch 127/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 30716.0955 - mean_absolute_error: 30716.0955 - val_loss: 23784.2561 - val_mean_absolute_error: 23784.2561\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 20858.02643\n",
      "Epoch 128/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 33172.6645 - mean_absolute_error: 33172.6645 - val_loss: 20932.1745 - val_mean_absolute_error: 20932.1745\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 20858.02643\n",
      "Epoch 129/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 32947.0138 - mean_absolute_error: 32947.0138 - val_loss: 21824.9072 - val_mean_absolute_error: 21824.9072\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 20858.02643\n",
      "Epoch 130/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 32634.1605 - mean_absolute_error: 32634.1605 - val_loss: 23856.7861 - val_mean_absolute_error: 23856.7861\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 20858.02643\n",
      "Epoch 131/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 32524.6818 - mean_absolute_error: 32524.6818 - val_loss: 21916.0743 - val_mean_absolute_error: 21916.0743\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 20858.02643\n",
      "Epoch 132/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 31904.5173 - mean_absolute_error: 31904.5173 - val_loss: 22061.5118 - val_mean_absolute_error: 22061.5118\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 20858.02643\n",
      "Epoch 133/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 30522.7451 - mean_absolute_error: 30522.7451 - val_loss: 21603.3943 - val_mean_absolute_error: 21603.3943\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 20858.02643\n",
      "Epoch 134/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 33692.2539 - mean_absolute_error: 33692.2539 - val_loss: 21391.1836 - val_mean_absolute_error: 21391.1836\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 20858.02643\n",
      "Epoch 135/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 32190.7093 - mean_absolute_error: 32190.7093 - val_loss: 22411.5144 - val_mean_absolute_error: 22411.5144\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 20858.02643\n",
      "Epoch 136/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 33890.3260 - mean_absolute_error: 33890.3260 - val_loss: 21197.2187 - val_mean_absolute_error: 21197.2187\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 20858.02643\n",
      "Epoch 137/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 32011.3489 - mean_absolute_error: 32011.3489 - val_loss: 25043.0966 - val_mean_absolute_error: 25043.0966\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 20858.02643\n",
      "Epoch 138/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 32386.8554 - mean_absolute_error: 32386.8554 - val_loss: 26214.0393 - val_mean_absolute_error: 26214.0393\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 20858.02643\n",
      "Epoch 139/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31825.1997 - mean_absolute_error: 31825.1997 - val_loss: 23689.7905 - val_mean_absolute_error: 23689.7905\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 20858.02643\n",
      "Epoch 140/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 33003.4433 - mean_absolute_error: 33003.4433 - val_loss: 29202.6182 - val_mean_absolute_error: 29202.6182\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 20858.02643\n",
      "Epoch 141/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 32195.7982 - mean_absolute_error: 32195.7982 - val_loss: 23310.0680 - val_mean_absolute_error: 23310.0680\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 20858.02643\n",
      "Epoch 142/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 31326.2607 - mean_absolute_error: 31326.2607 - val_loss: 20976.8099 - val_mean_absolute_error: 20976.8099\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 20858.02643\n",
      "Epoch 143/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 31924.7190 - mean_absolute_error: 31924.7190 - val_loss: 20837.3022 - val_mean_absolute_error: 20837.3022\n",
      "\n",
      "Epoch 00143: val_loss improved from 20858.02643 to 20837.30221, saving model to Weights-143--20837.30221.hdf5\n",
      "Epoch 144/1500\n",
      "934/934 [==============================] - 0s 293us/step - loss: 33906.9400 - mean_absolute_error: 33906.9400 - val_loss: 23532.8097 - val_mean_absolute_error: 23532.8097\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 20837.30221\n",
      "Epoch 145/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 31220.5248 - mean_absolute_error: 31220.5248 - val_loss: 20708.5931 - val_mean_absolute_error: 20708.5931\n",
      "\n",
      "Epoch 00145: val_loss improved from 20837.30221 to 20708.59315, saving model to Weights-145--20708.59315.hdf5\n",
      "Epoch 146/1500\n",
      "934/934 [==============================] - 0s 321us/step - loss: 32116.2196 - mean_absolute_error: 32116.2196 - val_loss: 21765.7232 - val_mean_absolute_error: 21765.7232\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 20708.59315\n",
      "Epoch 147/1500\n",
      "934/934 [==============================] - 0s 325us/step - loss: 31616.5791 - mean_absolute_error: 31616.5791 - val_loss: 21441.3998 - val_mean_absolute_error: 21441.3998\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 20708.59315\n",
      "Epoch 148/1500\n",
      "934/934 [==============================] - 0s 308us/step - loss: 30820.9652 - mean_absolute_error: 30820.9652 - val_loss: 20739.0855 - val_mean_absolute_error: 20739.0855\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 20708.59315\n",
      "Epoch 149/1500\n",
      "934/934 [==============================] - 0s 300us/step - loss: 32004.4891 - mean_absolute_error: 32004.4891 - val_loss: 20572.0958 - val_mean_absolute_error: 20572.0958\n",
      "\n",
      "Epoch 00149: val_loss improved from 20708.59315 to 20572.09579, saving model to Weights-149--20572.09579.hdf5\n",
      "Epoch 150/1500\n",
      "934/934 [==============================] - 0s 306us/step - loss: 31302.0522 - mean_absolute_error: 31302.0522 - val_loss: 26111.4889 - val_mean_absolute_error: 26111.4889\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 20572.09579\n",
      "Epoch 151/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 31645.7389 - mean_absolute_error: 31645.7389 - val_loss: 21410.4132 - val_mean_absolute_error: 21410.4132\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 20572.09579\n",
      "Epoch 152/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 32975.9569 - mean_absolute_error: 32975.9569 - val_loss: 21442.5057 - val_mean_absolute_error: 21442.5057\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 20572.09579\n",
      "Epoch 153/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 32785.4658 - mean_absolute_error: 32785.4658 - val_loss: 21139.7714 - val_mean_absolute_error: 21139.7714\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 20572.09579\n",
      "Epoch 154/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 33001.9685 - mean_absolute_error: 33001.9685 - val_loss: 23162.5792 - val_mean_absolute_error: 23162.5792\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 20572.09579\n",
      "Epoch 155/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 32075.6941 - mean_absolute_error: 32075.6941 - val_loss: 20709.7456 - val_mean_absolute_error: 20709.7456\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 20572.09579\n",
      "Epoch 156/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 33203.4940 - mean_absolute_error: 33203.4940 - val_loss: 22515.4614 - val_mean_absolute_error: 22515.4614\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 20572.09579\n",
      "Epoch 157/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 30940.3292 - mean_absolute_error: 30940.3292 - val_loss: 21051.9208 - val_mean_absolute_error: 21051.9208\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 20572.09579\n",
      "Epoch 158/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 31689.6482 - mean_absolute_error: 31689.6482 - val_loss: 20839.6413 - val_mean_absolute_error: 20839.6413\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 20572.09579\n",
      "Epoch 159/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 32437.2185 - mean_absolute_error: 32437.2185 - val_loss: 21923.2968 - val_mean_absolute_error: 21923.2968\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 20572.09579\n",
      "Epoch 160/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 32556.9593 - mean_absolute_error: 32556.9593 - val_loss: 21746.7915 - val_mean_absolute_error: 21746.7915\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 20572.09579\n",
      "Epoch 161/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 30733.2223 - mean_absolute_error: 30733.2223 - val_loss: 23639.7031 - val_mean_absolute_error: 23639.7031\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 20572.09579\n",
      "Epoch 162/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 31919.9306 - mean_absolute_error: 31919.9306 - val_loss: 23267.7458 - val_mean_absolute_error: 23267.7458\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 20572.09579\n",
      "Epoch 163/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 275us/step - loss: 31479.8526 - mean_absolute_error: 31479.8526 - val_loss: 22227.2036 - val_mean_absolute_error: 22227.2036\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 20572.09579\n",
      "Epoch 164/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 31766.1029 - mean_absolute_error: 31766.1029 - val_loss: 20785.8351 - val_mean_absolute_error: 20785.8351\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 20572.09579\n",
      "Epoch 165/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 31198.7767 - mean_absolute_error: 31198.7767 - val_loss: 21417.8482 - val_mean_absolute_error: 21417.8482\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 20572.09579\n",
      "Epoch 166/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 32177.6214 - mean_absolute_error: 32177.6214 - val_loss: 23843.4942 - val_mean_absolute_error: 23843.4942\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 20572.09579\n",
      "Epoch 167/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 31087.8292 - mean_absolute_error: 31087.8292 - val_loss: 20788.2515 - val_mean_absolute_error: 20788.2515\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 20572.09579\n",
      "Epoch 168/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 30296.7905 - mean_absolute_error: 30296.7905 - val_loss: 21202.2039 - val_mean_absolute_error: 21202.2039\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 20572.09579\n",
      "Epoch 169/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 32521.8214 - mean_absolute_error: 32521.8214 - val_loss: 20989.8899 - val_mean_absolute_error: 20989.8899\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 20572.09579\n",
      "Epoch 170/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 30747.0132 - mean_absolute_error: 30747.0132 - val_loss: 20737.4423 - val_mean_absolute_error: 20737.4423\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 20572.09579\n",
      "Epoch 171/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31518.2075 - mean_absolute_error: 31518.2075 - val_loss: 23656.7642 - val_mean_absolute_error: 23656.7642\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 20572.09579\n",
      "Epoch 172/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 31291.8825 - mean_absolute_error: 31291.8825 - val_loss: 20323.6106 - val_mean_absolute_error: 20323.6106\n",
      "\n",
      "Epoch 00172: val_loss improved from 20572.09579 to 20323.61057, saving model to Weights-172--20323.61057.hdf5\n",
      "Epoch 173/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 31627.2786 - mean_absolute_error: 31627.2786 - val_loss: 22296.8760 - val_mean_absolute_error: 22296.8760\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 20323.61057\n",
      "Epoch 174/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 30241.0082 - mean_absolute_error: 30241.0082 - val_loss: 21687.4343 - val_mean_absolute_error: 21687.4343\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 20323.61057\n",
      "Epoch 175/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 30291.6104 - mean_absolute_error: 30291.6104 - val_loss: 21416.3571 - val_mean_absolute_error: 21416.3571\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 20323.61057\n",
      "Epoch 176/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 31502.1413 - mean_absolute_error: 31502.1413 - val_loss: 25893.1428 - val_mean_absolute_error: 25893.1428\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 20323.61057\n",
      "Epoch 177/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 29998.7687 - mean_absolute_error: 29998.7687 - val_loss: 22008.2617 - val_mean_absolute_error: 22008.2617\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 20323.61057\n",
      "Epoch 178/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 30527.6558 - mean_absolute_error: 30527.6558 - val_loss: 20672.5942 - val_mean_absolute_error: 20672.5942\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 20323.61057\n",
      "Epoch 179/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 31667.5952 - mean_absolute_error: 31667.5952 - val_loss: 25279.2538 - val_mean_absolute_error: 25279.2538\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 20323.61057\n",
      "Epoch 180/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 31438.9951 - mean_absolute_error: 31438.9951 - val_loss: 21305.2585 - val_mean_absolute_error: 21305.2585\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 20323.61057\n",
      "Epoch 181/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 32214.9421 - mean_absolute_error: 32214.9421 - val_loss: 21146.7454 - val_mean_absolute_error: 21146.7454\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 20323.61057\n",
      "Epoch 182/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 31821.4675 - mean_absolute_error: 31821.4675 - val_loss: 20737.7370 - val_mean_absolute_error: 20737.7370\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 20323.61057\n",
      "Epoch 183/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 30613.3095 - mean_absolute_error: 30613.3095 - val_loss: 21090.8027 - val_mean_absolute_error: 21090.8027\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 20323.61057\n",
      "Epoch 184/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31010.7505 - mean_absolute_error: 31010.7505 - val_loss: 24523.5061 - val_mean_absolute_error: 24523.5061\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 20323.61057\n",
      "Epoch 185/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 30770.5919 - mean_absolute_error: 30770.5919 - val_loss: 26254.3877 - val_mean_absolute_error: 26254.3877\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 20323.61057\n",
      "Epoch 186/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 31552.9254 - mean_absolute_error: 31552.9254 - val_loss: 20604.0788 - val_mean_absolute_error: 20604.0788\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 20323.61057\n",
      "Epoch 187/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 29982.9710 - mean_absolute_error: 29982.9710 - val_loss: 20376.2127 - val_mean_absolute_error: 20376.2127\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 20323.61057\n",
      "Epoch 188/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 30962.6749 - mean_absolute_error: 30962.6749 - val_loss: 20260.7436 - val_mean_absolute_error: 20260.7436\n",
      "\n",
      "Epoch 00188: val_loss improved from 20323.61057 to 20260.74363, saving model to Weights-188--20260.74363.hdf5\n",
      "Epoch 189/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 30228.2979 - mean_absolute_error: 30228.2979 - val_loss: 21537.8259 - val_mean_absolute_error: 21537.8259\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 20260.74363\n",
      "Epoch 190/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 31688.4139 - mean_absolute_error: 31688.4139 - val_loss: 20456.3390 - val_mean_absolute_error: 20456.3390\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 20260.74363\n",
      "Epoch 191/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 31501.0215 - mean_absolute_error: 31501.0215 - val_loss: 26394.9155 - val_mean_absolute_error: 26394.9155\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 20260.74363\n",
      "Epoch 192/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 31955.6624 - mean_absolute_error: 31955.6624 - val_loss: 26732.7854 - val_mean_absolute_error: 26732.7854\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 20260.74363\n",
      "Epoch 193/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 32438.2952 - mean_absolute_error: 32438.2952 - val_loss: 20296.0935 - val_mean_absolute_error: 20296.0935\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 20260.74363\n",
      "Epoch 194/1500\n",
      "934/934 [==============================] - 0s 281us/step - loss: 32200.2898 - mean_absolute_error: 32200.2898 - val_loss: 22706.9634 - val_mean_absolute_error: 22706.9634\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 20260.74363\n",
      "Epoch 195/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 31999.2010 - mean_absolute_error: 31999.2010 - val_loss: 20383.2664 - val_mean_absolute_error: 20383.2664\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 20260.74363\n",
      "Epoch 196/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 30930.5712 - mean_absolute_error: 30930.5712 - val_loss: 20409.0043 - val_mean_absolute_error: 20409.0043\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 20260.74363\n",
      "Epoch 197/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 30060.0696 - mean_absolute_error: 30060.0696 - val_loss: 20738.8530 - val_mean_absolute_error: 20738.8530\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 20260.74363\n",
      "Epoch 198/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 31373.1975 - mean_absolute_error: 31373.1975 - val_loss: 26658.3373 - val_mean_absolute_error: 26658.3373\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 20260.74363\n",
      "Epoch 199/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 29987.2899 - mean_absolute_error: 29987.2899 - val_loss: 21112.4849 - val_mean_absolute_error: 21112.4849\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 20260.74363\n",
      "Epoch 200/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 30467.8280 - mean_absolute_error: 30467.8280 - val_loss: 20706.7098 - val_mean_absolute_error: 20706.7098\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 20260.74363\n",
      "Epoch 201/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 31580.8797 - mean_absolute_error: 31580.8797 - val_loss: 22971.7746 - val_mean_absolute_error: 22971.7746\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 20260.74363\n",
      "Epoch 202/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31034.6879 - mean_absolute_error: 31034.6879 - val_loss: 21605.3463 - val_mean_absolute_error: 21605.3463\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 20260.74363\n",
      "Epoch 203/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 31473.1257 - mean_absolute_error: 31473.1257 - val_loss: 23030.7604 - val_mean_absolute_error: 23030.7604\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 20260.74363\n",
      "Epoch 204/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 30097.0940 - mean_absolute_error: 30097.0940 - val_loss: 20569.2585 - val_mean_absolute_error: 20569.2585\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 20260.74363\n",
      "Epoch 205/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 31520.1034 - mean_absolute_error: 31520.1034 - val_loss: 22745.1833 - val_mean_absolute_error: 22745.1833\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 20260.74363\n",
      "Epoch 206/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 30254.6053 - mean_absolute_error: 30254.6053 - val_loss: 22411.6514 - val_mean_absolute_error: 22411.6514\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 20260.74363\n",
      "Epoch 207/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31503.5072 - mean_absolute_error: 31503.5072 - val_loss: 25238.5643 - val_mean_absolute_error: 25238.5643\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 20260.74363\n",
      "Epoch 208/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 30868.2198 - mean_absolute_error: 30868.2198 - val_loss: 25456.5975 - val_mean_absolute_error: 25456.5975\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 20260.74363\n",
      "Epoch 209/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 30401.8758 - mean_absolute_error: 30401.8758 - val_loss: 21076.0263 - val_mean_absolute_error: 21076.0263\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 20260.74363\n",
      "Epoch 210/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 30517.0776 - mean_absolute_error: 30517.0776 - val_loss: 21282.9764 - val_mean_absolute_error: 21282.9764\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 20260.74363\n",
      "Epoch 211/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 31853.3735 - mean_absolute_error: 31853.3735 - val_loss: 20069.8109 - val_mean_absolute_error: 20069.8109\n",
      "\n",
      "Epoch 00211: val_loss improved from 20260.74363 to 20069.81090, saving model to Weights-211--20069.81090.hdf5\n",
      "Epoch 212/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 32548.8734 - mean_absolute_error: 32548.8734 - val_loss: 30804.9636 - val_mean_absolute_error: 30804.9636\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 20069.81090\n",
      "Epoch 213/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 31548.6393 - mean_absolute_error: 31548.6393 - val_loss: 20726.2472 - val_mean_absolute_error: 20726.2472\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 20069.81090\n",
      "Epoch 214/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 30394.8303 - mean_absolute_error: 30394.8303 - val_loss: 22169.6820 - val_mean_absolute_error: 22169.6820\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 20069.81090\n",
      "Epoch 215/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 30605.9820 - mean_absolute_error: 30605.9820 - val_loss: 21717.6749 - val_mean_absolute_error: 21717.6749\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 20069.81090\n",
      "Epoch 216/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 31099.0523 - mean_absolute_error: 31099.0523 - val_loss: 20077.8075 - val_mean_absolute_error: 20077.8075\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 20069.81090\n",
      "Epoch 217/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 30076.2501 - mean_absolute_error: 30076.2501 - val_loss: 21068.4026 - val_mean_absolute_error: 21068.4026\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 20069.81090\n",
      "Epoch 218/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 29640.9600 - mean_absolute_error: 29640.9600 - val_loss: 23070.5937 - val_mean_absolute_error: 23070.5937\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 20069.81090\n",
      "Epoch 219/1500\n",
      "934/934 [==============================] - 0s 281us/step - loss: 29924.7864 - mean_absolute_error: 29924.7864 - val_loss: 23408.7038 - val_mean_absolute_error: 23408.7038\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 20069.81090\n",
      "Epoch 220/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 30319.4405 - mean_absolute_error: 30319.4405 - val_loss: 20163.1935 - val_mean_absolute_error: 20163.1935\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 20069.81090\n",
      "Epoch 221/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 30510.6523 - mean_absolute_error: 30510.6523 - val_loss: 22343.9547 - val_mean_absolute_error: 22343.9547\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 20069.81090\n",
      "Epoch 222/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 30479.0039 - mean_absolute_error: 30479.0039 - val_loss: 24012.0209 - val_mean_absolute_error: 24012.0209\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 20069.81090\n",
      "Epoch 223/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 31781.3585 - mean_absolute_error: 31781.3585 - val_loss: 19876.9744 - val_mean_absolute_error: 19876.9744\n",
      "\n",
      "Epoch 00223: val_loss improved from 20069.81090 to 19876.97443, saving model to Weights-223--19876.97443.hdf5\n",
      "Epoch 224/1500\n",
      "934/934 [==============================] - 0s 309us/step - loss: 30255.1447 - mean_absolute_error: 30255.1447 - val_loss: 20106.5381 - val_mean_absolute_error: 20106.5381\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 19876.97443\n",
      "Epoch 225/1500\n",
      "934/934 [==============================] - 0s 301us/step - loss: 30251.2949 - mean_absolute_error: 30251.2949 - val_loss: 22200.5071 - val_mean_absolute_error: 22200.5071\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 19876.97443\n",
      "Epoch 226/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 31456.9860 - mean_absolute_error: 31456.9860 - val_loss: 20171.6976 - val_mean_absolute_error: 20171.6976\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 19876.97443\n",
      "Epoch 227/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 29694.6940 - mean_absolute_error: 29694.6940 - val_loss: 22065.1847 - val_mean_absolute_error: 22065.1847\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 19876.97443\n",
      "Epoch 228/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 30272.1071 - mean_absolute_error: 30272.1071 - val_loss: 20931.8125 - val_mean_absolute_error: 20931.8125\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 19876.97443\n",
      "Epoch 229/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 265us/step - loss: 30315.6376 - mean_absolute_error: 30315.6376 - val_loss: 20205.7742 - val_mean_absolute_error: 20205.7742\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 19876.97443\n",
      "Epoch 230/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 30530.8537 - mean_absolute_error: 30530.8537 - val_loss: 21861.4281 - val_mean_absolute_error: 21861.4281\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 19876.97443\n",
      "Epoch 231/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 30744.3497 - mean_absolute_error: 30744.3497 - val_loss: 20395.5347 - val_mean_absolute_error: 20395.5347\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 19876.97443\n",
      "Epoch 232/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 30573.2619 - mean_absolute_error: 30573.2619 - val_loss: 23512.6258 - val_mean_absolute_error: 23512.6258\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 19876.97443\n",
      "Epoch 233/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 31186.4022 - mean_absolute_error: 31186.4022 - val_loss: 19891.6517 - val_mean_absolute_error: 19891.6517\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 19876.97443\n",
      "Epoch 234/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 29323.7874 - mean_absolute_error: 29323.7874 - val_loss: 19792.4798 - val_mean_absolute_error: 19792.4798\n",
      "\n",
      "Epoch 00234: val_loss improved from 19876.97443 to 19792.47978, saving model to Weights-234--19792.47978.hdf5\n",
      "Epoch 235/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 29585.3349 - mean_absolute_error: 29585.3349 - val_loss: 21152.5191 - val_mean_absolute_error: 21152.5191\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 19792.47978\n",
      "Epoch 236/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 29857.7629 - mean_absolute_error: 29857.7629 - val_loss: 21342.7019 - val_mean_absolute_error: 21342.7019\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 19792.47978\n",
      "Epoch 237/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 30044.1265 - mean_absolute_error: 30044.1265 - val_loss: 19867.9196 - val_mean_absolute_error: 19867.9196\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 19792.47978\n",
      "Epoch 238/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 29580.6768 - mean_absolute_error: 29580.6768 - val_loss: 23118.1576 - val_mean_absolute_error: 23118.1576\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 19792.47978\n",
      "Epoch 239/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 31237.2966 - mean_absolute_error: 31237.2966 - val_loss: 20408.4640 - val_mean_absolute_error: 20408.4640\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 19792.47978\n",
      "Epoch 240/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 29167.5179 - mean_absolute_error: 29167.5179 - val_loss: 26338.0582 - val_mean_absolute_error: 26338.0582\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 19792.47978\n",
      "Epoch 241/1500\n",
      "934/934 [==============================] - 0s 290us/step - loss: 30939.4447 - mean_absolute_error: 30939.4447 - val_loss: 19919.1170 - val_mean_absolute_error: 19919.1170\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 19792.47978\n",
      "Epoch 242/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 29215.9987 - mean_absolute_error: 29215.9987 - val_loss: 21768.9334 - val_mean_absolute_error: 21768.9334\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 19792.47978\n",
      "Epoch 243/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 29148.7860 - mean_absolute_error: 29148.7860 - val_loss: 26225.1153 - val_mean_absolute_error: 26225.1153\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 19792.47978\n",
      "Epoch 244/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 30196.6357 - mean_absolute_error: 30196.6357 - val_loss: 24820.1085 - val_mean_absolute_error: 24820.1085\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 19792.47978\n",
      "Epoch 245/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 30216.4898 - mean_absolute_error: 30216.4898 - val_loss: 31689.5868 - val_mean_absolute_error: 31689.5868\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 19792.47978\n",
      "Epoch 246/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 28677.3374 - mean_absolute_error: 28677.3374 - val_loss: 23270.9968 - val_mean_absolute_error: 23270.9968\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 19792.47978\n",
      "Epoch 247/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 30612.1718 - mean_absolute_error: 30612.1718 - val_loss: 20966.0451 - val_mean_absolute_error: 20966.0451\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 19792.47978\n",
      "Epoch 248/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 30369.2801 - mean_absolute_error: 30369.2801 - val_loss: 21816.0245 - val_mean_absolute_error: 21816.0245\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 19792.47978\n",
      "Epoch 249/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 30332.9287 - mean_absolute_error: 30332.9287 - val_loss: 22811.9506 - val_mean_absolute_error: 22811.9506\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 19792.47978\n",
      "Epoch 250/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 30384.9524 - mean_absolute_error: 30384.9524 - val_loss: 19487.9365 - val_mean_absolute_error: 19487.9365\n",
      "\n",
      "Epoch 00250: val_loss improved from 19792.47978 to 19487.93654, saving model to Weights-250--19487.93654.hdf5\n",
      "Epoch 251/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 29745.8720 - mean_absolute_error: 29745.8720 - val_loss: 19850.3255 - val_mean_absolute_error: 19850.3255\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 19487.93654\n",
      "Epoch 252/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 30715.3363 - mean_absolute_error: 30715.3363 - val_loss: 24996.9681 - val_mean_absolute_error: 24996.9681\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 19487.93654\n",
      "Epoch 253/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 30126.9432 - mean_absolute_error: 30126.9432 - val_loss: 20300.2096 - val_mean_absolute_error: 20300.2096\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 19487.93654\n",
      "Epoch 254/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 30120.3897 - mean_absolute_error: 30120.3897 - val_loss: 19390.4609 - val_mean_absolute_error: 19390.4609\n",
      "\n",
      "Epoch 00254: val_loss improved from 19487.93654 to 19390.46093, saving model to Weights-254--19390.46093.hdf5\n",
      "Epoch 255/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28952.1523 - mean_absolute_error: 28952.1523 - val_loss: 21540.3550 - val_mean_absolute_error: 21540.3550\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 19390.46093\n",
      "Epoch 256/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 30898.0958 - mean_absolute_error: 30898.0958 - val_loss: 21149.7477 - val_mean_absolute_error: 21149.7477\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 19390.46093\n",
      "Epoch 257/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 30172.3347 - mean_absolute_error: 30172.3347 - val_loss: 22604.0838 - val_mean_absolute_error: 22604.0838\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 19390.46093\n",
      "Epoch 258/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 28874.6225 - mean_absolute_error: 28874.6225 - val_loss: 21128.9913 - val_mean_absolute_error: 21128.9913\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 19390.46093\n",
      "Epoch 259/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 29519.4639 - mean_absolute_error: 29519.4639 - val_loss: 19661.9526 - val_mean_absolute_error: 19661.9526\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 19390.46093\n",
      "Epoch 260/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 29234.1923 - mean_absolute_error: 29234.1923 - val_loss: 20821.7257 - val_mean_absolute_error: 20821.7257\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 19390.46093\n",
      "Epoch 261/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 30633.1859 - mean_absolute_error: 30633.1859 - val_loss: 21297.9343 - val_mean_absolute_error: 21297.9343\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 19390.46093\n",
      "Epoch 262/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 29765.2604 - mean_absolute_error: 29765.2604 - val_loss: 22727.0602 - val_mean_absolute_error: 22727.0602\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 19390.46093\n",
      "Epoch 263/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 28974.8197 - mean_absolute_error: 28974.8197 - val_loss: 19253.6114 - val_mean_absolute_error: 19253.6114\n",
      "\n",
      "Epoch 00263: val_loss improved from 19390.46093 to 19253.61144, saving model to Weights-263--19253.61144.hdf5\n",
      "Epoch 264/1500\n",
      "934/934 [==============================] - 0s 319us/step - loss: 29980.8389 - mean_absolute_error: 29980.8389 - val_loss: 21050.5715 - val_mean_absolute_error: 21050.5715\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 19253.61144\n",
      "Epoch 265/1500\n",
      "934/934 [==============================] - 0s 316us/step - loss: 30022.0071 - mean_absolute_error: 30022.0071 - val_loss: 23697.3532 - val_mean_absolute_error: 23697.3532\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 19253.61144\n",
      "Epoch 266/1500\n",
      "934/934 [==============================] - 0s 311us/step - loss: 28613.5689 - mean_absolute_error: 28613.5689 - val_loss: 21880.2970 - val_mean_absolute_error: 21880.2970\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 19253.61144\n",
      "Epoch 267/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 29229.7165 - mean_absolute_error: 29229.7165 - val_loss: 20268.8827 - val_mean_absolute_error: 20268.8827\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 19253.61144\n",
      "Epoch 268/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 30159.9008 - mean_absolute_error: 30159.9008 - val_loss: 23048.4511 - val_mean_absolute_error: 23048.4511\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 19253.61144\n",
      "Epoch 269/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 30738.0083 - mean_absolute_error: 30738.0083 - val_loss: 23661.9230 - val_mean_absolute_error: 23661.9230\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 19253.61144\n",
      "Epoch 270/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 29932.6203 - mean_absolute_error: 29932.6203 - val_loss: 24387.4841 - val_mean_absolute_error: 24387.4841\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 19253.61144\n",
      "Epoch 271/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 28906.9989 - mean_absolute_error: 28906.9989 - val_loss: 24062.5555 - val_mean_absolute_error: 24062.5555\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 19253.61144\n",
      "Epoch 272/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 29250.0501 - mean_absolute_error: 29250.0501 - val_loss: 27946.2697 - val_mean_absolute_error: 27946.2697\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 19253.61144\n",
      "Epoch 273/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 30694.6857 - mean_absolute_error: 30694.6857 - val_loss: 24749.7585 - val_mean_absolute_error: 24749.7585\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 19253.61144\n",
      "Epoch 274/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 29949.3548 - mean_absolute_error: 29949.3548 - val_loss: 23428.2330 - val_mean_absolute_error: 23428.2330\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 19253.61144\n",
      "Epoch 275/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 29329.5386 - mean_absolute_error: 29329.5386 - val_loss: 20190.6878 - val_mean_absolute_error: 20190.6878\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 19253.61144\n",
      "Epoch 276/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 29121.5787 - mean_absolute_error: 29121.5787 - val_loss: 27186.1023 - val_mean_absolute_error: 27186.1023\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 19253.61144\n",
      "Epoch 277/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 30172.1823 - mean_absolute_error: 30172.1823 - val_loss: 21658.1520 - val_mean_absolute_error: 21658.1520\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 19253.61144\n",
      "Epoch 278/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 29996.7980 - mean_absolute_error: 29996.7980 - val_loss: 29084.6687 - val_mean_absolute_error: 29084.6687\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 19253.61144\n",
      "Epoch 279/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 28811.5538 - mean_absolute_error: 28811.5538 - val_loss: 19796.1690 - val_mean_absolute_error: 19796.1690\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 19253.61144\n",
      "Epoch 280/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 29567.8334 - mean_absolute_error: 29567.8334 - val_loss: 29262.4241 - val_mean_absolute_error: 29262.4241\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 19253.61144\n",
      "Epoch 281/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 29678.4768 - mean_absolute_error: 29678.4768 - val_loss: 20107.8346 - val_mean_absolute_error: 20107.8346\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 19253.61144\n",
      "Epoch 282/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 29147.9159 - mean_absolute_error: 29147.9159 - val_loss: 22063.0155 - val_mean_absolute_error: 22063.0155\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 19253.61144\n",
      "Epoch 283/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 28835.1795 - mean_absolute_error: 28835.1795 - val_loss: 23383.4784 - val_mean_absolute_error: 23383.4784\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 19253.61144\n",
      "Epoch 284/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 29020.4135 - mean_absolute_error: 29020.4135 - val_loss: 19513.0740 - val_mean_absolute_error: 19513.0740\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 19253.61144\n",
      "Epoch 285/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 30170.7693 - mean_absolute_error: 30170.7693 - val_loss: 22922.7293 - val_mean_absolute_error: 22922.7293\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 19253.61144\n",
      "Epoch 286/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 29237.5704 - mean_absolute_error: 29237.5704 - val_loss: 27183.7393 - val_mean_absolute_error: 27183.7393\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 19253.61144\n",
      "Epoch 287/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 28948.6517 - mean_absolute_error: 28948.6517 - val_loss: 20636.5939 - val_mean_absolute_error: 20636.5939\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 19253.61144\n",
      "Epoch 288/1500\n",
      "934/934 [==============================] - 0s 281us/step - loss: 28409.1765 - mean_absolute_error: 28409.1765 - val_loss: 19480.5833 - val_mean_absolute_error: 19480.5833\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 19253.61144\n",
      "Epoch 289/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 28734.3417 - mean_absolute_error: 28734.3417 - val_loss: 21623.4382 - val_mean_absolute_error: 21623.4382\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 19253.61144\n",
      "Epoch 290/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27896.5789 - mean_absolute_error: 27896.5789 - val_loss: 27092.2527 - val_mean_absolute_error: 27092.2527\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 19253.61144\n",
      "Epoch 291/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 28971.2678 - mean_absolute_error: 28971.2678 - val_loss: 31734.1510 - val_mean_absolute_error: 31734.1510\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 19253.61144\n",
      "Epoch 292/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 30212.6838 - mean_absolute_error: 30212.6838 - val_loss: 21167.5044 - val_mean_absolute_error: 21167.5044\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 19253.61144\n",
      "Epoch 293/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28789.4897 - mean_absolute_error: 28789.4897 - val_loss: 21406.7209 - val_mean_absolute_error: 21406.7209\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 19253.61144\n",
      "Epoch 294/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 29160.5807 - mean_absolute_error: 29160.5807 - val_loss: 26238.0076 - val_mean_absolute_error: 26238.0076\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 19253.61144\n",
      "Epoch 295/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 29291.3525 - mean_absolute_error: 29291.3525 - val_loss: 19596.3142 - val_mean_absolute_error: 19596.3142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00295: val_loss did not improve from 19253.61144\n",
      "Epoch 296/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 29049.1415 - mean_absolute_error: 29049.1415 - val_loss: 19436.7239 - val_mean_absolute_error: 19436.7239\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 19253.61144\n",
      "Epoch 297/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 29343.1963 - mean_absolute_error: 29343.1963 - val_loss: 23915.9966 - val_mean_absolute_error: 23915.9966\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 19253.61144\n",
      "Epoch 298/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 28746.0971 - mean_absolute_error: 28746.0971 - val_loss: 20072.8841 - val_mean_absolute_error: 20072.8841\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 19253.61144\n",
      "Epoch 299/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 29487.9657 - mean_absolute_error: 29487.9657 - val_loss: 22532.8610 - val_mean_absolute_error: 22532.8610\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 19253.61144\n",
      "Epoch 300/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 29171.6561 - mean_absolute_error: 29171.6561 - val_loss: 31654.3099 - val_mean_absolute_error: 31654.3099\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 19253.61144\n",
      "Epoch 301/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 29432.7048 - mean_absolute_error: 29432.7048 - val_loss: 20989.2055 - val_mean_absolute_error: 20989.2055\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 19253.61144\n",
      "Epoch 302/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 28897.5015 - mean_absolute_error: 28897.5015 - val_loss: 19815.3624 - val_mean_absolute_error: 19815.3624\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 19253.61144\n",
      "Epoch 303/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 28467.4096 - mean_absolute_error: 28467.4096 - val_loss: 19528.5454 - val_mean_absolute_error: 19528.5454\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 19253.61144\n",
      "Epoch 304/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 29937.1637 - mean_absolute_error: 29937.1637 - val_loss: 19973.8374 - val_mean_absolute_error: 19973.8374\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 19253.61144\n",
      "Epoch 305/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 29387.8501 - mean_absolute_error: 29387.8501 - val_loss: 26706.8733 - val_mean_absolute_error: 26706.8733\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 19253.61144\n",
      "Epoch 306/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 29140.5574 - mean_absolute_error: 29140.5574 - val_loss: 31036.8244 - val_mean_absolute_error: 31036.8244\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 19253.61144\n",
      "Epoch 307/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28892.9882 - mean_absolute_error: 28892.9882 - val_loss: 18987.4326 - val_mean_absolute_error: 18987.4326\n",
      "\n",
      "Epoch 00307: val_loss improved from 19253.61144 to 18987.43257, saving model to Weights-307--18987.43257.hdf5\n",
      "Epoch 308/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 28864.3735 - mean_absolute_error: 28864.3735 - val_loss: 19160.1783 - val_mean_absolute_error: 19160.1783\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 18987.43257\n",
      "Epoch 309/1500\n",
      "934/934 [==============================] - 0s 300us/step - loss: 29539.0583 - mean_absolute_error: 29539.0583 - val_loss: 23620.1830 - val_mean_absolute_error: 23620.1830\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 18987.43257\n",
      "Epoch 310/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 29676.3518 - mean_absolute_error: 29676.3518 - val_loss: 22183.1392 - val_mean_absolute_error: 22183.1392\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 18987.43257\n",
      "Epoch 311/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 29835.5491 - mean_absolute_error: 29835.5491 - val_loss: 19819.7840 - val_mean_absolute_error: 19819.7840\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 18987.43257\n",
      "Epoch 312/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 28991.5829 - mean_absolute_error: 28991.5829 - val_loss: 20690.7768 - val_mean_absolute_error: 20690.7768\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 18987.43257\n",
      "Epoch 313/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 29832.0196 - mean_absolute_error: 29832.0196 - val_loss: 18955.9871 - val_mean_absolute_error: 18955.9871\n",
      "\n",
      "Epoch 00313: val_loss improved from 18987.43257 to 18955.98710, saving model to Weights-313--18955.98710.hdf5\n",
      "Epoch 314/1500\n",
      "934/934 [==============================] - 0s 324us/step - loss: 28800.5028 - mean_absolute_error: 28800.5028 - val_loss: 20212.3579 - val_mean_absolute_error: 20212.3579\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 18955.98710\n",
      "Epoch 315/1500\n",
      "934/934 [==============================] - 0s 322us/step - loss: 28366.1125 - mean_absolute_error: 28366.1125 - val_loss: 18915.1622 - val_mean_absolute_error: 18915.1622\n",
      "\n",
      "Epoch 00315: val_loss improved from 18955.98710 to 18915.16218, saving model to Weights-315--18915.16218.hdf5\n",
      "Epoch 316/1500\n",
      "934/934 [==============================] - 0s 304us/step - loss: 29906.6342 - mean_absolute_error: 29906.6342 - val_loss: 18926.4885 - val_mean_absolute_error: 18926.4885\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 18915.16218\n",
      "Epoch 317/1500\n",
      "934/934 [==============================] - 0s 311us/step - loss: 29110.8997 - mean_absolute_error: 29110.8997 - val_loss: 19181.6589 - val_mean_absolute_error: 19181.6589\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 18915.16218\n",
      "Epoch 318/1500\n",
      "934/934 [==============================] - 0s 316us/step - loss: 29765.7939 - mean_absolute_error: 29765.7939 - val_loss: 23820.2291 - val_mean_absolute_error: 23820.2291\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 18915.16218\n",
      "Epoch 319/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 28953.0805 - mean_absolute_error: 28953.0805 - val_loss: 25911.6181 - val_mean_absolute_error: 25911.6181\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 18915.16218\n",
      "Epoch 320/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 28833.2841 - mean_absolute_error: 28833.2841 - val_loss: 29528.1069 - val_mean_absolute_error: 29528.1069\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 18915.16218\n",
      "Epoch 321/1500\n",
      "934/934 [==============================] - 0s 293us/step - loss: 30315.9284 - mean_absolute_error: 30315.9284 - val_loss: 21881.7959 - val_mean_absolute_error: 21881.7959\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 18915.16218\n",
      "Epoch 322/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 28516.6468 - mean_absolute_error: 28516.6468 - val_loss: 21880.0971 - val_mean_absolute_error: 21880.0971\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 18915.16218\n",
      "Epoch 323/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28503.8612 - mean_absolute_error: 28503.8612 - val_loss: 19327.2103 - val_mean_absolute_error: 19327.2103\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 18915.16218\n",
      "Epoch 324/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 29453.4762 - mean_absolute_error: 29453.4762 - val_loss: 19181.2150 - val_mean_absolute_error: 19181.2150\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 18915.16218\n",
      "Epoch 325/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 29427.4469 - mean_absolute_error: 29427.4469 - val_loss: 19702.9830 - val_mean_absolute_error: 19702.9830\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 18915.16218\n",
      "Epoch 326/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 28592.8311 - mean_absolute_error: 28592.8311 - val_loss: 18955.4212 - val_mean_absolute_error: 18955.4212\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 18915.16218\n",
      "Epoch 327/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27419.3728 - mean_absolute_error: 27419.3728 - val_loss: 23386.4276 - val_mean_absolute_error: 23386.4276\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 18915.16218\n",
      "Epoch 328/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28586.9919 - mean_absolute_error: 28586.9919 - val_loss: 28289.6233 - val_mean_absolute_error: 28289.6233\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 18915.16218\n",
      "Epoch 329/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 257us/step - loss: 29038.5963 - mean_absolute_error: 29038.5963 - val_loss: 18824.7761 - val_mean_absolute_error: 18824.7761\n",
      "\n",
      "Epoch 00329: val_loss improved from 18915.16218 to 18824.77605, saving model to Weights-329--18824.77605.hdf5\n",
      "Epoch 330/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 29540.2326 - mean_absolute_error: 29540.2326 - val_loss: 23735.4489 - val_mean_absolute_error: 23735.4489\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 18824.77605\n",
      "Epoch 331/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 29742.3030 - mean_absolute_error: 29742.3030 - val_loss: 28426.6260 - val_mean_absolute_error: 28426.6260\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 18824.77605\n",
      "Epoch 332/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 29457.6360 - mean_absolute_error: 29457.6360 - val_loss: 32155.9737 - val_mean_absolute_error: 32155.9737\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 18824.77605\n",
      "Epoch 333/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 27930.6943 - mean_absolute_error: 27930.6943 - val_loss: 24825.8040 - val_mean_absolute_error: 24825.8040\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 18824.77605\n",
      "Epoch 334/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 28182.9117 - mean_absolute_error: 28182.9117 - val_loss: 19522.5619 - val_mean_absolute_error: 19522.5619\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 18824.77605\n",
      "Epoch 335/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 28540.6583 - mean_absolute_error: 28540.6583 - val_loss: 24175.5175 - val_mean_absolute_error: 24175.5175\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 18824.77605\n",
      "Epoch 336/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 28449.4080 - mean_absolute_error: 28449.4080 - val_loss: 24587.0390 - val_mean_absolute_error: 24587.0390\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 18824.77605\n",
      "Epoch 337/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27744.9478 - mean_absolute_error: 27744.9478 - val_loss: 19134.0555 - val_mean_absolute_error: 19134.0555\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 18824.77605\n",
      "Epoch 338/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 29259.6096 - mean_absolute_error: 29259.6096 - val_loss: 18713.9902 - val_mean_absolute_error: 18713.9902\n",
      "\n",
      "Epoch 00338: val_loss improved from 18824.77605 to 18713.99016, saving model to Weights-338--18713.99016.hdf5\n",
      "Epoch 339/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 29870.5073 - mean_absolute_error: 29870.5073 - val_loss: 22626.5455 - val_mean_absolute_error: 22626.5455\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 18713.99016\n",
      "Epoch 340/1500\n",
      "934/934 [==============================] - 0s 290us/step - loss: 28121.5588 - mean_absolute_error: 28121.5588 - val_loss: 35004.1416 - val_mean_absolute_error: 35004.1416\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 18713.99016\n",
      "Epoch 341/1500\n",
      "934/934 [==============================] - 0s 289us/step - loss: 30175.2992 - mean_absolute_error: 30175.2992 - val_loss: 24671.3612 - val_mean_absolute_error: 24671.3612\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 18713.99016\n",
      "Epoch 342/1500\n",
      "934/934 [==============================] - 0s 286us/step - loss: 28052.4413 - mean_absolute_error: 28052.4413 - val_loss: 22639.4474 - val_mean_absolute_error: 22639.4474\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 18713.99016\n",
      "Epoch 343/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 29109.8478 - mean_absolute_error: 29109.8478 - val_loss: 22875.0330 - val_mean_absolute_error: 22875.0330\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 18713.99016\n",
      "Epoch 344/1500\n",
      "934/934 [==============================] - 0s 287us/step - loss: 28093.7314 - mean_absolute_error: 28093.7314 - val_loss: 22139.1304 - val_mean_absolute_error: 22139.1304\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 18713.99016\n",
      "Epoch 345/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 27941.5597 - mean_absolute_error: 27941.5597 - val_loss: 24766.2241 - val_mean_absolute_error: 24766.2241\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 18713.99016\n",
      "Epoch 346/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 30061.5095 - mean_absolute_error: 30061.5095 - val_loss: 23567.9577 - val_mean_absolute_error: 23567.9577\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 18713.99016\n",
      "Epoch 347/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 28325.1937 - mean_absolute_error: 28325.1937 - val_loss: 22300.1600 - val_mean_absolute_error: 22300.1600\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 18713.99016\n",
      "Epoch 348/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 28075.7708 - mean_absolute_error: 28075.7708 - val_loss: 21908.1366 - val_mean_absolute_error: 21908.1366\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 18713.99016\n",
      "Epoch 349/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 28680.4100 - mean_absolute_error: 28680.4100 - val_loss: 18711.5524 - val_mean_absolute_error: 18711.5524\n",
      "\n",
      "Epoch 00349: val_loss improved from 18713.99016 to 18711.55245, saving model to Weights-349--18711.55245.hdf5\n",
      "Epoch 350/1500\n",
      "934/934 [==============================] - 0s 293us/step - loss: 27112.7895 - mean_absolute_error: 27112.7895 - val_loss: 18818.5964 - val_mean_absolute_error: 18818.5964\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 18711.55245\n",
      "Epoch 351/1500\n",
      "934/934 [==============================] - 0s 286us/step - loss: 28169.9308 - mean_absolute_error: 28169.9308 - val_loss: 20148.0305 - val_mean_absolute_error: 20148.0305\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 18711.55245\n",
      "Epoch 352/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 28185.3928 - mean_absolute_error: 28185.3928 - val_loss: 18678.1011 - val_mean_absolute_error: 18678.1011\n",
      "\n",
      "Epoch 00352: val_loss improved from 18711.55245 to 18678.10112, saving model to Weights-352--18678.10112.hdf5\n",
      "Epoch 353/1500\n",
      "934/934 [==============================] - 0s 303us/step - loss: 29308.5082 - mean_absolute_error: 29308.5082 - val_loss: 27640.2347 - val_mean_absolute_error: 27640.2347\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 18678.10112\n",
      "Epoch 354/1500\n",
      "934/934 [==============================] - 0s 286us/step - loss: 27883.3592 - mean_absolute_error: 27883.3592 - val_loss: 25795.0522 - val_mean_absolute_error: 25795.0522\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 18678.10112\n",
      "Epoch 355/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 28930.2981 - mean_absolute_error: 28930.2981 - val_loss: 18442.1149 - val_mean_absolute_error: 18442.1149\n",
      "\n",
      "Epoch 00355: val_loss improved from 18678.10112 to 18442.11488, saving model to Weights-355--18442.11488.hdf5\n",
      "Epoch 356/1500\n",
      "934/934 [==============================] - 0s 326us/step - loss: 28804.5732 - mean_absolute_error: 28804.5732 - val_loss: 20265.7829 - val_mean_absolute_error: 20265.7829\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 18442.11488\n",
      "Epoch 357/1500\n",
      "934/934 [==============================] - 0s 290us/step - loss: 28916.9563 - mean_absolute_error: 28916.9563 - val_loss: 20587.5251 - val_mean_absolute_error: 20587.5251\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 18442.11488\n",
      "Epoch 358/1500\n",
      "934/934 [==============================] - 0s 308us/step - loss: 29512.0932 - mean_absolute_error: 29512.0932 - val_loss: 18251.2210 - val_mean_absolute_error: 18251.2210\n",
      "\n",
      "Epoch 00358: val_loss improved from 18442.11488 to 18251.22101, saving model to Weights-358--18251.22101.hdf5\n",
      "Epoch 359/1500\n",
      "934/934 [==============================] - 0s 331us/step - loss: 27851.0376 - mean_absolute_error: 27851.0376 - val_loss: 21288.8765 - val_mean_absolute_error: 21288.8765\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 18251.22101\n",
      "Epoch 360/1500\n",
      "934/934 [==============================] - 0s 313us/step - loss: 27621.2441 - mean_absolute_error: 27621.2441 - val_loss: 22118.0953 - val_mean_absolute_error: 22118.0953\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 18251.22101\n",
      "Epoch 361/1500\n",
      "934/934 [==============================] - 0s 324us/step - loss: 28166.2103 - mean_absolute_error: 28166.2103 - val_loss: 18642.3209 - val_mean_absolute_error: 18642.3209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00361: val_loss did not improve from 18251.22101\n",
      "Epoch 362/1500\n",
      "934/934 [==============================] - 0s 333us/step - loss: 28006.5145 - mean_absolute_error: 28006.5145 - val_loss: 20012.8965 - val_mean_absolute_error: 20012.8965\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 18251.22101\n",
      "Epoch 363/1500\n",
      "934/934 [==============================] - 0s 292us/step - loss: 27741.8481 - mean_absolute_error: 27741.8481 - val_loss: 24784.4481 - val_mean_absolute_error: 24784.4481\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 18251.22101\n",
      "Epoch 364/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 28257.9180 - mean_absolute_error: 28257.9180 - val_loss: 29675.1642 - val_mean_absolute_error: 29675.1642\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 18251.22101\n",
      "Epoch 365/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 28516.2326 - mean_absolute_error: 28516.2326 - val_loss: 20474.5209 - val_mean_absolute_error: 20474.5209\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 18251.22101\n",
      "Epoch 366/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27979.3005 - mean_absolute_error: 27979.3005 - val_loss: 22093.9156 - val_mean_absolute_error: 22093.9156\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 18251.22101\n",
      "Epoch 367/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28607.0238 - mean_absolute_error: 28607.0238 - val_loss: 27284.8463 - val_mean_absolute_error: 27284.8463\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 18251.22101\n",
      "Epoch 368/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27947.3031 - mean_absolute_error: 27947.3031 - val_loss: 21173.9433 - val_mean_absolute_error: 21173.9433\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 18251.22101\n",
      "Epoch 369/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27881.8764 - mean_absolute_error: 27881.8764 - val_loss: 29628.1693 - val_mean_absolute_error: 29628.1693\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 18251.22101\n",
      "Epoch 370/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 27921.1351 - mean_absolute_error: 27921.1351 - val_loss: 18545.5700 - val_mean_absolute_error: 18545.5700\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 18251.22101\n",
      "Epoch 371/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 29333.8274 - mean_absolute_error: 29333.8274 - val_loss: 18352.3047 - val_mean_absolute_error: 18352.3047\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 18251.22101\n",
      "Epoch 372/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27862.2473 - mean_absolute_error: 27862.2473 - val_loss: 19352.8970 - val_mean_absolute_error: 19352.8970\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 18251.22101\n",
      "Epoch 373/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 28730.7935 - mean_absolute_error: 28730.7935 - val_loss: 19785.6918 - val_mean_absolute_error: 19785.6918\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 18251.22101\n",
      "Epoch 374/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 28233.4586 - mean_absolute_error: 28233.4586 - val_loss: 32830.4833 - val_mean_absolute_error: 32830.4833\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 18251.22101\n",
      "Epoch 375/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28570.3533 - mean_absolute_error: 28570.3533 - val_loss: 19573.3981 - val_mean_absolute_error: 19573.3981\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 18251.22101\n",
      "Epoch 376/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 28395.6632 - mean_absolute_error: 28395.6632 - val_loss: 19626.6738 - val_mean_absolute_error: 19626.6738\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 18251.22101\n",
      "Epoch 377/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 29222.4847 - mean_absolute_error: 29222.4847 - val_loss: 22178.0479 - val_mean_absolute_error: 22178.0479\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 18251.22101\n",
      "Epoch 378/1500\n",
      "934/934 [==============================] - 0s 295us/step - loss: 29318.6091 - mean_absolute_error: 29318.6091 - val_loss: 26826.2566 - val_mean_absolute_error: 26826.2566\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 18251.22101\n",
      "Epoch 379/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 27474.3464 - mean_absolute_error: 27474.3464 - val_loss: 18308.5333 - val_mean_absolute_error: 18308.5333\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 18251.22101\n",
      "Epoch 380/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 28931.8353 - mean_absolute_error: 28931.8353 - val_loss: 20038.0982 - val_mean_absolute_error: 20038.0982\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 18251.22101\n",
      "Epoch 381/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28957.3145 - mean_absolute_error: 28957.3145 - val_loss: 17979.5238 - val_mean_absolute_error: 17979.5238\n",
      "\n",
      "Epoch 00381: val_loss improved from 18251.22101 to 17979.52379, saving model to Weights-381--17979.52379.hdf5\n",
      "Epoch 382/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 28630.2467 - mean_absolute_error: 28630.2467 - val_loss: 22296.1647 - val_mean_absolute_error: 22296.1647\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 17979.52379\n",
      "Epoch 383/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 29002.2148 - mean_absolute_error: 29002.2148 - val_loss: 24482.1012 - val_mean_absolute_error: 24482.1012\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 17979.52379\n",
      "Epoch 384/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 27906.2028 - mean_absolute_error: 27906.2028 - val_loss: 22557.7132 - val_mean_absolute_error: 22557.7132\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 17979.52379\n",
      "Epoch 385/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 29989.5826 - mean_absolute_error: 29989.5826 - val_loss: 20987.2255 - val_mean_absolute_error: 20987.2255\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 17979.52379\n",
      "Epoch 386/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27704.9289 - mean_absolute_error: 27704.9289 - val_loss: 22053.6050 - val_mean_absolute_error: 22053.6050\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 17979.52379\n",
      "Epoch 387/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 28456.5055 - mean_absolute_error: 28456.5055 - val_loss: 18322.8337 - val_mean_absolute_error: 18322.8337\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 17979.52379\n",
      "Epoch 388/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 28046.1193 - mean_absolute_error: 28046.1193 - val_loss: 20864.1994 - val_mean_absolute_error: 20864.1994\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 17979.52379\n",
      "Epoch 389/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 28480.3509 - mean_absolute_error: 28480.3509 - val_loss: 23874.9884 - val_mean_absolute_error: 23874.9884\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 17979.52379\n",
      "Epoch 390/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27933.7290 - mean_absolute_error: 27933.7290 - val_loss: 18318.4763 - val_mean_absolute_error: 18318.4763\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 17979.52379\n",
      "Epoch 391/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 28543.9518 - mean_absolute_error: 28543.9518 - val_loss: 27325.6913 - val_mean_absolute_error: 27325.6913\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 17979.52379\n",
      "Epoch 392/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27094.6914 - mean_absolute_error: 27094.6914 - val_loss: 29998.3272 - val_mean_absolute_error: 29998.3272\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 17979.52379\n",
      "Epoch 393/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 28525.9284 - mean_absolute_error: 28525.9284 - val_loss: 21944.1125 - val_mean_absolute_error: 21944.1125\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 17979.52379\n",
      "Epoch 394/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 28348.2973 - mean_absolute_error: 28348.2973 - val_loss: 18591.1226 - val_mean_absolute_error: 18591.1226\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 17979.52379\n",
      "Epoch 395/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27770.0841 - mean_absolute_error: 27770.0841 - val_loss: 19414.6971 - val_mean_absolute_error: 19414.6971\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 17979.52379\n",
      "Epoch 396/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27919.5340 - mean_absolute_error: 27919.5340 - val_loss: 20999.1302 - val_mean_absolute_error: 20999.1302\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 17979.52379\n",
      "Epoch 397/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27766.0397 - mean_absolute_error: 27766.0397 - val_loss: 21931.4083 - val_mean_absolute_error: 21931.4083\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 17979.52379\n",
      "Epoch 398/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27563.2614 - mean_absolute_error: 27563.2614 - val_loss: 18204.6300 - val_mean_absolute_error: 18204.6300\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 17979.52379\n",
      "Epoch 399/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27411.3965 - mean_absolute_error: 27411.3965 - val_loss: 22955.3624 - val_mean_absolute_error: 22955.3624\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 17979.52379\n",
      "Epoch 400/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27670.1093 - mean_absolute_error: 27670.1093 - val_loss: 31837.4823 - val_mean_absolute_error: 31837.4823\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 17979.52379\n",
      "Epoch 401/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 28175.5905 - mean_absolute_error: 28175.5905 - val_loss: 19458.9874 - val_mean_absolute_error: 19458.9874\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 17979.52379\n",
      "Epoch 402/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27545.5517 - mean_absolute_error: 27545.5517 - val_loss: 18580.7885 - val_mean_absolute_error: 18580.7885\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 17979.52379\n",
      "Epoch 403/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27308.7883 - mean_absolute_error: 27308.7883 - val_loss: 22101.1105 - val_mean_absolute_error: 22101.1105\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 17979.52379\n",
      "Epoch 404/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27597.2673 - mean_absolute_error: 27597.2673 - val_loss: 27747.1236 - val_mean_absolute_error: 27747.1236\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 17979.52379\n",
      "Epoch 405/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 29728.7384 - mean_absolute_error: 29728.7384 - val_loss: 19635.2556 - val_mean_absolute_error: 19635.2556\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 17979.52379\n",
      "Epoch 406/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28304.0403 - mean_absolute_error: 28304.0403 - val_loss: 19012.1038 - val_mean_absolute_error: 19012.1038\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 17979.52379\n",
      "Epoch 407/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28628.4814 - mean_absolute_error: 28628.4814 - val_loss: 18263.9566 - val_mean_absolute_error: 18263.9566\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 17979.52379\n",
      "Epoch 408/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 28680.7962 - mean_absolute_error: 28680.7962 - val_loss: 29387.3524 - val_mean_absolute_error: 29387.3524\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 17979.52379\n",
      "Epoch 409/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27278.6115 - mean_absolute_error: 27278.6115 - val_loss: 24938.0740 - val_mean_absolute_error: 24938.0740\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 17979.52379\n",
      "Epoch 410/1500\n",
      "934/934 [==============================] - 0s 304us/step - loss: 27514.8236 - mean_absolute_error: 27514.8236 - val_loss: 26803.9367 - val_mean_absolute_error: 26803.9367\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 17979.52379\n",
      "Epoch 411/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28608.4601 - mean_absolute_error: 28608.4601 - val_loss: 18297.6088 - val_mean_absolute_error: 18297.6088\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 17979.52379\n",
      "Epoch 412/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 27176.3847 - mean_absolute_error: 27176.3847 - val_loss: 19249.7547 - val_mean_absolute_error: 19249.7547\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 17979.52379\n",
      "Epoch 413/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 28830.6165 - mean_absolute_error: 28830.6165 - val_loss: 18193.6414 - val_mean_absolute_error: 18193.6414\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 17979.52379\n",
      "Epoch 414/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 27166.4073 - mean_absolute_error: 27166.4073 - val_loss: 21459.8932 - val_mean_absolute_error: 21459.8932\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 17979.52379\n",
      "Epoch 415/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 28046.2238 - mean_absolute_error: 28046.2238 - val_loss: 18960.6042 - val_mean_absolute_error: 18960.6042\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 17979.52379\n",
      "Epoch 416/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 28536.2560 - mean_absolute_error: 28536.2560 - val_loss: 17934.6417 - val_mean_absolute_error: 17934.6417\n",
      "\n",
      "Epoch 00416: val_loss improved from 17979.52379 to 17934.64165, saving model to Weights-416--17934.64165.hdf5\n",
      "Epoch 417/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27460.4519 - mean_absolute_error: 27460.4519 - val_loss: 28450.3149 - val_mean_absolute_error: 28450.3149\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 17934.64165\n",
      "Epoch 418/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 28573.6954 - mean_absolute_error: 28573.6954 - val_loss: 21142.3889 - val_mean_absolute_error: 21142.3889\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 17934.64165\n",
      "Epoch 419/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27888.6361 - mean_absolute_error: 27888.6361 - val_loss: 20940.6163 - val_mean_absolute_error: 20940.6163\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 17934.64165\n",
      "Epoch 420/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 28008.5643 - mean_absolute_error: 28008.5643 - val_loss: 20271.0522 - val_mean_absolute_error: 20271.0522\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 17934.64165\n",
      "Epoch 421/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27838.8261 - mean_absolute_error: 27838.8261 - val_loss: 19108.8465 - val_mean_absolute_error: 19108.8465\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 17934.64165\n",
      "Epoch 422/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 28304.6278 - mean_absolute_error: 28304.6278 - val_loss: 22988.2226 - val_mean_absolute_error: 22988.2226\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 17934.64165\n",
      "Epoch 423/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27966.2342 - mean_absolute_error: 27966.2342 - val_loss: 19773.6042 - val_mean_absolute_error: 19773.6042\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 17934.64165\n",
      "Epoch 424/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 28048.2813 - mean_absolute_error: 28048.2813 - val_loss: 20824.2573 - val_mean_absolute_error: 20824.2573\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 17934.64165\n",
      "Epoch 425/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 28518.1087 - mean_absolute_error: 28518.1087 - val_loss: 22687.2737 - val_mean_absolute_error: 22687.2737\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 17934.64165\n",
      "Epoch 426/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 28932.0710 - mean_absolute_error: 28932.0710 - val_loss: 23052.8029 - val_mean_absolute_error: 23052.8029\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 17934.64165\n",
      "Epoch 427/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 28149.9963 - mean_absolute_error: 28149.9963 - val_loss: 27069.0735 - val_mean_absolute_error: 27069.0735\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 17934.64165\n",
      "Epoch 428/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27598.8757 - mean_absolute_error: 27598.8757 - val_loss: 21146.7814 - val_mean_absolute_error: 21146.7814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00428: val_loss did not improve from 17934.64165\n",
      "Epoch 429/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27626.0548 - mean_absolute_error: 27626.0548 - val_loss: 18915.4989 - val_mean_absolute_error: 18915.4989\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 17934.64165\n",
      "Epoch 430/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 27546.2047 - mean_absolute_error: 27546.2047 - val_loss: 18348.2155 - val_mean_absolute_error: 18348.2155\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 17934.64165\n",
      "Epoch 431/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26900.2868 - mean_absolute_error: 26900.2868 - val_loss: 22618.1305 - val_mean_absolute_error: 22618.1305\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 17934.64165\n",
      "Epoch 432/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 27511.4176 - mean_absolute_error: 27511.4176 - val_loss: 23172.5630 - val_mean_absolute_error: 23172.5630\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 17934.64165\n",
      "Epoch 433/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27619.0383 - mean_absolute_error: 27619.0383 - val_loss: 26161.4237 - val_mean_absolute_error: 26161.4237\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 17934.64165\n",
      "Epoch 434/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26753.5134 - mean_absolute_error: 26753.5134 - val_loss: 26649.5682 - val_mean_absolute_error: 26649.5682\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 17934.64165\n",
      "Epoch 435/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26981.9291 - mean_absolute_error: 26981.9291 - val_loss: 21879.9579 - val_mean_absolute_error: 21879.9579\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 17934.64165\n",
      "Epoch 436/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 28142.6527 - mean_absolute_error: 28142.6527 - val_loss: 18763.6219 - val_mean_absolute_error: 18763.6219\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 17934.64165\n",
      "Epoch 437/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27032.8972 - mean_absolute_error: 27032.8972 - val_loss: 19007.1164 - val_mean_absolute_error: 19007.1164\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 17934.64165\n",
      "Epoch 438/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 28522.8782 - mean_absolute_error: 28522.8782 - val_loss: 23766.2989 - val_mean_absolute_error: 23766.2989\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 17934.64165\n",
      "Epoch 439/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27565.1558 - mean_absolute_error: 27565.1558 - val_loss: 19148.3818 - val_mean_absolute_error: 19148.3818\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 17934.64165\n",
      "Epoch 440/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27618.3840 - mean_absolute_error: 27618.3840 - val_loss: 21275.7118 - val_mean_absolute_error: 21275.7118\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 17934.64165\n",
      "Epoch 441/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26999.3210 - mean_absolute_error: 26999.3210 - val_loss: 24699.3503 - val_mean_absolute_error: 24699.3503\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 17934.64165\n",
      "Epoch 442/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 27618.6676 - mean_absolute_error: 27618.6676 - val_loss: 25668.9089 - val_mean_absolute_error: 25668.9089\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 17934.64165\n",
      "Epoch 443/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 29093.8345 - mean_absolute_error: 29093.8345 - val_loss: 20787.8180 - val_mean_absolute_error: 20787.8180\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 17934.64165\n",
      "Epoch 444/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27523.9507 - mean_absolute_error: 27523.9507 - val_loss: 18741.6435 - val_mean_absolute_error: 18741.6435\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 17934.64165\n",
      "Epoch 445/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 27654.6609 - mean_absolute_error: 27654.6609 - val_loss: 24617.8188 - val_mean_absolute_error: 24617.8188\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 17934.64165\n",
      "Epoch 446/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27694.7003 - mean_absolute_error: 27694.7003 - val_loss: 20005.2817 - val_mean_absolute_error: 20005.2817\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 17934.64165\n",
      "Epoch 447/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27216.2078 - mean_absolute_error: 27216.2078 - val_loss: 19527.1385 - val_mean_absolute_error: 19527.1385\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 17934.64165\n",
      "Epoch 448/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26904.3448 - mean_absolute_error: 26904.3448 - val_loss: 22953.0097 - val_mean_absolute_error: 22953.0097\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 17934.64165\n",
      "Epoch 449/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27471.4922 - mean_absolute_error: 27471.4922 - val_loss: 20136.9322 - val_mean_absolute_error: 20136.9322\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 17934.64165\n",
      "Epoch 450/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27887.4147 - mean_absolute_error: 27887.4147 - val_loss: 19432.5387 - val_mean_absolute_error: 19432.5387\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 17934.64165\n",
      "Epoch 451/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27378.6586 - mean_absolute_error: 27378.6586 - val_loss: 27880.8990 - val_mean_absolute_error: 27880.8990\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 17934.64165\n",
      "Epoch 452/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26323.1089 - mean_absolute_error: 26323.1089 - val_loss: 21882.4166 - val_mean_absolute_error: 21882.4166\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 17934.64165\n",
      "Epoch 453/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27352.0902 - mean_absolute_error: 27352.0902 - val_loss: 26145.4787 - val_mean_absolute_error: 26145.4787\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 17934.64165\n",
      "Epoch 454/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 27787.9238 - mean_absolute_error: 27787.9238 - val_loss: 25120.3496 - val_mean_absolute_error: 25120.3496\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 17934.64165\n",
      "Epoch 455/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 27927.1585 - mean_absolute_error: 27927.1585 - val_loss: 22924.7567 - val_mean_absolute_error: 22924.7567\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 17934.64165\n",
      "Epoch 456/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 26728.0789 - mean_absolute_error: 26728.0789 - val_loss: 19914.4208 - val_mean_absolute_error: 19914.4208\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 17934.64165\n",
      "Epoch 457/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27398.7382 - mean_absolute_error: 27398.7382 - val_loss: 17844.5893 - val_mean_absolute_error: 17844.5893\n",
      "\n",
      "Epoch 00457: val_loss improved from 17934.64165 to 17844.58928, saving model to Weights-457--17844.58928.hdf5\n",
      "Epoch 458/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 28133.8099 - mean_absolute_error: 28133.8099 - val_loss: 19526.7519 - val_mean_absolute_error: 19526.7519\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 17844.58928\n",
      "Epoch 459/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27190.0622 - mean_absolute_error: 27190.0622 - val_loss: 25797.4954 - val_mean_absolute_error: 25797.4954\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 17844.58928\n",
      "Epoch 460/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27969.3950 - mean_absolute_error: 27969.3950 - val_loss: 18422.3953 - val_mean_absolute_error: 18422.3953\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 17844.58928\n",
      "Epoch 461/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27375.8935 - mean_absolute_error: 27375.8935 - val_loss: 22593.7222 - val_mean_absolute_error: 22593.7222\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 17844.58928\n",
      "Epoch 462/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27359.1688 - mean_absolute_error: 27359.1688 - val_loss: 19100.7938 - val_mean_absolute_error: 19100.7938\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 17844.58928\n",
      "Epoch 463/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 28184.2246 - mean_absolute_error: 28184.2246 - val_loss: 19197.0152 - val_mean_absolute_error: 19197.0152\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 17844.58928\n",
      "Epoch 464/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 28571.8430 - mean_absolute_error: 28571.8430 - val_loss: 18784.1212 - val_mean_absolute_error: 18784.1212\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 17844.58928\n",
      "Epoch 465/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26549.6239 - mean_absolute_error: 26549.6239 - val_loss: 19540.1616 - val_mean_absolute_error: 19540.1616\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 17844.58928\n",
      "Epoch 466/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 27452.9750 - mean_absolute_error: 27452.9750 - val_loss: 19077.0703 - val_mean_absolute_error: 19077.0703\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 17844.58928\n",
      "Epoch 467/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27091.3206 - mean_absolute_error: 27091.3206 - val_loss: 17775.5727 - val_mean_absolute_error: 17775.5727\n",
      "\n",
      "Epoch 00467: val_loss improved from 17844.58928 to 17775.57270, saving model to Weights-467--17775.57270.hdf5\n",
      "Epoch 468/1500\n",
      "934/934 [==============================] - 0s 289us/step - loss: 27036.2178 - mean_absolute_error: 27036.2178 - val_loss: 19011.6937 - val_mean_absolute_error: 19011.6937\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 17775.57270\n",
      "Epoch 469/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 28499.1349 - mean_absolute_error: 28499.1349 - val_loss: 17773.8579 - val_mean_absolute_error: 17773.8579\n",
      "\n",
      "Epoch 00469: val_loss improved from 17775.57270 to 17773.85792, saving model to Weights-469--17773.85792.hdf5\n",
      "Epoch 470/1500\n",
      "934/934 [==============================] - 0s 304us/step - loss: 27432.4852 - mean_absolute_error: 27432.4852 - val_loss: 20266.2249 - val_mean_absolute_error: 20266.2249\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 17773.85792\n",
      "Epoch 471/1500\n",
      "934/934 [==============================] - 0s 308us/step - loss: 27726.3022 - mean_absolute_error: 27726.3022 - val_loss: 26184.3576 - val_mean_absolute_error: 26184.3576\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 17773.85792\n",
      "Epoch 472/1500\n",
      "934/934 [==============================] - 0s 310us/step - loss: 28565.8985 - mean_absolute_error: 28565.8985 - val_loss: 22947.3971 - val_mean_absolute_error: 22947.3971\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 17773.85792\n",
      "Epoch 473/1500\n",
      "934/934 [==============================] - 0s 315us/step - loss: 26805.4491 - mean_absolute_error: 26805.4491 - val_loss: 20866.4586 - val_mean_absolute_error: 20866.4586\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 17773.85792\n",
      "Epoch 474/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28029.1665 - mean_absolute_error: 28029.1665 - val_loss: 28246.3600 - val_mean_absolute_error: 28246.3600\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 17773.85792\n",
      "Epoch 475/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27873.3037 - mean_absolute_error: 27873.3037 - val_loss: 35336.7770 - val_mean_absolute_error: 35336.7770\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 17773.85792\n",
      "Epoch 476/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26763.5895 - mean_absolute_error: 26763.5895 - val_loss: 22884.4392 - val_mean_absolute_error: 22884.4392\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 17773.85792\n",
      "Epoch 477/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27801.5779 - mean_absolute_error: 27801.5779 - val_loss: 20366.6123 - val_mean_absolute_error: 20366.6123\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 17773.85792\n",
      "Epoch 478/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28119.7818 - mean_absolute_error: 28119.7818 - val_loss: 25693.3251 - val_mean_absolute_error: 25693.3251\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 17773.85792\n",
      "Epoch 479/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27540.2739 - mean_absolute_error: 27540.2739 - val_loss: 26512.8316 - val_mean_absolute_error: 26512.8316\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 17773.85792\n",
      "Epoch 480/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27471.9189 - mean_absolute_error: 27471.9189 - val_loss: 24235.1176 - val_mean_absolute_error: 24235.1176\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 17773.85792\n",
      "Epoch 481/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27430.0239 - mean_absolute_error: 27430.0239 - val_loss: 22926.0234 - val_mean_absolute_error: 22926.0234\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 17773.85792\n",
      "Epoch 482/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27544.6414 - mean_absolute_error: 27544.6414 - val_loss: 17811.9644 - val_mean_absolute_error: 17811.9644\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 17773.85792\n",
      "Epoch 483/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 29612.9415 - mean_absolute_error: 29612.9415 - val_loss: 29801.5225 - val_mean_absolute_error: 29801.5225\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 17773.85792\n",
      "Epoch 484/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26539.3245 - mean_absolute_error: 26539.3245 - val_loss: 25221.5597 - val_mean_absolute_error: 25221.5597\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 17773.85792\n",
      "Epoch 485/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 28062.7300 - mean_absolute_error: 28062.7300 - val_loss: 22788.9955 - val_mean_absolute_error: 22788.9955\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 17773.85792\n",
      "Epoch 486/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27243.3968 - mean_absolute_error: 27243.3968 - val_loss: 18575.4095 - val_mean_absolute_error: 18575.4095\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 17773.85792\n",
      "Epoch 487/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26968.7716 - mean_absolute_error: 26968.7716 - val_loss: 23247.2248 - val_mean_absolute_error: 23247.2248\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 17773.85792\n",
      "Epoch 488/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 28683.3786 - mean_absolute_error: 28683.3786 - val_loss: 18303.6480 - val_mean_absolute_error: 18303.6480\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 17773.85792\n",
      "Epoch 489/1500\n",
      "934/934 [==============================] - 0s 290us/step - loss: 27457.6697 - mean_absolute_error: 27457.6697 - val_loss: 18289.0751 - val_mean_absolute_error: 18289.0751\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 17773.85792\n",
      "Epoch 490/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 28751.4846 - mean_absolute_error: 28751.4846 - val_loss: 29458.6724 - val_mean_absolute_error: 29458.6724\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 17773.85792\n",
      "Epoch 491/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27061.8161 - mean_absolute_error: 27061.8161 - val_loss: 23113.5504 - val_mean_absolute_error: 23113.5504\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 17773.85792\n",
      "Epoch 492/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27446.4574 - mean_absolute_error: 27446.4574 - val_loss: 20333.7744 - val_mean_absolute_error: 20333.7744\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 17773.85792\n",
      "Epoch 493/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27864.0620 - mean_absolute_error: 27864.0620 - val_loss: 28287.4752 - val_mean_absolute_error: 28287.4752\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 17773.85792\n",
      "Epoch 494/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 28368.5153 - mean_absolute_error: 28368.5153 - val_loss: 18666.2090 - val_mean_absolute_error: 18666.2090\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 17773.85792\n",
      "Epoch 495/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 262us/step - loss: 27550.5730 - mean_absolute_error: 27550.5730 - val_loss: 26070.2082 - val_mean_absolute_error: 26070.2082\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 17773.85792\n",
      "Epoch 496/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26959.0961 - mean_absolute_error: 26959.0961 - val_loss: 22515.6927 - val_mean_absolute_error: 22515.6927\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 17773.85792\n",
      "Epoch 497/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26220.5020 - mean_absolute_error: 26220.5020 - val_loss: 22041.6160 - val_mean_absolute_error: 22041.6160\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 17773.85792\n",
      "Epoch 498/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 27437.5048 - mean_absolute_error: 27437.5048 - val_loss: 18107.1324 - val_mean_absolute_error: 18107.1324\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 17773.85792\n",
      "Epoch 499/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 28260.2824 - mean_absolute_error: 28260.2824 - val_loss: 17875.3913 - val_mean_absolute_error: 17875.3913\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 17773.85792\n",
      "Epoch 500/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26462.1934 - mean_absolute_error: 26462.1934 - val_loss: 27654.1808 - val_mean_absolute_error: 27654.1808\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 17773.85792\n",
      "Epoch 501/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27701.3611 - mean_absolute_error: 27701.3611 - val_loss: 18509.3160 - val_mean_absolute_error: 18509.3160\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 17773.85792\n",
      "Epoch 502/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27687.1861 - mean_absolute_error: 27687.1861 - val_loss: 22798.4472 - val_mean_absolute_error: 22798.4472\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 17773.85792\n",
      "Epoch 503/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 28069.1857 - mean_absolute_error: 28069.1857 - val_loss: 18536.0166 - val_mean_absolute_error: 18536.0166\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 17773.85792\n",
      "Epoch 504/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27433.5347 - mean_absolute_error: 27433.5347 - val_loss: 20557.3962 - val_mean_absolute_error: 20557.3962\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 17773.85792\n",
      "Epoch 505/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 28746.8608 - mean_absolute_error: 28746.8608 - val_loss: 20805.9035 - val_mean_absolute_error: 20805.9035\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 17773.85792\n",
      "Epoch 506/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27219.3354 - mean_absolute_error: 27219.3354 - val_loss: 17863.0678 - val_mean_absolute_error: 17863.0678\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 17773.85792\n",
      "Epoch 507/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 28411.9568 - mean_absolute_error: 28411.9568 - val_loss: 23616.7912 - val_mean_absolute_error: 23616.7912\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 17773.85792\n",
      "Epoch 508/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27452.3694 - mean_absolute_error: 27452.3694 - val_loss: 27181.9994 - val_mean_absolute_error: 27181.9994\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 17773.85792\n",
      "Epoch 509/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26501.6089 - mean_absolute_error: 26501.6089 - val_loss: 22214.2273 - val_mean_absolute_error: 22214.2273\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 17773.85792\n",
      "Epoch 510/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28555.5155 - mean_absolute_error: 28555.5155 - val_loss: 25816.0755 - val_mean_absolute_error: 25816.0755\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 17773.85792\n",
      "Epoch 511/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26918.3159 - mean_absolute_error: 26918.3159 - val_loss: 21740.1449 - val_mean_absolute_error: 21740.1449\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 17773.85792\n",
      "Epoch 512/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 28057.2855 - mean_absolute_error: 28057.2855 - val_loss: 18781.6465 - val_mean_absolute_error: 18781.6465\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 17773.85792\n",
      "Epoch 513/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27108.7900 - mean_absolute_error: 27108.7900 - val_loss: 18595.0433 - val_mean_absolute_error: 18595.0433\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 17773.85792\n",
      "Epoch 514/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26921.8600 - mean_absolute_error: 26921.8600 - val_loss: 26702.5942 - val_mean_absolute_error: 26702.5942\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 17773.85792\n",
      "Epoch 515/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 28874.1865 - mean_absolute_error: 28874.1865 - val_loss: 19553.5571 - val_mean_absolute_error: 19553.5571\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 17773.85792\n",
      "Epoch 516/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26761.6156 - mean_absolute_error: 26761.6156 - val_loss: 24289.2222 - val_mean_absolute_error: 24289.2222\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 17773.85792\n",
      "Epoch 517/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27400.5639 - mean_absolute_error: 27400.5639 - val_loss: 23967.4151 - val_mean_absolute_error: 23967.4151\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 17773.85792\n",
      "Epoch 518/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26080.5112 - mean_absolute_error: 26080.5112 - val_loss: 23570.1922 - val_mean_absolute_error: 23570.1922\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 17773.85792\n",
      "Epoch 519/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27863.6895 - mean_absolute_error: 27863.6895 - val_loss: 34274.3793 - val_mean_absolute_error: 34274.3793\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 17773.85792\n",
      "Epoch 520/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27468.3663 - mean_absolute_error: 27468.3663 - val_loss: 20411.4118 - val_mean_absolute_error: 20411.4118\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 17773.85792\n",
      "Epoch 521/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 27124.9693 - mean_absolute_error: 27124.9693 - val_loss: 18987.0444 - val_mean_absolute_error: 18987.0444\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 17773.85792\n",
      "Epoch 522/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27304.6192 - mean_absolute_error: 27304.6192 - val_loss: 17967.5770 - val_mean_absolute_error: 17967.5770\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 17773.85792\n",
      "Epoch 523/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27574.5258 - mean_absolute_error: 27574.5258 - val_loss: 18792.1604 - val_mean_absolute_error: 18792.1604\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 17773.85792\n",
      "Epoch 524/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 27655.6854 - mean_absolute_error: 27655.6854 - val_loss: 22027.2155 - val_mean_absolute_error: 22027.2155\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 17773.85792\n",
      "Epoch 525/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 28160.5674 - mean_absolute_error: 28160.5674 - val_loss: 20589.6190 - val_mean_absolute_error: 20589.6190\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 17773.85792\n",
      "Epoch 526/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 25948.0158 - mean_absolute_error: 25948.0158 - val_loss: 32539.7458 - val_mean_absolute_error: 32539.7458\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 17773.85792\n",
      "Epoch 527/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27647.7306 - mean_absolute_error: 27647.7306 - val_loss: 33266.4053 - val_mean_absolute_error: 33266.4053\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 17773.85792\n",
      "Epoch 528/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 27661.5851 - mean_absolute_error: 27661.5851 - val_loss: 19817.5999 - val_mean_absolute_error: 19817.5999\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 17773.85792\n",
      "Epoch 529/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 264us/step - loss: 25791.2353 - mean_absolute_error: 25791.2353 - val_loss: 25213.6736 - val_mean_absolute_error: 25213.6736\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 17773.85792\n",
      "Epoch 530/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 27523.5468 - mean_absolute_error: 27523.5468 - val_loss: 18371.7088 - val_mean_absolute_error: 18371.7088\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 17773.85792\n",
      "Epoch 531/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27519.9199 - mean_absolute_error: 27519.9199 - val_loss: 18338.6019 - val_mean_absolute_error: 18338.6019\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 17773.85792\n",
      "Epoch 532/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 26706.4896 - mean_absolute_error: 26706.4896 - val_loss: 26663.3794 - val_mean_absolute_error: 26663.3794\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 17773.85792\n",
      "Epoch 533/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27255.5932 - mean_absolute_error: 27255.5932 - val_loss: 18322.9976 - val_mean_absolute_error: 18322.9976\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 17773.85792\n",
      "Epoch 534/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26387.5619 - mean_absolute_error: 26387.5619 - val_loss: 25126.3953 - val_mean_absolute_error: 25126.3953\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 17773.85792\n",
      "Epoch 535/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 27411.4715 - mean_absolute_error: 27411.4715 - val_loss: 18958.5919 - val_mean_absolute_error: 18958.5919\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 17773.85792\n",
      "Epoch 536/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 27594.1830 - mean_absolute_error: 27594.1830 - val_loss: 24221.8996 - val_mean_absolute_error: 24221.8996\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 17773.85792\n",
      "Epoch 537/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 27609.9196 - mean_absolute_error: 27609.9196 - val_loss: 18370.1813 - val_mean_absolute_error: 18370.1813\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 17773.85792\n",
      "Epoch 538/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 28464.9150 - mean_absolute_error: 28464.9150 - val_loss: 22052.4342 - val_mean_absolute_error: 22052.4342\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 17773.85792\n",
      "Epoch 539/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 27013.9989 - mean_absolute_error: 27013.9989 - val_loss: 23097.4455 - val_mean_absolute_error: 23097.4455\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 17773.85792\n",
      "Epoch 540/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26028.0683 - mean_absolute_error: 26028.0683 - val_loss: 41048.0534 - val_mean_absolute_error: 41048.0534\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 17773.85792\n",
      "Epoch 541/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27940.1132 - mean_absolute_error: 27940.1132 - val_loss: 23723.3864 - val_mean_absolute_error: 23723.3864\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 17773.85792\n",
      "Epoch 542/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27533.9332 - mean_absolute_error: 27533.9332 - val_loss: 18639.5937 - val_mean_absolute_error: 18639.5937\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 17773.85792\n",
      "Epoch 543/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27720.2850 - mean_absolute_error: 27720.2850 - val_loss: 19773.5899 - val_mean_absolute_error: 19773.5899\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 17773.85792\n",
      "Epoch 544/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26875.2324 - mean_absolute_error: 26875.2324 - val_loss: 22467.8703 - val_mean_absolute_error: 22467.8703\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 17773.85792\n",
      "Epoch 545/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26428.7153 - mean_absolute_error: 26428.7153 - val_loss: 18655.7420 - val_mean_absolute_error: 18655.7420\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 17773.85792\n",
      "Epoch 546/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27125.4310 - mean_absolute_error: 27125.4310 - val_loss: 17380.5002 - val_mean_absolute_error: 17380.5002\n",
      "\n",
      "Epoch 00546: val_loss improved from 17773.85792 to 17380.50020, saving model to Weights-546--17380.50020.hdf5\n",
      "Epoch 547/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 26533.0948 - mean_absolute_error: 26533.0948 - val_loss: 19122.6106 - val_mean_absolute_error: 19122.6106\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 17380.50020\n",
      "Epoch 548/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25133.4814 - mean_absolute_error: 25133.4814 - val_loss: 19681.4204 - val_mean_absolute_error: 19681.4204\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 17380.50020\n",
      "Epoch 549/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27721.5364 - mean_absolute_error: 27721.5364 - val_loss: 19404.0458 - val_mean_absolute_error: 19404.0458\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 17380.50020\n",
      "Epoch 550/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27442.3329 - mean_absolute_error: 27442.3329 - val_loss: 19235.1350 - val_mean_absolute_error: 19235.1350\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 17380.50020\n",
      "Epoch 551/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26535.8110 - mean_absolute_error: 26535.8110 - val_loss: 28161.3170 - val_mean_absolute_error: 28161.3170\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 17380.50020\n",
      "Epoch 552/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27904.6484 - mean_absolute_error: 27904.6484 - val_loss: 17988.5601 - val_mean_absolute_error: 17988.5601\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 17380.50020\n",
      "Epoch 553/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 26722.7696 - mean_absolute_error: 26722.7696 - val_loss: 32321.8645 - val_mean_absolute_error: 32321.8645\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 17380.50020\n",
      "Epoch 554/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26734.3874 - mean_absolute_error: 26734.3874 - val_loss: 21106.5237 - val_mean_absolute_error: 21106.5237\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 17380.50020\n",
      "Epoch 555/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 27438.1048 - mean_absolute_error: 27438.1048 - val_loss: 19051.1528 - val_mean_absolute_error: 19051.1528\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 17380.50020\n",
      "Epoch 556/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27790.7049 - mean_absolute_error: 27790.7049 - val_loss: 25143.4930 - val_mean_absolute_error: 25143.4930\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 17380.50020\n",
      "Epoch 557/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 28226.9356 - mean_absolute_error: 28226.9356 - val_loss: 26234.0503 - val_mean_absolute_error: 26234.0503\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 17380.50020\n",
      "Epoch 558/1500\n",
      "934/934 [==============================] - 0s 289us/step - loss: 28458.3186 - mean_absolute_error: 28458.3186 - val_loss: 22355.8527 - val_mean_absolute_error: 22355.8527\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 17380.50020\n",
      "Epoch 559/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 27149.7893 - mean_absolute_error: 27149.7893 - val_loss: 19177.2542 - val_mean_absolute_error: 19177.2542\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 17380.50020\n",
      "Epoch 560/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 26836.7276 - mean_absolute_error: 26836.7276 - val_loss: 20231.2605 - val_mean_absolute_error: 20231.2605\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 17380.50020\n",
      "Epoch 561/1500\n",
      "934/934 [==============================] - 0s 314us/step - loss: 26624.0211 - mean_absolute_error: 26624.0211 - val_loss: 17669.0071 - val_mean_absolute_error: 17669.0071\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 17380.50020\n",
      "Epoch 562/1500\n",
      "934/934 [==============================] - 0s 340us/step - loss: 27931.3406 - mean_absolute_error: 27931.3406 - val_loss: 18563.7758 - val_mean_absolute_error: 18563.7758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00562: val_loss did not improve from 17380.50020\n",
      "Epoch 563/1500\n",
      "934/934 [==============================] - 0s 372us/step - loss: 27515.8167 - mean_absolute_error: 27515.8167 - val_loss: 28566.7052 - val_mean_absolute_error: 28566.7052\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 17380.50020\n",
      "Epoch 564/1500\n",
      "934/934 [==============================] - 0s 343us/step - loss: 27903.1457 - mean_absolute_error: 27903.1457 - val_loss: 18721.9301 - val_mean_absolute_error: 18721.9301\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 17380.50020\n",
      "Epoch 565/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 27400.3370 - mean_absolute_error: 27400.3370 - val_loss: 26478.9862 - val_mean_absolute_error: 26478.9862\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 17380.50020\n",
      "Epoch 566/1500\n",
      "934/934 [==============================] - 0s 299us/step - loss: 26148.2510 - mean_absolute_error: 26148.2510 - val_loss: 20478.3092 - val_mean_absolute_error: 20478.3092\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 17380.50020\n",
      "Epoch 567/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 27626.2281 - mean_absolute_error: 27626.2281 - val_loss: 19866.7047 - val_mean_absolute_error: 19866.7047\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 17380.50020\n",
      "Epoch 568/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 27679.2267 - mean_absolute_error: 27679.2267 - val_loss: 22548.4574 - val_mean_absolute_error: 22548.4574\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 17380.50020\n",
      "Epoch 569/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27628.5156 - mean_absolute_error: 27628.5156 - val_loss: 21345.3703 - val_mean_absolute_error: 21345.3703\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 17380.50020\n",
      "Epoch 570/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 26980.4117 - mean_absolute_error: 26980.4117 - val_loss: 27339.6135 - val_mean_absolute_error: 27339.6135\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 17380.50020\n",
      "Epoch 571/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 28257.4880 - mean_absolute_error: 28257.4880 - val_loss: 21728.5335 - val_mean_absolute_error: 21728.5335\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 17380.50020\n",
      "Epoch 572/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 29111.1408 - mean_absolute_error: 29111.1408 - val_loss: 22192.3965 - val_mean_absolute_error: 22192.3965\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 17380.50020\n",
      "Epoch 573/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 25996.4143 - mean_absolute_error: 25996.4143 - val_loss: 21877.5743 - val_mean_absolute_error: 21877.5743\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 17380.50020\n",
      "Epoch 574/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 27100.0942 - mean_absolute_error: 27100.0942 - val_loss: 20144.5890 - val_mean_absolute_error: 20144.5890\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 17380.50020\n",
      "Epoch 575/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24897.9594 - mean_absolute_error: 24897.9594 - val_loss: 21177.0125 - val_mean_absolute_error: 21177.0125\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 17380.50020\n",
      "Epoch 576/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26549.5309 - mean_absolute_error: 26549.5309 - val_loss: 18791.7627 - val_mean_absolute_error: 18791.7627\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 17380.50020\n",
      "Epoch 577/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26208.7412 - mean_absolute_error: 26208.7412 - val_loss: 27612.0563 - val_mean_absolute_error: 27612.0563\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 17380.50020\n",
      "Epoch 578/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26006.1292 - mean_absolute_error: 26006.1292 - val_loss: 25161.3588 - val_mean_absolute_error: 25161.3588\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 17380.50020\n",
      "Epoch 579/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26461.9726 - mean_absolute_error: 26461.9726 - val_loss: 23235.4178 - val_mean_absolute_error: 23235.4178\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 17380.50020\n",
      "Epoch 580/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25682.0617 - mean_absolute_error: 25682.0617 - val_loss: 19335.2135 - val_mean_absolute_error: 19335.2135\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 17380.50020\n",
      "Epoch 581/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27673.2750 - mean_absolute_error: 27673.2750 - val_loss: 18991.1180 - val_mean_absolute_error: 18991.1180\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 17380.50020\n",
      "Epoch 582/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27609.9429 - mean_absolute_error: 27609.9429 - val_loss: 28217.7830 - val_mean_absolute_error: 28217.7830\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 17380.50020\n",
      "Epoch 583/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26910.3269 - mean_absolute_error: 26910.3269 - val_loss: 20271.3529 - val_mean_absolute_error: 20271.3529\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 17380.50020\n",
      "Epoch 584/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 27309.9013 - mean_absolute_error: 27309.9013 - val_loss: 18102.7600 - val_mean_absolute_error: 18102.7600\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 17380.50020\n",
      "Epoch 585/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25746.9239 - mean_absolute_error: 25746.9239 - val_loss: 20149.6808 - val_mean_absolute_error: 20149.6808\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 17380.50020\n",
      "Epoch 586/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27031.0631 - mean_absolute_error: 27031.0631 - val_loss: 18236.6857 - val_mean_absolute_error: 18236.6857\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 17380.50020\n",
      "Epoch 587/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25670.4049 - mean_absolute_error: 25670.4049 - val_loss: 23347.9475 - val_mean_absolute_error: 23347.9475\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 17380.50020\n",
      "Epoch 588/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26324.4057 - mean_absolute_error: 26324.4057 - val_loss: 18624.6609 - val_mean_absolute_error: 18624.6609\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 17380.50020\n",
      "Epoch 589/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26701.0988 - mean_absolute_error: 26701.0988 - val_loss: 17119.0334 - val_mean_absolute_error: 17119.0334\n",
      "\n",
      "Epoch 00589: val_loss improved from 17380.50020 to 17119.03343, saving model to Weights-589--17119.03343.hdf5\n",
      "Epoch 590/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 27464.4482 - mean_absolute_error: 27464.4482 - val_loss: 24105.6658 - val_mean_absolute_error: 24105.6658\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 17119.03343\n",
      "Epoch 591/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 26758.0826 - mean_absolute_error: 26758.0826 - val_loss: 18360.4680 - val_mean_absolute_error: 18360.4680\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 17119.03343\n",
      "Epoch 592/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 26257.6564 - mean_absolute_error: 26257.6564 - val_loss: 19142.1944 - val_mean_absolute_error: 19142.1944\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 17119.03343\n",
      "Epoch 593/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 26586.4794 - mean_absolute_error: 26586.4794 - val_loss: 20984.6564 - val_mean_absolute_error: 20984.6564\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 17119.03343\n",
      "Epoch 594/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 26849.9851 - mean_absolute_error: 26849.9851 - val_loss: 18559.0934 - val_mean_absolute_error: 18559.0934\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 17119.03343\n",
      "Epoch 595/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27359.5475 - mean_absolute_error: 27359.5475 - val_loss: 27262.5410 - val_mean_absolute_error: 27262.5410\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 17119.03343\n",
      "Epoch 596/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26514.6555 - mean_absolute_error: 26514.6555 - val_loss: 21053.2367 - val_mean_absolute_error: 21053.2367\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 17119.03343\n",
      "Epoch 597/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 27774.9107 - mean_absolute_error: 27774.9107 - val_loss: 19039.2882 - val_mean_absolute_error: 19039.2882\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 17119.03343\n",
      "Epoch 598/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 28176.1213 - mean_absolute_error: 28176.1213 - val_loss: 17523.3041 - val_mean_absolute_error: 17523.3041\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 17119.03343\n",
      "Epoch 599/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27115.2680 - mean_absolute_error: 27115.2680 - val_loss: 26771.8842 - val_mean_absolute_error: 26771.8842\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 17119.03343\n",
      "Epoch 600/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 28396.8555 - mean_absolute_error: 28396.8555 - val_loss: 18139.7722 - val_mean_absolute_error: 18139.7722\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 17119.03343\n",
      "Epoch 601/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25933.9070 - mean_absolute_error: 25933.9070 - val_loss: 25939.7037 - val_mean_absolute_error: 25939.7037\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 17119.03343\n",
      "Epoch 602/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 26147.6315 - mean_absolute_error: 26147.6315 - val_loss: 20279.2322 - val_mean_absolute_error: 20279.2322\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 17119.03343\n",
      "Epoch 603/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27085.2408 - mean_absolute_error: 27085.2408 - val_loss: 19269.0632 - val_mean_absolute_error: 19269.0632\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 17119.03343\n",
      "Epoch 604/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 28156.6851 - mean_absolute_error: 28156.6851 - val_loss: 17414.9132 - val_mean_absolute_error: 17414.9132\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 17119.03343\n",
      "Epoch 605/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26343.0956 - mean_absolute_error: 26343.0956 - val_loss: 20612.8011 - val_mean_absolute_error: 20612.8011\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 17119.03343\n",
      "Epoch 606/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26181.3935 - mean_absolute_error: 26181.3935 - val_loss: 28043.7236 - val_mean_absolute_error: 28043.7236\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 17119.03343\n",
      "Epoch 607/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26693.0910 - mean_absolute_error: 26693.0910 - val_loss: 18754.8323 - val_mean_absolute_error: 18754.8323\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 17119.03343\n",
      "Epoch 608/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26730.4529 - mean_absolute_error: 26730.4529 - val_loss: 26813.1214 - val_mean_absolute_error: 26813.1214\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 17119.03343\n",
      "Epoch 609/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27247.0366 - mean_absolute_error: 27247.0366 - val_loss: 20704.7089 - val_mean_absolute_error: 20704.7089\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 17119.03343\n",
      "Epoch 610/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26164.0718 - mean_absolute_error: 26164.0718 - val_loss: 19183.3241 - val_mean_absolute_error: 19183.3241\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 17119.03343\n",
      "Epoch 611/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26481.7710 - mean_absolute_error: 26481.7710 - val_loss: 21789.8722 - val_mean_absolute_error: 21789.8722\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 17119.03343\n",
      "Epoch 612/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 27680.2604 - mean_absolute_error: 27680.2604 - val_loss: 21711.6021 - val_mean_absolute_error: 21711.6021\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 17119.03343\n",
      "Epoch 613/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26254.8549 - mean_absolute_error: 26254.8549 - val_loss: 20100.2719 - val_mean_absolute_error: 20100.2719\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 17119.03343\n",
      "Epoch 614/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25552.0635 - mean_absolute_error: 25552.0635 - val_loss: 23856.9934 - val_mean_absolute_error: 23856.9934\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 17119.03343\n",
      "Epoch 615/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25553.6426 - mean_absolute_error: 25553.6426 - val_loss: 18912.8773 - val_mean_absolute_error: 18912.8773\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 17119.03343\n",
      "Epoch 616/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 26321.4070 - mean_absolute_error: 26321.4070 - val_loss: 29168.2018 - val_mean_absolute_error: 29168.2018\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 17119.03343\n",
      "Epoch 617/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27316.8877 - mean_absolute_error: 27316.8877 - val_loss: 23404.7676 - val_mean_absolute_error: 23404.7676\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 17119.03343\n",
      "Epoch 618/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26327.4777 - mean_absolute_error: 26327.4777 - val_loss: 18666.7274 - val_mean_absolute_error: 18666.7274\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 17119.03343\n",
      "Epoch 619/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26551.3931 - mean_absolute_error: 26551.3931 - val_loss: 24553.9409 - val_mean_absolute_error: 24553.9409\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 17119.03343\n",
      "Epoch 620/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 27417.1923 - mean_absolute_error: 27417.1923 - val_loss: 20845.6673 - val_mean_absolute_error: 20845.6673\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 17119.03343\n",
      "Epoch 621/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26103.5707 - mean_absolute_error: 26103.5707 - val_loss: 26161.0866 - val_mean_absolute_error: 26161.0866\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 17119.03343\n",
      "Epoch 622/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26773.8780 - mean_absolute_error: 26773.8780 - val_loss: 17866.5195 - val_mean_absolute_error: 17866.5195\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 17119.03343\n",
      "Epoch 623/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27500.9541 - mean_absolute_error: 27500.9541 - val_loss: 24996.6797 - val_mean_absolute_error: 24996.6797\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 17119.03343\n",
      "Epoch 624/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27526.3236 - mean_absolute_error: 27526.3236 - val_loss: 24670.9622 - val_mean_absolute_error: 24670.9622\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 17119.03343\n",
      "Epoch 625/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27415.1364 - mean_absolute_error: 27415.1364 - val_loss: 32100.0475 - val_mean_absolute_error: 32100.0475\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 17119.03343\n",
      "Epoch 626/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26556.7075 - mean_absolute_error: 26556.7075 - val_loss: 17629.3120 - val_mean_absolute_error: 17629.3120\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 17119.03343\n",
      "Epoch 627/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27237.3788 - mean_absolute_error: 27237.3788 - val_loss: 21070.4220 - val_mean_absolute_error: 21070.4220\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 17119.03343\n",
      "Epoch 628/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26721.9351 - mean_absolute_error: 26721.9351 - val_loss: 21498.0577 - val_mean_absolute_error: 21498.0577\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 17119.03343\n",
      "Epoch 629/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27630.6822 - mean_absolute_error: 27630.6822 - val_loss: 33448.7597 - val_mean_absolute_error: 33448.7597\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 17119.03343\n",
      "Epoch 630/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 267us/step - loss: 26881.5519 - mean_absolute_error: 26881.5519 - val_loss: 28715.0330 - val_mean_absolute_error: 28715.0330\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 17119.03343\n",
      "Epoch 631/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 28613.2880 - mean_absolute_error: 28613.2880 - val_loss: 21835.9909 - val_mean_absolute_error: 21835.9909\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 17119.03343\n",
      "Epoch 632/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 26710.8285 - mean_absolute_error: 26710.8285 - val_loss: 25445.6635 - val_mean_absolute_error: 25445.6635\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 17119.03343\n",
      "Epoch 633/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26802.1859 - mean_absolute_error: 26802.1859 - val_loss: 17488.5876 - val_mean_absolute_error: 17488.5876\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 17119.03343\n",
      "Epoch 634/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27409.4524 - mean_absolute_error: 27409.4524 - val_loss: 18628.0586 - val_mean_absolute_error: 18628.0586\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 17119.03343\n",
      "Epoch 635/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26339.2360 - mean_absolute_error: 26339.2360 - val_loss: 24178.5846 - val_mean_absolute_error: 24178.5846\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 17119.03343\n",
      "Epoch 636/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 26219.6342 - mean_absolute_error: 26219.6342 - val_loss: 18200.8439 - val_mean_absolute_error: 18200.8439\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 17119.03343\n",
      "Epoch 637/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25268.1231 - mean_absolute_error: 25268.1231 - val_loss: 18034.5981 - val_mean_absolute_error: 18034.5981\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 17119.03343\n",
      "Epoch 638/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27101.6515 - mean_absolute_error: 27101.6515 - val_loss: 17861.2800 - val_mean_absolute_error: 17861.2800\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 17119.03343\n",
      "Epoch 639/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27200.1818 - mean_absolute_error: 27200.1818 - val_loss: 19684.6358 - val_mean_absolute_error: 19684.6358\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 17119.03343\n",
      "Epoch 640/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27311.8849 - mean_absolute_error: 27311.8849 - val_loss: 22627.3914 - val_mean_absolute_error: 22627.3914\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 17119.03343\n",
      "Epoch 641/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25556.0776 - mean_absolute_error: 25556.0776 - val_loss: 23886.2975 - val_mean_absolute_error: 23886.2975\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 17119.03343\n",
      "Epoch 642/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27581.2446 - mean_absolute_error: 27581.2446 - val_loss: 17320.6913 - val_mean_absolute_error: 17320.6913\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 17119.03343\n",
      "Epoch 643/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26861.0910 - mean_absolute_error: 26861.0910 - val_loss: 17936.2714 - val_mean_absolute_error: 17936.2714\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 17119.03343\n",
      "Epoch 644/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25554.3758 - mean_absolute_error: 25554.3758 - val_loss: 21629.0668 - val_mean_absolute_error: 21629.0668\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 17119.03343\n",
      "Epoch 645/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26193.3365 - mean_absolute_error: 26193.3365 - val_loss: 20556.6407 - val_mean_absolute_error: 20556.6407\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 17119.03343\n",
      "Epoch 646/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25856.3816 - mean_absolute_error: 25856.3816 - val_loss: 28660.7972 - val_mean_absolute_error: 28660.7972\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 17119.03343\n",
      "Epoch 647/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26477.0462 - mean_absolute_error: 26477.0462 - val_loss: 21134.5447 - val_mean_absolute_error: 21134.5447\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 17119.03343\n",
      "Epoch 648/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27701.2032 - mean_absolute_error: 27701.2032 - val_loss: 25971.1914 - val_mean_absolute_error: 25971.1914\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 17119.03343\n",
      "Epoch 649/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 26385.9644 - mean_absolute_error: 26385.9644 - val_loss: 30804.9399 - val_mean_absolute_error: 30804.9399\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 17119.03343\n",
      "Epoch 650/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 27984.6603 - mean_absolute_error: 27984.6603 - val_loss: 18070.4003 - val_mean_absolute_error: 18070.4003\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 17119.03343\n",
      "Epoch 651/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27985.6057 - mean_absolute_error: 27985.6057 - val_loss: 19163.8924 - val_mean_absolute_error: 19163.8924\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 17119.03343\n",
      "Epoch 652/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26648.4810 - mean_absolute_error: 26648.4810 - val_loss: 22803.7274 - val_mean_absolute_error: 22803.7274\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 17119.03343\n",
      "Epoch 653/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26065.3857 - mean_absolute_error: 26065.3857 - val_loss: 17968.6076 - val_mean_absolute_error: 17968.6076\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 17119.03343\n",
      "Epoch 654/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26893.9067 - mean_absolute_error: 26893.9067 - val_loss: 28577.9788 - val_mean_absolute_error: 28577.9788\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 17119.03343\n",
      "Epoch 655/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26707.3809 - mean_absolute_error: 26707.3809 - val_loss: 17185.0681 - val_mean_absolute_error: 17185.0681\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 17119.03343\n",
      "Epoch 656/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27546.8596 - mean_absolute_error: 27546.8596 - val_loss: 17137.8915 - val_mean_absolute_error: 17137.8915\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 17119.03343\n",
      "Epoch 657/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26817.0312 - mean_absolute_error: 26817.0312 - val_loss: 28048.9965 - val_mean_absolute_error: 28048.9965\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 17119.03343\n",
      "Epoch 658/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27423.9745 - mean_absolute_error: 27423.9745 - val_loss: 28306.0874 - val_mean_absolute_error: 28306.0874\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 17119.03343\n",
      "Epoch 659/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27211.2604 - mean_absolute_error: 27211.2604 - val_loss: 18668.6504 - val_mean_absolute_error: 18668.6504\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 17119.03343\n",
      "Epoch 660/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27799.4520 - mean_absolute_error: 27799.4520 - val_loss: 22227.0603 - val_mean_absolute_error: 22227.0603\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 17119.03343\n",
      "Epoch 661/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 27288.3869 - mean_absolute_error: 27288.3869 - val_loss: 30230.0234 - val_mean_absolute_error: 30230.0234\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 17119.03343\n",
      "Epoch 662/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 28583.3697 - mean_absolute_error: 28583.3697 - val_loss: 29150.4171 - val_mean_absolute_error: 29150.4171\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 17119.03343\n",
      "Epoch 663/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25960.6444 - mean_absolute_error: 25960.6444 - val_loss: 20579.1444 - val_mean_absolute_error: 20579.1444\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 17119.03343\n",
      "Epoch 664/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 255us/step - loss: 26150.8496 - mean_absolute_error: 26150.8496 - val_loss: 23369.5906 - val_mean_absolute_error: 23369.5906\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 17119.03343\n",
      "Epoch 665/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 27747.2407 - mean_absolute_error: 27747.2407 - val_loss: 19757.9397 - val_mean_absolute_error: 19757.9397\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 17119.03343\n",
      "Epoch 666/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25479.9245 - mean_absolute_error: 25479.9245 - val_loss: 25987.4074 - val_mean_absolute_error: 25987.4074\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 17119.03343\n",
      "Epoch 667/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 28525.5561 - mean_absolute_error: 28525.5561 - val_loss: 37904.9902 - val_mean_absolute_error: 37904.9902\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 17119.03343\n",
      "Epoch 668/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27926.8541 - mean_absolute_error: 27926.8541 - val_loss: 30203.9089 - val_mean_absolute_error: 30203.9089\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 17119.03343\n",
      "Epoch 669/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26879.5879 - mean_absolute_error: 26879.5879 - val_loss: 17658.2092 - val_mean_absolute_error: 17658.2092\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 17119.03343\n",
      "Epoch 670/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 27359.3285 - mean_absolute_error: 27359.3285 - val_loss: 18931.3897 - val_mean_absolute_error: 18931.3897\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 17119.03343\n",
      "Epoch 671/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26615.7440 - mean_absolute_error: 26615.7440 - val_loss: 24841.7972 - val_mean_absolute_error: 24841.7972\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 17119.03343\n",
      "Epoch 672/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25736.4313 - mean_absolute_error: 25736.4313 - val_loss: 27485.5344 - val_mean_absolute_error: 27485.5344\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 17119.03343\n",
      "Epoch 673/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27038.1364 - mean_absolute_error: 27038.1364 - val_loss: 27816.8374 - val_mean_absolute_error: 27816.8374\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 17119.03343\n",
      "Epoch 674/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25781.4186 - mean_absolute_error: 25781.4186 - val_loss: 21461.7275 - val_mean_absolute_error: 21461.7275\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 17119.03343\n",
      "Epoch 675/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27133.6790 - mean_absolute_error: 27133.6790 - val_loss: 17210.3424 - val_mean_absolute_error: 17210.3424\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 17119.03343\n",
      "Epoch 676/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25334.8684 - mean_absolute_error: 25334.8684 - val_loss: 17928.0122 - val_mean_absolute_error: 17928.0122\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 17119.03343\n",
      "Epoch 677/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25386.4380 - mean_absolute_error: 25386.4380 - val_loss: 19823.2547 - val_mean_absolute_error: 19823.2547\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 17119.03343\n",
      "Epoch 678/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26878.9047 - mean_absolute_error: 26878.9047 - val_loss: 17946.0534 - val_mean_absolute_error: 17946.0534\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 17119.03343\n",
      "Epoch 679/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26049.2737 - mean_absolute_error: 26049.2737 - val_loss: 27103.3023 - val_mean_absolute_error: 27103.3023\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 17119.03343\n",
      "Epoch 680/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25772.5369 - mean_absolute_error: 25772.5369 - val_loss: 34966.0793 - val_mean_absolute_error: 34966.0793\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 17119.03343\n",
      "Epoch 681/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 26914.2130 - mean_absolute_error: 26914.2130 - val_loss: 17706.6201 - val_mean_absolute_error: 17706.6201\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 17119.03343\n",
      "Epoch 682/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 28378.2272 - mean_absolute_error: 28378.2272 - val_loss: 16744.5417 - val_mean_absolute_error: 16744.5417\n",
      "\n",
      "Epoch 00682: val_loss improved from 17119.03343 to 16744.54169, saving model to Weights-682--16744.54169.hdf5\n",
      "Epoch 683/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 26877.7512 - mean_absolute_error: 26877.7512 - val_loss: 18443.7449 - val_mean_absolute_error: 18443.7449\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 16744.54169\n",
      "Epoch 684/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26037.4151 - mean_absolute_error: 26037.4151 - val_loss: 25454.6893 - val_mean_absolute_error: 25454.6893\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 16744.54169\n",
      "Epoch 685/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26778.5201 - mean_absolute_error: 26778.5201 - val_loss: 27611.3952 - val_mean_absolute_error: 27611.3952\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 16744.54169\n",
      "Epoch 686/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 27009.6729 - mean_absolute_error: 27009.6729 - val_loss: 19579.4569 - val_mean_absolute_error: 19579.4569\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 16744.54169\n",
      "Epoch 687/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 26124.7680 - mean_absolute_error: 26124.7680 - val_loss: 19522.9784 - val_mean_absolute_error: 19522.9784\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 16744.54169\n",
      "Epoch 688/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25730.7244 - mean_absolute_error: 25730.7244 - val_loss: 24480.7770 - val_mean_absolute_error: 24480.7770\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 16744.54169\n",
      "Epoch 689/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26900.1275 - mean_absolute_error: 26900.1275 - val_loss: 24281.6576 - val_mean_absolute_error: 24281.6576\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 16744.54169\n",
      "Epoch 690/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27421.3973 - mean_absolute_error: 27421.3973 - val_loss: 17918.8966 - val_mean_absolute_error: 17918.8966\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 16744.54169\n",
      "Epoch 691/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27515.4237 - mean_absolute_error: 27515.4237 - val_loss: 20374.7841 - val_mean_absolute_error: 20374.7841\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 16744.54169\n",
      "Epoch 692/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26731.5983 - mean_absolute_error: 26731.5983 - val_loss: 19891.5114 - val_mean_absolute_error: 19891.5114\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 16744.54169\n",
      "Epoch 693/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27295.7106 - mean_absolute_error: 27295.7106 - val_loss: 26276.4222 - val_mean_absolute_error: 26276.4222\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 16744.54169\n",
      "Epoch 694/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26996.8745 - mean_absolute_error: 26996.8745 - val_loss: 18670.0484 - val_mean_absolute_error: 18670.0484\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 16744.54169\n",
      "Epoch 695/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 28349.7841 - mean_absolute_error: 28349.7841 - val_loss: 25153.7450 - val_mean_absolute_error: 25153.7450\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 16744.54169\n",
      "Epoch 696/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25254.1933 - mean_absolute_error: 25254.1933 - val_loss: 33223.8390 - val_mean_absolute_error: 33223.8390\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 16744.54169\n",
      "Epoch 697/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25811.8580 - mean_absolute_error: 25811.8580 - val_loss: 17783.7118 - val_mean_absolute_error: 17783.7118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00697: val_loss did not improve from 16744.54169\n",
      "Epoch 698/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26349.4265 - mean_absolute_error: 26349.4265 - val_loss: 18516.3706 - val_mean_absolute_error: 18516.3706\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 16744.54169\n",
      "Epoch 699/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25411.4442 - mean_absolute_error: 25411.4442 - val_loss: 29514.1685 - val_mean_absolute_error: 29514.1685\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 16744.54169\n",
      "Epoch 700/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26441.6897 - mean_absolute_error: 26441.6897 - val_loss: 22627.0233 - val_mean_absolute_error: 22627.0233\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 16744.54169\n",
      "Epoch 701/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26999.1976 - mean_absolute_error: 26999.1976 - val_loss: 20025.8745 - val_mean_absolute_error: 20025.8745\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 16744.54169\n",
      "Epoch 702/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26715.2982 - mean_absolute_error: 26715.2982 - val_loss: 21085.1581 - val_mean_absolute_error: 21085.1581\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 16744.54169\n",
      "Epoch 703/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25691.5611 - mean_absolute_error: 25691.5611 - val_loss: 30409.3067 - val_mean_absolute_error: 30409.3067\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 16744.54169\n",
      "Epoch 704/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 26522.6424 - mean_absolute_error: 26522.6424 - val_loss: 27745.1993 - val_mean_absolute_error: 27745.1993\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 16744.54169\n",
      "Epoch 705/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27562.0957 - mean_absolute_error: 27562.0957 - val_loss: 23448.4342 - val_mean_absolute_error: 23448.4342\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 16744.54169\n",
      "Epoch 706/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26589.1986 - mean_absolute_error: 26589.1986 - val_loss: 17208.6099 - val_mean_absolute_error: 17208.6099\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 16744.54169\n",
      "Epoch 707/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26904.0144 - mean_absolute_error: 26904.0144 - val_loss: 20591.7118 - val_mean_absolute_error: 20591.7118\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 16744.54169\n",
      "Epoch 708/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26919.5504 - mean_absolute_error: 26919.5504 - val_loss: 19966.3794 - val_mean_absolute_error: 19966.3794\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 16744.54169\n",
      "Epoch 709/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26529.0676 - mean_absolute_error: 26529.0676 - val_loss: 20547.8389 - val_mean_absolute_error: 20547.8389\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 16744.54169\n",
      "Epoch 710/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26241.9183 - mean_absolute_error: 26241.9183 - val_loss: 21710.6194 - val_mean_absolute_error: 21710.6194\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 16744.54169\n",
      "Epoch 711/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26632.4291 - mean_absolute_error: 26632.4291 - val_loss: 19075.2969 - val_mean_absolute_error: 19075.2969\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 16744.54169\n",
      "Epoch 712/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 27929.6603 - mean_absolute_error: 27929.6603 - val_loss: 22226.8565 - val_mean_absolute_error: 22226.8565\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 16744.54169\n",
      "Epoch 713/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27575.7414 - mean_absolute_error: 27575.7414 - val_loss: 17043.3943 - val_mean_absolute_error: 17043.3943\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 16744.54169\n",
      "Epoch 714/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 25815.9507 - mean_absolute_error: 25815.9507 - val_loss: 26206.5920 - val_mean_absolute_error: 26206.5920\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 16744.54169\n",
      "Epoch 715/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27743.3045 - mean_absolute_error: 27743.3045 - val_loss: 35613.1716 - val_mean_absolute_error: 35613.1716\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 16744.54169\n",
      "Epoch 716/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26068.3687 - mean_absolute_error: 26068.3687 - val_loss: 17925.5306 - val_mean_absolute_error: 17925.5306\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 16744.54169\n",
      "Epoch 717/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27744.8612 - mean_absolute_error: 27744.8612 - val_loss: 27980.4653 - val_mean_absolute_error: 27980.4653\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 16744.54169\n",
      "Epoch 718/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 24591.2563 - mean_absolute_error: 24591.2563 - val_loss: 18756.9457 - val_mean_absolute_error: 18756.9457\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 16744.54169\n",
      "Epoch 719/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25973.7008 - mean_absolute_error: 25973.7008 - val_loss: 33655.2983 - val_mean_absolute_error: 33655.2983\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 16744.54169\n",
      "Epoch 720/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26712.5134 - mean_absolute_error: 26712.5134 - val_loss: 19113.9116 - val_mean_absolute_error: 19113.9116\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 16744.54169\n",
      "Epoch 721/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27184.3576 - mean_absolute_error: 27184.3576 - val_loss: 25593.5567 - val_mean_absolute_error: 25593.5567\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 16744.54169\n",
      "Epoch 722/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27214.3191 - mean_absolute_error: 27214.3191 - val_loss: 22982.1843 - val_mean_absolute_error: 22982.1843\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 16744.54169\n",
      "Epoch 723/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 25555.5011 - mean_absolute_error: 25555.5011 - val_loss: 26000.7210 - val_mean_absolute_error: 26000.7210\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 16744.54169\n",
      "Epoch 724/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26832.6392 - mean_absolute_error: 26832.6392 - val_loss: 25111.9523 - val_mean_absolute_error: 25111.9523\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 16744.54169\n",
      "Epoch 725/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26734.3114 - mean_absolute_error: 26734.3114 - val_loss: 20101.4242 - val_mean_absolute_error: 20101.4242\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 16744.54169\n",
      "Epoch 726/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27906.7697 - mean_absolute_error: 27906.7697 - val_loss: 23683.8835 - val_mean_absolute_error: 23683.8835\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 16744.54169\n",
      "Epoch 727/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26178.8126 - mean_absolute_error: 26178.8126 - val_loss: 26731.4861 - val_mean_absolute_error: 26731.4861\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 16744.54169\n",
      "Epoch 728/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26498.7425 - mean_absolute_error: 26498.7425 - val_loss: 18382.2781 - val_mean_absolute_error: 18382.2781\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 16744.54169\n",
      "Epoch 729/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26457.2977 - mean_absolute_error: 26457.2977 - val_loss: 17990.1448 - val_mean_absolute_error: 17990.1448\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 16744.54169\n",
      "Epoch 730/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25593.7454 - mean_absolute_error: 25593.7454 - val_loss: 25615.4950 - val_mean_absolute_error: 25615.4950\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 16744.54169\n",
      "Epoch 731/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27911.4104 - mean_absolute_error: 27911.4104 - val_loss: 28569.5846 - val_mean_absolute_error: 28569.5846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00731: val_loss did not improve from 16744.54169\n",
      "Epoch 732/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 27656.4770 - mean_absolute_error: 27656.4770 - val_loss: 17801.5712 - val_mean_absolute_error: 17801.5712\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 16744.54169\n",
      "Epoch 733/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 25853.6902 - mean_absolute_error: 25853.6902 - val_loss: 32233.8033 - val_mean_absolute_error: 32233.8033\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 16744.54169\n",
      "Epoch 734/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 27468.3946 - mean_absolute_error: 27468.3946 - val_loss: 35706.7831 - val_mean_absolute_error: 35706.7831\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 16744.54169\n",
      "Epoch 735/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27185.7034 - mean_absolute_error: 27185.7034 - val_loss: 23387.7254 - val_mean_absolute_error: 23387.7254\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 16744.54169\n",
      "Epoch 736/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26116.1531 - mean_absolute_error: 26116.1531 - val_loss: 17705.4772 - val_mean_absolute_error: 17705.4772\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 16744.54169\n",
      "Epoch 737/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26903.1453 - mean_absolute_error: 26903.1453 - val_loss: 22113.7687 - val_mean_absolute_error: 22113.7687\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 16744.54169\n",
      "Epoch 738/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 28381.1096 - mean_absolute_error: 28381.1096 - val_loss: 26035.6961 - val_mean_absolute_error: 26035.6961\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 16744.54169\n",
      "Epoch 739/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26614.1198 - mean_absolute_error: 26614.1198 - val_loss: 21540.8770 - val_mean_absolute_error: 21540.8770\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 16744.54169\n",
      "Epoch 740/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26577.8056 - mean_absolute_error: 26577.8056 - val_loss: 17436.3088 - val_mean_absolute_error: 17436.3088\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 16744.54169\n",
      "Epoch 741/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25900.1841 - mean_absolute_error: 25900.1841 - val_loss: 30556.4755 - val_mean_absolute_error: 30556.4755\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 16744.54169\n",
      "Epoch 742/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27155.2034 - mean_absolute_error: 27155.2034 - val_loss: 30409.0925 - val_mean_absolute_error: 30409.0925\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 16744.54169\n",
      "Epoch 743/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26120.2659 - mean_absolute_error: 26120.2659 - val_loss: 20178.0362 - val_mean_absolute_error: 20178.0362\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 16744.54169\n",
      "Epoch 744/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26071.3616 - mean_absolute_error: 26071.3616 - val_loss: 20741.2701 - val_mean_absolute_error: 20741.2701\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 16744.54169\n",
      "Epoch 745/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 28010.5774 - mean_absolute_error: 28010.5774 - val_loss: 21954.4345 - val_mean_absolute_error: 21954.4345\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 16744.54169\n",
      "Epoch 746/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 26637.9543 - mean_absolute_error: 26637.9543 - val_loss: 18966.4876 - val_mean_absolute_error: 18966.4876\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 16744.54169\n",
      "Epoch 747/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25619.7374 - mean_absolute_error: 25619.7374 - val_loss: 18892.2070 - val_mean_absolute_error: 18892.2070\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 16744.54169\n",
      "Epoch 748/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25799.8913 - mean_absolute_error: 25799.8913 - val_loss: 23272.7011 - val_mean_absolute_error: 23272.7011\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 16744.54169\n",
      "Epoch 749/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27108.5921 - mean_absolute_error: 27108.5921 - val_loss: 21074.6758 - val_mean_absolute_error: 21074.6758\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 16744.54169\n",
      "Epoch 750/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25659.3156 - mean_absolute_error: 25659.3156 - val_loss: 18513.3772 - val_mean_absolute_error: 18513.3772\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 16744.54169\n",
      "Epoch 751/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26586.6469 - mean_absolute_error: 26586.6469 - val_loss: 21646.4693 - val_mean_absolute_error: 21646.4693\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 16744.54169\n",
      "Epoch 752/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26762.1690 - mean_absolute_error: 26762.1690 - val_loss: 17032.0685 - val_mean_absolute_error: 17032.0685\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 16744.54169\n",
      "Epoch 753/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25475.2310 - mean_absolute_error: 25475.2310 - val_loss: 18730.8787 - val_mean_absolute_error: 18730.8787\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 16744.54169\n",
      "Epoch 754/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25978.7355 - mean_absolute_error: 25978.7355 - val_loss: 19163.4589 - val_mean_absolute_error: 19163.4589\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 16744.54169\n",
      "Epoch 755/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27462.4109 - mean_absolute_error: 27462.4109 - val_loss: 22039.6273 - val_mean_absolute_error: 22039.6273\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 16744.54169\n",
      "Epoch 756/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25845.9171 - mean_absolute_error: 25845.9171 - val_loss: 20242.1213 - val_mean_absolute_error: 20242.1213\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 16744.54169\n",
      "Epoch 757/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26147.1532 - mean_absolute_error: 26147.1532 - val_loss: 20894.9023 - val_mean_absolute_error: 20894.9023\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 16744.54169\n",
      "Epoch 758/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26528.0567 - mean_absolute_error: 26528.0567 - val_loss: 29189.7502 - val_mean_absolute_error: 29189.7502\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 16744.54169\n",
      "Epoch 759/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27128.9152 - mean_absolute_error: 27128.9152 - val_loss: 25680.5141 - val_mean_absolute_error: 25680.5141\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 16744.54169\n",
      "Epoch 760/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25976.5118 - mean_absolute_error: 25976.5118 - val_loss: 26862.0436 - val_mean_absolute_error: 26862.0436\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 16744.54169\n",
      "Epoch 761/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25621.9325 - mean_absolute_error: 25621.9325 - val_loss: 23764.3904 - val_mean_absolute_error: 23764.3904\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 16744.54169\n",
      "Epoch 762/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26099.3837 - mean_absolute_error: 26099.3837 - val_loss: 20545.1817 - val_mean_absolute_error: 20545.1817\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 16744.54169\n",
      "Epoch 763/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 26370.7074 - mean_absolute_error: 26370.7074 - val_loss: 23419.4552 - val_mean_absolute_error: 23419.4552\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 16744.54169\n",
      "Epoch 764/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26452.3835 - mean_absolute_error: 26452.3835 - val_loss: 18371.4056 - val_mean_absolute_error: 18371.4056\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 16744.54169\n",
      "Epoch 765/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26125.6349 - mean_absolute_error: 26125.6349 - val_loss: 18938.8234 - val_mean_absolute_error: 18938.8234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00765: val_loss did not improve from 16744.54169\n",
      "Epoch 766/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 24463.1903 - mean_absolute_error: 24463.1903 - val_loss: 20264.2527 - val_mean_absolute_error: 20264.2527\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 16744.54169\n",
      "Epoch 767/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25971.1525 - mean_absolute_error: 25971.1525 - val_loss: 19968.2842 - val_mean_absolute_error: 19968.2842\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 16744.54169\n",
      "Epoch 768/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26713.8800 - mean_absolute_error: 26713.8800 - val_loss: 21390.3062 - val_mean_absolute_error: 21390.3062\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 16744.54169\n",
      "Epoch 769/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26288.6522 - mean_absolute_error: 26288.6522 - val_loss: 18086.0131 - val_mean_absolute_error: 18086.0131\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 16744.54169\n",
      "Epoch 770/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26516.8100 - mean_absolute_error: 26516.8100 - val_loss: 29504.7280 - val_mean_absolute_error: 29504.7280\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 16744.54169\n",
      "Epoch 771/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26297.1783 - mean_absolute_error: 26297.1783 - val_loss: 17458.4746 - val_mean_absolute_error: 17458.4746\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 16744.54169\n",
      "Epoch 772/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27385.3908 - mean_absolute_error: 27385.3908 - val_loss: 20758.3839 - val_mean_absolute_error: 20758.3839\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 16744.54169\n",
      "Epoch 773/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26994.4247 - mean_absolute_error: 26994.4247 - val_loss: 21155.7198 - val_mean_absolute_error: 21155.7198\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 16744.54169\n",
      "Epoch 774/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25532.1132 - mean_absolute_error: 25532.1132 - val_loss: 17119.6044 - val_mean_absolute_error: 17119.6044\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 16744.54169\n",
      "Epoch 775/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26438.1530 - mean_absolute_error: 26438.1530 - val_loss: 22415.4740 - val_mean_absolute_error: 22415.4740\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 16744.54169\n",
      "Epoch 776/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26542.2972 - mean_absolute_error: 26542.2972 - val_loss: 36191.9687 - val_mean_absolute_error: 36191.9687\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 16744.54169\n",
      "Epoch 777/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26039.5857 - mean_absolute_error: 26039.5857 - val_loss: 26548.2129 - val_mean_absolute_error: 26548.2129\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 16744.54169\n",
      "Epoch 778/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25419.1185 - mean_absolute_error: 25419.1185 - val_loss: 26997.3445 - val_mean_absolute_error: 26997.3445\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 16744.54169\n",
      "Epoch 779/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25927.8858 - mean_absolute_error: 25927.8858 - val_loss: 22901.8922 - val_mean_absolute_error: 22901.8922\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 16744.54169\n",
      "Epoch 780/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26262.8744 - mean_absolute_error: 26262.8744 - val_loss: 19184.8698 - val_mean_absolute_error: 19184.8698\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 16744.54169\n",
      "Epoch 781/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 27113.4608 - mean_absolute_error: 27113.4608 - val_loss: 26234.5104 - val_mean_absolute_error: 26234.5104\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 16744.54169\n",
      "Epoch 782/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27382.6464 - mean_absolute_error: 27382.6464 - val_loss: 19800.0516 - val_mean_absolute_error: 19800.0516\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 16744.54169\n",
      "Epoch 783/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 24872.4661 - mean_absolute_error: 24872.4661 - val_loss: 18662.2288 - val_mean_absolute_error: 18662.2288\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 16744.54169\n",
      "Epoch 784/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26164.3514 - mean_absolute_error: 26164.3514 - val_loss: 17961.8535 - val_mean_absolute_error: 17961.8535\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 16744.54169\n",
      "Epoch 785/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26984.6980 - mean_absolute_error: 26984.6980 - val_loss: 32655.3153 - val_mean_absolute_error: 32655.3153\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 16744.54169\n",
      "Epoch 786/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25830.8478 - mean_absolute_error: 25830.8478 - val_loss: 26982.8667 - val_mean_absolute_error: 26982.8667\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 16744.54169\n",
      "Epoch 787/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26812.5690 - mean_absolute_error: 26812.5690 - val_loss: 24460.4524 - val_mean_absolute_error: 24460.4524\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 16744.54169\n",
      "Epoch 788/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26996.5816 - mean_absolute_error: 26996.5816 - val_loss: 32832.4877 - val_mean_absolute_error: 32832.4877\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 16744.54169\n",
      "Epoch 789/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26005.2738 - mean_absolute_error: 26005.2738 - val_loss: 19727.0800 - val_mean_absolute_error: 19727.0800\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 16744.54169\n",
      "Epoch 790/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26974.2122 - mean_absolute_error: 26974.2122 - val_loss: 24952.6316 - val_mean_absolute_error: 24952.6316\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 16744.54169\n",
      "Epoch 791/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25780.1302 - mean_absolute_error: 25780.1302 - val_loss: 22762.4625 - val_mean_absolute_error: 22762.4625\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 16744.54169\n",
      "Epoch 792/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26049.2059 - mean_absolute_error: 26049.2059 - val_loss: 23503.1154 - val_mean_absolute_error: 23503.1154\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 16744.54169\n",
      "Epoch 793/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25879.6932 - mean_absolute_error: 25879.6932 - val_loss: 18651.8287 - val_mean_absolute_error: 18651.8287\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 16744.54169\n",
      "Epoch 794/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26312.4783 - mean_absolute_error: 26312.4783 - val_loss: 22823.9823 - val_mean_absolute_error: 22823.9823\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 16744.54169\n",
      "Epoch 795/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26637.7341 - mean_absolute_error: 26637.7341 - val_loss: 25716.6811 - val_mean_absolute_error: 25716.6811\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 16744.54169\n",
      "Epoch 796/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 27219.3295 - mean_absolute_error: 27219.3295 - val_loss: 26468.4948 - val_mean_absolute_error: 26468.4948\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 16744.54169\n",
      "Epoch 797/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 27930.2876 - mean_absolute_error: 27930.2876 - val_loss: 21919.3871 - val_mean_absolute_error: 21919.3871\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 16744.54169\n",
      "Epoch 798/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27611.1890 - mean_absolute_error: 27611.1890 - val_loss: 17874.6346 - val_mean_absolute_error: 17874.6346\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 16744.54169\n",
      "Epoch 799/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25696.7244 - mean_absolute_error: 25696.7244 - val_loss: 20464.2376 - val_mean_absolute_error: 20464.2376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00799: val_loss did not improve from 16744.54169\n",
      "Epoch 800/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26437.3602 - mean_absolute_error: 26437.3602 - val_loss: 28125.0927 - val_mean_absolute_error: 28125.0927\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 16744.54169\n",
      "Epoch 801/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25593.7857 - mean_absolute_error: 25593.7857 - val_loss: 24379.6477 - val_mean_absolute_error: 24379.6477\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 16744.54169\n",
      "Epoch 802/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 26949.4611 - mean_absolute_error: 26949.4611 - val_loss: 30655.0604 - val_mean_absolute_error: 30655.0604\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 16744.54169\n",
      "Epoch 803/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25468.2595 - mean_absolute_error: 25468.2595 - val_loss: 26392.0503 - val_mean_absolute_error: 26392.0503\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 16744.54169\n",
      "Epoch 804/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26701.1509 - mean_absolute_error: 26701.1509 - val_loss: 23450.8967 - val_mean_absolute_error: 23450.8967\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 16744.54169\n",
      "Epoch 805/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25636.3883 - mean_absolute_error: 25636.3883 - val_loss: 21708.3745 - val_mean_absolute_error: 21708.3745\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 16744.54169\n",
      "Epoch 806/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25622.8560 - mean_absolute_error: 25622.8560 - val_loss: 29718.4650 - val_mean_absolute_error: 29718.4650\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 16744.54169\n",
      "Epoch 807/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25897.0161 - mean_absolute_error: 25897.0161 - val_loss: 22634.3127 - val_mean_absolute_error: 22634.3127\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 16744.54169\n",
      "Epoch 808/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27501.2891 - mean_absolute_error: 27501.2891 - val_loss: 21157.3554 - val_mean_absolute_error: 21157.3554\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 16744.54169\n",
      "Epoch 809/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25992.8092 - mean_absolute_error: 25992.8092 - val_loss: 20234.0013 - val_mean_absolute_error: 20234.0013\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 16744.54169\n",
      "Epoch 810/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26669.3079 - mean_absolute_error: 26669.3079 - val_loss: 19905.0791 - val_mean_absolute_error: 19905.0791\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 16744.54169\n",
      "Epoch 811/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26264.1991 - mean_absolute_error: 26264.1991 - val_loss: 18864.3589 - val_mean_absolute_error: 18864.3589\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 16744.54169\n",
      "Epoch 812/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 27249.9854 - mean_absolute_error: 27249.9854 - val_loss: 19146.4199 - val_mean_absolute_error: 19146.4199\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 16744.54169\n",
      "Epoch 813/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 27576.1258 - mean_absolute_error: 27576.1258 - val_loss: 21121.5139 - val_mean_absolute_error: 21121.5139\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 16744.54169\n",
      "Epoch 814/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25836.0983 - mean_absolute_error: 25836.0983 - val_loss: 17638.6304 - val_mean_absolute_error: 17638.6304\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 16744.54169\n",
      "Epoch 815/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25512.7390 - mean_absolute_error: 25512.7390 - val_loss: 20206.3428 - val_mean_absolute_error: 20206.3428\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 16744.54169\n",
      "Epoch 816/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25954.8732 - mean_absolute_error: 25954.8732 - val_loss: 17584.2922 - val_mean_absolute_error: 17584.2922\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 16744.54169\n",
      "Epoch 817/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27096.3797 - mean_absolute_error: 27096.3797 - val_loss: 43374.2799 - val_mean_absolute_error: 43374.2799\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 16744.54169\n",
      "Epoch 818/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27598.9901 - mean_absolute_error: 27598.9901 - val_loss: 23993.7450 - val_mean_absolute_error: 23993.7450\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 16744.54169\n",
      "Epoch 819/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25765.7372 - mean_absolute_error: 25765.7372 - val_loss: 21070.8734 - val_mean_absolute_error: 21070.8734\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 16744.54169\n",
      "Epoch 820/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26185.1868 - mean_absolute_error: 26185.1868 - val_loss: 20098.6233 - val_mean_absolute_error: 20098.6233\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 16744.54169\n",
      "Epoch 821/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27165.9161 - mean_absolute_error: 27165.9161 - val_loss: 27932.9567 - val_mean_absolute_error: 27932.9567\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 16744.54169\n",
      "Epoch 822/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25891.5403 - mean_absolute_error: 25891.5403 - val_loss: 17841.8201 - val_mean_absolute_error: 17841.8201\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 16744.54169\n",
      "Epoch 823/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26115.6530 - mean_absolute_error: 26115.6530 - val_loss: 24371.5788 - val_mean_absolute_error: 24371.5788\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 16744.54169\n",
      "Epoch 824/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26483.9865 - mean_absolute_error: 26483.9865 - val_loss: 17967.0644 - val_mean_absolute_error: 17967.0644\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 16744.54169\n",
      "Epoch 825/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26476.7651 - mean_absolute_error: 26476.7651 - val_loss: 25115.1233 - val_mean_absolute_error: 25115.1233\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 16744.54169\n",
      "Epoch 826/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26347.0097 - mean_absolute_error: 26347.0097 - val_loss: 18472.4187 - val_mean_absolute_error: 18472.4187\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 16744.54169\n",
      "Epoch 827/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26545.7682 - mean_absolute_error: 26545.7682 - val_loss: 26699.3572 - val_mean_absolute_error: 26699.3572\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 16744.54169\n",
      "Epoch 828/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25952.2963 - mean_absolute_error: 25952.2963 - val_loss: 18087.4114 - val_mean_absolute_error: 18087.4114\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 16744.54169\n",
      "Epoch 829/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 25314.2249 - mean_absolute_error: 25314.2249 - val_loss: 18813.1289 - val_mean_absolute_error: 18813.1289\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 16744.54169\n",
      "Epoch 830/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 27233.4757 - mean_absolute_error: 27233.4757 - val_loss: 28468.3859 - val_mean_absolute_error: 28468.3859\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 16744.54169\n",
      "Epoch 831/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25388.8919 - mean_absolute_error: 25388.8919 - val_loss: 25959.1114 - val_mean_absolute_error: 25959.1114\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 16744.54169\n",
      "Epoch 832/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 26609.3157 - mean_absolute_error: 26609.3157 - val_loss: 36702.5828 - val_mean_absolute_error: 36702.5828\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 16744.54169\n",
      "Epoch 833/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 25881.2675 - mean_absolute_error: 25881.2675 - val_loss: 20342.3022 - val_mean_absolute_error: 20342.3022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00833: val_loss did not improve from 16744.54169\n",
      "Epoch 834/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25954.9401 - mean_absolute_error: 25954.9401 - val_loss: 23394.4062 - val_mean_absolute_error: 23394.4062\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 16744.54169\n",
      "Epoch 835/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26647.7543 - mean_absolute_error: 26647.7543 - val_loss: 36313.7335 - val_mean_absolute_error: 36313.7335\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 16744.54169\n",
      "Epoch 836/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26426.9596 - mean_absolute_error: 26426.9596 - val_loss: 26954.0544 - val_mean_absolute_error: 26954.0544\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 16744.54169\n",
      "Epoch 837/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 26729.4631 - mean_absolute_error: 26729.4631 - val_loss: 20875.4545 - val_mean_absolute_error: 20875.4545\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 16744.54169\n",
      "Epoch 838/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26054.5817 - mean_absolute_error: 26054.5817 - val_loss: 21441.1956 - val_mean_absolute_error: 21441.1956\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 16744.54169\n",
      "Epoch 839/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25330.2962 - mean_absolute_error: 25330.2962 - val_loss: 29957.6625 - val_mean_absolute_error: 29957.6625\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 16744.54169\n",
      "Epoch 840/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26918.3804 - mean_absolute_error: 26918.3804 - val_loss: 24202.1680 - val_mean_absolute_error: 24202.1680\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 16744.54169\n",
      "Epoch 841/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25194.1586 - mean_absolute_error: 25194.1586 - val_loss: 19400.8188 - val_mean_absolute_error: 19400.8188\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 16744.54169\n",
      "Epoch 842/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25633.4146 - mean_absolute_error: 25633.4146 - val_loss: 18444.7869 - val_mean_absolute_error: 18444.7869\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 16744.54169\n",
      "Epoch 843/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25216.5243 - mean_absolute_error: 25216.5243 - val_loss: 26355.8306 - val_mean_absolute_error: 26355.8306\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 16744.54169\n",
      "Epoch 844/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27176.2292 - mean_absolute_error: 27176.2292 - val_loss: 18370.5895 - val_mean_absolute_error: 18370.5895\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 16744.54169\n",
      "Epoch 845/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 26708.6726 - mean_absolute_error: 26708.6726 - val_loss: 22523.8755 - val_mean_absolute_error: 22523.8755\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 16744.54169\n",
      "Epoch 846/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26292.0669 - mean_absolute_error: 26292.0669 - val_loss: 30940.7582 - val_mean_absolute_error: 30940.7582\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 16744.54169\n",
      "Epoch 847/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25494.2688 - mean_absolute_error: 25494.2688 - val_loss: 20303.5781 - val_mean_absolute_error: 20303.5781\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 16744.54169\n",
      "Epoch 848/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26116.8872 - mean_absolute_error: 26116.8872 - val_loss: 22525.9456 - val_mean_absolute_error: 22525.9456\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 16744.54169\n",
      "Epoch 849/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26126.6911 - mean_absolute_error: 26126.6911 - val_loss: 18941.2233 - val_mean_absolute_error: 18941.2233\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 16744.54169\n",
      "Epoch 850/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26235.8928 - mean_absolute_error: 26235.8928 - val_loss: 22788.9138 - val_mean_absolute_error: 22788.9138\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 16744.54169\n",
      "Epoch 851/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25381.0117 - mean_absolute_error: 25381.0117 - val_loss: 17788.4931 - val_mean_absolute_error: 17788.4931\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 16744.54169\n",
      "Epoch 852/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25432.8596 - mean_absolute_error: 25432.8596 - val_loss: 24351.3925 - val_mean_absolute_error: 24351.3925\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 16744.54169\n",
      "Epoch 853/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26472.5099 - mean_absolute_error: 26472.5099 - val_loss: 18282.6688 - val_mean_absolute_error: 18282.6688\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 16744.54169\n",
      "Epoch 854/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26061.1694 - mean_absolute_error: 26061.1694 - val_loss: 18203.0406 - val_mean_absolute_error: 18203.0406\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 16744.54169\n",
      "Epoch 855/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25593.5530 - mean_absolute_error: 25593.5530 - val_loss: 17067.9845 - val_mean_absolute_error: 17067.9845\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 16744.54169\n",
      "Epoch 856/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25748.9021 - mean_absolute_error: 25748.9021 - val_loss: 17087.0003 - val_mean_absolute_error: 17087.0003\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 16744.54169\n",
      "Epoch 857/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26537.4058 - mean_absolute_error: 26537.4058 - val_loss: 22327.0679 - val_mean_absolute_error: 22327.0679\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 16744.54169\n",
      "Epoch 858/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25266.1151 - mean_absolute_error: 25266.1151 - val_loss: 33271.5602 - val_mean_absolute_error: 33271.5602\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 16744.54169\n",
      "Epoch 859/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25724.6495 - mean_absolute_error: 25724.6495 - val_loss: 21782.3141 - val_mean_absolute_error: 21782.3141\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 16744.54169\n",
      "Epoch 860/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27046.1326 - mean_absolute_error: 27046.1326 - val_loss: 20256.8040 - val_mean_absolute_error: 20256.8040\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 16744.54169\n",
      "Epoch 861/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 26406.5032 - mean_absolute_error: 26406.5032 - val_loss: 19492.9287 - val_mean_absolute_error: 19492.9287\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 16744.54169\n",
      "Epoch 862/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 25539.5507 - mean_absolute_error: 25539.5507 - val_loss: 19921.1639 - val_mean_absolute_error: 19921.1639\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 16744.54169\n",
      "Epoch 863/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 25756.9773 - mean_absolute_error: 25756.9773 - val_loss: 19027.0875 - val_mean_absolute_error: 19027.0875\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 16744.54169\n",
      "Epoch 864/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 27157.7951 - mean_absolute_error: 27157.7951 - val_loss: 18737.6477 - val_mean_absolute_error: 18737.6477\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 16744.54169\n",
      "Epoch 865/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 25738.6337 - mean_absolute_error: 25738.6337 - val_loss: 21434.7056 - val_mean_absolute_error: 21434.7056\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 16744.54169\n",
      "Epoch 866/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 24915.3873 - mean_absolute_error: 24915.3873 - val_loss: 28066.3580 - val_mean_absolute_error: 28066.3580\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 16744.54169\n",
      "Epoch 867/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26132.1062 - mean_absolute_error: 26132.1062 - val_loss: 31516.4624 - val_mean_absolute_error: 31516.4624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00867: val_loss did not improve from 16744.54169\n",
      "Epoch 868/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26317.8326 - mean_absolute_error: 26317.8326 - val_loss: 21811.2828 - val_mean_absolute_error: 21811.2828\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 16744.54169\n",
      "Epoch 869/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25258.3180 - mean_absolute_error: 25258.3180 - val_loss: 23440.0412 - val_mean_absolute_error: 23440.0412\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 16744.54169\n",
      "Epoch 870/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26682.7101 - mean_absolute_error: 26682.7101 - val_loss: 23908.6099 - val_mean_absolute_error: 23908.6099\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 16744.54169\n",
      "Epoch 871/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 25990.5790 - mean_absolute_error: 25990.5790 - val_loss: 28533.7850 - val_mean_absolute_error: 28533.7850\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 16744.54169\n",
      "Epoch 872/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27910.1780 - mean_absolute_error: 27910.1780 - val_loss: 22319.3763 - val_mean_absolute_error: 22319.3763\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 16744.54169\n",
      "Epoch 873/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25756.8883 - mean_absolute_error: 25756.8883 - val_loss: 23272.9385 - val_mean_absolute_error: 23272.9385\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 16744.54169\n",
      "Epoch 874/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26544.4601 - mean_absolute_error: 26544.4601 - val_loss: 20426.4310 - val_mean_absolute_error: 20426.4310\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 16744.54169\n",
      "Epoch 875/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26410.5807 - mean_absolute_error: 26410.5807 - val_loss: 21265.4280 - val_mean_absolute_error: 21265.4280\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 16744.54169\n",
      "Epoch 876/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26029.4664 - mean_absolute_error: 26029.4664 - val_loss: 28908.5670 - val_mean_absolute_error: 28908.5670\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 16744.54169\n",
      "Epoch 877/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25507.8735 - mean_absolute_error: 25507.8735 - val_loss: 29623.2896 - val_mean_absolute_error: 29623.2896\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 16744.54169\n",
      "Epoch 878/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25421.7834 - mean_absolute_error: 25421.7834 - val_loss: 23838.8935 - val_mean_absolute_error: 23838.8935\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 16744.54169\n",
      "Epoch 879/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25568.2811 - mean_absolute_error: 25568.2811 - val_loss: 18989.0089 - val_mean_absolute_error: 18989.0089\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 16744.54169\n",
      "Epoch 880/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26258.1515 - mean_absolute_error: 26258.1515 - val_loss: 22645.8846 - val_mean_absolute_error: 22645.8846\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 16744.54169\n",
      "Epoch 881/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25859.5315 - mean_absolute_error: 25859.5315 - val_loss: 27503.5671 - val_mean_absolute_error: 27503.5671\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 16744.54169\n",
      "Epoch 882/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26668.0084 - mean_absolute_error: 26668.0084 - val_loss: 28468.0659 - val_mean_absolute_error: 28468.0659\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 16744.54169\n",
      "Epoch 883/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26412.4159 - mean_absolute_error: 26412.4159 - val_loss: 24022.5909 - val_mean_absolute_error: 24022.5909\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 16744.54169\n",
      "Epoch 884/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26933.4436 - mean_absolute_error: 26933.4436 - val_loss: 23094.4463 - val_mean_absolute_error: 23094.4463\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 16744.54169\n",
      "Epoch 885/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25686.2649 - mean_absolute_error: 25686.2649 - val_loss: 23503.7680 - val_mean_absolute_error: 23503.7680\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 16744.54169\n",
      "Epoch 886/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26717.3961 - mean_absolute_error: 26717.3961 - val_loss: 27986.6760 - val_mean_absolute_error: 27986.6760\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 16744.54169\n",
      "Epoch 887/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26545.0045 - mean_absolute_error: 26545.0045 - val_loss: 23339.2267 - val_mean_absolute_error: 23339.2267\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 16744.54169\n",
      "Epoch 888/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26878.1755 - mean_absolute_error: 26878.1755 - val_loss: 22271.8636 - val_mean_absolute_error: 22271.8636\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 16744.54169\n",
      "Epoch 889/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26041.8297 - mean_absolute_error: 26041.8297 - val_loss: 20604.7021 - val_mean_absolute_error: 20604.7021\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 16744.54169\n",
      "Epoch 890/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26289.8286 - mean_absolute_error: 26289.8286 - val_loss: 24743.3493 - val_mean_absolute_error: 24743.3493\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 16744.54169\n",
      "Epoch 891/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26305.9647 - mean_absolute_error: 26305.9647 - val_loss: 20059.6295 - val_mean_absolute_error: 20059.6295\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 16744.54169\n",
      "Epoch 892/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25471.8383 - mean_absolute_error: 25471.8383 - val_loss: 18652.0891 - val_mean_absolute_error: 18652.0891\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 16744.54169\n",
      "Epoch 893/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25489.5186 - mean_absolute_error: 25489.5186 - val_loss: 26746.2684 - val_mean_absolute_error: 26746.2684\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 16744.54169\n",
      "Epoch 894/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 25104.4551 - mean_absolute_error: 25104.4551 - val_loss: 25973.7666 - val_mean_absolute_error: 25973.7666\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 16744.54169\n",
      "Epoch 895/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25957.0145 - mean_absolute_error: 25957.0145 - val_loss: 20414.5927 - val_mean_absolute_error: 20414.5927\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 16744.54169\n",
      "Epoch 896/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24800.0382 - mean_absolute_error: 24800.0382 - val_loss: 27157.1480 - val_mean_absolute_error: 27157.1480\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 16744.54169\n",
      "Epoch 897/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 24823.1094 - mean_absolute_error: 24823.1094 - val_loss: 22427.8862 - val_mean_absolute_error: 22427.8862\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 16744.54169\n",
      "Epoch 898/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25954.1979 - mean_absolute_error: 25954.1979 - val_loss: 20248.5310 - val_mean_absolute_error: 20248.5310\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 16744.54169\n",
      "Epoch 899/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 24976.0556 - mean_absolute_error: 24976.0556 - val_loss: 18051.0673 - val_mean_absolute_error: 18051.0673\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 16744.54169\n",
      "Epoch 900/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24815.0776 - mean_absolute_error: 24815.0776 - val_loss: 24725.9529 - val_mean_absolute_error: 24725.9529\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 16744.54169\n",
      "Epoch 901/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26234.8788 - mean_absolute_error: 26234.8788 - val_loss: 18581.9765 - val_mean_absolute_error: 18581.9765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00901: val_loss did not improve from 16744.54169\n",
      "Epoch 902/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26752.6561 - mean_absolute_error: 26752.6561 - val_loss: 20667.2288 - val_mean_absolute_error: 20667.2288\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 16744.54169\n",
      "Epoch 903/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26811.4257 - mean_absolute_error: 26811.4257 - val_loss: 18435.6375 - val_mean_absolute_error: 18435.6375\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 16744.54169\n",
      "Epoch 904/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25441.6659 - mean_absolute_error: 25441.6659 - val_loss: 17467.3824 - val_mean_absolute_error: 17467.3824\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 16744.54169\n",
      "Epoch 905/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26482.2833 - mean_absolute_error: 26482.2833 - val_loss: 22795.2036 - val_mean_absolute_error: 22795.2036\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 16744.54169\n",
      "Epoch 906/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26372.9509 - mean_absolute_error: 26372.9509 - val_loss: 31803.1582 - val_mean_absolute_error: 31803.1582\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 16744.54169\n",
      "Epoch 907/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 27067.6493 - mean_absolute_error: 27067.6493 - val_loss: 24436.0488 - val_mean_absolute_error: 24436.0488\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 16744.54169\n",
      "Epoch 908/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25659.3458 - mean_absolute_error: 25659.3458 - val_loss: 19415.2908 - val_mean_absolute_error: 19415.2908\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 16744.54169\n",
      "Epoch 909/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25866.2004 - mean_absolute_error: 25866.2004 - val_loss: 21748.4378 - val_mean_absolute_error: 21748.4378\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 16744.54169\n",
      "Epoch 910/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 24908.7072 - mean_absolute_error: 24908.7072 - val_loss: 18298.1399 - val_mean_absolute_error: 18298.1399\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 16744.54169\n",
      "Epoch 911/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25366.4394 - mean_absolute_error: 25366.4394 - val_loss: 17054.6158 - val_mean_absolute_error: 17054.6158\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 16744.54169\n",
      "Epoch 912/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26478.8532 - mean_absolute_error: 26478.8532 - val_loss: 20126.4610 - val_mean_absolute_error: 20126.4610\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 16744.54169\n",
      "Epoch 913/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26982.4981 - mean_absolute_error: 26982.4981 - val_loss: 17444.1362 - val_mean_absolute_error: 17444.1362\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 16744.54169\n",
      "Epoch 914/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25681.5334 - mean_absolute_error: 25681.5334 - val_loss: 27353.3078 - val_mean_absolute_error: 27353.3078\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 16744.54169\n",
      "Epoch 915/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25776.3916 - mean_absolute_error: 25776.3916 - val_loss: 19488.0409 - val_mean_absolute_error: 19488.0409\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 16744.54169\n",
      "Epoch 916/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26152.6515 - mean_absolute_error: 26152.6515 - val_loss: 17791.4320 - val_mean_absolute_error: 17791.4320\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 16744.54169\n",
      "Epoch 917/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 25694.6573 - mean_absolute_error: 25694.6573 - val_loss: 22404.2896 - val_mean_absolute_error: 22404.2896\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 16744.54169\n",
      "Epoch 918/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27510.7087 - mean_absolute_error: 27510.7087 - val_loss: 19999.3048 - val_mean_absolute_error: 19999.3048\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 16744.54169\n",
      "Epoch 919/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25795.8244 - mean_absolute_error: 25795.8244 - val_loss: 17692.3917 - val_mean_absolute_error: 17692.3917\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 16744.54169\n",
      "Epoch 920/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25014.5629 - mean_absolute_error: 25014.5629 - val_loss: 20827.0066 - val_mean_absolute_error: 20827.0066\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 16744.54169\n",
      "Epoch 921/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25348.3625 - mean_absolute_error: 25348.3625 - val_loss: 24694.0704 - val_mean_absolute_error: 24694.0704\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 16744.54169\n",
      "Epoch 922/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26253.4080 - mean_absolute_error: 26253.4080 - val_loss: 22135.3735 - val_mean_absolute_error: 22135.3735\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 16744.54169\n",
      "Epoch 923/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26988.1367 - mean_absolute_error: 26988.1367 - val_loss: 21443.0904 - val_mean_absolute_error: 21443.0904\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 16744.54169\n",
      "Epoch 924/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25618.9126 - mean_absolute_error: 25618.9126 - val_loss: 29986.4148 - val_mean_absolute_error: 29986.4148\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 16744.54169\n",
      "Epoch 925/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26425.1368 - mean_absolute_error: 26425.1368 - val_loss: 21361.8549 - val_mean_absolute_error: 21361.8549\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 16744.54169\n",
      "Epoch 926/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26046.5736 - mean_absolute_error: 26046.5736 - val_loss: 21011.3961 - val_mean_absolute_error: 21011.3961\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 16744.54169\n",
      "Epoch 927/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 24787.9537 - mean_absolute_error: 24787.9537 - val_loss: 23691.7938 - val_mean_absolute_error: 23691.7938\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 16744.54169\n",
      "Epoch 928/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25456.4783 - mean_absolute_error: 25456.4783 - val_loss: 17798.9210 - val_mean_absolute_error: 17798.9210\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 16744.54169\n",
      "Epoch 929/1500\n",
      "934/934 [==============================] - 0s 308us/step - loss: 26865.7398 - mean_absolute_error: 26865.7398 - val_loss: 28450.0630 - val_mean_absolute_error: 28450.0630\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 16744.54169\n",
      "Epoch 930/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 26611.4169 - mean_absolute_error: 26611.4169 - val_loss: 31235.1351 - val_mean_absolute_error: 31235.1351\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 16744.54169\n",
      "Epoch 931/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25779.1997 - mean_absolute_error: 25779.1997 - val_loss: 32511.6978 - val_mean_absolute_error: 32511.6978\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 16744.54169\n",
      "Epoch 932/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26745.7759 - mean_absolute_error: 26745.7759 - val_loss: 21185.0174 - val_mean_absolute_error: 21185.0174\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 16744.54169\n",
      "Epoch 933/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26043.9035 - mean_absolute_error: 26043.9035 - val_loss: 24943.7444 - val_mean_absolute_error: 24943.7444\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 16744.54169\n",
      "Epoch 934/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26112.2669 - mean_absolute_error: 26112.2669 - val_loss: 18112.8505 - val_mean_absolute_error: 18112.8505\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 16744.54169\n",
      "Epoch 935/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26885.2308 - mean_absolute_error: 26885.2308 - val_loss: 23140.3025 - val_mean_absolute_error: 23140.3025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00935: val_loss did not improve from 16744.54169\n",
      "Epoch 936/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25994.3143 - mean_absolute_error: 25994.3143 - val_loss: 23281.3065 - val_mean_absolute_error: 23281.3065\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 16744.54169\n",
      "Epoch 937/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26047.1687 - mean_absolute_error: 26047.1687 - val_loss: 27685.3162 - val_mean_absolute_error: 27685.3162\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 16744.54169\n",
      "Epoch 938/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26602.3800 - mean_absolute_error: 26602.3800 - val_loss: 24145.1999 - val_mean_absolute_error: 24145.1999\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 16744.54169\n",
      "Epoch 939/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26772.1571 - mean_absolute_error: 26772.1571 - val_loss: 18588.2461 - val_mean_absolute_error: 18588.2461\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 16744.54169\n",
      "Epoch 940/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25686.1182 - mean_absolute_error: 25686.1182 - val_loss: 24460.3718 - val_mean_absolute_error: 24460.3718\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 16744.54169\n",
      "Epoch 941/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25124.2277 - mean_absolute_error: 25124.2277 - val_loss: 26619.2905 - val_mean_absolute_error: 26619.2905\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 16744.54169\n",
      "Epoch 942/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26820.3177 - mean_absolute_error: 26820.3177 - val_loss: 18293.8424 - val_mean_absolute_error: 18293.8424\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 16744.54169\n",
      "Epoch 943/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 26271.7333 - mean_absolute_error: 26271.7333 - val_loss: 21748.9803 - val_mean_absolute_error: 21748.9803\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 16744.54169\n",
      "Epoch 944/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26130.8065 - mean_absolute_error: 26130.8065 - val_loss: 21329.3508 - val_mean_absolute_error: 21329.3508\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 16744.54169\n",
      "Epoch 945/1500\n",
      "934/934 [==============================] - ETA: 0s - loss: 26189.5179 - mean_absolute_error: 26189.517 - 0s 256us/step - loss: 25911.5293 - mean_absolute_error: 25911.5293 - val_loss: 18528.7580 - val_mean_absolute_error: 18528.7580\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 16744.54169\n",
      "Epoch 946/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25704.5763 - mean_absolute_error: 25704.5763 - val_loss: 21214.1882 - val_mean_absolute_error: 21214.1882\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 16744.54169\n",
      "Epoch 947/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26434.3322 - mean_absolute_error: 26434.3322 - val_loss: 26521.2601 - val_mean_absolute_error: 26521.2601\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 16744.54169\n",
      "Epoch 948/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26466.0620 - mean_absolute_error: 26466.0620 - val_loss: 17240.0869 - val_mean_absolute_error: 17240.0869\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 16744.54169\n",
      "Epoch 949/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 26608.4302 - mean_absolute_error: 26608.4302 - val_loss: 26063.4873 - val_mean_absolute_error: 26063.4873\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 16744.54169\n",
      "Epoch 950/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25150.6913 - mean_absolute_error: 25150.6913 - val_loss: 33886.1346 - val_mean_absolute_error: 33886.1346\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 16744.54169\n",
      "Epoch 951/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26803.1216 - mean_absolute_error: 26803.1216 - val_loss: 27129.5163 - val_mean_absolute_error: 27129.5163\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 16744.54169\n",
      "Epoch 952/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26363.9525 - mean_absolute_error: 26363.9525 - val_loss: 22028.0054 - val_mean_absolute_error: 22028.0054\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 16744.54169\n",
      "Epoch 953/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26504.2340 - mean_absolute_error: 26504.2340 - val_loss: 23342.1064 - val_mean_absolute_error: 23342.1064\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 16744.54169\n",
      "Epoch 954/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25539.9261 - mean_absolute_error: 25539.9261 - val_loss: 23758.0603 - val_mean_absolute_error: 23758.0603\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 16744.54169\n",
      "Epoch 955/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25531.4937 - mean_absolute_error: 25531.4937 - val_loss: 26349.4680 - val_mean_absolute_error: 26349.4680\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 16744.54169\n",
      "Epoch 956/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25985.9074 - mean_absolute_error: 25985.9074 - val_loss: 29625.4090 - val_mean_absolute_error: 29625.4090\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 16744.54169\n",
      "Epoch 957/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26773.1249 - mean_absolute_error: 26773.1249 - val_loss: 25670.3172 - val_mean_absolute_error: 25670.3172\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 16744.54169\n",
      "Epoch 958/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25075.8232 - mean_absolute_error: 25075.8232 - val_loss: 17123.8672 - val_mean_absolute_error: 17123.8672\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 16744.54169\n",
      "Epoch 959/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26833.3823 - mean_absolute_error: 26833.3823 - val_loss: 24645.3859 - val_mean_absolute_error: 24645.3859\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 16744.54169\n",
      "Epoch 960/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26374.4234 - mean_absolute_error: 26374.4234 - val_loss: 20857.7870 - val_mean_absolute_error: 20857.7870\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 16744.54169\n",
      "Epoch 961/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25861.3691 - mean_absolute_error: 25861.3691 - val_loss: 19152.0514 - val_mean_absolute_error: 19152.0514\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 16744.54169\n",
      "Epoch 962/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25845.7596 - mean_absolute_error: 25845.7596 - val_loss: 18999.1742 - val_mean_absolute_error: 18999.1742\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 16744.54169\n",
      "Epoch 963/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26673.6772 - mean_absolute_error: 26673.6772 - val_loss: 21684.6823 - val_mean_absolute_error: 21684.6823\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 16744.54169\n",
      "Epoch 964/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25147.3843 - mean_absolute_error: 25147.3843 - val_loss: 20038.2620 - val_mean_absolute_error: 20038.2620\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 16744.54169\n",
      "Epoch 965/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25331.3736 - mean_absolute_error: 25331.3736 - val_loss: 26902.9986 - val_mean_absolute_error: 26902.9986\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 16744.54169\n",
      "Epoch 966/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 24796.5849 - mean_absolute_error: 24796.5849 - val_loss: 28565.9746 - val_mean_absolute_error: 28565.9746\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 16744.54169\n",
      "Epoch 967/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27497.7809 - mean_absolute_error: 27497.7809 - val_loss: 21492.7792 - val_mean_absolute_error: 21492.7792\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 16744.54169\n",
      "Epoch 968/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26729.9101 - mean_absolute_error: 26729.9101 - val_loss: 25180.3541 - val_mean_absolute_error: 25180.3541\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 16744.54169\n",
      "Epoch 969/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25310.7378 - mean_absolute_error: 25310.7378 - val_loss: 17935.4241 - val_mean_absolute_error: 17935.4241\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 16744.54169\n",
      "Epoch 970/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25910.2421 - mean_absolute_error: 25910.2421 - val_loss: 19949.3431 - val_mean_absolute_error: 19949.3431\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 16744.54169\n",
      "Epoch 971/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25773.0831 - mean_absolute_error: 25773.0831 - val_loss: 26259.4921 - val_mean_absolute_error: 26259.4921\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 16744.54169\n",
      "Epoch 972/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26756.2419 - mean_absolute_error: 26756.2419 - val_loss: 26492.8958 - val_mean_absolute_error: 26492.8958\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 16744.54169\n",
      "Epoch 973/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26241.9207 - mean_absolute_error: 26241.9207 - val_loss: 23262.0177 - val_mean_absolute_error: 23262.0177\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 16744.54169\n",
      "Epoch 974/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27318.1242 - mean_absolute_error: 27318.1242 - val_loss: 23457.6648 - val_mean_absolute_error: 23457.6648\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 16744.54169\n",
      "Epoch 975/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26111.9693 - mean_absolute_error: 26111.9693 - val_loss: 21988.8923 - val_mean_absolute_error: 21988.8923\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 16744.54169\n",
      "Epoch 976/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 25016.0238 - mean_absolute_error: 25016.0238 - val_loss: 17096.8432 - val_mean_absolute_error: 17096.8432\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 16744.54169\n",
      "Epoch 977/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25976.5045 - mean_absolute_error: 25976.5045 - val_loss: 25826.9006 - val_mean_absolute_error: 25826.9006\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 16744.54169\n",
      "Epoch 978/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25739.7533 - mean_absolute_error: 25739.7533 - val_loss: 18244.7475 - val_mean_absolute_error: 18244.7475\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 16744.54169\n",
      "Epoch 979/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27698.7679 - mean_absolute_error: 27698.7679 - val_loss: 24031.0043 - val_mean_absolute_error: 24031.0043\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 16744.54169\n",
      "Epoch 980/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26493.7008 - mean_absolute_error: 26493.7008 - val_loss: 18955.9996 - val_mean_absolute_error: 18955.9996\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 16744.54169\n",
      "Epoch 981/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26616.4205 - mean_absolute_error: 26616.4205 - val_loss: 22327.7013 - val_mean_absolute_error: 22327.7013\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 16744.54169\n",
      "Epoch 982/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 27230.5183 - mean_absolute_error: 27230.5183 - val_loss: 19441.2221 - val_mean_absolute_error: 19441.2221\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 16744.54169\n",
      "Epoch 983/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26295.4793 - mean_absolute_error: 26295.4793 - val_loss: 20582.9921 - val_mean_absolute_error: 20582.9921\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 16744.54169\n",
      "Epoch 984/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25953.3199 - mean_absolute_error: 25953.3199 - val_loss: 18423.7972 - val_mean_absolute_error: 18423.7972\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 16744.54169\n",
      "Epoch 985/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25657.1402 - mean_absolute_error: 25657.1402 - val_loss: 17314.1724 - val_mean_absolute_error: 17314.1724\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 16744.54169\n",
      "Epoch 986/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26279.1348 - mean_absolute_error: 26279.1348 - val_loss: 19384.7461 - val_mean_absolute_error: 19384.7461\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 16744.54169\n",
      "Epoch 987/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25803.3211 - mean_absolute_error: 25803.3211 - val_loss: 20544.9866 - val_mean_absolute_error: 20544.9866\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 16744.54169\n",
      "Epoch 988/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 24662.8124 - mean_absolute_error: 24662.8124 - val_loss: 24085.0161 - val_mean_absolute_error: 24085.0161\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 16744.54169\n",
      "Epoch 989/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26733.9943 - mean_absolute_error: 26733.9943 - val_loss: 23711.5016 - val_mean_absolute_error: 23711.5016\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 16744.54169\n",
      "Epoch 990/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25369.1974 - mean_absolute_error: 25369.1974 - val_loss: 32698.6143 - val_mean_absolute_error: 32698.6143\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 16744.54169\n",
      "Epoch 991/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25851.2148 - mean_absolute_error: 25851.2148 - val_loss: 29539.9911 - val_mean_absolute_error: 29539.9911\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 16744.54169\n",
      "Epoch 992/1500\n",
      "934/934 [==============================] - 0s 287us/step - loss: 26123.4494 - mean_absolute_error: 26123.4494 - val_loss: 23556.0465 - val_mean_absolute_error: 23556.0465\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 16744.54169\n",
      "Epoch 993/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25070.0574 - mean_absolute_error: 25070.0574 - val_loss: 25527.4222 - val_mean_absolute_error: 25527.4222\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 16744.54169\n",
      "Epoch 994/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26491.6501 - mean_absolute_error: 26491.6501 - val_loss: 21042.0879 - val_mean_absolute_error: 21042.0879\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 16744.54169\n",
      "Epoch 995/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25430.0329 - mean_absolute_error: 25430.0329 - val_loss: 22922.3965 - val_mean_absolute_error: 22922.3965\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 16744.54169\n",
      "Epoch 996/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25678.2141 - mean_absolute_error: 25678.2141 - val_loss: 17382.2995 - val_mean_absolute_error: 17382.2995\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 16744.54169\n",
      "Epoch 997/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26170.1613 - mean_absolute_error: 26170.1613 - val_loss: 21130.1909 - val_mean_absolute_error: 21130.1909\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 16744.54169\n",
      "Epoch 998/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25085.2985 - mean_absolute_error: 25085.2985 - val_loss: 22578.5350 - val_mean_absolute_error: 22578.5350\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 16744.54169\n",
      "Epoch 999/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25830.1555 - mean_absolute_error: 25830.1555 - val_loss: 19558.4593 - val_mean_absolute_error: 19558.4593\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 16744.54169\n",
      "Epoch 1000/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25167.7846 - mean_absolute_error: 25167.7846 - val_loss: 18842.1480 - val_mean_absolute_error: 18842.1480\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 16744.54169\n",
      "Epoch 1001/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26065.3909 - mean_absolute_error: 26065.3909 - val_loss: 17614.7212 - val_mean_absolute_error: 17614.7212\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 16744.54169\n",
      "Epoch 1002/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26589.3029 - mean_absolute_error: 26589.3029 - val_loss: 21956.2320 - val_mean_absolute_error: 21956.2320\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 16744.54169\n",
      "Epoch 1003/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 250us/step - loss: 26626.1646 - mean_absolute_error: 26626.1646 - val_loss: 17980.0018 - val_mean_absolute_error: 17980.0018\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 16744.54169\n",
      "Epoch 1004/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26280.6384 - mean_absolute_error: 26280.6384 - val_loss: 19140.4926 - val_mean_absolute_error: 19140.4926\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 16744.54169\n",
      "Epoch 1005/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24976.6588 - mean_absolute_error: 24976.6588 - val_loss: 20357.5147 - val_mean_absolute_error: 20357.5147\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 16744.54169\n",
      "Epoch 1006/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25648.9967 - mean_absolute_error: 25648.9967 - val_loss: 18018.3207 - val_mean_absolute_error: 18018.3207\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 16744.54169\n",
      "Epoch 1007/1500\n",
      "934/934 [==============================] - ETA: 0s - loss: 25820.9453 - mean_absolute_error: 25820.945 - 0s 255us/step - loss: 25806.3157 - mean_absolute_error: 25806.3157 - val_loss: 33149.0869 - val_mean_absolute_error: 33149.0869\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 16744.54169\n",
      "Epoch 1008/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 25359.7435 - mean_absolute_error: 25359.7435 - val_loss: 17100.7356 - val_mean_absolute_error: 17100.7356\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 16744.54169\n",
      "Epoch 1009/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25653.1954 - mean_absolute_error: 25653.1954 - val_loss: 30435.6633 - val_mean_absolute_error: 30435.6633\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 16744.54169\n",
      "Epoch 1010/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25516.2513 - mean_absolute_error: 25516.2513 - val_loss: 21043.6632 - val_mean_absolute_error: 21043.6632\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 16744.54169\n",
      "Epoch 1011/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25402.1218 - mean_absolute_error: 25402.1218 - val_loss: 19423.7091 - val_mean_absolute_error: 19423.7091\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 16744.54169\n",
      "Epoch 1012/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 25148.6165 - mean_absolute_error: 25148.6165 - val_loss: 27984.7176 - val_mean_absolute_error: 27984.7176\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 16744.54169\n",
      "Epoch 1013/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25263.0160 - mean_absolute_error: 25263.0160 - val_loss: 21125.1328 - val_mean_absolute_error: 21125.1328\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 16744.54169\n",
      "Epoch 1014/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25205.7025 - mean_absolute_error: 25205.7025 - val_loss: 17358.1915 - val_mean_absolute_error: 17358.1915\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 16744.54169\n",
      "Epoch 1015/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25849.6616 - mean_absolute_error: 25849.6616 - val_loss: 29979.7773 - val_mean_absolute_error: 29979.7773\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 16744.54169\n",
      "Epoch 1016/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25475.3043 - mean_absolute_error: 25475.3043 - val_loss: 21359.8754 - val_mean_absolute_error: 21359.8754\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 16744.54169\n",
      "Epoch 1017/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26860.0252 - mean_absolute_error: 26860.0252 - val_loss: 24474.6342 - val_mean_absolute_error: 24474.6342\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 16744.54169\n",
      "Epoch 1018/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25537.2033 - mean_absolute_error: 25537.2033 - val_loss: 19783.9847 - val_mean_absolute_error: 19783.9847\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 16744.54169\n",
      "Epoch 1019/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26613.3700 - mean_absolute_error: 26613.3700 - val_loss: 26807.4861 - val_mean_absolute_error: 26807.4861\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 16744.54169\n",
      "Epoch 1020/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25554.5206 - mean_absolute_error: 25554.5206 - val_loss: 20417.2809 - val_mean_absolute_error: 20417.2809\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 16744.54169\n",
      "Epoch 1021/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25944.1560 - mean_absolute_error: 25944.1560 - val_loss: 23968.9706 - val_mean_absolute_error: 23968.9706\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 16744.54169\n",
      "Epoch 1022/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25184.9691 - mean_absolute_error: 25184.9691 - val_loss: 29580.6963 - val_mean_absolute_error: 29580.6963\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 16744.54169\n",
      "Epoch 1023/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25883.9554 - mean_absolute_error: 25883.9554 - val_loss: 29097.1781 - val_mean_absolute_error: 29097.1781\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 16744.54169\n",
      "Epoch 1024/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26849.5711 - mean_absolute_error: 26849.5711 - val_loss: 27456.1144 - val_mean_absolute_error: 27456.1144\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 16744.54169\n",
      "Epoch 1025/1500\n",
      "934/934 [==============================] - 0s 284us/step - loss: 26473.0112 - mean_absolute_error: 26473.0112 - val_loss: 28160.0330 - val_mean_absolute_error: 28160.0330\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 16744.54169\n",
      "Epoch 1026/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25154.9110 - mean_absolute_error: 25154.9110 - val_loss: 17499.5947 - val_mean_absolute_error: 17499.5947\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 16744.54169\n",
      "Epoch 1027/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 25557.2301 - mean_absolute_error: 25557.2301 - val_loss: 23516.3538 - val_mean_absolute_error: 23516.3538\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 16744.54169\n",
      "Epoch 1028/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25758.4604 - mean_absolute_error: 25758.4604 - val_loss: 18016.2846 - val_mean_absolute_error: 18016.2846\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 16744.54169\n",
      "Epoch 1029/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25786.2786 - mean_absolute_error: 25786.2786 - val_loss: 17255.6895 - val_mean_absolute_error: 17255.6895\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 16744.54169\n",
      "Epoch 1030/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26174.5434 - mean_absolute_error: 26174.5434 - val_loss: 33168.3407 - val_mean_absolute_error: 33168.3407\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 16744.54169\n",
      "Epoch 1031/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26454.7562 - mean_absolute_error: 26454.7562 - val_loss: 19033.0655 - val_mean_absolute_error: 19033.0655\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 16744.54169\n",
      "Epoch 1032/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25727.9072 - mean_absolute_error: 25727.9072 - val_loss: 26949.4649 - val_mean_absolute_error: 26949.4649\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 16744.54169\n",
      "Epoch 1033/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25813.7803 - mean_absolute_error: 25813.7803 - val_loss: 21368.6930 - val_mean_absolute_error: 21368.6930\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 16744.54169\n",
      "Epoch 1034/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26391.4412 - mean_absolute_error: 26391.4412 - val_loss: 24571.5627 - val_mean_absolute_error: 24571.5627\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 16744.54169\n",
      "Epoch 1035/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25957.1901 - mean_absolute_error: 25957.1901 - val_loss: 30839.5048 - val_mean_absolute_error: 30839.5048\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 16744.54169\n",
      "Epoch 1036/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25918.9176 - mean_absolute_error: 25918.9176 - val_loss: 18578.6945 - val_mean_absolute_error: 18578.6945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01036: val_loss did not improve from 16744.54169\n",
      "Epoch 1037/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26918.7725 - mean_absolute_error: 26918.7725 - val_loss: 20620.4025 - val_mean_absolute_error: 20620.4025\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 16744.54169\n",
      "Epoch 1038/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 26403.1060 - mean_absolute_error: 26403.1060 - val_loss: 17710.2691 - val_mean_absolute_error: 17710.2691\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 16744.54169\n",
      "Epoch 1039/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25376.2553 - mean_absolute_error: 25376.2553 - val_loss: 23701.2366 - val_mean_absolute_error: 23701.2366\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 16744.54169\n",
      "Epoch 1040/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26715.0409 - mean_absolute_error: 26715.0409 - val_loss: 17628.7370 - val_mean_absolute_error: 17628.7370\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 16744.54169\n",
      "Epoch 1041/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 26517.0876 - mean_absolute_error: 26517.0876 - val_loss: 30824.2326 - val_mean_absolute_error: 30824.2326\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 16744.54169\n",
      "Epoch 1042/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27290.2763 - mean_absolute_error: 27290.2763 - val_loss: 19884.9980 - val_mean_absolute_error: 19884.9980\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 16744.54169\n",
      "Epoch 1043/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25577.1325 - mean_absolute_error: 25577.1325 - val_loss: 26213.5814 - val_mean_absolute_error: 26213.5814\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 16744.54169\n",
      "Epoch 1044/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24669.1524 - mean_absolute_error: 24669.1524 - val_loss: 26649.1560 - val_mean_absolute_error: 26649.1560\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 16744.54169\n",
      "Epoch 1045/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26418.7004 - mean_absolute_error: 26418.7004 - val_loss: 21399.9407 - val_mean_absolute_error: 21399.9407\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 16744.54169\n",
      "Epoch 1046/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26288.0299 - mean_absolute_error: 26288.0299 - val_loss: 29370.9393 - val_mean_absolute_error: 29370.9393\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 16744.54169\n",
      "Epoch 1047/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26059.7009 - mean_absolute_error: 26059.7009 - val_loss: 18852.9344 - val_mean_absolute_error: 18852.9344\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 16744.54169\n",
      "Epoch 1048/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26066.0785 - mean_absolute_error: 26066.0785 - val_loss: 17354.7580 - val_mean_absolute_error: 17354.7580\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 16744.54169\n",
      "Epoch 1049/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 25861.9732 - mean_absolute_error: 25861.9732 - val_loss: 17793.2089 - val_mean_absolute_error: 17793.2089\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 16744.54169\n",
      "Epoch 1050/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25431.1725 - mean_absolute_error: 25431.1725 - val_loss: 22112.6786 - val_mean_absolute_error: 22112.6786\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 16744.54169\n",
      "Epoch 1051/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26856.3298 - mean_absolute_error: 26856.3298 - val_loss: 29483.9002 - val_mean_absolute_error: 29483.9002\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 16744.54169\n",
      "Epoch 1052/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25261.5579 - mean_absolute_error: 25261.5579 - val_loss: 24342.9590 - val_mean_absolute_error: 24342.9590\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 16744.54169\n",
      "Epoch 1053/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26663.6286 - mean_absolute_error: 26663.6286 - val_loss: 17909.4384 - val_mean_absolute_error: 17909.4384\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 16744.54169\n",
      "Epoch 1054/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26373.8667 - mean_absolute_error: 26373.8667 - val_loss: 22560.8345 - val_mean_absolute_error: 22560.8345\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 16744.54169\n",
      "Epoch 1055/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 25873.7705 - mean_absolute_error: 25873.7705 - val_loss: 24825.1355 - val_mean_absolute_error: 24825.1355\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 16744.54169\n",
      "Epoch 1056/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26544.5706 - mean_absolute_error: 26544.5706 - val_loss: 25241.8912 - val_mean_absolute_error: 25241.8912\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 16744.54169\n",
      "Epoch 1057/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26339.9885 - mean_absolute_error: 26339.9885 - val_loss: 19259.9858 - val_mean_absolute_error: 19259.9858\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 16744.54169\n",
      "Epoch 1058/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 24893.0121 - mean_absolute_error: 24893.0121 - val_loss: 21997.9085 - val_mean_absolute_error: 21997.9085\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 16744.54169\n",
      "Epoch 1059/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26183.0588 - mean_absolute_error: 26183.0588 - val_loss: 17975.9915 - val_mean_absolute_error: 17975.9915\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 16744.54169\n",
      "Epoch 1060/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25677.9296 - mean_absolute_error: 25677.9296 - val_loss: 20780.6690 - val_mean_absolute_error: 20780.6690\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 16744.54169\n",
      "Epoch 1061/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24620.6989 - mean_absolute_error: 24620.6989 - val_loss: 20118.5313 - val_mean_absolute_error: 20118.5313\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 16744.54169\n",
      "Epoch 1062/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26088.9269 - mean_absolute_error: 26088.9269 - val_loss: 20372.1044 - val_mean_absolute_error: 20372.1044\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 16744.54169\n",
      "Epoch 1063/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 24857.8400 - mean_absolute_error: 24857.8400 - val_loss: 26692.6845 - val_mean_absolute_error: 26692.6845\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 16744.54169\n",
      "Epoch 1064/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26401.9453 - mean_absolute_error: 26401.9453 - val_loss: 18765.4465 - val_mean_absolute_error: 18765.4465\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 16744.54169\n",
      "Epoch 1065/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26170.7848 - mean_absolute_error: 26170.7848 - val_loss: 17770.1530 - val_mean_absolute_error: 17770.1530\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 16744.54169\n",
      "Epoch 1066/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26763.7877 - mean_absolute_error: 26763.7877 - val_loss: 18638.4979 - val_mean_absolute_error: 18638.4979\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 16744.54169\n",
      "Epoch 1067/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25613.5775 - mean_absolute_error: 25613.5775 - val_loss: 26096.0693 - val_mean_absolute_error: 26096.0693\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 16744.54169\n",
      "Epoch 1068/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25185.5404 - mean_absolute_error: 25185.5404 - val_loss: 17541.2546 - val_mean_absolute_error: 17541.2546\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 16744.54169\n",
      "Epoch 1069/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 25357.9052 - mean_absolute_error: 25357.9052 - val_loss: 18829.9799 - val_mean_absolute_error: 18829.9799\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 16744.54169\n",
      "Epoch 1070/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26321.6268 - mean_absolute_error: 26321.6268 - val_loss: 25275.3144 - val_mean_absolute_error: 25275.3144\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 16744.54169\n",
      "Epoch 1071/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25021.1569 - mean_absolute_error: 25021.1569 - val_loss: 17345.4188 - val_mean_absolute_error: 17345.4188\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 16744.54169\n",
      "Epoch 1072/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25048.4920 - mean_absolute_error: 25048.4920 - val_loss: 19197.4707 - val_mean_absolute_error: 19197.4707\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 16744.54169\n",
      "Epoch 1073/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 24800.2974 - mean_absolute_error: 24800.2974 - val_loss: 17854.4680 - val_mean_absolute_error: 17854.4680\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 16744.54169\n",
      "Epoch 1074/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26323.0979 - mean_absolute_error: 26323.0979 - val_loss: 29923.3711 - val_mean_absolute_error: 29923.3711\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 16744.54169\n",
      "Epoch 1075/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26335.4947 - mean_absolute_error: 26335.4947 - val_loss: 17992.5472 - val_mean_absolute_error: 17992.5472\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 16744.54169\n",
      "Epoch 1076/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 26642.6353 - mean_absolute_error: 26642.6353 - val_loss: 18732.1709 - val_mean_absolute_error: 18732.1709\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 16744.54169\n",
      "Epoch 1077/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25588.4772 - mean_absolute_error: 25588.4772 - val_loss: 26524.1111 - val_mean_absolute_error: 26524.1111\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 16744.54169\n",
      "Epoch 1078/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25235.5366 - mean_absolute_error: 25235.5366 - val_loss: 22677.6881 - val_mean_absolute_error: 22677.6881\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 16744.54169\n",
      "Epoch 1079/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24742.3980 - mean_absolute_error: 24742.3980 - val_loss: 20813.3380 - val_mean_absolute_error: 20813.3380\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 16744.54169\n",
      "Epoch 1080/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 24679.2078 - mean_absolute_error: 24679.2078 - val_loss: 22898.7684 - val_mean_absolute_error: 22898.7684\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 16744.54169\n",
      "Epoch 1081/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26162.2267 - mean_absolute_error: 26162.2267 - val_loss: 26943.2491 - val_mean_absolute_error: 26943.2491\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 16744.54169\n",
      "Epoch 1082/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 25500.1076 - mean_absolute_error: 25500.1076 - val_loss: 23699.0095 - val_mean_absolute_error: 23699.0095\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 16744.54169\n",
      "Epoch 1083/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25513.1209 - mean_absolute_error: 25513.1209 - val_loss: 30191.7921 - val_mean_absolute_error: 30191.7921\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 16744.54169\n",
      "Epoch 1084/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25395.2975 - mean_absolute_error: 25395.2975 - val_loss: 18015.8047 - val_mean_absolute_error: 18015.8047\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 16744.54169\n",
      "Epoch 1085/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25764.0322 - mean_absolute_error: 25764.0322 - val_loss: 21160.8290 - val_mean_absolute_error: 21160.8290\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 16744.54169\n",
      "Epoch 1086/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26801.5503 - mean_absolute_error: 26801.5503 - val_loss: 20651.6731 - val_mean_absolute_error: 20651.6731\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 16744.54169\n",
      "Epoch 1087/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26085.3845 - mean_absolute_error: 26085.3845 - val_loss: 20979.5132 - val_mean_absolute_error: 20979.5132\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 16744.54169\n",
      "Epoch 1088/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27280.0284 - mean_absolute_error: 27280.0284 - val_loss: 18030.6561 - val_mean_absolute_error: 18030.6561\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 16744.54169\n",
      "Epoch 1089/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26183.5893 - mean_absolute_error: 26183.5893 - val_loss: 27463.2827 - val_mean_absolute_error: 27463.2827\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 16744.54169\n",
      "Epoch 1090/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 26808.9349 - mean_absolute_error: 26808.9349 - val_loss: 23730.4192 - val_mean_absolute_error: 23730.4192\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 16744.54169\n",
      "Epoch 1091/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25811.4579 - mean_absolute_error: 25811.4579 - val_loss: 21346.9123 - val_mean_absolute_error: 21346.9123\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 16744.54169\n",
      "Epoch 1092/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26611.7745 - mean_absolute_error: 26611.7745 - val_loss: 30685.6293 - val_mean_absolute_error: 30685.6293\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 16744.54169\n",
      "Epoch 1093/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25659.1935 - mean_absolute_error: 25659.1935 - val_loss: 20979.1163 - val_mean_absolute_error: 20979.1163\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 16744.54169\n",
      "Epoch 1094/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26418.6845 - mean_absolute_error: 26418.6845 - val_loss: 20931.0546 - val_mean_absolute_error: 20931.0546\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 16744.54169\n",
      "Epoch 1095/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26077.7795 - mean_absolute_error: 26077.7795 - val_loss: 19969.5260 - val_mean_absolute_error: 19969.5260\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 16744.54169\n",
      "Epoch 1096/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25705.6578 - mean_absolute_error: 25705.6578 - val_loss: 25457.5192 - val_mean_absolute_error: 25457.5192\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 16744.54169\n",
      "Epoch 1097/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24202.9791 - mean_absolute_error: 24202.9791 - val_loss: 28837.8386 - val_mean_absolute_error: 28837.8386\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 16744.54169\n",
      "Epoch 1098/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26181.7923 - mean_absolute_error: 26181.7923 - val_loss: 18336.7521 - val_mean_absolute_error: 18336.7521\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 16744.54169\n",
      "Epoch 1099/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25779.9709 - mean_absolute_error: 25779.9709 - val_loss: 19811.0169 - val_mean_absolute_error: 19811.0169\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 16744.54169\n",
      "Epoch 1100/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 24203.8381 - mean_absolute_error: 24203.8381 - val_loss: 21276.4077 - val_mean_absolute_error: 21276.4077\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 16744.54169\n",
      "Epoch 1101/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25812.6420 - mean_absolute_error: 25812.6420 - val_loss: 18174.9417 - val_mean_absolute_error: 18174.9417\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 16744.54169\n",
      "Epoch 1102/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26168.0480 - mean_absolute_error: 26168.0480 - val_loss: 26367.3117 - val_mean_absolute_error: 26367.3117\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 16744.54169\n",
      "Epoch 1103/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26027.3790 - mean_absolute_error: 26027.3790 - val_loss: 21164.5729 - val_mean_absolute_error: 21164.5729\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 16744.54169\n",
      "Epoch 1104/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 259us/step - loss: 25838.5255 - mean_absolute_error: 25838.5255 - val_loss: 30206.3323 - val_mean_absolute_error: 30206.3323\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 16744.54169\n",
      "Epoch 1105/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24967.6569 - mean_absolute_error: 24967.6569 - val_loss: 18378.1375 - val_mean_absolute_error: 18378.1375\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 16744.54169\n",
      "Epoch 1106/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26270.4355 - mean_absolute_error: 26270.4355 - val_loss: 27428.0400 - val_mean_absolute_error: 27428.0400\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 16744.54169\n",
      "Epoch 1107/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 27362.2590 - mean_absolute_error: 27362.2590 - val_loss: 24074.7757 - val_mean_absolute_error: 24074.7757\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 16744.54169\n",
      "Epoch 1108/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25609.0111 - mean_absolute_error: 25609.0111 - val_loss: 24684.4890 - val_mean_absolute_error: 24684.4890\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 16744.54169\n",
      "Epoch 1109/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 24949.1876 - mean_absolute_error: 24949.1876 - val_loss: 20346.0826 - val_mean_absolute_error: 20346.0826\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 16744.54169\n",
      "Epoch 1110/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25921.0554 - mean_absolute_error: 25921.0554 - val_loss: 21128.0586 - val_mean_absolute_error: 21128.0586\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 16744.54169\n",
      "Epoch 1111/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24980.8830 - mean_absolute_error: 24980.8830 - val_loss: 17425.8414 - val_mean_absolute_error: 17425.8414\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 16744.54169\n",
      "Epoch 1112/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25611.9544 - mean_absolute_error: 25611.9544 - val_loss: 17724.5777 - val_mean_absolute_error: 17724.5777\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 16744.54169\n",
      "Epoch 1113/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25185.7497 - mean_absolute_error: 25185.7497 - val_loss: 17721.3510 - val_mean_absolute_error: 17721.3510\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 16744.54169\n",
      "Epoch 1114/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 27016.9666 - mean_absolute_error: 27016.9666 - val_loss: 18465.3142 - val_mean_absolute_error: 18465.3142\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 16744.54169\n",
      "Epoch 1115/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25887.2328 - mean_absolute_error: 25887.2328 - val_loss: 27763.4954 - val_mean_absolute_error: 27763.4954\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 16744.54169\n",
      "Epoch 1116/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26282.4001 - mean_absolute_error: 26282.4001 - val_loss: 17208.1095 - val_mean_absolute_error: 17208.1095\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 16744.54169\n",
      "Epoch 1117/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27244.1974 - mean_absolute_error: 27244.1974 - val_loss: 18899.9511 - val_mean_absolute_error: 18899.9511\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 16744.54169\n",
      "Epoch 1118/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 24793.1923 - mean_absolute_error: 24793.1923 - val_loss: 17322.4151 - val_mean_absolute_error: 17322.4151\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 16744.54169\n",
      "Epoch 1119/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25742.0275 - mean_absolute_error: 25742.0275 - val_loss: 18001.9316 - val_mean_absolute_error: 18001.9316\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 16744.54169\n",
      "Epoch 1120/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26097.1329 - mean_absolute_error: 26097.1329 - val_loss: 23467.3695 - val_mean_absolute_error: 23467.3695\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 16744.54169\n",
      "Epoch 1121/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25499.6191 - mean_absolute_error: 25499.6191 - val_loss: 17746.2164 - val_mean_absolute_error: 17746.2164\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 16744.54169\n",
      "Epoch 1122/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25562.8493 - mean_absolute_error: 25562.8493 - val_loss: 19141.0860 - val_mean_absolute_error: 19141.0860\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 16744.54169\n",
      "Epoch 1123/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25765.4595 - mean_absolute_error: 25765.4595 - val_loss: 26006.9918 - val_mean_absolute_error: 26006.9918\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 16744.54169\n",
      "Epoch 1124/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26170.3221 - mean_absolute_error: 26170.3221 - val_loss: 17845.7749 - val_mean_absolute_error: 17845.7749\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 16744.54169\n",
      "Epoch 1125/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25836.5142 - mean_absolute_error: 25836.5142 - val_loss: 23017.7277 - val_mean_absolute_error: 23017.7277\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 16744.54169\n",
      "Epoch 1126/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25476.9015 - mean_absolute_error: 25476.9015 - val_loss: 22804.0148 - val_mean_absolute_error: 22804.0148\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 16744.54169\n",
      "Epoch 1127/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25145.4558 - mean_absolute_error: 25145.4558 - val_loss: 18884.3272 - val_mean_absolute_error: 18884.3272\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 16744.54169\n",
      "Epoch 1128/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26070.6826 - mean_absolute_error: 26070.6826 - val_loss: 23132.3542 - val_mean_absolute_error: 23132.3542\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 16744.54169\n",
      "Epoch 1129/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26632.6682 - mean_absolute_error: 26632.6682 - val_loss: 20421.6531 - val_mean_absolute_error: 20421.6531\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 16744.54169\n",
      "Epoch 1130/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25364.6114 - mean_absolute_error: 25364.6114 - val_loss: 27264.0312 - val_mean_absolute_error: 27264.0312\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 16744.54169\n",
      "Epoch 1131/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26471.4611 - mean_absolute_error: 26471.4611 - val_loss: 22499.8748 - val_mean_absolute_error: 22499.8748\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 16744.54169\n",
      "Epoch 1132/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25529.0267 - mean_absolute_error: 25529.0267 - val_loss: 23260.0344 - val_mean_absolute_error: 23260.0344\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 16744.54169\n",
      "Epoch 1133/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26351.2740 - mean_absolute_error: 26351.2740 - val_loss: 23864.9976 - val_mean_absolute_error: 23864.9976\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 16744.54169\n",
      "Epoch 1134/1500\n",
      "934/934 [==============================] - ETA: 0s - loss: 25451.2811 - mean_absolute_error: 25451.281 - 0s 256us/step - loss: 25354.0783 - mean_absolute_error: 25354.0783 - val_loss: 23660.3445 - val_mean_absolute_error: 23660.3445\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 16744.54169\n",
      "Epoch 1135/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26173.1425 - mean_absolute_error: 26173.1425 - val_loss: 16806.9312 - val_mean_absolute_error: 16806.9312\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 16744.54169\n",
      "Epoch 1136/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26252.9771 - mean_absolute_error: 26252.9771 - val_loss: 19284.8361 - val_mean_absolute_error: 19284.8361\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 16744.54169\n",
      "Epoch 1137/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25640.0653 - mean_absolute_error: 25640.0653 - val_loss: 30714.7236 - val_mean_absolute_error: 30714.7236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01137: val_loss did not improve from 16744.54169\n",
      "Epoch 1138/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25291.8699 - mean_absolute_error: 25291.8699 - val_loss: 17365.9641 - val_mean_absolute_error: 17365.9641\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 16744.54169\n",
      "Epoch 1139/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25952.0159 - mean_absolute_error: 25952.0159 - val_loss: 27746.4886 - val_mean_absolute_error: 27746.4886\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 16744.54169\n",
      "Epoch 1140/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 26807.5643 - mean_absolute_error: 26807.5643 - val_loss: 18969.6885 - val_mean_absolute_error: 18969.6885\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 16744.54169\n",
      "Epoch 1141/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26141.1417 - mean_absolute_error: 26141.1417 - val_loss: 28483.5158 - val_mean_absolute_error: 28483.5158\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 16744.54169\n",
      "Epoch 1142/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 24854.6788 - mean_absolute_error: 24854.6788 - val_loss: 19482.2479 - val_mean_absolute_error: 19482.2479\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 16744.54169\n",
      "Epoch 1143/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25524.5163 - mean_absolute_error: 25524.5163 - val_loss: 22540.7133 - val_mean_absolute_error: 22540.7133\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 16744.54169\n",
      "Epoch 1144/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25504.6977 - mean_absolute_error: 25504.6977 - val_loss: 30207.8019 - val_mean_absolute_error: 30207.8019\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 16744.54169\n",
      "Epoch 1145/1500\n",
      "934/934 [==============================] - 0s 281us/step - loss: 25350.4403 - mean_absolute_error: 25350.4403 - val_loss: 28403.4261 - val_mean_absolute_error: 28403.4261\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 16744.54169\n",
      "Epoch 1146/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26205.0946 - mean_absolute_error: 26205.0946 - val_loss: 17680.3231 - val_mean_absolute_error: 17680.3231\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 16744.54169\n",
      "Epoch 1147/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26959.7947 - mean_absolute_error: 26959.7947 - val_loss: 23541.9692 - val_mean_absolute_error: 23541.9692\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 16744.54169\n",
      "Epoch 1148/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25993.0856 - mean_absolute_error: 25993.0856 - val_loss: 27310.3269 - val_mean_absolute_error: 27310.3269\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 16744.54169\n",
      "Epoch 1149/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25714.4924 - mean_absolute_error: 25714.4924 - val_loss: 16979.9855 - val_mean_absolute_error: 16979.9855\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 16744.54169\n",
      "Epoch 1150/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 25088.8834 - mean_absolute_error: 25088.8834 - val_loss: 24921.7324 - val_mean_absolute_error: 24921.7324\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 16744.54169\n",
      "Epoch 1151/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 27927.0515 - mean_absolute_error: 27927.0515 - val_loss: 18806.3792 - val_mean_absolute_error: 18806.3792\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 16744.54169\n",
      "Epoch 1152/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25221.3484 - mean_absolute_error: 25221.3484 - val_loss: 22864.9938 - val_mean_absolute_error: 22864.9938\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 16744.54169\n",
      "Epoch 1153/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25218.0708 - mean_absolute_error: 25218.0708 - val_loss: 20553.1164 - val_mean_absolute_error: 20553.1164\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 16744.54169\n",
      "Epoch 1154/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25868.8480 - mean_absolute_error: 25868.8480 - val_loss: 20625.5725 - val_mean_absolute_error: 20625.5725\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 16744.54169\n",
      "Epoch 1155/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25517.2937 - mean_absolute_error: 25517.2937 - val_loss: 18046.1327 - val_mean_absolute_error: 18046.1327\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 16744.54169\n",
      "Epoch 1156/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 25342.0928 - mean_absolute_error: 25342.0928 - val_loss: 24887.6935 - val_mean_absolute_error: 24887.6935\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 16744.54169\n",
      "Epoch 1157/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26117.5349 - mean_absolute_error: 26117.5349 - val_loss: 23978.9351 - val_mean_absolute_error: 23978.9351\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 16744.54169\n",
      "Epoch 1158/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25248.6777 - mean_absolute_error: 25248.6777 - val_loss: 25068.7649 - val_mean_absolute_error: 25068.7649\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 16744.54169\n",
      "Epoch 1159/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 27713.0883 - mean_absolute_error: 27713.0883 - val_loss: 31631.7409 - val_mean_absolute_error: 31631.7409\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 16744.54169\n",
      "Epoch 1160/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26519.0143 - mean_absolute_error: 26519.0143 - val_loss: 26686.9740 - val_mean_absolute_error: 26686.9740\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 16744.54169\n",
      "Epoch 1161/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26304.1941 - mean_absolute_error: 26304.1941 - val_loss: 25284.2794 - val_mean_absolute_error: 25284.2794\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 16744.54169\n",
      "Epoch 1162/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 27167.0178 - mean_absolute_error: 27167.0178 - val_loss: 29679.7641 - val_mean_absolute_error: 29679.7641\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 16744.54169\n",
      "Epoch 1163/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 24825.5576 - mean_absolute_error: 24825.5576 - val_loss: 22746.2125 - val_mean_absolute_error: 22746.2125\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 16744.54169\n",
      "Epoch 1164/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25156.2997 - mean_absolute_error: 25156.2997 - val_loss: 21612.8165 - val_mean_absolute_error: 21612.8165\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 16744.54169\n",
      "Epoch 1165/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25053.3098 - mean_absolute_error: 25053.3098 - val_loss: 17931.7196 - val_mean_absolute_error: 17931.7196\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 16744.54169\n",
      "Epoch 1166/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25655.4856 - mean_absolute_error: 25655.4856 - val_loss: 24674.1968 - val_mean_absolute_error: 24674.1968\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 16744.54169\n",
      "Epoch 1167/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26216.0435 - mean_absolute_error: 26216.0435 - val_loss: 26099.2352 - val_mean_absolute_error: 26099.2352\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 16744.54169\n",
      "Epoch 1168/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 24636.4269 - mean_absolute_error: 24636.4269 - val_loss: 24531.1986 - val_mean_absolute_error: 24531.1986\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 16744.54169\n",
      "Epoch 1169/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25741.5805 - mean_absolute_error: 25741.5805 - val_loss: 24285.9134 - val_mean_absolute_error: 24285.9134\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 16744.54169\n",
      "Epoch 1170/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 24698.3473 - mean_absolute_error: 24698.3473 - val_loss: 18974.3295 - val_mean_absolute_error: 18974.3295\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 16744.54169\n",
      "Epoch 1171/1500\n",
      "934/934 [==============================] - 0s 249us/step - loss: 25666.3029 - mean_absolute_error: 25666.3029 - val_loss: 22485.1895 - val_mean_absolute_error: 22485.1895\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 16744.54169\n",
      "Epoch 1172/1500\n",
      "934/934 [==============================] - 0s 287us/step - loss: 25021.9230 - mean_absolute_error: 25021.9230 - val_loss: 24502.0346 - val_mean_absolute_error: 24502.0346\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 16744.54169\n",
      "Epoch 1173/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25807.8131 - mean_absolute_error: 25807.8131 - val_loss: 25410.0859 - val_mean_absolute_error: 25410.0859\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 16744.54169\n",
      "Epoch 1174/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26087.9994 - mean_absolute_error: 26087.9994 - val_loss: 17708.0652 - val_mean_absolute_error: 17708.0652\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 16744.54169\n",
      "Epoch 1175/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26199.6369 - mean_absolute_error: 26199.6369 - val_loss: 21181.3654 - val_mean_absolute_error: 21181.3654\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 16744.54169\n",
      "Epoch 1176/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25861.9117 - mean_absolute_error: 25861.9117 - val_loss: 17862.9773 - val_mean_absolute_error: 17862.9773\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 16744.54169\n",
      "Epoch 1177/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25613.4829 - mean_absolute_error: 25613.4829 - val_loss: 24612.9162 - val_mean_absolute_error: 24612.9162\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 16744.54169\n",
      "Epoch 1178/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25921.8314 - mean_absolute_error: 25921.8314 - val_loss: 22556.4356 - val_mean_absolute_error: 22556.4356\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 16744.54169\n",
      "Epoch 1179/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25127.1857 - mean_absolute_error: 25127.1857 - val_loss: 20065.6421 - val_mean_absolute_error: 20065.6421\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 16744.54169\n",
      "Epoch 1180/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 26037.5065 - mean_absolute_error: 26037.5065 - val_loss: 21399.9961 - val_mean_absolute_error: 21399.9961\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 16744.54169\n",
      "Epoch 1181/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26268.2841 - mean_absolute_error: 26268.2841 - val_loss: 17577.8617 - val_mean_absolute_error: 17577.8617\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 16744.54169\n",
      "Epoch 1182/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26001.4847 - mean_absolute_error: 26001.4847 - val_loss: 22930.2880 - val_mean_absolute_error: 22930.2880\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 16744.54169\n",
      "Epoch 1183/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 25072.4253 - mean_absolute_error: 25072.4253 - val_loss: 25435.9811 - val_mean_absolute_error: 25435.9811\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 16744.54169\n",
      "Epoch 1184/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25203.7817 - mean_absolute_error: 25203.7817 - val_loss: 18436.2609 - val_mean_absolute_error: 18436.2609\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 16744.54169\n",
      "Epoch 1185/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25306.1516 - mean_absolute_error: 25306.1516 - val_loss: 17286.7372 - val_mean_absolute_error: 17286.7372\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 16744.54169\n",
      "Epoch 1186/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25994.8789 - mean_absolute_error: 25994.8789 - val_loss: 17820.5634 - val_mean_absolute_error: 17820.5634\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 16744.54169\n",
      "Epoch 1187/1500\n",
      "934/934 [==============================] - 0s 249us/step - loss: 24472.1511 - mean_absolute_error: 24472.1511 - val_loss: 23841.0298 - val_mean_absolute_error: 23841.0298\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 16744.54169\n",
      "Epoch 1188/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26027.9742 - mean_absolute_error: 26027.9742 - val_loss: 23992.5039 - val_mean_absolute_error: 23992.5039\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 16744.54169\n",
      "Epoch 1189/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 26226.8537 - mean_absolute_error: 26226.8537 - val_loss: 21250.3466 - val_mean_absolute_error: 21250.3466\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 16744.54169\n",
      "Epoch 1190/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25845.6007 - mean_absolute_error: 25845.6007 - val_loss: 17192.8627 - val_mean_absolute_error: 17192.8627\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 16744.54169\n",
      "Epoch 1191/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25863.7341 - mean_absolute_error: 25863.7341 - val_loss: 16632.1370 - val_mean_absolute_error: 16632.1370\n",
      "\n",
      "Epoch 01191: val_loss improved from 16744.54169 to 16632.13696, saving model to Weights-1191--16632.13696.hdf5\n",
      "Epoch 1192/1500\n",
      "934/934 [==============================] - 0s 286us/step - loss: 25399.4095 - mean_absolute_error: 25399.4095 - val_loss: 25492.1678 - val_mean_absolute_error: 25492.1678\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 16632.13696\n",
      "Epoch 1193/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 26213.7121 - mean_absolute_error: 26213.7121 - val_loss: 19052.9155 - val_mean_absolute_error: 19052.9155\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 16632.13696\n",
      "Epoch 1194/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25501.9618 - mean_absolute_error: 25501.9618 - val_loss: 19181.8842 - val_mean_absolute_error: 19181.8842\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 16632.13696\n",
      "Epoch 1195/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 27074.9145 - mean_absolute_error: 27074.9145 - val_loss: 17025.8071 - val_mean_absolute_error: 17025.8071\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 16632.13696\n",
      "Epoch 1196/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25370.7393 - mean_absolute_error: 25370.7393 - val_loss: 17818.1890 - val_mean_absolute_error: 17818.1890\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 16632.13696\n",
      "Epoch 1197/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 24684.4077 - mean_absolute_error: 24684.4077 - val_loss: 18088.9919 - val_mean_absolute_error: 18088.9919\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 16632.13696\n",
      "Epoch 1198/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 25417.4914 - mean_absolute_error: 25417.4914 - val_loss: 25140.6212 - val_mean_absolute_error: 25140.6212\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 16632.13696\n",
      "Epoch 1199/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26378.4652 - mean_absolute_error: 26378.4652 - val_loss: 35180.9595 - val_mean_absolute_error: 35180.9595\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 16632.13696\n",
      "Epoch 1200/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26511.5248 - mean_absolute_error: 26511.5248 - val_loss: 20859.1719 - val_mean_absolute_error: 20859.1719\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 16632.13696\n",
      "Epoch 1201/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 24938.3257 - mean_absolute_error: 24938.3257 - val_loss: 24922.6374 - val_mean_absolute_error: 24922.6374\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 16632.13696\n",
      "Epoch 1202/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25050.8703 - mean_absolute_error: 25050.8703 - val_loss: 18332.4912 - val_mean_absolute_error: 18332.4912\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 16632.13696\n",
      "Epoch 1203/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25459.2032 - mean_absolute_error: 25459.2032 - val_loss: 26835.3344 - val_mean_absolute_error: 26835.3344\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 16632.13696\n",
      "Epoch 1204/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25121.2385 - mean_absolute_error: 25121.2385 - val_loss: 22519.8493 - val_mean_absolute_error: 22519.8493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01204: val_loss did not improve from 16632.13696\n",
      "Epoch 1205/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 24602.7883 - mean_absolute_error: 24602.7883 - val_loss: 28496.1563 - val_mean_absolute_error: 28496.1563\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 16632.13696\n",
      "Epoch 1206/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25943.8546 - mean_absolute_error: 25943.8546 - val_loss: 17534.0180 - val_mean_absolute_error: 17534.0180\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 16632.13696\n",
      "Epoch 1207/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26564.7802 - mean_absolute_error: 26564.7802 - val_loss: 23852.8051 - val_mean_absolute_error: 23852.8051\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 16632.13696\n",
      "Epoch 1208/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25543.3049 - mean_absolute_error: 25543.3049 - val_loss: 21780.0145 - val_mean_absolute_error: 21780.0145\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 16632.13696\n",
      "Epoch 1209/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26226.7088 - mean_absolute_error: 26226.7088 - val_loss: 29464.2220 - val_mean_absolute_error: 29464.2220\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 16632.13696\n",
      "Epoch 1210/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24550.1722 - mean_absolute_error: 24550.1722 - val_loss: 26186.2797 - val_mean_absolute_error: 26186.2797\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 16632.13696\n",
      "Epoch 1211/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26227.1688 - mean_absolute_error: 26227.1688 - val_loss: 19560.1615 - val_mean_absolute_error: 19560.1615\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 16632.13696\n",
      "Epoch 1212/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26608.2469 - mean_absolute_error: 26608.2469 - val_loss: 18409.5221 - val_mean_absolute_error: 18409.5221\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 16632.13696\n",
      "Epoch 1213/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24676.5967 - mean_absolute_error: 24676.5967 - val_loss: 25253.0694 - val_mean_absolute_error: 25253.0694\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 16632.13696\n",
      "Epoch 1214/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25659.4141 - mean_absolute_error: 25659.4141 - val_loss: 23544.8288 - val_mean_absolute_error: 23544.8288\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 16632.13696\n",
      "Epoch 1215/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 26960.6087 - mean_absolute_error: 26960.6087 - val_loss: 18943.2378 - val_mean_absolute_error: 18943.2378\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 16632.13696\n",
      "Epoch 1216/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26799.8081 - mean_absolute_error: 26799.8081 - val_loss: 18493.2703 - val_mean_absolute_error: 18493.2703\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 16632.13696\n",
      "Epoch 1217/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26045.0995 - mean_absolute_error: 26045.0995 - val_loss: 20303.8692 - val_mean_absolute_error: 20303.8692\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 16632.13696\n",
      "Epoch 1218/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 24815.3072 - mean_absolute_error: 24815.3072 - val_loss: 26054.2319 - val_mean_absolute_error: 26054.2319\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 16632.13696\n",
      "Epoch 1219/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26135.6772 - mean_absolute_error: 26135.6772 - val_loss: 17864.7976 - val_mean_absolute_error: 17864.7976\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 16632.13696\n",
      "Epoch 1220/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26800.0233 - mean_absolute_error: 26800.0233 - val_loss: 17902.1676 - val_mean_absolute_error: 17902.1676\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 16632.13696\n",
      "Epoch 1221/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25856.4076 - mean_absolute_error: 25856.4076 - val_loss: 17560.4018 - val_mean_absolute_error: 17560.4018\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 16632.13696\n",
      "Epoch 1222/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25705.7046 - mean_absolute_error: 25705.7046 - val_loss: 35867.8735 - val_mean_absolute_error: 35867.8735\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 16632.13696\n",
      "Epoch 1223/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25074.5259 - mean_absolute_error: 25074.5259 - val_loss: 18603.1044 - val_mean_absolute_error: 18603.1044\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 16632.13696\n",
      "Epoch 1224/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26593.0159 - mean_absolute_error: 26593.0159 - val_loss: 21234.3195 - val_mean_absolute_error: 21234.3195\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 16632.13696\n",
      "Epoch 1225/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26294.2635 - mean_absolute_error: 26294.2635 - val_loss: 25430.6539 - val_mean_absolute_error: 25430.6539\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 16632.13696\n",
      "Epoch 1226/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 25714.0721 - mean_absolute_error: 25714.0721 - val_loss: 28168.2835 - val_mean_absolute_error: 28168.2835\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 16632.13696\n",
      "Epoch 1227/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25429.9061 - mean_absolute_error: 25429.9061 - val_loss: 26993.8550 - val_mean_absolute_error: 26993.8550\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 16632.13696\n",
      "Epoch 1228/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 26445.2379 - mean_absolute_error: 26445.2379 - val_loss: 20321.9337 - val_mean_absolute_error: 20321.9337\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 16632.13696\n",
      "Epoch 1229/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25561.8515 - mean_absolute_error: 25561.8515 - val_loss: 23098.7658 - val_mean_absolute_error: 23098.7658\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 16632.13696\n",
      "Epoch 1230/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25473.3965 - mean_absolute_error: 25473.3965 - val_loss: 39148.1148 - val_mean_absolute_error: 39148.1148\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 16632.13696\n",
      "Epoch 1231/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26303.7948 - mean_absolute_error: 26303.7948 - val_loss: 28637.1372 - val_mean_absolute_error: 28637.1372\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 16632.13696\n",
      "Epoch 1232/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25533.0058 - mean_absolute_error: 25533.0058 - val_loss: 24743.1454 - val_mean_absolute_error: 24743.1454\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 16632.13696\n",
      "Epoch 1233/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25028.0671 - mean_absolute_error: 25028.0671 - val_loss: 22570.8919 - val_mean_absolute_error: 22570.8919\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 16632.13696\n",
      "Epoch 1234/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26405.2962 - mean_absolute_error: 26405.2962 - val_loss: 20044.7022 - val_mean_absolute_error: 20044.7022\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 16632.13696\n",
      "Epoch 1235/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25808.0826 - mean_absolute_error: 25808.0826 - val_loss: 19612.7687 - val_mean_absolute_error: 19612.7687\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 16632.13696\n",
      "Epoch 1236/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 25185.5103 - mean_absolute_error: 25185.5103 - val_loss: 22955.8255 - val_mean_absolute_error: 22955.8255\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 16632.13696\n",
      "Epoch 1237/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25242.2190 - mean_absolute_error: 25242.2190 - val_loss: 19951.2225 - val_mean_absolute_error: 19951.2225\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 16632.13696\n",
      "Epoch 1238/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 27555.8276 - mean_absolute_error: 27555.8276 - val_loss: 22071.2749 - val_mean_absolute_error: 22071.2749\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 16632.13696\n",
      "Epoch 1239/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25410.0401 - mean_absolute_error: 25410.0401 - val_loss: 21182.7889 - val_mean_absolute_error: 21182.7889\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 16632.13696\n",
      "Epoch 1240/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25085.8354 - mean_absolute_error: 25085.8354 - val_loss: 19274.6946 - val_mean_absolute_error: 19274.6946\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 16632.13696\n",
      "Epoch 1241/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25709.1542 - mean_absolute_error: 25709.1542 - val_loss: 26687.3221 - val_mean_absolute_error: 26687.3221\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 16632.13696\n",
      "Epoch 1242/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26932.4983 - mean_absolute_error: 26932.4983 - val_loss: 25445.0228 - val_mean_absolute_error: 25445.0228\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 16632.13696\n",
      "Epoch 1243/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 26354.3417 - mean_absolute_error: 26354.3417 - val_loss: 25134.3560 - val_mean_absolute_error: 25134.3560\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 16632.13696\n",
      "Epoch 1244/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 24856.9944 - mean_absolute_error: 24856.9944 - val_loss: 20109.4823 - val_mean_absolute_error: 20109.4823\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 16632.13696\n",
      "Epoch 1245/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25238.1558 - mean_absolute_error: 25238.1558 - val_loss: 22751.3409 - val_mean_absolute_error: 22751.3409\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 16632.13696\n",
      "Epoch 1246/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25793.9315 - mean_absolute_error: 25793.9315 - val_loss: 29799.4597 - val_mean_absolute_error: 29799.4597\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 16632.13696\n",
      "Epoch 1247/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 27222.5727 - mean_absolute_error: 27222.5727 - val_loss: 23705.0381 - val_mean_absolute_error: 23705.0381\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 16632.13696\n",
      "Epoch 1248/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 24823.4659 - mean_absolute_error: 24823.4659 - val_loss: 19711.8838 - val_mean_absolute_error: 19711.8838\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 16632.13696\n",
      "Epoch 1249/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 24186.4669 - mean_absolute_error: 24186.4669 - val_loss: 21439.3724 - val_mean_absolute_error: 21439.3724\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 16632.13696\n",
      "Epoch 1250/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25986.5208 - mean_absolute_error: 25986.5208 - val_loss: 18093.9398 - val_mean_absolute_error: 18093.9398\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 16632.13696\n",
      "Epoch 1251/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26078.6468 - mean_absolute_error: 26078.6468 - val_loss: 17624.7216 - val_mean_absolute_error: 17624.7216\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 16632.13696\n",
      "Epoch 1252/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25001.8607 - mean_absolute_error: 25001.8607 - val_loss: 18245.2119 - val_mean_absolute_error: 18245.2119\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 16632.13696\n",
      "Epoch 1253/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25547.9842 - mean_absolute_error: 25547.9842 - val_loss: 26025.4084 - val_mean_absolute_error: 26025.4084\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 16632.13696\n",
      "Epoch 1254/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25732.9030 - mean_absolute_error: 25732.9030 - val_loss: 20067.1231 - val_mean_absolute_error: 20067.1231\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 16632.13696\n",
      "Epoch 1255/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25694.9664 - mean_absolute_error: 25694.9664 - val_loss: 19097.7752 - val_mean_absolute_error: 19097.7752\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 16632.13696\n",
      "Epoch 1256/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25982.6904 - mean_absolute_error: 25982.6904 - val_loss: 17439.6278 - val_mean_absolute_error: 17439.6278\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 16632.13696\n",
      "Epoch 1257/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25270.9953 - mean_absolute_error: 25270.9953 - val_loss: 23605.0230 - val_mean_absolute_error: 23605.0230\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 16632.13696\n",
      "Epoch 1258/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 24412.9351 - mean_absolute_error: 24412.9351 - val_loss: 21129.5812 - val_mean_absolute_error: 21129.5812\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 16632.13696\n",
      "Epoch 1259/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 25589.6888 - mean_absolute_error: 25589.6888 - val_loss: 26414.6072 - val_mean_absolute_error: 26414.6072\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 16632.13696\n",
      "Epoch 1260/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25787.9851 - mean_absolute_error: 25787.9851 - val_loss: 17881.7113 - val_mean_absolute_error: 17881.7113\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 16632.13696\n",
      "Epoch 1261/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25685.7734 - mean_absolute_error: 25685.7734 - val_loss: 21373.8459 - val_mean_absolute_error: 21373.8459\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 16632.13696\n",
      "Epoch 1262/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25310.6995 - mean_absolute_error: 25310.6995 - val_loss: 23754.0378 - val_mean_absolute_error: 23754.0378\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 16632.13696\n",
      "Epoch 1263/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 26301.1648 - mean_absolute_error: 26301.1648 - val_loss: 22494.3230 - val_mean_absolute_error: 22494.3230\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 16632.13696\n",
      "Epoch 1264/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25523.8155 - mean_absolute_error: 25523.8155 - val_loss: 22040.5143 - val_mean_absolute_error: 22040.5143\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 16632.13696\n",
      "Epoch 1265/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24270.9409 - mean_absolute_error: 24270.9409 - val_loss: 20477.9685 - val_mean_absolute_error: 20477.9685\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 16632.13696\n",
      "Epoch 1266/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25225.3355 - mean_absolute_error: 25225.3355 - val_loss: 22618.9079 - val_mean_absolute_error: 22618.9079\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 16632.13696\n",
      "Epoch 1267/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 24768.4101 - mean_absolute_error: 24768.4101 - val_loss: 28296.7455 - val_mean_absolute_error: 28296.7455\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 16632.13696\n",
      "Epoch 1268/1500\n",
      "934/934 [==============================] - 0s 301us/step - loss: 24905.7704 - mean_absolute_error: 24905.7704 - val_loss: 23923.8159 - val_mean_absolute_error: 23923.8159\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 16632.13696\n",
      "Epoch 1269/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25390.6418 - mean_absolute_error: 25390.6418 - val_loss: 22527.9177 - val_mean_absolute_error: 22527.9177\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 16632.13696\n",
      "Epoch 1270/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 26093.9760 - mean_absolute_error: 26093.9760 - val_loss: 19019.0361 - val_mean_absolute_error: 19019.0361\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 16632.13696\n",
      "Epoch 1271/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25951.0239 - mean_absolute_error: 25951.0239 - val_loss: 17044.5560 - val_mean_absolute_error: 17044.5560\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 16632.13696\n",
      "Epoch 1272/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 270us/step - loss: 25730.6197 - mean_absolute_error: 25730.6197 - val_loss: 25021.3624 - val_mean_absolute_error: 25021.3624\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 16632.13696\n",
      "Epoch 1273/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27247.2161 - mean_absolute_error: 27247.2161 - val_loss: 33751.1406 - val_mean_absolute_error: 33751.1406\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 16632.13696\n",
      "Epoch 1274/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26529.7164 - mean_absolute_error: 26529.7164 - val_loss: 22614.8667 - val_mean_absolute_error: 22614.8667\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 16632.13696\n",
      "Epoch 1275/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25117.8285 - mean_absolute_error: 25117.8285 - val_loss: 24959.6984 - val_mean_absolute_error: 24959.6984\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 16632.13696\n",
      "Epoch 1276/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26750.0454 - mean_absolute_error: 26750.0454 - val_loss: 18712.2218 - val_mean_absolute_error: 18712.2218\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 16632.13696\n",
      "Epoch 1277/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 24832.7617 - mean_absolute_error: 24832.7617 - val_loss: 19839.7275 - val_mean_absolute_error: 19839.7275\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 16632.13696\n",
      "Epoch 1278/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 25807.1391 - mean_absolute_error: 25807.1391 - val_loss: 22921.0316 - val_mean_absolute_error: 22921.0316\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 16632.13696\n",
      "Epoch 1279/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25907.7904 - mean_absolute_error: 25907.7904 - val_loss: 17505.0342 - val_mean_absolute_error: 17505.0342\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 16632.13696\n",
      "Epoch 1280/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25953.4545 - mean_absolute_error: 25953.4545 - val_loss: 16820.3897 - val_mean_absolute_error: 16820.3897\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 16632.13696\n",
      "Epoch 1281/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25359.5010 - mean_absolute_error: 25359.5010 - val_loss: 24575.7108 - val_mean_absolute_error: 24575.7108\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 16632.13696\n",
      "Epoch 1282/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 26304.2151 - mean_absolute_error: 26304.2151 - val_loss: 25132.9570 - val_mean_absolute_error: 25132.9570\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 16632.13696\n",
      "Epoch 1283/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25830.3714 - mean_absolute_error: 25830.3714 - val_loss: 22195.4513 - val_mean_absolute_error: 22195.4513\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 16632.13696\n",
      "Epoch 1284/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25471.4417 - mean_absolute_error: 25471.4417 - val_loss: 28113.6364 - val_mean_absolute_error: 28113.6364\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 16632.13696\n",
      "Epoch 1285/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 24915.0610 - mean_absolute_error: 24915.0610 - val_loss: 22663.3800 - val_mean_absolute_error: 22663.3800\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 16632.13696\n",
      "Epoch 1286/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25642.7638 - mean_absolute_error: 25642.7638 - val_loss: 23906.6969 - val_mean_absolute_error: 23906.6969\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 16632.13696\n",
      "Epoch 1287/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25912.8685 - mean_absolute_error: 25912.8685 - val_loss: 20336.8599 - val_mean_absolute_error: 20336.8599\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 16632.13696\n",
      "Epoch 1288/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26068.6921 - mean_absolute_error: 26068.6921 - val_loss: 19917.3086 - val_mean_absolute_error: 19917.3086\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 16632.13696\n",
      "Epoch 1289/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25644.0007 - mean_absolute_error: 25644.0007 - val_loss: 23128.0139 - val_mean_absolute_error: 23128.0139\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 16632.13696\n",
      "Epoch 1290/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 26959.9408 - mean_absolute_error: 26959.9408 - val_loss: 19557.2940 - val_mean_absolute_error: 19557.2940\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 16632.13696\n",
      "Epoch 1291/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25778.4511 - mean_absolute_error: 25778.4511 - val_loss: 18394.9269 - val_mean_absolute_error: 18394.9269\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 16632.13696\n",
      "Epoch 1292/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25372.1620 - mean_absolute_error: 25372.1620 - val_loss: 23745.5824 - val_mean_absolute_error: 23745.5824\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 16632.13696\n",
      "Epoch 1293/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25181.0825 - mean_absolute_error: 25181.0825 - val_loss: 22341.3874 - val_mean_absolute_error: 22341.3874\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 16632.13696\n",
      "Epoch 1294/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 26022.7511 - mean_absolute_error: 26022.7511 - val_loss: 20845.4439 - val_mean_absolute_error: 20845.4439\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 16632.13696\n",
      "Epoch 1295/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26088.5014 - mean_absolute_error: 26088.5014 - val_loss: 25893.0186 - val_mean_absolute_error: 25893.0186\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 16632.13696\n",
      "Epoch 1296/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25878.0964 - mean_absolute_error: 25878.0964 - val_loss: 22107.7647 - val_mean_absolute_error: 22107.7647\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 16632.13696\n",
      "Epoch 1297/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 25731.3052 - mean_absolute_error: 25731.3052 - val_loss: 19872.2023 - val_mean_absolute_error: 19872.2023\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 16632.13696\n",
      "Epoch 1298/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 26095.5267 - mean_absolute_error: 26095.5267 - val_loss: 25684.5782 - val_mean_absolute_error: 25684.5782\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 16632.13696\n",
      "Epoch 1299/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26028.5047 - mean_absolute_error: 26028.5047 - val_loss: 28673.3515 - val_mean_absolute_error: 28673.3515\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 16632.13696\n",
      "Epoch 1300/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25439.6932 - mean_absolute_error: 25439.6932 - val_loss: 22420.2082 - val_mean_absolute_error: 22420.2082\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 16632.13696\n",
      "Epoch 1301/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26337.5521 - mean_absolute_error: 26337.5521 - val_loss: 32879.8421 - val_mean_absolute_error: 32879.8421\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 16632.13696\n",
      "Epoch 1302/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26163.9977 - mean_absolute_error: 26163.9977 - val_loss: 25763.2919 - val_mean_absolute_error: 25763.2919\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 16632.13696\n",
      "Epoch 1303/1500\n",
      "934/934 [==============================] - 0s 280us/step - loss: 26091.1786 - mean_absolute_error: 26091.1786 - val_loss: 29311.8967 - val_mean_absolute_error: 29311.8967\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 16632.13696\n",
      "Epoch 1304/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26572.7655 - mean_absolute_error: 26572.7655 - val_loss: 17845.7095 - val_mean_absolute_error: 17845.7095\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 16632.13696\n",
      "Epoch 1305/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25963.6864 - mean_absolute_error: 25963.6864 - val_loss: 17235.4532 - val_mean_absolute_error: 17235.4532\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 16632.13696\n",
      "Epoch 1306/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 271us/step - loss: 25537.0869 - mean_absolute_error: 25537.0869 - val_loss: 24280.9617 - val_mean_absolute_error: 24280.9617\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 16632.13696\n",
      "Epoch 1307/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26530.8153 - mean_absolute_error: 26530.8153 - val_loss: 23987.4984 - val_mean_absolute_error: 23987.4984\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 16632.13696\n",
      "Epoch 1308/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25854.6597 - mean_absolute_error: 25854.6597 - val_loss: 18314.3998 - val_mean_absolute_error: 18314.3998\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 16632.13696\n",
      "Epoch 1309/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26676.5107 - mean_absolute_error: 26676.5107 - val_loss: 25029.0509 - val_mean_absolute_error: 25029.0509\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 16632.13696\n",
      "Epoch 1310/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25148.3758 - mean_absolute_error: 25148.3758 - val_loss: 16869.5478 - val_mean_absolute_error: 16869.5478\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 16632.13696\n",
      "Epoch 1311/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25974.5752 - mean_absolute_error: 25974.5752 - val_loss: 19246.2093 - val_mean_absolute_error: 19246.2093\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 16632.13696\n",
      "Epoch 1312/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26141.3273 - mean_absolute_error: 26141.3273 - val_loss: 29371.8038 - val_mean_absolute_error: 29371.8038\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 16632.13696\n",
      "Epoch 1313/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26622.3187 - mean_absolute_error: 26622.3187 - val_loss: 21986.4793 - val_mean_absolute_error: 21986.4793\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 16632.13696\n",
      "Epoch 1314/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25838.5395 - mean_absolute_error: 25838.5395 - val_loss: 19559.1344 - val_mean_absolute_error: 19559.1344\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 16632.13696\n",
      "Epoch 1315/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25233.6184 - mean_absolute_error: 25233.6184 - val_loss: 19299.3997 - val_mean_absolute_error: 19299.3997\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 16632.13696\n",
      "Epoch 1316/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24582.7019 - mean_absolute_error: 24582.7019 - val_loss: 23659.3735 - val_mean_absolute_error: 23659.3735\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 16632.13696\n",
      "Epoch 1317/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25798.3015 - mean_absolute_error: 25798.3015 - val_loss: 17485.6651 - val_mean_absolute_error: 17485.6651\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 16632.13696\n",
      "Epoch 1318/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25647.3703 - mean_absolute_error: 25647.3703 - val_loss: 25518.2055 - val_mean_absolute_error: 25518.2055\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 16632.13696\n",
      "Epoch 1319/1500\n",
      "934/934 [==============================] - 0s 279us/step - loss: 25050.3534 - mean_absolute_error: 25050.3534 - val_loss: 22573.0395 - val_mean_absolute_error: 22573.0395\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 16632.13696\n",
      "Epoch 1320/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25640.1188 - mean_absolute_error: 25640.1188 - val_loss: 16917.5427 - val_mean_absolute_error: 16917.5427\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 16632.13696\n",
      "Epoch 1321/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25110.3844 - mean_absolute_error: 25110.3844 - val_loss: 21504.0157 - val_mean_absolute_error: 21504.0157\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 16632.13696\n",
      "Epoch 1322/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 26103.3863 - mean_absolute_error: 26103.3863 - val_loss: 20161.5843 - val_mean_absolute_error: 20161.5843\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 16632.13696\n",
      "Epoch 1323/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25284.7136 - mean_absolute_error: 25284.7136 - val_loss: 19518.1562 - val_mean_absolute_error: 19518.1562\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 16632.13696\n",
      "Epoch 1324/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27689.1501 - mean_absolute_error: 27689.1501 - val_loss: 25449.2343 - val_mean_absolute_error: 25449.2343\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 16632.13696\n",
      "Epoch 1325/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26087.4571 - mean_absolute_error: 26087.4571 - val_loss: 18967.2137 - val_mean_absolute_error: 18967.2137\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 16632.13696\n",
      "Epoch 1326/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25267.5891 - mean_absolute_error: 25267.5891 - val_loss: 31602.1991 - val_mean_absolute_error: 31602.1991\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 16632.13696\n",
      "Epoch 1327/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25010.0211 - mean_absolute_error: 25010.0211 - val_loss: 20856.3562 - val_mean_absolute_error: 20856.3562\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 16632.13696\n",
      "Epoch 1328/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25469.7505 - mean_absolute_error: 25469.7505 - val_loss: 17175.5240 - val_mean_absolute_error: 17175.5240\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 16632.13696\n",
      "Epoch 1329/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26823.5733 - mean_absolute_error: 26823.5733 - val_loss: 23430.6306 - val_mean_absolute_error: 23430.6306\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 16632.13696\n",
      "Epoch 1330/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 24154.1810 - mean_absolute_error: 24154.1810 - val_loss: 22371.9636 - val_mean_absolute_error: 22371.9636\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 16632.13696\n",
      "Epoch 1331/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25451.3731 - mean_absolute_error: 25451.3731 - val_loss: 17527.8851 - val_mean_absolute_error: 17527.8851\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 16632.13696\n",
      "Epoch 1332/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25493.6941 - mean_absolute_error: 25493.6941 - val_loss: 17924.0803 - val_mean_absolute_error: 17924.0803\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 16632.13696\n",
      "Epoch 1333/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25507.3507 - mean_absolute_error: 25507.3507 - val_loss: 23835.7787 - val_mean_absolute_error: 23835.7787\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 16632.13696\n",
      "Epoch 1334/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25268.4278 - mean_absolute_error: 25268.4278 - val_loss: 26846.5406 - val_mean_absolute_error: 26846.5406\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 16632.13696\n",
      "Epoch 1335/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26172.0561 - mean_absolute_error: 26172.0561 - val_loss: 30639.6960 - val_mean_absolute_error: 30639.6960\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 16632.13696\n",
      "Epoch 1336/1500\n",
      "934/934 [==============================] - 0s 281us/step - loss: 26232.7681 - mean_absolute_error: 26232.7681 - val_loss: 20353.5102 - val_mean_absolute_error: 20353.5102\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 16632.13696\n",
      "Epoch 1337/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25262.2872 - mean_absolute_error: 25262.2872 - val_loss: 19136.6266 - val_mean_absolute_error: 19136.6266\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 16632.13696\n",
      "Epoch 1338/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25030.2192 - mean_absolute_error: 25030.2192 - val_loss: 27550.2538 - val_mean_absolute_error: 27550.2538\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 16632.13696\n",
      "Epoch 1339/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25253.1721 - mean_absolute_error: 25253.1721 - val_loss: 35115.1809 - val_mean_absolute_error: 35115.1809\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 16632.13696\n",
      "Epoch 1340/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 259us/step - loss: 24274.6483 - mean_absolute_error: 24274.6483 - val_loss: 19381.2490 - val_mean_absolute_error: 19381.2490\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 16632.13696\n",
      "Epoch 1341/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26630.9062 - mean_absolute_error: 26630.9062 - val_loss: 19527.1399 - val_mean_absolute_error: 19527.1399\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 16632.13696\n",
      "Epoch 1342/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26172.1381 - mean_absolute_error: 26172.1381 - val_loss: 22146.9135 - val_mean_absolute_error: 22146.9135\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 16632.13696\n",
      "Epoch 1343/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25455.4793 - mean_absolute_error: 25455.4793 - val_loss: 17924.9895 - val_mean_absolute_error: 17924.9895\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 16632.13696\n",
      "Epoch 1344/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25508.7488 - mean_absolute_error: 25508.7488 - val_loss: 23162.4855 - val_mean_absolute_error: 23162.4855\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 16632.13696\n",
      "Epoch 1345/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25939.9091 - mean_absolute_error: 25939.9091 - val_loss: 18096.1274 - val_mean_absolute_error: 18096.1274\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 16632.13696\n",
      "Epoch 1346/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26939.7409 - mean_absolute_error: 26939.7409 - val_loss: 24985.8457 - val_mean_absolute_error: 24985.8457\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 16632.13696\n",
      "Epoch 1347/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 25379.1058 - mean_absolute_error: 25379.1058 - val_loss: 30938.5317 - val_mean_absolute_error: 30938.5317\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 16632.13696\n",
      "Epoch 1348/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25232.5375 - mean_absolute_error: 25232.5375 - val_loss: 21245.7753 - val_mean_absolute_error: 21245.7753\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 16632.13696\n",
      "Epoch 1349/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24718.7575 - mean_absolute_error: 24718.7575 - val_loss: 17819.4500 - val_mean_absolute_error: 17819.4500\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 16632.13696\n",
      "Epoch 1350/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26044.1389 - mean_absolute_error: 26044.1389 - val_loss: 22699.4616 - val_mean_absolute_error: 22699.4616\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 16632.13696\n",
      "Epoch 1351/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 24909.9903 - mean_absolute_error: 24909.9903 - val_loss: 18740.1908 - val_mean_absolute_error: 18740.1908\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 16632.13696\n",
      "Epoch 1352/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 24479.0948 - mean_absolute_error: 24479.0948 - val_loss: 20935.1819 - val_mean_absolute_error: 20935.1819\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 16632.13696\n",
      "Epoch 1353/1500\n",
      "934/934 [==============================] - 0s 287us/step - loss: 25201.7195 - mean_absolute_error: 25201.7195 - val_loss: 21879.0941 - val_mean_absolute_error: 21879.0941\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 16632.13696\n",
      "Epoch 1354/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25037.8292 - mean_absolute_error: 25037.8292 - val_loss: 29639.2723 - val_mean_absolute_error: 29639.2723\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 16632.13696\n",
      "Epoch 1355/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 26172.2909 - mean_absolute_error: 26172.2909 - val_loss: 21919.0065 - val_mean_absolute_error: 21919.0065\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 16632.13696\n",
      "Epoch 1356/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 24126.5388 - mean_absolute_error: 24126.5388 - val_loss: 33144.4953 - val_mean_absolute_error: 33144.4953\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 16632.13696\n",
      "Epoch 1357/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26150.5114 - mean_absolute_error: 26150.5114 - val_loss: 18128.0746 - val_mean_absolute_error: 18128.0746\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 16632.13696\n",
      "Epoch 1358/1500\n",
      "934/934 [==============================] - 0s 289us/step - loss: 25682.1025 - mean_absolute_error: 25682.1025 - val_loss: 19119.7638 - val_mean_absolute_error: 19119.7638\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 16632.13696\n",
      "Epoch 1359/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 25935.1264 - mean_absolute_error: 25935.1264 - val_loss: 18780.0753 - val_mean_absolute_error: 18780.0753\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 16632.13696\n",
      "Epoch 1360/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26858.9601 - mean_absolute_error: 26858.9601 - val_loss: 18878.9358 - val_mean_absolute_error: 18878.9358\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 16632.13696\n",
      "Epoch 1361/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 25309.7922 - mean_absolute_error: 25309.7922 - val_loss: 22127.4252 - val_mean_absolute_error: 22127.4252\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 16632.13696\n",
      "Epoch 1362/1500\n",
      "934/934 [==============================] - 0s 249us/step - loss: 25866.6252 - mean_absolute_error: 25866.6252 - val_loss: 21446.3149 - val_mean_absolute_error: 21446.3149\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 16632.13696\n",
      "Epoch 1363/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25511.8884 - mean_absolute_error: 25511.8884 - val_loss: 18258.5160 - val_mean_absolute_error: 18258.5160\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 16632.13696\n",
      "Epoch 1364/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 24949.8281 - mean_absolute_error: 24949.8281 - val_loss: 17267.4256 - val_mean_absolute_error: 17267.4256\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 16632.13696\n",
      "Epoch 1365/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25203.7041 - mean_absolute_error: 25203.7041 - val_loss: 24228.1197 - val_mean_absolute_error: 24228.1197\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 16632.13696\n",
      "Epoch 1366/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25167.0120 - mean_absolute_error: 25167.0120 - val_loss: 17284.5938 - val_mean_absolute_error: 17284.5938\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 16632.13696\n",
      "Epoch 1367/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25070.5139 - mean_absolute_error: 25070.5139 - val_loss: 19208.2641 - val_mean_absolute_error: 19208.2641\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 16632.13696\n",
      "Epoch 1368/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 24722.3887 - mean_absolute_error: 24722.3887 - val_loss: 17255.0205 - val_mean_absolute_error: 17255.0205\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 16632.13696\n",
      "Epoch 1369/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25856.9906 - mean_absolute_error: 25856.9906 - val_loss: 19661.9410 - val_mean_absolute_error: 19661.9410\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 16632.13696\n",
      "Epoch 1370/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25496.9293 - mean_absolute_error: 25496.9293 - val_loss: 19929.0267 - val_mean_absolute_error: 19929.0267\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 16632.13696\n",
      "Epoch 1371/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24793.5876 - mean_absolute_error: 24793.5876 - val_loss: 22341.2013 - val_mean_absolute_error: 22341.2013\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 16632.13696\n",
      "Epoch 1372/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26019.0620 - mean_absolute_error: 26019.0620 - val_loss: 23484.2985 - val_mean_absolute_error: 23484.2985\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 16632.13696\n",
      "Epoch 1373/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25792.2523 - mean_absolute_error: 25792.2523 - val_loss: 19583.4039 - val_mean_absolute_error: 19583.4039\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 16632.13696\n",
      "Epoch 1374/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 263us/step - loss: 25273.8250 - mean_absolute_error: 25273.8250 - val_loss: 17710.6573 - val_mean_absolute_error: 17710.6573\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 16632.13696\n",
      "Epoch 1375/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25766.9889 - mean_absolute_error: 25766.9889 - val_loss: 20913.2896 - val_mean_absolute_error: 20913.2896\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 16632.13696\n",
      "Epoch 1376/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25053.3582 - mean_absolute_error: 25053.3582 - val_loss: 28506.8082 - val_mean_absolute_error: 28506.8082\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 16632.13696\n",
      "Epoch 1377/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25760.8639 - mean_absolute_error: 25760.8639 - val_loss: 21861.0092 - val_mean_absolute_error: 21861.0092\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 16632.13696\n",
      "Epoch 1378/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26026.4248 - mean_absolute_error: 26026.4248 - val_loss: 23672.7119 - val_mean_absolute_error: 23672.7119\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 16632.13696\n",
      "Epoch 1379/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 24702.6734 - mean_absolute_error: 24702.6734 - val_loss: 17435.2703 - val_mean_absolute_error: 17435.2703\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 16632.13696\n",
      "Epoch 1380/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24487.2873 - mean_absolute_error: 24487.2873 - val_loss: 18261.9620 - val_mean_absolute_error: 18261.9620\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 16632.13696\n",
      "Epoch 1381/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24407.3375 - mean_absolute_error: 24407.3375 - val_loss: 22298.4654 - val_mean_absolute_error: 22298.4654\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 16632.13696\n",
      "Epoch 1382/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25852.2810 - mean_absolute_error: 25852.2810 - val_loss: 19038.7102 - val_mean_absolute_error: 19038.7102\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 16632.13696\n",
      "Epoch 1383/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 26580.1996 - mean_absolute_error: 26580.1996 - val_loss: 21970.6944 - val_mean_absolute_error: 21970.6944\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 16632.13696\n",
      "Epoch 1384/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25383.7123 - mean_absolute_error: 25383.7123 - val_loss: 29938.1117 - val_mean_absolute_error: 29938.1117\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 16632.13696\n",
      "Epoch 1385/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25549.7291 - mean_absolute_error: 25549.7291 - val_loss: 19517.8678 - val_mean_absolute_error: 19517.8678\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 16632.13696\n",
      "Epoch 1386/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26373.3111 - mean_absolute_error: 26373.3111 - val_loss: 23070.1666 - val_mean_absolute_error: 23070.1666\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 16632.13696\n",
      "Epoch 1387/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 25418.7590 - mean_absolute_error: 25418.7590 - val_loss: 17748.1911 - val_mean_absolute_error: 17748.1911\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 16632.13696\n",
      "Epoch 1388/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26025.7295 - mean_absolute_error: 26025.7295 - val_loss: 17244.7901 - val_mean_absolute_error: 17244.7901\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 16632.13696\n",
      "Epoch 1389/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25048.8851 - mean_absolute_error: 25048.8851 - val_loss: 19931.8056 - val_mean_absolute_error: 19931.8056\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 16632.13696\n",
      "Epoch 1390/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 25648.0073 - mean_absolute_error: 25648.0073 - val_loss: 22781.9553 - val_mean_absolute_error: 22781.9553\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 16632.13696\n",
      "Epoch 1391/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 24950.3593 - mean_absolute_error: 24950.3593 - val_loss: 21776.5137 - val_mean_absolute_error: 21776.5137\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 16632.13696\n",
      "Epoch 1392/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25878.5475 - mean_absolute_error: 25878.5475 - val_loss: 24896.8786 - val_mean_absolute_error: 24896.8786\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 16632.13696\n",
      "Epoch 1393/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26540.2048 - mean_absolute_error: 26540.2048 - val_loss: 29912.3801 - val_mean_absolute_error: 29912.3801\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 16632.13696\n",
      "Epoch 1394/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 24634.1462 - mean_absolute_error: 24634.1462 - val_loss: 19304.8708 - val_mean_absolute_error: 19304.8708\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 16632.13696\n",
      "Epoch 1395/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 24890.1947 - mean_absolute_error: 24890.1947 - val_loss: 22055.1988 - val_mean_absolute_error: 22055.1988\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 16632.13696\n",
      "Epoch 1396/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24968.2198 - mean_absolute_error: 24968.2198 - val_loss: 25489.3440 - val_mean_absolute_error: 25489.3440\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 16632.13696\n",
      "Epoch 1397/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24555.1658 - mean_absolute_error: 24555.1658 - val_loss: 22819.3772 - val_mean_absolute_error: 22819.3772\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 16632.13696\n",
      "Epoch 1398/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25739.2323 - mean_absolute_error: 25739.2323 - val_loss: 18169.7018 - val_mean_absolute_error: 18169.7018\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 16632.13696\n",
      "Epoch 1399/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 24568.6226 - mean_absolute_error: 24568.6226 - val_loss: 19074.2069 - val_mean_absolute_error: 19074.2069\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 16632.13696\n",
      "Epoch 1400/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 24410.1607 - mean_absolute_error: 24410.1607 - val_loss: 18146.5781 - val_mean_absolute_error: 18146.5781\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 16632.13696\n",
      "Epoch 1401/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 26498.7374 - mean_absolute_error: 26498.7374 - val_loss: 21111.1262 - val_mean_absolute_error: 21111.1262\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 16632.13696\n",
      "Epoch 1402/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 26110.3241 - mean_absolute_error: 26110.3241 - val_loss: 19536.2700 - val_mean_absolute_error: 19536.2700\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 16632.13696\n",
      "Epoch 1403/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25470.1984 - mean_absolute_error: 25470.1984 - val_loss: 19876.9211 - val_mean_absolute_error: 19876.9211\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 16632.13696\n",
      "Epoch 1404/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 24955.7275 - mean_absolute_error: 24955.7275 - val_loss: 26491.1812 - val_mean_absolute_error: 26491.1812\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 16632.13696\n",
      "Epoch 1405/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25372.0230 - mean_absolute_error: 25372.0230 - val_loss: 24912.7057 - val_mean_absolute_error: 24912.7057\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 16632.13696\n",
      "Epoch 1406/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25866.6599 - mean_absolute_error: 25866.6599 - val_loss: 24076.2240 - val_mean_absolute_error: 24076.2240\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 16632.13696\n",
      "Epoch 1407/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25204.2974 - mean_absolute_error: 25204.2974 - val_loss: 22734.4100 - val_mean_absolute_error: 22734.4100\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 16632.13696\n",
      "Epoch 1408/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 259us/step - loss: 25878.6610 - mean_absolute_error: 25878.6610 - val_loss: 19118.2604 - val_mean_absolute_error: 19118.2604\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 16632.13696\n",
      "Epoch 1409/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25393.3690 - mean_absolute_error: 25393.3690 - val_loss: 21322.6799 - val_mean_absolute_error: 21322.6799\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 16632.13696\n",
      "Epoch 1410/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25135.0392 - mean_absolute_error: 25135.0392 - val_loss: 20356.2044 - val_mean_absolute_error: 20356.2044\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 16632.13696\n",
      "Epoch 1411/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26861.7086 - mean_absolute_error: 26861.7086 - val_loss: 36171.4420 - val_mean_absolute_error: 36171.4420\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 16632.13696\n",
      "Epoch 1412/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25147.9884 - mean_absolute_error: 25147.9884 - val_loss: 23883.2795 - val_mean_absolute_error: 23883.2795\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 16632.13696\n",
      "Epoch 1413/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25760.1691 - mean_absolute_error: 25760.1691 - val_loss: 20902.3278 - val_mean_absolute_error: 20902.3278\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 16632.13696\n",
      "Epoch 1414/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26080.2953 - mean_absolute_error: 26080.2953 - val_loss: 23835.8236 - val_mean_absolute_error: 23835.8236\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 16632.13696\n",
      "Epoch 1415/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25638.6889 - mean_absolute_error: 25638.6889 - val_loss: 17846.4665 - val_mean_absolute_error: 17846.4665\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 16632.13696\n",
      "Epoch 1416/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25731.2797 - mean_absolute_error: 25731.2797 - val_loss: 19030.5414 - val_mean_absolute_error: 19030.5414\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 16632.13696\n",
      "Epoch 1417/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25236.6851 - mean_absolute_error: 25236.6851 - val_loss: 17114.2833 - val_mean_absolute_error: 17114.2833\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 16632.13696\n",
      "Epoch 1418/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25396.6038 - mean_absolute_error: 25396.6038 - val_loss: 17033.8110 - val_mean_absolute_error: 17033.8110\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 16632.13696\n",
      "Epoch 1419/1500\n",
      "934/934 [==============================] - 0s 267us/step - loss: 24053.3618 - mean_absolute_error: 24053.3618 - val_loss: 26292.3047 - val_mean_absolute_error: 26292.3047\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 16632.13696\n",
      "Epoch 1420/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 26299.8238 - mean_absolute_error: 26299.8238 - val_loss: 17334.8000 - val_mean_absolute_error: 17334.8000\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 16632.13696\n",
      "Epoch 1421/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25255.2053 - mean_absolute_error: 25255.2053 - val_loss: 20781.1749 - val_mean_absolute_error: 20781.1749\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 16632.13696\n",
      "Epoch 1422/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 23369.9581 - mean_absolute_error: 23369.9581 - val_loss: 21722.9702 - val_mean_absolute_error: 21722.9702\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 16632.13696\n",
      "Epoch 1423/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25646.8602 - mean_absolute_error: 25646.8602 - val_loss: 17656.7106 - val_mean_absolute_error: 17656.7106\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 16632.13696\n",
      "Epoch 1424/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26017.9400 - mean_absolute_error: 26017.9400 - val_loss: 16805.5970 - val_mean_absolute_error: 16805.5970\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 16632.13696\n",
      "Epoch 1425/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25476.5841 - mean_absolute_error: 25476.5841 - val_loss: 31109.4288 - val_mean_absolute_error: 31109.4288\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 16632.13696\n",
      "Epoch 1426/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25594.8423 - mean_absolute_error: 25594.8423 - val_loss: 26526.2782 - val_mean_absolute_error: 26526.2782\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 16632.13696\n",
      "Epoch 1427/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25901.3499 - mean_absolute_error: 25901.3499 - val_loss: 20088.8878 - val_mean_absolute_error: 20088.8878\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 16632.13696\n",
      "Epoch 1428/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25486.2912 - mean_absolute_error: 25486.2912 - val_loss: 29438.7420 - val_mean_absolute_error: 29438.7420\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 16632.13696\n",
      "Epoch 1429/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 24705.5957 - mean_absolute_error: 24705.5957 - val_loss: 25711.0968 - val_mean_absolute_error: 25711.0968\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 16632.13696\n",
      "Epoch 1430/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 24840.5485 - mean_absolute_error: 24840.5485 - val_loss: 29729.5187 - val_mean_absolute_error: 29729.5187\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 16632.13696\n",
      "Epoch 1431/1500\n",
      "934/934 [==============================] - 0s 270us/step - loss: 25244.1152 - mean_absolute_error: 25244.1152 - val_loss: 21904.5030 - val_mean_absolute_error: 21904.5030\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 16632.13696\n",
      "Epoch 1432/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25879.0568 - mean_absolute_error: 25879.0568 - val_loss: 24149.5083 - val_mean_absolute_error: 24149.5083\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 16632.13696\n",
      "Epoch 1433/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 25250.2838 - mean_absolute_error: 25250.2838 - val_loss: 21991.0230 - val_mean_absolute_error: 21991.0230\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 16632.13696\n",
      "Epoch 1434/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 25668.4894 - mean_absolute_error: 25668.4894 - val_loss: 27406.8437 - val_mean_absolute_error: 27406.8437\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 16632.13696\n",
      "Epoch 1435/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 24844.7495 - mean_absolute_error: 24844.7495 - val_loss: 24596.6340 - val_mean_absolute_error: 24596.6340\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 16632.13696\n",
      "Epoch 1436/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 24885.3671 - mean_absolute_error: 24885.3671 - val_loss: 22505.9157 - val_mean_absolute_error: 22505.9157\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 16632.13696\n",
      "Epoch 1437/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24728.4741 - mean_absolute_error: 24728.4741 - val_loss: 22734.9622 - val_mean_absolute_error: 22734.9622\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 16632.13696\n",
      "Epoch 1438/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25118.1900 - mean_absolute_error: 25118.1900 - val_loss: 22436.9433 - val_mean_absolute_error: 22436.9433\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 16632.13696\n",
      "Epoch 1439/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25435.9070 - mean_absolute_error: 25435.9070 - val_loss: 17970.2752 - val_mean_absolute_error: 17970.2752\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 16632.13696\n",
      "Epoch 1440/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 23922.6061 - mean_absolute_error: 23922.6061 - val_loss: 21682.0417 - val_mean_absolute_error: 21682.0417\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 16632.13696\n",
      "Epoch 1441/1500\n",
      "934/934 [==============================] - 0s 278us/step - loss: 25001.5703 - mean_absolute_error: 25001.5703 - val_loss: 24830.4770 - val_mean_absolute_error: 24830.4770\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 16632.13696\n",
      "Epoch 1442/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 254us/step - loss: 25321.6234 - mean_absolute_error: 25321.6234 - val_loss: 30267.5463 - val_mean_absolute_error: 30267.5463\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 16632.13696\n",
      "Epoch 1443/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 25362.6144 - mean_absolute_error: 25362.6144 - val_loss: 17787.7524 - val_mean_absolute_error: 17787.7524\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 16632.13696\n",
      "Epoch 1444/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24974.3989 - mean_absolute_error: 24974.3989 - val_loss: 24932.8732 - val_mean_absolute_error: 24932.8732\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 16632.13696\n",
      "Epoch 1445/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 24440.9343 - mean_absolute_error: 24440.9343 - val_loss: 20813.7556 - val_mean_absolute_error: 20813.7556\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 16632.13696\n",
      "Epoch 1446/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 25184.5234 - mean_absolute_error: 25184.5234 - val_loss: 19325.9231 - val_mean_absolute_error: 19325.9231\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 16632.13696\n",
      "Epoch 1447/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 26428.3503 - mean_absolute_error: 26428.3503 - val_loss: 26117.9398 - val_mean_absolute_error: 26117.9398\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 16632.13696\n",
      "Epoch 1448/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 24378.0205 - mean_absolute_error: 24378.0205 - val_loss: 19771.7568 - val_mean_absolute_error: 19771.7568\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 16632.13696\n",
      "Epoch 1449/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25802.2872 - mean_absolute_error: 25802.2872 - val_loss: 17312.2760 - val_mean_absolute_error: 17312.2760\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 16632.13696\n",
      "Epoch 1450/1500\n",
      "934/934 [==============================] - 0s 272us/step - loss: 26012.6851 - mean_absolute_error: 26012.6851 - val_loss: 29530.5552 - val_mean_absolute_error: 29530.5552\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 16632.13696\n",
      "Epoch 1451/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 24762.6672 - mean_absolute_error: 24762.6672 - val_loss: 24389.0168 - val_mean_absolute_error: 24389.0168\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 16632.13696\n",
      "Epoch 1452/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26269.8464 - mean_absolute_error: 26269.8464 - val_loss: 16960.9868 - val_mean_absolute_error: 16960.9868\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 16632.13696\n",
      "Epoch 1453/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25974.7095 - mean_absolute_error: 25974.7095 - val_loss: 19942.0733 - val_mean_absolute_error: 19942.0733\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 16632.13696\n",
      "Epoch 1454/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 26094.1424 - mean_absolute_error: 26094.1424 - val_loss: 21523.8547 - val_mean_absolute_error: 21523.8547\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 16632.13696\n",
      "Epoch 1455/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25290.1337 - mean_absolute_error: 25290.1337 - val_loss: 17837.3810 - val_mean_absolute_error: 17837.3810\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 16632.13696\n",
      "Epoch 1456/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26253.7922 - mean_absolute_error: 26253.7922 - val_loss: 19717.3905 - val_mean_absolute_error: 19717.3905\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 16632.13696\n",
      "Epoch 1457/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25045.2570 - mean_absolute_error: 25045.2570 - val_loss: 23191.0378 - val_mean_absolute_error: 23191.0378\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 16632.13696\n",
      "Epoch 1458/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 25742.5329 - mean_absolute_error: 25742.5329 - val_loss: 18306.6942 - val_mean_absolute_error: 18306.6942\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 16632.13696\n",
      "Epoch 1459/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25625.6959 - mean_absolute_error: 25625.6959 - val_loss: 18782.1222 - val_mean_absolute_error: 18782.1222\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 16632.13696\n",
      "Epoch 1460/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 23959.1169 - mean_absolute_error: 23959.1169 - val_loss: 24743.3315 - val_mean_absolute_error: 24743.3315\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 16632.13696\n",
      "Epoch 1461/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 26064.5149 - mean_absolute_error: 26064.5149 - val_loss: 29097.7433 - val_mean_absolute_error: 29097.7433\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 16632.13696\n",
      "Epoch 1462/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 24160.4309 - mean_absolute_error: 24160.4309 - val_loss: 17056.2696 - val_mean_absolute_error: 17056.2696\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 16632.13696\n",
      "Epoch 1463/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 27131.5980 - mean_absolute_error: 27131.5980 - val_loss: 22910.3019 - val_mean_absolute_error: 22910.3019\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 16632.13696\n",
      "Epoch 1464/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25698.3560 - mean_absolute_error: 25698.3560 - val_loss: 23969.2900 - val_mean_absolute_error: 23969.2900\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 16632.13696\n",
      "Epoch 1465/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 25806.9974 - mean_absolute_error: 25806.9974 - val_loss: 21797.3979 - val_mean_absolute_error: 21797.3979\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 16632.13696\n",
      "Epoch 1466/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25551.2972 - mean_absolute_error: 25551.2972 - val_loss: 24889.6201 - val_mean_absolute_error: 24889.6201\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 16632.13696\n",
      "Epoch 1467/1500\n",
      "934/934 [==============================] - 0s 275us/step - loss: 25745.2970 - mean_absolute_error: 25745.2970 - val_loss: 20282.3294 - val_mean_absolute_error: 20282.3294\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 16632.13696\n",
      "Epoch 1468/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25703.7069 - mean_absolute_error: 25703.7069 - val_loss: 23832.2388 - val_mean_absolute_error: 23832.2388\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 16632.13696\n",
      "Epoch 1469/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25319.1122 - mean_absolute_error: 25319.1122 - val_loss: 17771.7807 - val_mean_absolute_error: 17771.7807\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 16632.13696\n",
      "Epoch 1470/1500\n",
      "934/934 [==============================] - 0s 266us/step - loss: 25447.9726 - mean_absolute_error: 25447.9726 - val_loss: 35897.9507 - val_mean_absolute_error: 35897.9507\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 16632.13696\n",
      "Epoch 1471/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 26380.6792 - mean_absolute_error: 26380.6792 - val_loss: 21337.9667 - val_mean_absolute_error: 21337.9667\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 16632.13696\n",
      "Epoch 1472/1500\n",
      "934/934 [==============================] - 0s 264us/step - loss: 26385.1939 - mean_absolute_error: 26385.1939 - val_loss: 23219.0175 - val_mean_absolute_error: 23219.0175\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 16632.13696\n",
      "Epoch 1473/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24803.0969 - mean_absolute_error: 24803.0969 - val_loss: 20499.8235 - val_mean_absolute_error: 20499.8235\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 16632.13696\n",
      "Epoch 1474/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25076.3381 - mean_absolute_error: 25076.3381 - val_loss: 24831.1127 - val_mean_absolute_error: 24831.1127\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 16632.13696\n",
      "Epoch 1475/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25114.4062 - mean_absolute_error: 25114.4062 - val_loss: 26365.6815 - val_mean_absolute_error: 26365.6815\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 16632.13696\n",
      "Epoch 1476/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 264us/step - loss: 24579.6252 - mean_absolute_error: 24579.6252 - val_loss: 28005.7725 - val_mean_absolute_error: 28005.7725\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 16632.13696\n",
      "Epoch 1477/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 25861.8617 - mean_absolute_error: 25861.8617 - val_loss: 16976.9958 - val_mean_absolute_error: 16976.9958\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 16632.13696\n",
      "Epoch 1478/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25193.6484 - mean_absolute_error: 25193.6484 - val_loss: 18984.8352 - val_mean_absolute_error: 18984.8352\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 16632.13696\n",
      "Epoch 1479/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 24288.1232 - mean_absolute_error: 24288.1232 - val_loss: 33245.6007 - val_mean_absolute_error: 33245.6007\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 16632.13696\n",
      "Epoch 1480/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25682.5171 - mean_absolute_error: 25682.5171 - val_loss: 18197.0430 - val_mean_absolute_error: 18197.0430\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 16632.13696\n",
      "Epoch 1481/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24766.8293 - mean_absolute_error: 24766.8293 - val_loss: 18654.6437 - val_mean_absolute_error: 18654.6437\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 16632.13696\n",
      "Epoch 1482/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24567.9888 - mean_absolute_error: 24567.9888 - val_loss: 17949.6595 - val_mean_absolute_error: 17949.6595\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 16632.13696\n",
      "Epoch 1483/1500\n",
      "934/934 [==============================] - 0s 282us/step - loss: 25674.2676 - mean_absolute_error: 25674.2676 - val_loss: 22394.0439 - val_mean_absolute_error: 22394.0439\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 16632.13696\n",
      "Epoch 1484/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25511.5646 - mean_absolute_error: 25511.5646 - val_loss: 23316.0310 - val_mean_absolute_error: 23316.0310\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 16632.13696\n",
      "Epoch 1485/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 25722.8099 - mean_absolute_error: 25722.8099 - val_loss: 20594.2697 - val_mean_absolute_error: 20594.2697\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 16632.13696\n",
      "Epoch 1486/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24522.2590 - mean_absolute_error: 24522.2590 - val_loss: 20915.7956 - val_mean_absolute_error: 20915.7956\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 16632.13696\n",
      "Epoch 1487/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25564.7987 - mean_absolute_error: 25564.7987 - val_loss: 18323.7425 - val_mean_absolute_error: 18323.7425\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 16632.13696\n",
      "Epoch 1488/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25205.6228 - mean_absolute_error: 25205.6228 - val_loss: 24367.8132 - val_mean_absolute_error: 24367.8132\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 16632.13696\n",
      "Epoch 1489/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 24684.7436 - mean_absolute_error: 24684.7436 - val_loss: 19886.2506 - val_mean_absolute_error: 19886.2506\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 16632.13696\n",
      "Epoch 1490/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 25620.6522 - mean_absolute_error: 25620.6522 - val_loss: 27251.9238 - val_mean_absolute_error: 27251.9238\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 16632.13696\n",
      "Epoch 1491/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 24240.8668 - mean_absolute_error: 24240.8668 - val_loss: 19228.6479 - val_mean_absolute_error: 19228.6479\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 16632.13696\n",
      "Epoch 1492/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 25083.6574 - mean_absolute_error: 25083.6574 - val_loss: 19549.8414 - val_mean_absolute_error: 19549.8414\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 16632.13696\n",
      "Epoch 1493/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 25621.4861 - mean_absolute_error: 25621.4861 - val_loss: 20482.4611 - val_mean_absolute_error: 20482.4611\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 16632.13696\n",
      "Epoch 1494/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26265.6619 - mean_absolute_error: 26265.6619 - val_loss: 17089.0813 - val_mean_absolute_error: 17089.0813\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 16632.13696\n",
      "Epoch 1495/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 24608.3253 - mean_absolute_error: 24608.3253 - val_loss: 17944.4018 - val_mean_absolute_error: 17944.4018\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 16632.13696\n",
      "Epoch 1496/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 25046.3103 - mean_absolute_error: 25046.3103 - val_loss: 26511.9018 - val_mean_absolute_error: 26511.9018\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 16632.13696\n",
      "Epoch 1497/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 25519.4228 - mean_absolute_error: 25519.4228 - val_loss: 35014.4776 - val_mean_absolute_error: 35014.4776\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 16632.13696\n",
      "Epoch 1498/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 25799.0073 - mean_absolute_error: 25799.0073 - val_loss: 22613.4195 - val_mean_absolute_error: 22613.4195\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 16632.13696\n",
      "Epoch 1499/1500\n",
      "934/934 [==============================] - 0s 263us/step - loss: 26127.8204 - mean_absolute_error: 26127.8204 - val_loss: 26345.9438 - val_mean_absolute_error: 26345.9438\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 16632.13696\n",
      "Epoch 1500/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 24505.9597 - mean_absolute_error: 24505.9597 - val_loss: 22016.1357 - val_mean_absolute_error: 22016.1357\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 16632.13696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b555bd8470>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(vxtrain, vytrain, epochs=1500, batch_size=5, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 22992.187807684077\n",
      "Mean Squared Error: 1293298063.3278923\n",
      "Root Mean Squared Error: 35962.45352208178\n"
     ]
    }
   ],
   "source": [
    "y_pred=NN_model.predict(xtest)\n",
    "vytest= ytest.to_numpy()\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(vytest, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(vytest, y_pred))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(vytest, y_pred))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3)\n",
    "axs[0].set_title('Actual value')\n",
    "axs[0].plot(vytest)\n",
    "axs[1].set_title('Predicted value')\n",
    "axs[1].plot(y_pred)\n",
    "axs[2].set_title('Compared values')\n",
    "axs[2].plot(vytest)\n",
    "axs[2].plot(y_pred)\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\non_n\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                2432      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Nmodel = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "Nmodel.add(Dense(32, kernel_initializer='normal',input_dim = vxtrain.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "Nmodel.add(Dense(16, kernel_initializer='normal',activation='relu'))\n",
    "# The Output Layer :\n",
    "Nmodel.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "Nmodel.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "Nmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\non_n\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/1500\n",
      "934/934 [==============================] - 1s 1ms/step - loss: 155556.5624 - mean_absolute_error: 155556.5624 - val_loss: 92493.1143 - val_mean_absolute_error: 92493.1143\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 92493.11427, saving model to Weights-001--92493.11427.hdf5\n",
      "Epoch 2/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 54656.6546 - mean_absolute_error: 54656.6546 - val_loss: 41933.5023 - val_mean_absolute_error: 41933.5023\n",
      "\n",
      "Epoch 00002: val_loss improved from 92493.11427 to 41933.50233, saving model to Weights-002--41933.50233.hdf5\n",
      "Epoch 3/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 46622.9397 - mean_absolute_error: 46622.9397 - val_loss: 40553.8611 - val_mean_absolute_error: 40553.8611\n",
      "\n",
      "Epoch 00003: val_loss improved from 41933.50233 to 40553.86112, saving model to Weights-003--40553.86112.hdf5\n",
      "Epoch 4/1500\n",
      "934/934 [==============================] - 0s 249us/step - loss: 44667.9400 - mean_absolute_error: 44667.9400 - val_loss: 39024.6675 - val_mean_absolute_error: 39024.6675\n",
      "\n",
      "Epoch 00004: val_loss improved from 40553.86112 to 39024.66748, saving model to Weights-004--39024.66748.hdf5\n",
      "Epoch 5/1500\n",
      "934/934 [==============================] - 0s 249us/step - loss: 43033.3680 - mean_absolute_error: 43033.3680 - val_loss: 37831.9685 - val_mean_absolute_error: 37831.9685\n",
      "\n",
      "Epoch 00005: val_loss improved from 39024.66748 to 37831.96849, saving model to Weights-005--37831.96849.hdf5\n",
      "Epoch 6/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 41162.5763 - mean_absolute_error: 41162.5763 - val_loss: 36028.3419 - val_mean_absolute_error: 36028.3419\n",
      "\n",
      "Epoch 00006: val_loss improved from 37831.96849 to 36028.34194, saving model to Weights-006--36028.34194.hdf5\n",
      "Epoch 7/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 39586.0885 - mean_absolute_error: 39586.0885 - val_loss: 34247.6745 - val_mean_absolute_error: 34247.6745\n",
      "\n",
      "Epoch 00007: val_loss improved from 36028.34194 to 34247.67451, saving model to Weights-007--34247.67451.hdf5\n",
      "Epoch 8/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 37499.2301 - mean_absolute_error: 37499.2301 - val_loss: 33672.5531 - val_mean_absolute_error: 33672.5531\n",
      "\n",
      "Epoch 00008: val_loss improved from 34247.67451 to 33672.55309, saving model to Weights-008--33672.55309.hdf5\n",
      "Epoch 9/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 35676.4223 - mean_absolute_error: 35676.4223 - val_loss: 31237.6334 - val_mean_absolute_error: 31237.6334\n",
      "\n",
      "Epoch 00009: val_loss improved from 33672.55309 to 31237.63336, saving model to Weights-009--31237.63336.hdf5\n",
      "Epoch 10/1500\n",
      "934/934 [==============================] - 0s 240us/step - loss: 34000.5650 - mean_absolute_error: 34000.5650 - val_loss: 29640.6280 - val_mean_absolute_error: 29640.6280\n",
      "\n",
      "Epoch 00010: val_loss improved from 31237.63336 to 29640.62795, saving model to Weights-010--29640.62795.hdf5\n",
      "Epoch 11/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 32571.2930 - mean_absolute_error: 32571.2930 - val_loss: 28420.3101 - val_mean_absolute_error: 28420.3101\n",
      "\n",
      "Epoch 00011: val_loss improved from 29640.62795 to 28420.31006, saving model to Weights-011--28420.31006.hdf5\n",
      "Epoch 12/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 31675.5759 - mean_absolute_error: 31675.5759 - val_loss: 27255.4316 - val_mean_absolute_error: 27255.4316\n",
      "\n",
      "Epoch 00012: val_loss improved from 28420.31006 to 27255.43163, saving model to Weights-012--27255.43163.hdf5\n",
      "Epoch 13/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 30668.0638 - mean_absolute_error: 30668.0638 - val_loss: 26675.6446 - val_mean_absolute_error: 26675.6446\n",
      "\n",
      "Epoch 00013: val_loss improved from 27255.43163 to 26675.64462, saving model to Weights-013--26675.64462.hdf5\n",
      "Epoch 14/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 29903.5102 - mean_absolute_error: 29903.5102 - val_loss: 26080.8631 - val_mean_absolute_error: 26080.8631\n",
      "\n",
      "Epoch 00014: val_loss improved from 26675.64462 to 26080.86309, saving model to Weights-014--26080.86309.hdf5\n",
      "Epoch 15/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 29473.2651 - mean_absolute_error: 29473.2651 - val_loss: 25355.3175 - val_mean_absolute_error: 25355.3175\n",
      "\n",
      "Epoch 00015: val_loss improved from 26080.86309 to 25355.31749, saving model to Weights-015--25355.31749.hdf5\n",
      "Epoch 16/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 29358.1426 - mean_absolute_error: 29358.1426 - val_loss: 25257.7346 - val_mean_absolute_error: 25257.7346\n",
      "\n",
      "Epoch 00016: val_loss improved from 25355.31749 to 25257.73456, saving model to Weights-016--25257.73456.hdf5\n",
      "Epoch 17/1500\n",
      "934/934 [==============================] - 0s 283us/step - loss: 28736.2122 - mean_absolute_error: 28736.2122 - val_loss: 25748.7563 - val_mean_absolute_error: 25748.7563\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 25257.73456\n",
      "Epoch 18/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 28842.8511 - mean_absolute_error: 28842.8511 - val_loss: 24695.5091 - val_mean_absolute_error: 24695.5091\n",
      "\n",
      "Epoch 00018: val_loss improved from 25257.73456 to 24695.50913, saving model to Weights-018--24695.50913.hdf5\n",
      "Epoch 19/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 28704.3508 - mean_absolute_error: 28704.3508 - val_loss: 24577.1883 - val_mean_absolute_error: 24577.1883\n",
      "\n",
      "Epoch 00019: val_loss improved from 24695.50913 to 24577.18831, saving model to Weights-019--24577.18831.hdf5\n",
      "Epoch 20/1500\n",
      "934/934 [==============================] - 0s 233us/step - loss: 28646.6279 - mean_absolute_error: 28646.6279 - val_loss: 24413.6811 - val_mean_absolute_error: 24413.6811\n",
      "\n",
      "Epoch 00020: val_loss improved from 24577.18831 to 24413.68114, saving model to Weights-020--24413.68114.hdf5\n",
      "Epoch 21/1500\n",
      "934/934 [==============================] - 0s 258us/step - loss: 28497.0161 - mean_absolute_error: 28497.0161 - val_loss: 25057.9558 - val_mean_absolute_error: 25057.9558\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 24413.68114\n",
      "Epoch 22/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 28239.9296 - mean_absolute_error: 28239.9296 - val_loss: 24242.8150 - val_mean_absolute_error: 24242.8150\n",
      "\n",
      "Epoch 00022: val_loss improved from 24413.68114 to 24242.81496, saving model to Weights-022--24242.81496.hdf5\n",
      "Epoch 23/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 28206.7660 - mean_absolute_error: 28206.7660 - val_loss: 24298.7261 - val_mean_absolute_error: 24298.7261\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 24242.81496\n",
      "Epoch 24/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 28298.1042 - mean_absolute_error: 28298.1042 - val_loss: 24345.0755 - val_mean_absolute_error: 24345.0755\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 24242.81496\n",
      "Epoch 25/1500\n",
      "934/934 [==============================] - 0s 222us/step - loss: 28091.0096 - mean_absolute_error: 28091.0096 - val_loss: 24198.3699 - val_mean_absolute_error: 24198.3699\n",
      "\n",
      "Epoch 00025: val_loss improved from 24242.81496 to 24198.36994, saving model to Weights-025--24198.36994.hdf5\n",
      "Epoch 26/1500\n",
      "934/934 [==============================] - 0s 221us/step - loss: 28222.8980 - mean_absolute_error: 28222.8980 - val_loss: 24115.0619 - val_mean_absolute_error: 24115.0619\n",
      "\n",
      "Epoch 00026: val_loss improved from 24198.36994 to 24115.06190, saving model to Weights-026--24115.06190.hdf5\n",
      "Epoch 27/1500\n",
      "934/934 [==============================] - 0s 265us/step - loss: 27974.1465 - mean_absolute_error: 27974.1465 - val_loss: 24218.9686 - val_mean_absolute_error: 24218.9686\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 24115.06190\n",
      "Epoch 28/1500\n",
      "934/934 [==============================] - 0s 285us/step - loss: 28005.8567 - mean_absolute_error: 28005.8567 - val_loss: 24057.0520 - val_mean_absolute_error: 24057.0520\n",
      "\n",
      "Epoch 00028: val_loss improved from 24115.06190 to 24057.05199, saving model to Weights-028--24057.05199.hdf5\n",
      "Epoch 29/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 224us/step - loss: 28037.8011 - mean_absolute_error: 28037.8011 - val_loss: 23923.7190 - val_mean_absolute_error: 23923.7190\n",
      "\n",
      "Epoch 00029: val_loss improved from 24057.05199 to 23923.71905, saving model to Weights-029--23923.71905.hdf5\n",
      "Epoch 30/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 27914.0397 - mean_absolute_error: 27914.0397 - val_loss: 23958.4944 - val_mean_absolute_error: 23958.4944\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23923.71905\n",
      "Epoch 31/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 28042.2316 - mean_absolute_error: 28042.2316 - val_loss: 23854.9764 - val_mean_absolute_error: 23854.9764\n",
      "\n",
      "Epoch 00031: val_loss improved from 23923.71905 to 23854.97645, saving model to Weights-031--23854.97645.hdf5\n",
      "Epoch 32/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 28017.8571 - mean_absolute_error: 28017.8571 - val_loss: 23865.7031 - val_mean_absolute_error: 23865.7031\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 23854.97645\n",
      "Epoch 33/1500\n",
      "934/934 [==============================] - 0s 222us/step - loss: 27998.2296 - mean_absolute_error: 27998.2296 - val_loss: 23908.6977 - val_mean_absolute_error: 23908.6977\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23854.97645\n",
      "Epoch 34/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 27880.4580 - mean_absolute_error: 27880.4580 - val_loss: 24357.7988 - val_mean_absolute_error: 24357.7988\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23854.97645\n",
      "Epoch 35/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 27808.4252 - mean_absolute_error: 27808.4252 - val_loss: 24025.7931 - val_mean_absolute_error: 24025.7931\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23854.97645\n",
      "Epoch 36/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 27930.3623 - mean_absolute_error: 27930.3623 - val_loss: 23967.0922 - val_mean_absolute_error: 23967.0922\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23854.97645\n",
      "Epoch 37/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 27870.6100 - mean_absolute_error: 27870.6100 - val_loss: 23773.4189 - val_mean_absolute_error: 23773.4189\n",
      "\n",
      "Epoch 00037: val_loss improved from 23854.97645 to 23773.41892, saving model to Weights-037--23773.41892.hdf5\n",
      "Epoch 38/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 27675.3908 - mean_absolute_error: 27675.3908 - val_loss: 24042.2245 - val_mean_absolute_error: 24042.2245\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23773.41892\n",
      "Epoch 39/1500\n",
      "934/934 [==============================] - 0s 232us/step - loss: 27730.1808 - mean_absolute_error: 27730.1808 - val_loss: 24057.0926 - val_mean_absolute_error: 24057.0926\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23773.41892\n",
      "Epoch 40/1500\n",
      "934/934 [==============================] - 0s 224us/step - loss: 27616.2868 - mean_absolute_error: 27616.2868 - val_loss: 23733.7186 - val_mean_absolute_error: 23733.7186\n",
      "\n",
      "Epoch 00040: val_loss improved from 23773.41892 to 23733.71862, saving model to Weights-040--23733.71862.hdf5\n",
      "Epoch 41/1500\n",
      "934/934 [==============================] - 0s 277us/step - loss: 27786.5430 - mean_absolute_error: 27786.5430 - val_loss: 23781.2923 - val_mean_absolute_error: 23781.2923\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23733.71862\n",
      "Epoch 42/1500\n",
      "934/934 [==============================] - 0s 269us/step - loss: 27560.4675 - mean_absolute_error: 27560.4675 - val_loss: 23779.6814 - val_mean_absolute_error: 23779.6814\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23733.71862\n",
      "Epoch 43/1500\n",
      "934/934 [==============================] - 0s 256us/step - loss: 27532.3427 - mean_absolute_error: 27532.3427 - val_loss: 23777.6561 - val_mean_absolute_error: 23777.6561\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23733.71862\n",
      "Epoch 44/1500\n",
      "934/934 [==============================] - 0s 246us/step - loss: 27602.3647 - mean_absolute_error: 27602.3647 - val_loss: 23668.0635 - val_mean_absolute_error: 23668.0635\n",
      "\n",
      "Epoch 00044: val_loss improved from 23733.71862 to 23668.06348, saving model to Weights-044--23668.06348.hdf5\n",
      "Epoch 45/1500\n",
      "934/934 [==============================] - 0s 281us/step - loss: 27699.8864 - mean_absolute_error: 27699.8864 - val_loss: 23683.4943 - val_mean_absolute_error: 23683.4943\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23668.06348\n",
      "Epoch 46/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 27579.4074 - mean_absolute_error: 27579.4074 - val_loss: 24124.5661 - val_mean_absolute_error: 24124.5661\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23668.06348\n",
      "Epoch 47/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 27712.8387 - mean_absolute_error: 27712.8387 - val_loss: 23668.3868 - val_mean_absolute_error: 23668.3868\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23668.06348\n",
      "Epoch 48/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 27377.3897 - mean_absolute_error: 27377.3897 - val_loss: 23788.8073 - val_mean_absolute_error: 23788.8073\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23668.06348\n",
      "Epoch 49/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 27705.2319 - mean_absolute_error: 27705.2319 - val_loss: 23655.9899 - val_mean_absolute_error: 23655.9899\n",
      "\n",
      "Epoch 00049: val_loss improved from 23668.06348 to 23655.98990, saving model to Weights-049--23655.98990.hdf5\n",
      "Epoch 50/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 27512.4729 - mean_absolute_error: 27512.4729 - val_loss: 26280.6016 - val_mean_absolute_error: 26280.6016\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23655.98990\n",
      "Epoch 51/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 27682.6327 - mean_absolute_error: 27682.6327 - val_loss: 24382.3121 - val_mean_absolute_error: 24382.3121\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 23655.98990\n",
      "Epoch 52/1500\n",
      "934/934 [==============================] - 0s 236us/step - loss: 27692.8389 - mean_absolute_error: 27692.8389 - val_loss: 23706.3192 - val_mean_absolute_error: 23706.3192\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 23655.98990\n",
      "Epoch 53/1500\n",
      "934/934 [==============================] - 0s 235us/step - loss: 27714.9896 - mean_absolute_error: 27714.9896 - val_loss: 23723.8901 - val_mean_absolute_error: 23723.8901\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 23655.98990\n",
      "Epoch 54/1500\n",
      "934/934 [==============================] - 0s 225us/step - loss: 27634.9115 - mean_absolute_error: 27634.9115 - val_loss: 23742.5763 - val_mean_absolute_error: 23742.5763\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 23655.98990\n",
      "Epoch 55/1500\n",
      "934/934 [==============================] - 0s 221us/step - loss: 27468.0584 - mean_absolute_error: 27468.0584 - val_loss: 24243.0846 - val_mean_absolute_error: 24243.0846\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 23655.98990\n",
      "Epoch 56/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 27397.1830 - mean_absolute_error: 27397.1830 - val_loss: 23670.5911 - val_mean_absolute_error: 23670.5911\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 23655.98990\n",
      "Epoch 57/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 27238.9282 - mean_absolute_error: 27238.9282 - val_loss: 23554.5151 - val_mean_absolute_error: 23554.5151\n",
      "\n",
      "Epoch 00057: val_loss improved from 23655.98990 to 23554.51506, saving model to Weights-057--23554.51506.hdf5\n",
      "Epoch 58/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 27622.6246 - mean_absolute_error: 27622.6246 - val_loss: 23902.4670 - val_mean_absolute_error: 23902.4670\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 23554.51506\n",
      "Epoch 59/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 27392.1662 - mean_absolute_error: 27392.1662 - val_loss: 23532.7322 - val_mean_absolute_error: 23532.7322\n",
      "\n",
      "Epoch 00059: val_loss improved from 23554.51506 to 23532.73218, saving model to Weights-059--23532.73218.hdf5\n",
      "Epoch 60/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 27268.1224 - mean_absolute_error: 27268.1224 - val_loss: 23567.1488 - val_mean_absolute_error: 23567.1488\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 23532.73218\n",
      "Epoch 61/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 27469.1423 - mean_absolute_error: 27469.1423 - val_loss: 23873.1008 - val_mean_absolute_error: 23873.1008\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 23532.73218\n",
      "Epoch 62/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 27533.6850 - mean_absolute_error: 27533.6850 - val_loss: 24475.0733 - val_mean_absolute_error: 24475.0733\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 23532.73218\n",
      "Epoch 63/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 27326.9009 - mean_absolute_error: 27326.9009 - val_loss: 23797.3508 - val_mean_absolute_error: 23797.3508\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 23532.73218\n",
      "Epoch 64/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 27244.2284 - mean_absolute_error: 27244.2284 - val_loss: 24478.2148 - val_mean_absolute_error: 24478.2148\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 23532.73218\n",
      "Epoch 65/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 27433.7050 - mean_absolute_error: 27433.7050 - val_loss: 23816.6688 - val_mean_absolute_error: 23816.6688\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 23532.73218\n",
      "Epoch 66/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 27493.7451 - mean_absolute_error: 27493.7451 - val_loss: 23748.1862 - val_mean_absolute_error: 23748.1862\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 23532.73218\n",
      "Epoch 67/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 27334.1439 - mean_absolute_error: 27334.1439 - val_loss: 24943.6514 - val_mean_absolute_error: 24943.6514\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 23532.73218\n",
      "Epoch 68/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 27375.2093 - mean_absolute_error: 27375.2093 - val_loss: 23651.5190 - val_mean_absolute_error: 23651.5190\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 23532.73218\n",
      "Epoch 69/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 27263.4366 - mean_absolute_error: 27263.4366 - val_loss: 23610.5514 - val_mean_absolute_error: 23610.5514\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 23532.73218\n",
      "Epoch 70/1500\n",
      "934/934 [==============================] - 0s 234us/step - loss: 27212.2421 - mean_absolute_error: 27212.2421 - val_loss: 23483.5567 - val_mean_absolute_error: 23483.5567\n",
      "\n",
      "Epoch 00070: val_loss improved from 23532.73218 to 23483.55669, saving model to Weights-070--23483.55669.hdf5\n",
      "Epoch 71/1500\n",
      "934/934 [==============================] - 0s 230us/step - loss: 27152.2418 - mean_absolute_error: 27152.2418 - val_loss: 23694.2946 - val_mean_absolute_error: 23694.2946\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 23483.55669\n",
      "Epoch 72/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 27212.0493 - mean_absolute_error: 27212.0493 - val_loss: 23874.6339 - val_mean_absolute_error: 23874.6339\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 23483.55669\n",
      "Epoch 73/1500\n",
      "934/934 [==============================] - 0s 224us/step - loss: 27327.7569 - mean_absolute_error: 27327.7569 - val_loss: 23532.5774 - val_mean_absolute_error: 23532.5774\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 23483.55669\n",
      "Epoch 74/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 27181.6801 - mean_absolute_error: 27181.6801 - val_loss: 24696.4806 - val_mean_absolute_error: 24696.4806\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 23483.55669\n",
      "Epoch 75/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 27430.0002 - mean_absolute_error: 27430.0002 - val_loss: 24047.5165 - val_mean_absolute_error: 24047.5165\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 23483.55669\n",
      "Epoch 76/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 27489.9424 - mean_absolute_error: 27489.9424 - val_loss: 23711.5938 - val_mean_absolute_error: 23711.5938\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 23483.55669\n",
      "Epoch 77/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 27242.5620 - mean_absolute_error: 27242.5620 - val_loss: 23766.1346 - val_mean_absolute_error: 23766.1346\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 23483.55669\n",
      "Epoch 78/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 26899.7919 - mean_absolute_error: 26899.7919 - val_loss: 23455.4715 - val_mean_absolute_error: 23455.4715\n",
      "\n",
      "Epoch 00078: val_loss improved from 23483.55669 to 23455.47146, saving model to Weights-078--23455.47146.hdf5\n",
      "Epoch 79/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 27119.4913 - mean_absolute_error: 27119.4913 - val_loss: 23655.2011 - val_mean_absolute_error: 23655.2011\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 23455.47146\n",
      "Epoch 80/1500\n",
      "934/934 [==============================] - 0s 230us/step - loss: 27008.6777 - mean_absolute_error: 27008.6777 - val_loss: 23855.6781 - val_mean_absolute_error: 23855.6781\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 23455.47146\n",
      "Epoch 81/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 27136.1622 - mean_absolute_error: 27136.1622 - val_loss: 23433.9249 - val_mean_absolute_error: 23433.9249\n",
      "\n",
      "Epoch 00081: val_loss improved from 23455.47146 to 23433.92491, saving model to Weights-081--23433.92491.hdf5\n",
      "Epoch 82/1500\n",
      "934/934 [==============================] - 0s 246us/step - loss: 27122.8119 - mean_absolute_error: 27122.8119 - val_loss: 23706.9160 - val_mean_absolute_error: 23706.9160\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 23433.92491\n",
      "Epoch 83/1500\n",
      "934/934 [==============================] - 0s 299us/step - loss: 27104.4469 - mean_absolute_error: 27104.4469 - val_loss: 23508.4130 - val_mean_absolute_error: 23508.4130\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 23433.92491\n",
      "Epoch 84/1500\n",
      "934/934 [==============================] - 0s 271us/step - loss: 27179.4564 - mean_absolute_error: 27179.4564 - val_loss: 23606.4206 - val_mean_absolute_error: 23606.4206\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 23433.92491\n",
      "Epoch 85/1500\n",
      "934/934 [==============================] - 0s 300us/step - loss: 27081.5057 - mean_absolute_error: 27081.5057 - val_loss: 23622.3588 - val_mean_absolute_error: 23622.3588\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 23433.92491\n",
      "Epoch 86/1500\n",
      "934/934 [==============================] - 0s 274us/step - loss: 27210.6695 - mean_absolute_error: 27210.6695 - val_loss: 23444.5696 - val_mean_absolute_error: 23444.5696\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 23433.92491\n",
      "Epoch 87/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 27161.7230 - mean_absolute_error: 27161.7230 - val_loss: 23576.7185 - val_mean_absolute_error: 23576.7185\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 23433.92491\n",
      "Epoch 88/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 27170.0728 - mean_absolute_error: 27170.0728 - val_loss: 23388.0074 - val_mean_absolute_error: 23388.0074\n",
      "\n",
      "Epoch 00088: val_loss improved from 23433.92491 to 23388.00743, saving model to Weights-088--23388.00743.hdf5\n",
      "Epoch 89/1500\n",
      "934/934 [==============================] - 0s 249us/step - loss: 27022.5447 - mean_absolute_error: 27022.5447 - val_loss: 23337.9227 - val_mean_absolute_error: 23337.9227\n",
      "\n",
      "Epoch 00089: val_loss improved from 23388.00743 to 23337.92275, saving model to Weights-089--23337.92275.hdf5\n",
      "Epoch 90/1500\n",
      "934/934 [==============================] - 0s 248us/step - loss: 26945.2398 - mean_absolute_error: 26945.2398 - val_loss: 23586.0154 - val_mean_absolute_error: 23586.0154\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 23337.92275\n",
      "Epoch 91/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 26973.7051 - mean_absolute_error: 26973.7051 - val_loss: 23863.6256 - val_mean_absolute_error: 23863.6256\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 23337.92275\n",
      "Epoch 92/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 27001.3528 - mean_absolute_error: 27001.3528 - val_loss: 26231.6423 - val_mean_absolute_error: 26231.6423\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 23337.92275\n",
      "Epoch 93/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 27041.8678 - mean_absolute_error: 27041.8678 - val_loss: 23880.8150 - val_mean_absolute_error: 23880.8150\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 23337.92275\n",
      "Epoch 94/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 214us/step - loss: 26968.3696 - mean_absolute_error: 26968.3696 - val_loss: 23532.7162 - val_mean_absolute_error: 23532.7162\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 23337.92275\n",
      "Epoch 95/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 26981.3456 - mean_absolute_error: 26981.3456 - val_loss: 23589.9703 - val_mean_absolute_error: 23589.9703\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 23337.92275\n",
      "Epoch 96/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 27036.5081 - mean_absolute_error: 27036.5081 - val_loss: 23741.0685 - val_mean_absolute_error: 23741.0685\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 23337.92275\n",
      "Epoch 97/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 26958.2280 - mean_absolute_error: 26958.2280 - val_loss: 23651.3555 - val_mean_absolute_error: 23651.3555\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 23337.92275\n",
      "Epoch 98/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 27016.9598 - mean_absolute_error: 27016.9598 - val_loss: 23815.8489 - val_mean_absolute_error: 23815.8489\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 23337.92275\n",
      "Epoch 99/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 26882.0886 - mean_absolute_error: 26882.0886 - val_loss: 23469.6345 - val_mean_absolute_error: 23469.6345\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 23337.92275\n",
      "Epoch 100/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 26984.3236 - mean_absolute_error: 26984.3236 - val_loss: 23457.9793 - val_mean_absolute_error: 23457.9793\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 23337.92275\n",
      "Epoch 101/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 27125.5766 - mean_absolute_error: 27125.5766 - val_loss: 23486.0109 - val_mean_absolute_error: 23486.0109\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 23337.92275\n",
      "Epoch 102/1500\n",
      "934/934 [==============================] - 0s 230us/step - loss: 26807.2569 - mean_absolute_error: 26807.2569 - val_loss: 23960.1606 - val_mean_absolute_error: 23960.1606\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 23337.92275\n",
      "Epoch 103/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 26935.0563 - mean_absolute_error: 26935.0563 - val_loss: 23453.3321 - val_mean_absolute_error: 23453.3321\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 23337.92275\n",
      "Epoch 104/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 27138.3140 - mean_absolute_error: 27138.3140 - val_loss: 23709.9549 - val_mean_absolute_error: 23709.9549\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 23337.92275\n",
      "Epoch 105/1500\n",
      "934/934 [==============================] - 0s 237us/step - loss: 26873.5449 - mean_absolute_error: 26873.5449 - val_loss: 23420.8083 - val_mean_absolute_error: 23420.8083\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 23337.92275\n",
      "Epoch 106/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 26880.7274 - mean_absolute_error: 26880.7274 - val_loss: 23807.9757 - val_mean_absolute_error: 23807.9757\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 23337.92275\n",
      "Epoch 107/1500\n",
      "934/934 [==============================] - 0s 236us/step - loss: 26974.8946 - mean_absolute_error: 26974.8946 - val_loss: 23466.4901 - val_mean_absolute_error: 23466.4901\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 23337.92275\n",
      "Epoch 108/1500\n",
      "934/934 [==============================] - 0s 268us/step - loss: 26651.3267 - mean_absolute_error: 26651.3267 - val_loss: 23690.8692 - val_mean_absolute_error: 23690.8692\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 23337.92275\n",
      "Epoch 109/1500\n",
      "934/934 [==============================] - 0s 251us/step - loss: 26780.5222 - mean_absolute_error: 26780.5222 - val_loss: 23431.6924 - val_mean_absolute_error: 23431.6924\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 23337.92275\n",
      "Epoch 110/1500\n",
      "934/934 [==============================] - 0s 224us/step - loss: 27047.4340 - mean_absolute_error: 27047.4340 - val_loss: 23409.8857 - val_mean_absolute_error: 23409.8857\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 23337.92275\n",
      "Epoch 111/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 26888.8537 - mean_absolute_error: 26888.8537 - val_loss: 23521.7130 - val_mean_absolute_error: 23521.7130\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 23337.92275\n",
      "Epoch 112/1500\n",
      "934/934 [==============================] - 0s 236us/step - loss: 26831.3117 - mean_absolute_error: 26831.3117 - val_loss: 24400.0804 - val_mean_absolute_error: 24400.0804\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 23337.92275\n",
      "Epoch 113/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 26685.8307 - mean_absolute_error: 26685.8307 - val_loss: 23441.6658 - val_mean_absolute_error: 23441.6658\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 23337.92275\n",
      "Epoch 114/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 26908.5048 - mean_absolute_error: 26908.5048 - val_loss: 23511.1374 - val_mean_absolute_error: 23511.1374\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 23337.92275\n",
      "Epoch 115/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 26849.5013 - mean_absolute_error: 26849.5013 - val_loss: 23446.9550 - val_mean_absolute_error: 23446.9550\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 23337.92275\n",
      "Epoch 116/1500\n",
      "934/934 [==============================] - 0s 237us/step - loss: 26815.2498 - mean_absolute_error: 26815.2498 - val_loss: 23276.9658 - val_mean_absolute_error: 23276.9658\n",
      "\n",
      "Epoch 00116: val_loss improved from 23337.92275 to 23276.96585, saving model to Weights-116--23276.96585.hdf5\n",
      "Epoch 117/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 26773.0971 - mean_absolute_error: 26773.0971 - val_loss: 23241.0524 - val_mean_absolute_error: 23241.0524\n",
      "\n",
      "Epoch 00117: val_loss improved from 23276.96585 to 23241.05242, saving model to Weights-117--23241.05242.hdf5\n",
      "Epoch 118/1500\n",
      "934/934 [==============================] - 0s 245us/step - loss: 26836.0111 - mean_absolute_error: 26836.0111 - val_loss: 23828.7520 - val_mean_absolute_error: 23828.7520\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 23241.05242\n",
      "Epoch 119/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 26620.8493 - mean_absolute_error: 26620.8493 - val_loss: 23327.2386 - val_mean_absolute_error: 23327.2386\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 23241.05242\n",
      "Epoch 120/1500\n",
      "934/934 [==============================] - 0s 235us/step - loss: 26794.9506 - mean_absolute_error: 26794.9506 - val_loss: 24040.9934 - val_mean_absolute_error: 24040.9934\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 23241.05242\n",
      "Epoch 121/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 27070.5297 - mean_absolute_error: 27070.5297 - val_loss: 23325.2034 - val_mean_absolute_error: 23325.2034\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 23241.05242\n",
      "Epoch 122/1500\n",
      "934/934 [==============================] - 0s 231us/step - loss: 26618.7397 - mean_absolute_error: 26618.7397 - val_loss: 23226.2649 - val_mean_absolute_error: 23226.2649\n",
      "\n",
      "Epoch 00122: val_loss improved from 23241.05242 to 23226.26488, saving model to Weights-122--23226.26488.hdf5\n",
      "Epoch 123/1500\n",
      "934/934 [==============================] - 0s 273us/step - loss: 26584.9132 - mean_absolute_error: 26584.9132 - val_loss: 23151.0388 - val_mean_absolute_error: 23151.0388\n",
      "\n",
      "Epoch 00123: val_loss improved from 23226.26488 to 23151.03883, saving model to Weights-123--23151.03883.hdf5\n",
      "Epoch 124/1500\n",
      "934/934 [==============================] - 0s 252us/step - loss: 26663.9939 - mean_absolute_error: 26663.9939 - val_loss: 23190.6741 - val_mean_absolute_error: 23190.6741\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 23151.03883\n",
      "Epoch 125/1500\n",
      "934/934 [==============================] - 0s 320us/step - loss: 26673.2712 - mean_absolute_error: 26673.2712 - val_loss: 23084.5839 - val_mean_absolute_error: 23084.5839\n",
      "\n",
      "Epoch 00125: val_loss improved from 23151.03883 to 23084.58385, saving model to Weights-125--23084.58385.hdf5\n",
      "Epoch 126/1500\n",
      "934/934 [==============================] - 0s 236us/step - loss: 26563.2492 - mean_absolute_error: 26563.2492 - val_loss: 23240.4530 - val_mean_absolute_error: 23240.4530\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 23084.58385\n",
      "Epoch 127/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - ETA: 0s - loss: 26509.0961 - mean_absolute_error: 26509.096 - 0s 253us/step - loss: 26472.1575 - mean_absolute_error: 26472.1575 - val_loss: 23254.1410 - val_mean_absolute_error: 23254.1410\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 23084.58385\n",
      "Epoch 128/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 26594.9031 - mean_absolute_error: 26594.9031 - val_loss: 23163.7208 - val_mean_absolute_error: 23163.7208\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 23084.58385\n",
      "Epoch 129/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 26399.2189 - mean_absolute_error: 26399.2189 - val_loss: 23379.8571 - val_mean_absolute_error: 23379.8571\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 23084.58385\n",
      "Epoch 130/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 26429.9275 - mean_absolute_error: 26429.9275 - val_loss: 23570.1934 - val_mean_absolute_error: 23570.1934\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 23084.58385\n",
      "Epoch 131/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 26558.9839 - mean_absolute_error: 26558.9839 - val_loss: 22985.1706 - val_mean_absolute_error: 22985.1706\n",
      "\n",
      "Epoch 00131: val_loss improved from 23084.58385 to 22985.17056, saving model to Weights-131--22985.17056.hdf5\n",
      "Epoch 132/1500\n",
      "934/934 [==============================] - 0s 261us/step - loss: 26634.5679 - mean_absolute_error: 26634.5679 - val_loss: 23751.4712 - val_mean_absolute_error: 23751.4712\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 22985.17056\n",
      "Epoch 133/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 26349.9502 - mean_absolute_error: 26349.9502 - val_loss: 22921.5381 - val_mean_absolute_error: 22921.5381\n",
      "\n",
      "Epoch 00133: val_loss improved from 22985.17056 to 22921.53814, saving model to Weights-133--22921.53814.hdf5\n",
      "Epoch 134/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 26358.3653 - mean_absolute_error: 26358.3653 - val_loss: 22876.1613 - val_mean_absolute_error: 22876.1613\n",
      "\n",
      "Epoch 00134: val_loss improved from 22921.53814 to 22876.16129, saving model to Weights-134--22876.16129.hdf5\n",
      "Epoch 135/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 26451.8817 - mean_absolute_error: 26451.8817 - val_loss: 23191.5227 - val_mean_absolute_error: 23191.5227\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 22876.16129\n",
      "Epoch 136/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 26406.3304 - mean_absolute_error: 26406.3304 - val_loss: 23488.9374 - val_mean_absolute_error: 23488.9374\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 22876.16129\n",
      "Epoch 137/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 26352.3923 - mean_absolute_error: 26352.3923 - val_loss: 24082.7529 - val_mean_absolute_error: 24082.7529\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 22876.16129\n",
      "Epoch 138/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 26422.9126 - mean_absolute_error: 26422.9126 - val_loss: 23077.3487 - val_mean_absolute_error: 23077.3487\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 22876.16129\n",
      "Epoch 139/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 26511.4107 - mean_absolute_error: 26511.4107 - val_loss: 22889.7548 - val_mean_absolute_error: 22889.7548\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 22876.16129\n",
      "Epoch 140/1500\n",
      "934/934 [==============================] - 0s 232us/step - loss: 26263.5566 - mean_absolute_error: 26263.5566 - val_loss: 23353.8905 - val_mean_absolute_error: 23353.8905\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 22876.16129\n",
      "Epoch 141/1500\n",
      "934/934 [==============================] - 0s 233us/step - loss: 26313.0467 - mean_absolute_error: 26313.0467 - val_loss: 22854.2646 - val_mean_absolute_error: 22854.2646\n",
      "\n",
      "Epoch 00141: val_loss improved from 22876.16129 to 22854.26456, saving model to Weights-141--22854.26456.hdf5\n",
      "Epoch 142/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 26270.0089 - mean_absolute_error: 26270.0089 - val_loss: 22863.5945 - val_mean_absolute_error: 22863.5945\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 22854.26456\n",
      "Epoch 143/1500\n",
      "934/934 [==============================] - 0s 221us/step - loss: 26299.3727 - mean_absolute_error: 26299.3727 - val_loss: 22731.0372 - val_mean_absolute_error: 22731.0372\n",
      "\n",
      "Epoch 00143: val_loss improved from 22854.26456 to 22731.03723, saving model to Weights-143--22731.03723.hdf5\n",
      "Epoch 144/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 26214.4147 - mean_absolute_error: 26214.4147 - val_loss: 22845.3495 - val_mean_absolute_error: 22845.3495\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 22731.03723\n",
      "Epoch 145/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 26059.8887 - mean_absolute_error: 26059.8887 - val_loss: 22883.9723 - val_mean_absolute_error: 22883.9723\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 22731.03723\n",
      "Epoch 146/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 26079.2792 - mean_absolute_error: 26079.2792 - val_loss: 22799.8705 - val_mean_absolute_error: 22799.8705\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 22731.03723\n",
      "Epoch 147/1500\n",
      "934/934 [==============================] - 0s 257us/step - loss: 26148.3124 - mean_absolute_error: 26148.3124 - val_loss: 22717.0684 - val_mean_absolute_error: 22717.0684\n",
      "\n",
      "Epoch 00147: val_loss improved from 22731.03723 to 22717.06838, saving model to Weights-147--22717.06838.hdf5\n",
      "Epoch 148/1500\n",
      "934/934 [==============================] - 0s 247us/step - loss: 26157.7703 - mean_absolute_error: 26157.7703 - val_loss: 22873.2964 - val_mean_absolute_error: 22873.2964\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 22717.06838\n",
      "Epoch 149/1500\n",
      "934/934 [==============================] - 0s 238us/step - loss: 26212.0448 - mean_absolute_error: 26212.0448 - val_loss: 22584.6287 - val_mean_absolute_error: 22584.6287\n",
      "\n",
      "Epoch 00149: val_loss improved from 22717.06838 to 22584.62866, saving model to Weights-149--22584.62866.hdf5\n",
      "Epoch 150/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 25964.9609 - mean_absolute_error: 25964.9609 - val_loss: 22731.6912 - val_mean_absolute_error: 22731.6912\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 22584.62866\n",
      "Epoch 151/1500\n",
      "934/934 [==============================] - 0s 225us/step - loss: 26193.7253 - mean_absolute_error: 26193.7253 - val_loss: 22685.3500 - val_mean_absolute_error: 22685.3500\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 22584.62866\n",
      "Epoch 152/1500\n",
      "934/934 [==============================] - 0s 240us/step - loss: 25935.9539 - mean_absolute_error: 25935.9539 - val_loss: 23250.2093 - val_mean_absolute_error: 23250.2093\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 22584.62866\n",
      "Epoch 153/1500\n",
      "934/934 [==============================] - 0s 234us/step - loss: 26030.2834 - mean_absolute_error: 26030.2834 - val_loss: 22666.2077 - val_mean_absolute_error: 22666.2077\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 22584.62866\n",
      "Epoch 154/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 25837.4281 - mean_absolute_error: 25837.4281 - val_loss: 23852.8525 - val_mean_absolute_error: 23852.8525\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 22584.62866\n",
      "Epoch 155/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 25954.6892 - mean_absolute_error: 25954.6892 - val_loss: 22461.2001 - val_mean_absolute_error: 22461.2001\n",
      "\n",
      "Epoch 00155: val_loss improved from 22584.62866 to 22461.20008, saving model to Weights-155--22461.20008.hdf5\n",
      "Epoch 156/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 25742.0099 - mean_absolute_error: 25742.0099 - val_loss: 22555.7233 - val_mean_absolute_error: 22555.7233\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 22461.20008\n",
      "Epoch 157/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25811.3746 - mean_absolute_error: 25811.3746 - val_loss: 22674.4118 - val_mean_absolute_error: 22674.4118\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 22461.20008\n",
      "Epoch 158/1500\n",
      "934/934 [==============================] - 0s 288us/step - loss: 25905.1402 - mean_absolute_error: 25905.1402 - val_loss: 22660.2672 - val_mean_absolute_error: 22660.2672\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 22461.20008\n",
      "Epoch 159/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 241us/step - loss: 26299.5041 - mean_absolute_error: 26299.5041 - val_loss: 23343.8323 - val_mean_absolute_error: 23343.8323\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 22461.20008\n",
      "Epoch 160/1500\n",
      "934/934 [==============================] - 0s 259us/step - loss: 25966.9045 - mean_absolute_error: 25966.9045 - val_loss: 22637.8432 - val_mean_absolute_error: 22637.8432\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 22461.20008\n",
      "Epoch 161/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 25824.0226 - mean_absolute_error: 25824.0226 - val_loss: 22866.4161 - val_mean_absolute_error: 22866.4161\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 22461.20008\n",
      "Epoch 162/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 25737.1721 - mean_absolute_error: 25737.1721 - val_loss: 22507.3551 - val_mean_absolute_error: 22507.3551\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 22461.20008\n",
      "Epoch 163/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 25852.7269 - mean_absolute_error: 25852.7269 - val_loss: 22449.6077 - val_mean_absolute_error: 22449.6077\n",
      "\n",
      "Epoch 00163: val_loss improved from 22461.20008 to 22449.60770, saving model to Weights-163--22449.60770.hdf5\n",
      "Epoch 164/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 25585.5051 - mean_absolute_error: 25585.5051 - val_loss: 23311.0287 - val_mean_absolute_error: 23311.0287\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 22449.60770\n",
      "Epoch 165/1500\n",
      "934/934 [==============================] - 0s 247us/step - loss: 25717.6073 - mean_absolute_error: 25717.6073 - val_loss: 22381.2322 - val_mean_absolute_error: 22381.2322\n",
      "\n",
      "Epoch 00165: val_loss improved from 22449.60770 to 22381.23218, saving model to Weights-165--22381.23218.hdf5\n",
      "Epoch 166/1500\n",
      "934/934 [==============================] - 0s 262us/step - loss: 25669.7375 - mean_absolute_error: 25669.7375 - val_loss: 22696.0483 - val_mean_absolute_error: 22696.0483\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 22381.23218\n",
      "Epoch 167/1500\n",
      "934/934 [==============================] - 0s 233us/step - loss: 25705.3679 - mean_absolute_error: 25705.3679 - val_loss: 23745.5305 - val_mean_absolute_error: 23745.5305\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 22381.23218\n",
      "Epoch 168/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 25645.8319 - mean_absolute_error: 25645.8319 - val_loss: 23190.6942 - val_mean_absolute_error: 23190.6942\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 22381.23218\n",
      "Epoch 169/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 25548.3448 - mean_absolute_error: 25548.3448 - val_loss: 22155.9489 - val_mean_absolute_error: 22155.9489\n",
      "\n",
      "Epoch 00169: val_loss improved from 22381.23218 to 22155.94888, saving model to Weights-169--22155.94888.hdf5\n",
      "Epoch 170/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 25541.5472 - mean_absolute_error: 25541.5472 - val_loss: 22454.8586 - val_mean_absolute_error: 22454.8586\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 22155.94888\n",
      "Epoch 171/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 25495.1044 - mean_absolute_error: 25495.1044 - val_loss: 22141.1654 - val_mean_absolute_error: 22141.1654\n",
      "\n",
      "Epoch 00171: val_loss improved from 22155.94888 to 22141.16541, saving model to Weights-171--22141.16541.hdf5\n",
      "Epoch 172/1500\n",
      "934/934 [==============================] - 0s 249us/step - loss: 25288.5338 - mean_absolute_error: 25288.5338 - val_loss: 22152.8896 - val_mean_absolute_error: 22152.8896\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 22141.16541\n",
      "Epoch 173/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 25395.9736 - mean_absolute_error: 25395.9736 - val_loss: 22333.2843 - val_mean_absolute_error: 22333.2843\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 22141.16541\n",
      "Epoch 174/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 25315.4089 - mean_absolute_error: 25315.4089 - val_loss: 22902.5264 - val_mean_absolute_error: 22902.5264\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 22141.16541\n",
      "Epoch 175/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 25560.8854 - mean_absolute_error: 25560.8854 - val_loss: 22221.8141 - val_mean_absolute_error: 22221.8141\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 22141.16541\n",
      "Epoch 176/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 25305.6607 - mean_absolute_error: 25305.6607 - val_loss: 22229.3293 - val_mean_absolute_error: 22229.3293\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 22141.16541\n",
      "Epoch 177/1500\n",
      "934/934 [==============================] - 0s 221us/step - loss: 25372.5397 - mean_absolute_error: 25372.5397 - val_loss: 22962.8834 - val_mean_absolute_error: 22962.8834\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 22141.16541\n",
      "Epoch 178/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 25409.0000 - mean_absolute_error: 25409.0000 - val_loss: 22250.7680 - val_mean_absolute_error: 22250.7680\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 22141.16541\n",
      "Epoch 179/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 25428.6203 - mean_absolute_error: 25428.6203 - val_loss: 22769.2122 - val_mean_absolute_error: 22769.2122\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 22141.16541\n",
      "Epoch 180/1500\n",
      "934/934 [==============================] - 0s 246us/step - loss: 25235.4743 - mean_absolute_error: 25235.4743 - val_loss: 22174.7791 - val_mean_absolute_error: 22174.7791\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 22141.16541\n",
      "Epoch 181/1500\n",
      "934/934 [==============================] - 0s 231us/step - loss: 25073.7036 - mean_absolute_error: 25073.7036 - val_loss: 22009.2024 - val_mean_absolute_error: 22009.2024\n",
      "\n",
      "Epoch 00181: val_loss improved from 22141.16541 to 22009.20239, saving model to Weights-181--22009.20239.hdf5\n",
      "Epoch 182/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 25257.8761 - mean_absolute_error: 25257.8761 - val_loss: 22334.3872 - val_mean_absolute_error: 22334.3872\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 22009.20239\n",
      "Epoch 183/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 25011.4823 - mean_absolute_error: 25011.4823 - val_loss: 22560.5876 - val_mean_absolute_error: 22560.5876\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 22009.20239\n",
      "Epoch 184/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 25151.3133 - mean_absolute_error: 25151.3133 - val_loss: 22153.5940 - val_mean_absolute_error: 22153.5940\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 22009.20239\n",
      "Epoch 185/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 25157.1112 - mean_absolute_error: 25157.1112 - val_loss: 22618.3942 - val_mean_absolute_error: 22618.3942\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 22009.20239\n",
      "Epoch 186/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 25071.8133 - mean_absolute_error: 25071.8133 - val_loss: 22226.8418 - val_mean_absolute_error: 22226.8418\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 22009.20239\n",
      "Epoch 187/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 25075.3737 - mean_absolute_error: 25075.3737 - val_loss: 22894.7757 - val_mean_absolute_error: 22894.7757\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 22009.20239\n",
      "Epoch 188/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 25147.3338 - mean_absolute_error: 25147.3338 - val_loss: 22490.2383 - val_mean_absolute_error: 22490.2383\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 22009.20239\n",
      "Epoch 189/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 25023.6931 - mean_absolute_error: 25023.6931 - val_loss: 22250.4841 - val_mean_absolute_error: 22250.4841\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 22009.20239\n",
      "Epoch 190/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 24856.9725 - mean_absolute_error: 24856.9725 - val_loss: 21832.6265 - val_mean_absolute_error: 21832.6265\n",
      "\n",
      "Epoch 00190: val_loss improved from 22009.20239 to 21832.62651, saving model to Weights-190--21832.62651.hdf5\n",
      "Epoch 191/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 24937.9244 - mean_absolute_error: 24937.9244 - val_loss: 21784.4009 - val_mean_absolute_error: 21784.4009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00191: val_loss improved from 21832.62651 to 21784.40095, saving model to Weights-191--21784.40095.hdf5\n",
      "Epoch 192/1500\n",
      "934/934 [==============================] - 0s 231us/step - loss: 24825.0745 - mean_absolute_error: 24825.0745 - val_loss: 22938.8288 - val_mean_absolute_error: 22938.8288\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 21784.40095\n",
      "Epoch 193/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 24803.4272 - mean_absolute_error: 24803.4272 - val_loss: 22338.4405 - val_mean_absolute_error: 22338.4405\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 21784.40095\n",
      "Epoch 194/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 24863.8233 - mean_absolute_error: 24863.8233 - val_loss: 21833.3823 - val_mean_absolute_error: 21833.3823\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 21784.40095\n",
      "Epoch 195/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 25119.7415 - mean_absolute_error: 25119.7415 - val_loss: 22384.0197 - val_mean_absolute_error: 22384.0197\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 21784.40095\n",
      "Epoch 196/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 24939.0915 - mean_absolute_error: 24939.0915 - val_loss: 22058.7855 - val_mean_absolute_error: 22058.7855\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 21784.40095\n",
      "Epoch 197/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 24870.7580 - mean_absolute_error: 24870.7580 - val_loss: 21912.4985 - val_mean_absolute_error: 21912.4985\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 21784.40095\n",
      "Epoch 198/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 24646.9610 - mean_absolute_error: 24646.9610 - val_loss: 23451.9920 - val_mean_absolute_error: 23451.9920\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 21784.40095\n",
      "Epoch 199/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 24614.7470 - mean_absolute_error: 24614.7470 - val_loss: 21755.2635 - val_mean_absolute_error: 21755.2635\n",
      "\n",
      "Epoch 00199: val_loss improved from 21784.40095 to 21755.26351, saving model to Weights-199--21755.26351.hdf5\n",
      "Epoch 200/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 24812.1876 - mean_absolute_error: 24812.1876 - val_loss: 21737.0288 - val_mean_absolute_error: 21737.0288\n",
      "\n",
      "Epoch 00200: val_loss improved from 21755.26351 to 21737.02880, saving model to Weights-200--21737.02880.hdf5\n",
      "Epoch 201/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 24578.0007 - mean_absolute_error: 24578.0007 - val_loss: 23207.9786 - val_mean_absolute_error: 23207.9786\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 21737.02880\n",
      "Epoch 202/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 24622.7971 - mean_absolute_error: 24622.7971 - val_loss: 21623.3093 - val_mean_absolute_error: 21623.3093\n",
      "\n",
      "Epoch 00202: val_loss improved from 21737.02880 to 21623.30934, saving model to Weights-202--21623.30934.hdf5\n",
      "Epoch 203/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 24546.4176 - mean_absolute_error: 24546.4176 - val_loss: 21848.8132 - val_mean_absolute_error: 21848.8132\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 21623.30934\n",
      "Epoch 204/1500\n",
      "934/934 [==============================] - 0s 220us/step - loss: 24908.2460 - mean_absolute_error: 24908.2460 - val_loss: 21621.7274 - val_mean_absolute_error: 21621.7274\n",
      "\n",
      "Epoch 00204: val_loss improved from 21623.30934 to 21621.72744, saving model to Weights-204--21621.72744.hdf5\n",
      "Epoch 205/1500\n",
      "934/934 [==============================] - 0s 222us/step - loss: 24612.3173 - mean_absolute_error: 24612.3173 - val_loss: 22018.1019 - val_mean_absolute_error: 22018.1019\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 21621.72744\n",
      "Epoch 206/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 24746.0857 - mean_absolute_error: 24746.0857 - val_loss: 22196.8410 - val_mean_absolute_error: 22196.8410\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 21621.72744\n",
      "Epoch 207/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 24502.2315 - mean_absolute_error: 24502.2315 - val_loss: 21679.6854 - val_mean_absolute_error: 21679.6854\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 21621.72744\n",
      "Epoch 208/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 24810.6729 - mean_absolute_error: 24810.6729 - val_loss: 21555.3703 - val_mean_absolute_error: 21555.3703\n",
      "\n",
      "Epoch 00208: val_loss improved from 21621.72744 to 21555.37033, saving model to Weights-208--21555.37033.hdf5\n",
      "Epoch 209/1500\n",
      "934/934 [==============================] - 0s 255us/step - loss: 24473.6267 - mean_absolute_error: 24473.6267 - val_loss: 21922.7745 - val_mean_absolute_error: 21922.7745\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 21555.37033\n",
      "Epoch 210/1500\n",
      "934/934 [==============================] - 0s 234us/step - loss: 24477.3058 - mean_absolute_error: 24477.3058 - val_loss: 21940.7724 - val_mean_absolute_error: 21940.7724\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 21555.37033\n",
      "Epoch 211/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 24568.6346 - mean_absolute_error: 24568.6346 - val_loss: 21522.6277 - val_mean_absolute_error: 21522.6277\n",
      "\n",
      "Epoch 00211: val_loss improved from 21555.37033 to 21522.62771, saving model to Weights-211--21522.62771.hdf5\n",
      "Epoch 212/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 24525.4552 - mean_absolute_error: 24525.4552 - val_loss: 21510.0301 - val_mean_absolute_error: 21510.0301\n",
      "\n",
      "Epoch 00212: val_loss improved from 21522.62771 to 21510.03015, saving model to Weights-212--21510.03015.hdf5\n",
      "Epoch 213/1500\n",
      "934/934 [==============================] - 0s 221us/step - loss: 24296.4735 - mean_absolute_error: 24296.4735 - val_loss: 21844.2545 - val_mean_absolute_error: 21844.2545\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 21510.03015\n",
      "Epoch 214/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 24593.8847 - mean_absolute_error: 24593.8847 - val_loss: 21481.7553 - val_mean_absolute_error: 21481.7553\n",
      "\n",
      "Epoch 00214: val_loss improved from 21510.03015 to 21481.75533, saving model to Weights-214--21481.75533.hdf5\n",
      "Epoch 215/1500\n",
      "934/934 [==============================] - 0s 238us/step - loss: 24686.0455 - mean_absolute_error: 24686.0455 - val_loss: 23972.5296 - val_mean_absolute_error: 23972.5296\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 21481.75533\n",
      "Epoch 216/1500\n",
      "934/934 [==============================] - 0s 253us/step - loss: 24475.6725 - mean_absolute_error: 24475.6725 - val_loss: 21748.6511 - val_mean_absolute_error: 21748.6511\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 21481.75533\n",
      "Epoch 217/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 24426.4275 - mean_absolute_error: 24426.4275 - val_loss: 21751.0868 - val_mean_absolute_error: 21751.0868\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 21481.75533\n",
      "Epoch 218/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 24454.1945 - mean_absolute_error: 24454.1945 - val_loss: 21531.2911 - val_mean_absolute_error: 21531.2911\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 21481.75533\n",
      "Epoch 219/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 24157.4695 - mean_absolute_error: 24157.4695 - val_loss: 21500.4659 - val_mean_absolute_error: 21500.4659\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 21481.75533\n",
      "Epoch 220/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 24243.6242 - mean_absolute_error: 24243.6242 - val_loss: 21580.6913 - val_mean_absolute_error: 21580.6913\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 21481.75533\n",
      "Epoch 221/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 24285.7408 - mean_absolute_error: 24285.7408 - val_loss: 21382.4424 - val_mean_absolute_error: 21382.4424\n",
      "\n",
      "Epoch 00221: val_loss improved from 21481.75533 to 21382.44243, saving model to Weights-221--21382.44243.hdf5\n",
      "Epoch 222/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 24108.9594 - mean_absolute_error: 24108.9594 - val_loss: 21212.3752 - val_mean_absolute_error: 21212.3752\n",
      "\n",
      "Epoch 00222: val_loss improved from 21382.44243 to 21212.37520, saving model to Weights-222--21212.37520.hdf5\n",
      "Epoch 223/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 24273.5850 - mean_absolute_error: 24273.5850 - val_loss: 21321.2935 - val_mean_absolute_error: 21321.2935\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 21212.37520\n",
      "Epoch 224/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 24094.9441 - mean_absolute_error: 24094.9441 - val_loss: 22621.7605 - val_mean_absolute_error: 22621.7605\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 21212.37520\n",
      "Epoch 225/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 24195.1415 - mean_absolute_error: 24195.1415 - val_loss: 21440.1894 - val_mean_absolute_error: 21440.1894\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 21212.37520\n",
      "Epoch 226/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 24195.4087 - mean_absolute_error: 24195.4087 - val_loss: 21513.2899 - val_mean_absolute_error: 21513.2899\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 21212.37520\n",
      "Epoch 227/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 24324.7138 - mean_absolute_error: 24324.7138 - val_loss: 21342.1193 - val_mean_absolute_error: 21342.1193\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 21212.37520\n",
      "Epoch 228/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 24151.1396 - mean_absolute_error: 24151.1396 - val_loss: 21446.0017 - val_mean_absolute_error: 21446.0017\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 21212.37520\n",
      "Epoch 229/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 24177.2908 - mean_absolute_error: 24177.2908 - val_loss: 21093.3339 - val_mean_absolute_error: 21093.3339\n",
      "\n",
      "Epoch 00229: val_loss improved from 21212.37520 to 21093.33388, saving model to Weights-229--21093.33388.hdf5\n",
      "Epoch 230/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 24097.6743 - mean_absolute_error: 24097.6743 - val_loss: 21209.9398 - val_mean_absolute_error: 21209.9398\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 21093.33388\n",
      "Epoch 231/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 24276.4950 - mean_absolute_error: 24276.4950 - val_loss: 21229.0883 - val_mean_absolute_error: 21229.0883\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 21093.33388\n",
      "Epoch 232/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 24181.0865 - mean_absolute_error: 24181.0865 - val_loss: 21396.0336 - val_mean_absolute_error: 21396.0336\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 21093.33388\n",
      "Epoch 233/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 24007.0176 - mean_absolute_error: 24007.0176 - val_loss: 21580.1904 - val_mean_absolute_error: 21580.1904\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 21093.33388\n",
      "Epoch 234/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 24036.9197 - mean_absolute_error: 24036.9197 - val_loss: 21330.6877 - val_mean_absolute_error: 21330.6877\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 21093.33388\n",
      "Epoch 235/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 24143.4103 - mean_absolute_error: 24143.4103 - val_loss: 21824.5178 - val_mean_absolute_error: 21824.5178\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 21093.33388\n",
      "Epoch 236/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 23849.3885 - mean_absolute_error: 23849.3885 - val_loss: 21421.0906 - val_mean_absolute_error: 21421.0906\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 21093.33388\n",
      "Epoch 237/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 24049.9205 - mean_absolute_error: 24049.9205 - val_loss: 21351.3083 - val_mean_absolute_error: 21351.3083\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 21093.33388\n",
      "Epoch 238/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 23787.9409 - mean_absolute_error: 23787.9409 - val_loss: 21160.4883 - val_mean_absolute_error: 21160.4883\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 21093.33388\n",
      "Epoch 239/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 23810.5531 - mean_absolute_error: 23810.5531 - val_loss: 21648.1140 - val_mean_absolute_error: 21648.1140\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 21093.33388\n",
      "Epoch 240/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 24007.7582 - mean_absolute_error: 24007.7582 - val_loss: 21225.1386 - val_mean_absolute_error: 21225.1386\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 21093.33388\n",
      "Epoch 241/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 23920.6751 - mean_absolute_error: 23920.6751 - val_loss: 21312.3801 - val_mean_absolute_error: 21312.3801\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 21093.33388\n",
      "Epoch 242/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 24026.1188 - mean_absolute_error: 24026.1188 - val_loss: 21215.6819 - val_mean_absolute_error: 21215.6819\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 21093.33388\n",
      "Epoch 243/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 23760.5875 - mean_absolute_error: 23760.5875 - val_loss: 20943.1565 - val_mean_absolute_error: 20943.1565\n",
      "\n",
      "Epoch 00243: val_loss improved from 21093.33388 to 20943.15647, saving model to Weights-243--20943.15647.hdf5\n",
      "Epoch 244/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 23829.0005 - mean_absolute_error: 23829.0005 - val_loss: 21331.3491 - val_mean_absolute_error: 21331.3491\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 20943.15647\n",
      "Epoch 245/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 23799.7536 - mean_absolute_error: 23799.7536 - val_loss: 21688.6064 - val_mean_absolute_error: 21688.6064\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 20943.15647\n",
      "Epoch 246/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 23713.8502 - mean_absolute_error: 23713.8502 - val_loss: 21290.6757 - val_mean_absolute_error: 21290.6757\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 20943.15647\n",
      "Epoch 247/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 23646.3431 - mean_absolute_error: 23646.3431 - val_loss: 21315.7074 - val_mean_absolute_error: 21315.7074\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 20943.15647\n",
      "Epoch 248/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 23620.4997 - mean_absolute_error: 23620.4997 - val_loss: 21123.1329 - val_mean_absolute_error: 21123.1329\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 20943.15647\n",
      "Epoch 249/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 23738.5027 - mean_absolute_error: 23738.5027 - val_loss: 21101.3368 - val_mean_absolute_error: 21101.3368\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 20943.15647\n",
      "Epoch 250/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 23690.1760 - mean_absolute_error: 23690.1760 - val_loss: 21060.6346 - val_mean_absolute_error: 21060.6346\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 20943.15647\n",
      "Epoch 251/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 23688.6197 - mean_absolute_error: 23688.6197 - val_loss: 21259.2182 - val_mean_absolute_error: 21259.2182\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 20943.15647\n",
      "Epoch 252/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 23633.6284 - mean_absolute_error: 23633.6284 - val_loss: 21140.2896 - val_mean_absolute_error: 21140.2896\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 20943.15647\n",
      "Epoch 253/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 23701.1440 - mean_absolute_error: 23701.1440 - val_loss: 22368.9416 - val_mean_absolute_error: 22368.9416\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 20943.15647\n",
      "Epoch 254/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 23638.5640 - mean_absolute_error: 23638.5640 - val_loss: 21018.3377 - val_mean_absolute_error: 21018.3377\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 20943.15647\n",
      "Epoch 255/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 23677.4560 - mean_absolute_error: 23677.4560 - val_loss: 21019.0726 - val_mean_absolute_error: 21019.0726\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 20943.15647\n",
      "Epoch 256/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 195us/step - loss: 23533.1106 - mean_absolute_error: 23533.1106 - val_loss: 21762.4367 - val_mean_absolute_error: 21762.4367\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 20943.15647\n",
      "Epoch 257/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 23564.0398 - mean_absolute_error: 23564.0398 - val_loss: 20859.9143 - val_mean_absolute_error: 20859.9143\n",
      "\n",
      "Epoch 00257: val_loss improved from 20943.15647 to 20859.91429, saving model to Weights-257--20859.91429.hdf5\n",
      "Epoch 258/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 23664.3846 - mean_absolute_error: 23664.3846 - val_loss: 21163.3240 - val_mean_absolute_error: 21163.3240\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 20859.91429\n",
      "Epoch 259/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 23513.9412 - mean_absolute_error: 23513.9412 - val_loss: 20780.9406 - val_mean_absolute_error: 20780.9406\n",
      "\n",
      "Epoch 00259: val_loss improved from 20859.91429 to 20780.94063, saving model to Weights-259--20780.94063.hdf5\n",
      "Epoch 260/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 23212.1910 - mean_absolute_error: 23212.1910 - val_loss: 21056.1594 - val_mean_absolute_error: 21056.1594\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 20780.94063\n",
      "Epoch 261/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 23469.0707 - mean_absolute_error: 23469.0707 - val_loss: 20755.7763 - val_mean_absolute_error: 20755.7763\n",
      "\n",
      "Epoch 00261: val_loss improved from 20780.94063 to 20755.77630, saving model to Weights-261--20755.77630.hdf5\n",
      "Epoch 262/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 23305.6675 - mean_absolute_error: 23305.6675 - val_loss: 20863.4070 - val_mean_absolute_error: 20863.4070\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 20755.77630\n",
      "Epoch 263/1500\n",
      "934/934 [==============================] - 0s 234us/step - loss: 23427.5149 - mean_absolute_error: 23427.5149 - val_loss: 20628.7780 - val_mean_absolute_error: 20628.7780\n",
      "\n",
      "Epoch 00263: val_loss improved from 20755.77630 to 20628.77799, saving model to Weights-263--20628.77799.hdf5\n",
      "Epoch 264/1500\n",
      "934/934 [==============================] - 0s 243us/step - loss: 23486.0612 - mean_absolute_error: 23486.0612 - val_loss: 20607.8511 - val_mean_absolute_error: 20607.8511\n",
      "\n",
      "Epoch 00264: val_loss improved from 20628.77799 to 20607.85107, saving model to Weights-264--20607.85107.hdf5\n",
      "Epoch 265/1500\n",
      "934/934 [==============================] - 0s 224us/step - loss: 23391.3475 - mean_absolute_error: 23391.3475 - val_loss: 21172.0866 - val_mean_absolute_error: 21172.0866\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 20607.85107\n",
      "Epoch 266/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 23578.3151 - mean_absolute_error: 23578.3151 - val_loss: 20736.6290 - val_mean_absolute_error: 20736.6290\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 20607.85107\n",
      "Epoch 267/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 23200.8375 - mean_absolute_error: 23200.8375 - val_loss: 20672.3680 - val_mean_absolute_error: 20672.3680\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 20607.85107\n",
      "Epoch 268/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 23419.8997 - mean_absolute_error: 23419.8997 - val_loss: 20574.4556 - val_mean_absolute_error: 20574.4556\n",
      "\n",
      "Epoch 00268: val_loss improved from 20607.85107 to 20574.45559, saving model to Weights-268--20574.45559.hdf5\n",
      "Epoch 269/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 23315.8194 - mean_absolute_error: 23315.8194 - val_loss: 21135.7114 - val_mean_absolute_error: 21135.7114\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 20574.45559\n",
      "Epoch 270/1500\n",
      "934/934 [==============================] - 0s 232us/step - loss: 23324.4317 - mean_absolute_error: 23324.4317 - val_loss: 20520.8818 - val_mean_absolute_error: 20520.8818\n",
      "\n",
      "Epoch 00270: val_loss improved from 20574.45559 to 20520.88177, saving model to Weights-270--20520.88177.hdf5\n",
      "Epoch 271/1500\n",
      "934/934 [==============================] - 0s 237us/step - loss: 23127.2527 - mean_absolute_error: 23127.2527 - val_loss: 20562.9367 - val_mean_absolute_error: 20562.9367\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 20520.88177\n",
      "Epoch 272/1500\n",
      "934/934 [==============================] - 0s 236us/step - loss: 23032.6705 - mean_absolute_error: 23032.6705 - val_loss: 21367.3825 - val_mean_absolute_error: 21367.3825\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 20520.88177\n",
      "Epoch 273/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 23005.8008 - mean_absolute_error: 23005.8008 - val_loss: 20396.2769 - val_mean_absolute_error: 20396.2769\n",
      "\n",
      "Epoch 00273: val_loss improved from 20520.88177 to 20396.27692, saving model to Weights-273--20396.27692.hdf5\n",
      "Epoch 274/1500\n",
      "934/934 [==============================] - 0s 235us/step - loss: 23109.1946 - mean_absolute_error: 23109.1946 - val_loss: 20334.7714 - val_mean_absolute_error: 20334.7714\n",
      "\n",
      "Epoch 00274: val_loss improved from 20396.27692 to 20334.77138, saving model to Weights-274--20334.77138.hdf5\n",
      "Epoch 275/1500\n",
      "934/934 [==============================] - 0s 238us/step - loss: 23055.5406 - mean_absolute_error: 23055.5406 - val_loss: 20453.8187 - val_mean_absolute_error: 20453.8187\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 20334.77138\n",
      "Epoch 276/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 22747.5411 - mean_absolute_error: 22747.5411 - val_loss: 20301.0880 - val_mean_absolute_error: 20301.0880\n",
      "\n",
      "Epoch 00276: val_loss improved from 20334.77138 to 20301.08797, saving model to Weights-276--20301.08797.hdf5\n",
      "Epoch 277/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 22794.8100 - mean_absolute_error: 22794.8100 - val_loss: 20497.6827 - val_mean_absolute_error: 20497.6827\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 20301.08797\n",
      "Epoch 278/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 23073.0426 - mean_absolute_error: 23073.0426 - val_loss: 20289.6361 - val_mean_absolute_error: 20289.6361\n",
      "\n",
      "Epoch 00278: val_loss improved from 20301.08797 to 20289.63608, saving model to Weights-278--20289.63608.hdf5\n",
      "Epoch 279/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 22796.0988 - mean_absolute_error: 22796.0988 - val_loss: 20850.9509 - val_mean_absolute_error: 20850.9509\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 20289.63608\n",
      "Epoch 280/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 22715.4180 - mean_absolute_error: 22715.4180 - val_loss: 20220.3046 - val_mean_absolute_error: 20220.3046\n",
      "\n",
      "Epoch 00280: val_loss improved from 20289.63608 to 20220.30457, saving model to Weights-280--20220.30457.hdf5\n",
      "Epoch 281/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 22953.9221 - mean_absolute_error: 22953.9221 - val_loss: 20066.4104 - val_mean_absolute_error: 20066.4104\n",
      "\n",
      "Epoch 00281: val_loss improved from 20220.30457 to 20066.41041, saving model to Weights-281--20066.41041.hdf5\n",
      "Epoch 282/1500\n",
      "934/934 [==============================] - 0s 235us/step - loss: 22614.9951 - mean_absolute_error: 22614.9951 - val_loss: 20415.1589 - val_mean_absolute_error: 20415.1589\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 20066.41041\n",
      "Epoch 283/1500\n",
      "934/934 [==============================] - 0s 241us/step - loss: 23123.1730 - mean_absolute_error: 23123.1730 - val_loss: 19936.4634 - val_mean_absolute_error: 19936.4634\n",
      "\n",
      "Epoch 00283: val_loss improved from 20066.41041 to 19936.46337, saving model to Weights-283--19936.46337.hdf5\n",
      "Epoch 284/1500\n",
      "934/934 [==============================] - 0s 231us/step - loss: 22578.7837 - mean_absolute_error: 22578.7837 - val_loss: 19949.8376 - val_mean_absolute_error: 19949.8376\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 19936.46337\n",
      "Epoch 285/1500\n",
      "934/934 [==============================] - 0s 236us/step - loss: 22886.1280 - mean_absolute_error: 22886.1280 - val_loss: 21809.1197 - val_mean_absolute_error: 21809.1197\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 19936.46337\n",
      "Epoch 286/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 22726.2649 - mean_absolute_error: 22726.2649 - val_loss: 19781.8306 - val_mean_absolute_error: 19781.8306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00286: val_loss improved from 19936.46337 to 19781.83063, saving model to Weights-286--19781.83063.hdf5\n",
      "Epoch 287/1500\n",
      "934/934 [==============================] - 0s 245us/step - loss: 22597.4749 - mean_absolute_error: 22597.4749 - val_loss: 19961.3024 - val_mean_absolute_error: 19961.3024\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 19781.83063\n",
      "Epoch 288/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 22683.7254 - mean_absolute_error: 22683.7254 - val_loss: 19895.9510 - val_mean_absolute_error: 19895.9510\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 19781.83063\n",
      "Epoch 289/1500\n",
      "934/934 [==============================] - 0s 241us/step - loss: 22475.5812 - mean_absolute_error: 22475.5812 - val_loss: 22048.0993 - val_mean_absolute_error: 22048.0993\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 19781.83063\n",
      "Epoch 290/1500\n",
      "934/934 [==============================] - 0s 228us/step - loss: 22679.4413 - mean_absolute_error: 22679.4413 - val_loss: 19680.2840 - val_mean_absolute_error: 19680.2840\n",
      "\n",
      "Epoch 00290: val_loss improved from 19781.83063 to 19680.28405, saving model to Weights-290--19680.28405.hdf5\n",
      "Epoch 291/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 22923.5953 - mean_absolute_error: 22923.5953 - val_loss: 20179.9187 - val_mean_absolute_error: 20179.9187\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 19680.28405\n",
      "Epoch 292/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 22461.6818 - mean_absolute_error: 22461.6818 - val_loss: 19547.7953 - val_mean_absolute_error: 19547.7953\n",
      "\n",
      "Epoch 00292: val_loss improved from 19680.28405 to 19547.79527, saving model to Weights-292--19547.79527.hdf5\n",
      "Epoch 293/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 22898.4041 - mean_absolute_error: 22898.4041 - val_loss: 19638.3481 - val_mean_absolute_error: 19638.3481\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 19547.79527\n",
      "Epoch 294/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 22471.4359 - mean_absolute_error: 22471.4359 - val_loss: 19706.6870 - val_mean_absolute_error: 19706.6870\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 19547.79527\n",
      "Epoch 295/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 22327.4322 - mean_absolute_error: 22327.4322 - val_loss: 19924.8845 - val_mean_absolute_error: 19924.8845\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 19547.79527\n",
      "Epoch 296/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 22355.8502 - mean_absolute_error: 22355.8502 - val_loss: 19476.6218 - val_mean_absolute_error: 19476.6218\n",
      "\n",
      "Epoch 00296: val_loss improved from 19547.79527 to 19476.62182, saving model to Weights-296--19476.62182.hdf5\n",
      "Epoch 297/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 22508.0172 - mean_absolute_error: 22508.0172 - val_loss: 19633.1922 - val_mean_absolute_error: 19633.1922\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 19476.62182\n",
      "Epoch 298/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 22071.1842 - mean_absolute_error: 22071.1842 - val_loss: 19521.5137 - val_mean_absolute_error: 19521.5137\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 19476.62182\n",
      "Epoch 299/1500\n",
      "934/934 [==============================] - 0s 225us/step - loss: 22379.2949 - mean_absolute_error: 22379.2949 - val_loss: 19464.4188 - val_mean_absolute_error: 19464.4188\n",
      "\n",
      "Epoch 00299: val_loss improved from 19476.62182 to 19464.41880, saving model to Weights-299--19464.41880.hdf5\n",
      "Epoch 300/1500\n",
      "934/934 [==============================] - 0s 225us/step - loss: 22291.7607 - mean_absolute_error: 22291.7607 - val_loss: 19792.1314 - val_mean_absolute_error: 19792.1314\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 19464.41880\n",
      "Epoch 301/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 22627.7009 - mean_absolute_error: 22627.7009 - val_loss: 19339.9245 - val_mean_absolute_error: 19339.9245\n",
      "\n",
      "Epoch 00301: val_loss improved from 19464.41880 to 19339.92446, saving model to Weights-301--19339.92446.hdf5\n",
      "Epoch 302/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 21904.8845 - mean_absolute_error: 21904.8845 - val_loss: 19962.8034 - val_mean_absolute_error: 19962.8034\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 19339.92446\n",
      "Epoch 303/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 22248.1983 - mean_absolute_error: 22248.1983 - val_loss: 19904.4244 - val_mean_absolute_error: 19904.4244\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 19339.92446\n",
      "Epoch 304/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 21904.4405 - mean_absolute_error: 21904.4405 - val_loss: 19253.3240 - val_mean_absolute_error: 19253.3240\n",
      "\n",
      "Epoch 00304: val_loss improved from 19339.92446 to 19253.32398, saving model to Weights-304--19253.32398.hdf5\n",
      "Epoch 305/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 22224.2599 - mean_absolute_error: 22224.2599 - val_loss: 19357.6864 - val_mean_absolute_error: 19357.6864\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 19253.32398\n",
      "Epoch 306/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 22275.7710 - mean_absolute_error: 22275.7710 - val_loss: 20361.1309 - val_mean_absolute_error: 20361.1309\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 19253.32398\n",
      "Epoch 307/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 22175.3608 - mean_absolute_error: 22175.3608 - val_loss: 19471.5103 - val_mean_absolute_error: 19471.5103\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 19253.32398\n",
      "Epoch 308/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 22180.6868 - mean_absolute_error: 22180.6868 - val_loss: 20217.5615 - val_mean_absolute_error: 20217.5615\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 19253.32398\n",
      "Epoch 309/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 22154.9053 - mean_absolute_error: 22154.9053 - val_loss: 20096.5947 - val_mean_absolute_error: 20096.5947\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 19253.32398\n",
      "Epoch 310/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 22199.1397 - mean_absolute_error: 22199.1397 - val_loss: 20151.7937 - val_mean_absolute_error: 20151.7937\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 19253.32398\n",
      "Epoch 311/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 22013.9273 - mean_absolute_error: 22013.9273 - val_loss: 19097.0615 - val_mean_absolute_error: 19097.0615\n",
      "\n",
      "Epoch 00311: val_loss improved from 19253.32398 to 19097.06152, saving model to Weights-311--19097.06152.hdf5\n",
      "Epoch 312/1500\n",
      "934/934 [==============================] - 0s 232us/step - loss: 21995.7538 - mean_absolute_error: 21995.7538 - val_loss: 19979.4682 - val_mean_absolute_error: 19979.4682\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 19097.06152\n",
      "Epoch 313/1500\n",
      "934/934 [==============================] - 0s 221us/step - loss: 22178.6240 - mean_absolute_error: 22178.6240 - val_loss: 20315.0406 - val_mean_absolute_error: 20315.0406\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 19097.06152\n",
      "Epoch 314/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 21976.9386 - mean_absolute_error: 21976.9386 - val_loss: 19127.0766 - val_mean_absolute_error: 19127.0766\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 19097.06152\n",
      "Epoch 315/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21862.2790 - mean_absolute_error: 21862.2790 - val_loss: 22274.3678 - val_mean_absolute_error: 22274.3678\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 19097.06152\n",
      "Epoch 316/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 21819.8375 - mean_absolute_error: 21819.8375 - val_loss: 18953.3786 - val_mean_absolute_error: 18953.3786\n",
      "\n",
      "Epoch 00316: val_loss improved from 19097.06152 to 18953.37862, saving model to Weights-316--18953.37862.hdf5\n",
      "Epoch 317/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 21860.9445 - mean_absolute_error: 21860.9445 - val_loss: 19096.4357 - val_mean_absolute_error: 19096.4357\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 18953.37862\n",
      "Epoch 318/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 21807.5566 - mean_absolute_error: 21807.5566 - val_loss: 19061.8970 - val_mean_absolute_error: 19061.8970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00318: val_loss did not improve from 18953.37862\n",
      "Epoch 319/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 21760.3975 - mean_absolute_error: 21760.3975 - val_loss: 20160.1713 - val_mean_absolute_error: 20160.1713\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 18953.37862\n",
      "Epoch 320/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 21994.1688 - mean_absolute_error: 21994.1688 - val_loss: 19483.5831 - val_mean_absolute_error: 19483.5831\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 18953.37862\n",
      "Epoch 321/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 22073.9425 - mean_absolute_error: 22073.9425 - val_loss: 19035.0978 - val_mean_absolute_error: 19035.0978\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 18953.37862\n",
      "Epoch 322/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 21849.7289 - mean_absolute_error: 21849.7289 - val_loss: 19153.7057 - val_mean_absolute_error: 19153.7057\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 18953.37862\n",
      "Epoch 323/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 21550.0623 - mean_absolute_error: 21550.0623 - val_loss: 22505.0071 - val_mean_absolute_error: 22505.0071\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 18953.37862\n",
      "Epoch 324/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 21840.9836 - mean_absolute_error: 21840.9836 - val_loss: 18947.6153 - val_mean_absolute_error: 18947.6153\n",
      "\n",
      "Epoch 00324: val_loss improved from 18953.37862 to 18947.61526, saving model to Weights-324--18947.61526.hdf5\n",
      "Epoch 325/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 21821.7404 - mean_absolute_error: 21821.7404 - val_loss: 22625.8608 - val_mean_absolute_error: 22625.8608\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 18947.61526\n",
      "Epoch 326/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21826.3056 - mean_absolute_error: 21826.3056 - val_loss: 20521.5465 - val_mean_absolute_error: 20521.5465\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 18947.61526\n",
      "Epoch 327/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 21952.3751 - mean_absolute_error: 21952.3751 - val_loss: 19810.1972 - val_mean_absolute_error: 19810.1972\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 18947.61526\n",
      "Epoch 328/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 21866.4390 - mean_absolute_error: 21866.4390 - val_loss: 18892.0493 - val_mean_absolute_error: 18892.0493\n",
      "\n",
      "Epoch 00328: val_loss improved from 18947.61526 to 18892.04931, saving model to Weights-328--18892.04931.hdf5\n",
      "Epoch 329/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 21457.4381 - mean_absolute_error: 21457.4381 - val_loss: 19158.7355 - val_mean_absolute_error: 19158.7355\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 18892.04931\n",
      "Epoch 330/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 21723.0817 - mean_absolute_error: 21723.0817 - val_loss: 18836.8165 - val_mean_absolute_error: 18836.8165\n",
      "\n",
      "Epoch 00330: val_loss improved from 18892.04931 to 18836.81654, saving model to Weights-330--18836.81654.hdf5\n",
      "Epoch 331/1500\n",
      "934/934 [==============================] - 0s 231us/step - loss: 21662.0925 - mean_absolute_error: 21662.0925 - val_loss: 18806.8215 - val_mean_absolute_error: 18806.8215\n",
      "\n",
      "Epoch 00331: val_loss improved from 18836.81654 to 18806.82147, saving model to Weights-331--18806.82147.hdf5\n",
      "Epoch 332/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 21482.4938 - mean_absolute_error: 21482.4938 - val_loss: 18885.5282 - val_mean_absolute_error: 18885.5282\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 18806.82147\n",
      "Epoch 333/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 21632.2464 - mean_absolute_error: 21632.2464 - val_loss: 19978.4690 - val_mean_absolute_error: 19978.4690\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 18806.82147\n",
      "Epoch 334/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21422.7938 - mean_absolute_error: 21422.7938 - val_loss: 19901.9184 - val_mean_absolute_error: 19901.9184\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 18806.82147\n",
      "Epoch 335/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 21805.1353 - mean_absolute_error: 21805.1353 - val_loss: 19669.7424 - val_mean_absolute_error: 19669.7424\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 18806.82147\n",
      "Epoch 336/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 21493.6423 - mean_absolute_error: 21493.6423 - val_loss: 19215.8181 - val_mean_absolute_error: 19215.8181\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 18806.82147\n",
      "Epoch 337/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 21452.6820 - mean_absolute_error: 21452.6820 - val_loss: 19199.2683 - val_mean_absolute_error: 19199.2683\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 18806.82147\n",
      "Epoch 338/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 21505.1515 - mean_absolute_error: 21505.1515 - val_loss: 18848.0722 - val_mean_absolute_error: 18848.0722\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 18806.82147\n",
      "Epoch 339/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21309.9118 - mean_absolute_error: 21309.9118 - val_loss: 18807.7551 - val_mean_absolute_error: 18807.7551\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 18806.82147\n",
      "Epoch 340/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 21507.8668 - mean_absolute_error: 21507.8668 - val_loss: 18777.8784 - val_mean_absolute_error: 18777.8784\n",
      "\n",
      "Epoch 00340: val_loss improved from 18806.82147 to 18777.87838, saving model to Weights-340--18777.87838.hdf5\n",
      "Epoch 341/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 21185.3508 - mean_absolute_error: 21185.3508 - val_loss: 19245.1599 - val_mean_absolute_error: 19245.1599\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 18777.87838\n",
      "Epoch 342/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21406.1679 - mean_absolute_error: 21406.1679 - val_loss: 18813.3930 - val_mean_absolute_error: 18813.3930\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 18777.87838\n",
      "Epoch 343/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 21309.4973 - mean_absolute_error: 21309.4973 - val_loss: 18851.5089 - val_mean_absolute_error: 18851.5089\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 18777.87838\n",
      "Epoch 344/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 21283.9078 - mean_absolute_error: 21283.9078 - val_loss: 19130.6003 - val_mean_absolute_error: 19130.6003\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 18777.87838\n",
      "Epoch 345/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 21219.1145 - mean_absolute_error: 21219.1145 - val_loss: 18768.5188 - val_mean_absolute_error: 18768.5188\n",
      "\n",
      "Epoch 00345: val_loss improved from 18777.87838 to 18768.51878, saving model to Weights-345--18768.51878.hdf5\n",
      "Epoch 346/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 21287.6892 - mean_absolute_error: 21287.6892 - val_loss: 18790.0856 - val_mean_absolute_error: 18790.0856\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 18768.51878\n",
      "Epoch 347/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 21290.4352 - mean_absolute_error: 21290.4352 - val_loss: 19017.7059 - val_mean_absolute_error: 19017.7059\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 18768.51878\n",
      "Epoch 348/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 21187.9288 - mean_absolute_error: 21187.9288 - val_loss: 19731.6279 - val_mean_absolute_error: 19731.6279\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 18768.51878\n",
      "Epoch 349/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 21358.0013 - mean_absolute_error: 21358.0013 - val_loss: 19067.7888 - val_mean_absolute_error: 19067.7888\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 18768.51878\n",
      "Epoch 350/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 21049.3409 - mean_absolute_error: 21049.3409 - val_loss: 19466.7296 - val_mean_absolute_error: 19466.7296\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 18768.51878\n",
      "Epoch 351/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 21228.4952 - mean_absolute_error: 21228.4952 - val_loss: 18840.5250 - val_mean_absolute_error: 18840.5250\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 18768.51878\n",
      "Epoch 352/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 21281.9178 - mean_absolute_error: 21281.9178 - val_loss: 18589.6973 - val_mean_absolute_error: 18589.6973\n",
      "\n",
      "Epoch 00352: val_loss improved from 18768.51878 to 18589.69727, saving model to Weights-352--18589.69727.hdf5\n",
      "Epoch 353/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 21264.0225 - mean_absolute_error: 21264.0225 - val_loss: 18489.2965 - val_mean_absolute_error: 18489.2965\n",
      "\n",
      "Epoch 00353: val_loss improved from 18589.69727 to 18489.29646, saving model to Weights-353--18489.29646.hdf5\n",
      "Epoch 354/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 21267.9241 - mean_absolute_error: 21267.9241 - val_loss: 18828.7972 - val_mean_absolute_error: 18828.7972\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 18489.29646\n",
      "Epoch 355/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 21317.3229 - mean_absolute_error: 21317.3229 - val_loss: 18985.1253 - val_mean_absolute_error: 18985.1253\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 18489.29646\n",
      "Epoch 356/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21235.3796 - mean_absolute_error: 21235.3796 - val_loss: 19333.2103 - val_mean_absolute_error: 19333.2103\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 18489.29646\n",
      "Epoch 357/1500\n",
      "934/934 [==============================] - 0s 213us/step - loss: 20949.4955 - mean_absolute_error: 20949.4955 - val_loss: 18725.2978 - val_mean_absolute_error: 18725.2978\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 18489.29646\n",
      "Epoch 358/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 21196.0154 - mean_absolute_error: 21196.0154 - val_loss: 19261.0414 - val_mean_absolute_error: 19261.0414\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 18489.29646\n",
      "Epoch 359/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21314.4179 - mean_absolute_error: 21314.4179 - val_loss: 18439.9045 - val_mean_absolute_error: 18439.9045\n",
      "\n",
      "Epoch 00359: val_loss improved from 18489.29646 to 18439.90452, saving model to Weights-359--18439.90452.hdf5\n",
      "Epoch 360/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 21089.5650 - mean_absolute_error: 21089.5650 - val_loss: 18467.3475 - val_mean_absolute_error: 18467.3475\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 18439.90452\n",
      "Epoch 361/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 20764.6363 - mean_absolute_error: 20764.6363 - val_loss: 18876.2941 - val_mean_absolute_error: 18876.2941\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 18439.90452\n",
      "Epoch 362/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 20821.5104 - mean_absolute_error: 20821.5104 - val_loss: 19063.2275 - val_mean_absolute_error: 19063.2275\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 18439.90452\n",
      "Epoch 363/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 20981.7540 - mean_absolute_error: 20981.7540 - val_loss: 18658.3019 - val_mean_absolute_error: 18658.3019\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 18439.90452\n",
      "Epoch 364/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 20961.0075 - mean_absolute_error: 20961.0075 - val_loss: 18951.3435 - val_mean_absolute_error: 18951.3435\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 18439.90452\n",
      "Epoch 365/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 20977.5611 - mean_absolute_error: 20977.5611 - val_loss: 18762.1612 - val_mean_absolute_error: 18762.1612\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 18439.90452\n",
      "Epoch 366/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 21033.6739 - mean_absolute_error: 21033.6739 - val_loss: 18421.7898 - val_mean_absolute_error: 18421.7898\n",
      "\n",
      "Epoch 00366: val_loss improved from 18439.90452 to 18421.78976, saving model to Weights-366--18421.78976.hdf5\n",
      "Epoch 367/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 20771.6569 - mean_absolute_error: 20771.6569 - val_loss: 18487.4148 - val_mean_absolute_error: 18487.4148\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 18421.78976\n",
      "Epoch 368/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 20824.4546 - mean_absolute_error: 20824.4546 - val_loss: 18452.9928 - val_mean_absolute_error: 18452.9928\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 18421.78976\n",
      "Epoch 369/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 20906.8766 - mean_absolute_error: 20906.8766 - val_loss: 18386.7823 - val_mean_absolute_error: 18386.7823\n",
      "\n",
      "Epoch 00369: val_loss improved from 18421.78976 to 18386.78226, saving model to Weights-369--18386.78226.hdf5\n",
      "Epoch 370/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 20753.4218 - mean_absolute_error: 20753.4218 - val_loss: 18393.1928 - val_mean_absolute_error: 18393.1928\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 18386.78226\n",
      "Epoch 371/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 20825.3177 - mean_absolute_error: 20825.3177 - val_loss: 18289.0417 - val_mean_absolute_error: 18289.0417\n",
      "\n",
      "Epoch 00371: val_loss improved from 18386.78226 to 18289.04168, saving model to Weights-371--18289.04168.hdf5\n",
      "Epoch 372/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 20818.3410 - mean_absolute_error: 20818.3410 - val_loss: 19407.9776 - val_mean_absolute_error: 19407.9776\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 18289.04168\n",
      "Epoch 373/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 20964.5562 - mean_absolute_error: 20964.5562 - val_loss: 18331.9455 - val_mean_absolute_error: 18331.9455\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 18289.04168\n",
      "Epoch 374/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 20623.5442 - mean_absolute_error: 20623.5442 - val_loss: 18482.8232 - val_mean_absolute_error: 18482.8232\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 18289.04168\n",
      "Epoch 375/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 20595.9697 - mean_absolute_error: 20595.9697 - val_loss: 18283.8938 - val_mean_absolute_error: 18283.8938\n",
      "\n",
      "Epoch 00375: val_loss improved from 18289.04168 to 18283.89378, saving model to Weights-375--18283.89378.hdf5\n",
      "Epoch 376/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 20824.2458 - mean_absolute_error: 20824.2458 - val_loss: 18400.5020 - val_mean_absolute_error: 18400.5020\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 18283.89378\n",
      "Epoch 377/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 20570.1591 - mean_absolute_error: 20570.1591 - val_loss: 18371.5038 - val_mean_absolute_error: 18371.5038\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 18283.89378\n",
      "Epoch 378/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 20920.7518 - mean_absolute_error: 20920.7518 - val_loss: 19951.5299 - val_mean_absolute_error: 19951.5299\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 18283.89378\n",
      "Epoch 379/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 21026.9207 - mean_absolute_error: 21026.9207 - val_loss: 18179.5185 - val_mean_absolute_error: 18179.5185\n",
      "\n",
      "Epoch 00379: val_loss improved from 18283.89378 to 18179.51845, saving model to Weights-379--18179.51845.hdf5\n",
      "Epoch 380/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 20727.4705 - mean_absolute_error: 20727.4705 - val_loss: 18597.8359 - val_mean_absolute_error: 18597.8359\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 18179.51845\n",
      "Epoch 381/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 20709.2197 - mean_absolute_error: 20709.2197 - val_loss: 18220.8548 - val_mean_absolute_error: 18220.8548\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 18179.51845\n",
      "Epoch 382/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 20531.4946 - mean_absolute_error: 20531.4946 - val_loss: 18395.3383 - val_mean_absolute_error: 18395.3383\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 18179.51845\n",
      "Epoch 383/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 219us/step - loss: 20431.4642 - mean_absolute_error: 20431.4642 - val_loss: 19546.4353 - val_mean_absolute_error: 19546.4353\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 18179.51845\n",
      "Epoch 384/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 20501.2983 - mean_absolute_error: 20501.2983 - val_loss: 18141.6400 - val_mean_absolute_error: 18141.6400\n",
      "\n",
      "Epoch 00384: val_loss improved from 18179.51845 to 18141.64002, saving model to Weights-384--18141.64002.hdf5\n",
      "Epoch 385/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 20931.3426 - mean_absolute_error: 20931.3426 - val_loss: 18266.5127 - val_mean_absolute_error: 18266.5127\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 18141.64002\n",
      "Epoch 386/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 20719.1435 - mean_absolute_error: 20719.1435 - val_loss: 18461.5419 - val_mean_absolute_error: 18461.5419\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 18141.64002\n",
      "Epoch 387/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 20761.0408 - mean_absolute_error: 20761.0408 - val_loss: 18591.7497 - val_mean_absolute_error: 18591.7497\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 18141.64002\n",
      "Epoch 388/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 20487.9242 - mean_absolute_error: 20487.9242 - val_loss: 21275.6968 - val_mean_absolute_error: 21275.6968\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 18141.64002\n",
      "Epoch 389/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 20426.1369 - mean_absolute_error: 20426.1369 - val_loss: 19437.2996 - val_mean_absolute_error: 19437.2996\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 18141.64002\n",
      "Epoch 390/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 20522.6983 - mean_absolute_error: 20522.6983 - val_loss: 18584.0707 - val_mean_absolute_error: 18584.0707\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 18141.64002\n",
      "Epoch 391/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 20361.4233 - mean_absolute_error: 20361.4233 - val_loss: 18169.6803 - val_mean_absolute_error: 18169.6803\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 18141.64002\n",
      "Epoch 392/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 20087.9310 - mean_absolute_error: 20087.9310 - val_loss: 18935.3668 - val_mean_absolute_error: 18935.3668\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 18141.64002\n",
      "Epoch 393/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 20425.1698 - mean_absolute_error: 20425.1698 - val_loss: 18336.8164 - val_mean_absolute_error: 18336.8164\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 18141.64002\n",
      "Epoch 394/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 20213.3249 - mean_absolute_error: 20213.3249 - val_loss: 18519.0391 - val_mean_absolute_error: 18519.0391\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 18141.64002\n",
      "Epoch 395/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 20442.6668 - mean_absolute_error: 20442.6668 - val_loss: 18203.6099 - val_mean_absolute_error: 18203.6099\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 18141.64002\n",
      "Epoch 396/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 20334.2341 - mean_absolute_error: 20334.2341 - val_loss: 18421.7988 - val_mean_absolute_error: 18421.7988\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 18141.64002\n",
      "Epoch 397/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 21062.7020 - mean_absolute_error: 21062.7020 - val_loss: 19634.9291 - val_mean_absolute_error: 19634.9291\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 18141.64002\n",
      "Epoch 398/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 20292.7349 - mean_absolute_error: 20292.7349 - val_loss: 18216.0912 - val_mean_absolute_error: 18216.0912\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 18141.64002\n",
      "Epoch 399/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 20296.2826 - mean_absolute_error: 20296.2826 - val_loss: 18025.6957 - val_mean_absolute_error: 18025.6957\n",
      "\n",
      "Epoch 00399: val_loss improved from 18141.64002 to 18025.69574, saving model to Weights-399--18025.69574.hdf5\n",
      "Epoch 400/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 20150.4683 - mean_absolute_error: 20150.4683 - val_loss: 18177.4981 - val_mean_absolute_error: 18177.4981\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 18025.69574\n",
      "Epoch 401/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 20256.8998 - mean_absolute_error: 20256.8998 - val_loss: 18526.2761 - val_mean_absolute_error: 18526.2761\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 18025.69574\n",
      "Epoch 402/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 20235.4884 - mean_absolute_error: 20235.4884 - val_loss: 18044.3694 - val_mean_absolute_error: 18044.3694\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 18025.69574\n",
      "Epoch 403/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 20084.2439 - mean_absolute_error: 20084.2439 - val_loss: 18155.0951 - val_mean_absolute_error: 18155.0951\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 18025.69574\n",
      "Epoch 404/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 20095.9432 - mean_absolute_error: 20095.9432 - val_loss: 18129.3388 - val_mean_absolute_error: 18129.3388\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 18025.69574\n",
      "Epoch 405/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 20306.9797 - mean_absolute_error: 20306.9797 - val_loss: 18505.1708 - val_mean_absolute_error: 18505.1708\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 18025.69574\n",
      "Epoch 406/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 20533.9228 - mean_absolute_error: 20533.9228 - val_loss: 18264.0970 - val_mean_absolute_error: 18264.0970\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 18025.69574\n",
      "Epoch 407/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 20209.5453 - mean_absolute_error: 20209.5453 - val_loss: 18150.4054 - val_mean_absolute_error: 18150.4054\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 18025.69574\n",
      "Epoch 408/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 20205.5704 - mean_absolute_error: 20205.5704 - val_loss: 18181.3502 - val_mean_absolute_error: 18181.3502\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 18025.69574\n",
      "Epoch 409/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 20234.5126 - mean_absolute_error: 20234.5126 - val_loss: 18899.5932 - val_mean_absolute_error: 18899.5932\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 18025.69574\n",
      "Epoch 410/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 20265.4385 - mean_absolute_error: 20265.4385 - val_loss: 18322.5167 - val_mean_absolute_error: 18322.5167\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 18025.69574\n",
      "Epoch 411/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 19814.2948 - mean_absolute_error: 19814.2948 - val_loss: 19418.7323 - val_mean_absolute_error: 19418.7323\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 18025.69574\n",
      "Epoch 412/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 20312.2506 - mean_absolute_error: 20312.2506 - val_loss: 18288.2182 - val_mean_absolute_error: 18288.2182\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 18025.69574\n",
      "Epoch 413/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 19736.3463 - mean_absolute_error: 19736.3463 - val_loss: 18878.3403 - val_mean_absolute_error: 18878.3403\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 18025.69574\n",
      "Epoch 414/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 20066.6842 - mean_absolute_error: 20066.6842 - val_loss: 17960.7066 - val_mean_absolute_error: 17960.7066\n",
      "\n",
      "Epoch 00414: val_loss improved from 18025.69574 to 17960.70664, saving model to Weights-414--17960.70664.hdf5\n",
      "Epoch 415/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 20050.6085 - mean_absolute_error: 20050.6085 - val_loss: 18176.0173 - val_mean_absolute_error: 18176.0173\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 17960.70664\n",
      "Epoch 416/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 19490.4868 - mean_absolute_error: 19490.4868 - val_loss: 18988.6548 - val_mean_absolute_error: 18988.6548\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 17960.70664\n",
      "Epoch 417/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 19721.2723 - mean_absolute_error: 19721.2723 - val_loss: 17995.5900 - val_mean_absolute_error: 17995.5900\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 17960.70664\n",
      "Epoch 418/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 19955.9683 - mean_absolute_error: 19955.9683 - val_loss: 17980.9092 - val_mean_absolute_error: 17980.9092\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 17960.70664\n",
      "Epoch 419/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 19803.5630 - mean_absolute_error: 19803.5630 - val_loss: 17951.7393 - val_mean_absolute_error: 17951.7393\n",
      "\n",
      "Epoch 00419: val_loss improved from 17960.70664 to 17951.73926, saving model to Weights-419--17951.73926.hdf5\n",
      "Epoch 420/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 19856.6259 - mean_absolute_error: 19856.6259 - val_loss: 18015.0293 - val_mean_absolute_error: 18015.0293\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 17951.73926\n",
      "Epoch 421/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 19790.7833 - mean_absolute_error: 19790.7833 - val_loss: 18734.0880 - val_mean_absolute_error: 18734.0880\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 17951.73926\n",
      "Epoch 422/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 19788.6960 - mean_absolute_error: 19788.6960 - val_loss: 18174.5833 - val_mean_absolute_error: 18174.5833\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 17951.73926\n",
      "Epoch 423/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 20191.4810 - mean_absolute_error: 20191.4810 - val_loss: 18307.1723 - val_mean_absolute_error: 18307.1723\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 17951.73926\n",
      "Epoch 424/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 20347.5270 - mean_absolute_error: 20347.5270 - val_loss: 18348.9617 - val_mean_absolute_error: 18348.9617\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 17951.73926\n",
      "Epoch 425/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 20140.4444 - mean_absolute_error: 20140.4444 - val_loss: 17942.5619 - val_mean_absolute_error: 17942.5619\n",
      "\n",
      "Epoch 00425: val_loss improved from 17951.73926 to 17942.56193, saving model to Weights-425--17942.56193.hdf5\n",
      "Epoch 426/1500\n",
      "934/934 [==============================] - 0s 225us/step - loss: 20089.7110 - mean_absolute_error: 20089.7110 - val_loss: 18195.6177 - val_mean_absolute_error: 18195.6177\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 17942.56193\n",
      "Epoch 427/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 19923.8526 - mean_absolute_error: 19923.8526 - val_loss: 18119.7630 - val_mean_absolute_error: 18119.7630\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 17942.56193\n",
      "Epoch 428/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 19682.8595 - mean_absolute_error: 19682.8595 - val_loss: 18185.5574 - val_mean_absolute_error: 18185.5574\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 17942.56193\n",
      "Epoch 429/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 19697.7222 - mean_absolute_error: 19697.7222 - val_loss: 18063.0040 - val_mean_absolute_error: 18063.0040\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 17942.56193\n",
      "Epoch 430/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 20327.7233 - mean_absolute_error: 20327.7233 - val_loss: 18297.0483 - val_mean_absolute_error: 18297.0483\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 17942.56193\n",
      "Epoch 431/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 19804.7283 - mean_absolute_error: 19804.7283 - val_loss: 17984.9415 - val_mean_absolute_error: 17984.9415\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 17942.56193\n",
      "Epoch 432/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 19563.3640 - mean_absolute_error: 19563.3640 - val_loss: 17825.8075 - val_mean_absolute_error: 17825.8075\n",
      "\n",
      "Epoch 00432: val_loss improved from 17942.56193 to 17825.80750, saving model to Weights-432--17825.80750.hdf5\n",
      "Epoch 433/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 19649.8622 - mean_absolute_error: 19649.8622 - val_loss: 18760.6746 - val_mean_absolute_error: 18760.6746\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 17825.80750\n",
      "Epoch 434/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 19763.2968 - mean_absolute_error: 19763.2968 - val_loss: 18029.2246 - val_mean_absolute_error: 18029.2246\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 17825.80750\n",
      "Epoch 435/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 20020.2546 - mean_absolute_error: 20020.2546 - val_loss: 17919.4684 - val_mean_absolute_error: 17919.4684\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 17825.80750\n",
      "Epoch 436/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 19894.5721 - mean_absolute_error: 19894.5721 - val_loss: 18798.7180 - val_mean_absolute_error: 18798.7180\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 17825.80750\n",
      "Epoch 437/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 19725.8566 - mean_absolute_error: 19725.8566 - val_loss: 20435.5383 - val_mean_absolute_error: 20435.5383\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 17825.80750\n",
      "Epoch 438/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 19496.1930 - mean_absolute_error: 19496.1930 - val_loss: 18005.1578 - val_mean_absolute_error: 18005.1578\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 17825.80750\n",
      "Epoch 439/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 19464.8812 - mean_absolute_error: 19464.8812 - val_loss: 17894.2473 - val_mean_absolute_error: 17894.2473\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 17825.80750\n",
      "Epoch 440/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 19577.5065 - mean_absolute_error: 19577.5065 - val_loss: 18131.8506 - val_mean_absolute_error: 18131.8506\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 17825.80750\n",
      "Epoch 441/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 19423.5488 - mean_absolute_error: 19423.5488 - val_loss: 18156.1621 - val_mean_absolute_error: 18156.1621\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 17825.80750\n",
      "Epoch 442/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 19429.0873 - mean_absolute_error: 19429.0873 - val_loss: 17915.9120 - val_mean_absolute_error: 17915.9120\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 17825.80750\n",
      "Epoch 443/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 19764.0546 - mean_absolute_error: 19764.0546 - val_loss: 17992.5657 - val_mean_absolute_error: 17992.5657\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 17825.80750\n",
      "Epoch 444/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 19620.4716 - mean_absolute_error: 19620.4716 - val_loss: 18621.9765 - val_mean_absolute_error: 18621.9765\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 17825.80750\n",
      "Epoch 445/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 19520.4512 - mean_absolute_error: 19520.4512 - val_loss: 19606.7002 - val_mean_absolute_error: 19606.7002\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 17825.80750\n",
      "Epoch 446/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 19565.9804 - mean_absolute_error: 19565.9804 - val_loss: 18392.0794 - val_mean_absolute_error: 18392.0794\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 17825.80750\n",
      "Epoch 447/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 19427.3465 - mean_absolute_error: 19427.3465 - val_loss: 17910.1087 - val_mean_absolute_error: 17910.1087\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 17825.80750\n",
      "Epoch 448/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 19814.4145 - mean_absolute_error: 19814.4145 - val_loss: 18121.5739 - val_mean_absolute_error: 18121.5739\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 17825.80750\n",
      "Epoch 449/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 193us/step - loss: 19586.9846 - mean_absolute_error: 19586.9846 - val_loss: 18015.6393 - val_mean_absolute_error: 18015.6393\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 17825.80750\n",
      "Epoch 450/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 19465.9808 - mean_absolute_error: 19465.9808 - val_loss: 17794.6136 - val_mean_absolute_error: 17794.6136\n",
      "\n",
      "Epoch 00450: val_loss improved from 17825.80750 to 17794.61359, saving model to Weights-450--17794.61359.hdf5\n",
      "Epoch 451/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 19695.2554 - mean_absolute_error: 19695.2554 - val_loss: 18182.7891 - val_mean_absolute_error: 18182.7891\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 17794.61359\n",
      "Epoch 452/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 19299.5064 - mean_absolute_error: 19299.5064 - val_loss: 17922.1858 - val_mean_absolute_error: 17922.1858\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 17794.61359\n",
      "Epoch 453/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 19423.5385 - mean_absolute_error: 19423.5385 - val_loss: 19495.7368 - val_mean_absolute_error: 19495.7368\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 17794.61359\n",
      "Epoch 454/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 19435.3449 - mean_absolute_error: 19435.3449 - val_loss: 18183.3103 - val_mean_absolute_error: 18183.3103\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 17794.61359\n",
      "Epoch 455/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 19415.4057 - mean_absolute_error: 19415.4057 - val_loss: 17969.5359 - val_mean_absolute_error: 17969.5359\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 17794.61359\n",
      "Epoch 456/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 19362.6237 - mean_absolute_error: 19362.6237 - val_loss: 17826.9331 - val_mean_absolute_error: 17826.9331\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 17794.61359\n",
      "Epoch 457/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 19879.7457 - mean_absolute_error: 19879.7457 - val_loss: 17954.6024 - val_mean_absolute_error: 17954.6024\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 17794.61359\n",
      "Epoch 458/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 19886.8727 - mean_absolute_error: 19886.8727 - val_loss: 17782.6696 - val_mean_absolute_error: 17782.6696\n",
      "\n",
      "Epoch 00458: val_loss improved from 17794.61359 to 17782.66963, saving model to Weights-458--17782.66963.hdf5\n",
      "Epoch 459/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 19562.4503 - mean_absolute_error: 19562.4503 - val_loss: 18113.7860 - val_mean_absolute_error: 18113.7860\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 17782.66963\n",
      "Epoch 460/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 19311.8269 - mean_absolute_error: 19311.8269 - val_loss: 18172.7464 - val_mean_absolute_error: 18172.7464\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 17782.66963\n",
      "Epoch 461/1500\n",
      "934/934 [==============================] - 0s 220us/step - loss: 19163.0482 - mean_absolute_error: 19163.0482 - val_loss: 18323.3099 - val_mean_absolute_error: 18323.3099\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 17782.66963\n",
      "Epoch 462/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 19460.6222 - mean_absolute_error: 19460.6222 - val_loss: 17889.9446 - val_mean_absolute_error: 17889.9446\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 17782.66963\n",
      "Epoch 463/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 19414.0543 - mean_absolute_error: 19414.0543 - val_loss: 18059.3877 - val_mean_absolute_error: 18059.3877\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 17782.66963\n",
      "Epoch 464/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 19155.0486 - mean_absolute_error: 19155.0486 - val_loss: 18007.0723 - val_mean_absolute_error: 18007.0723\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 17782.66963\n",
      "Epoch 465/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 19056.7124 - mean_absolute_error: 19056.7124 - val_loss: 18985.3653 - val_mean_absolute_error: 18985.3653\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 17782.66963\n",
      "Epoch 466/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 19464.9440 - mean_absolute_error: 19464.9440 - val_loss: 17850.5888 - val_mean_absolute_error: 17850.5888\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 17782.66963\n",
      "Epoch 467/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 19266.5553 - mean_absolute_error: 19266.5553 - val_loss: 17870.6720 - val_mean_absolute_error: 17870.6720\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 17782.66963\n",
      "Epoch 468/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 19059.1625 - mean_absolute_error: 19059.1625 - val_loss: 18175.5228 - val_mean_absolute_error: 18175.5228\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 17782.66963\n",
      "Epoch 469/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 19497.0310 - mean_absolute_error: 19497.0310 - val_loss: 19501.5667 - val_mean_absolute_error: 19501.5667\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 17782.66963\n",
      "Epoch 470/1500\n",
      "934/934 [==============================] - 0s 220us/step - loss: 19019.4050 - mean_absolute_error: 19019.4050 - val_loss: 17937.1979 - val_mean_absolute_error: 17937.1979\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 17782.66963\n",
      "Epoch 471/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 19524.6212 - mean_absolute_error: 19524.6212 - val_loss: 18008.5226 - val_mean_absolute_error: 18008.5226\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 17782.66963\n",
      "Epoch 472/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 19518.8937 - mean_absolute_error: 19518.8937 - val_loss: 17955.9967 - val_mean_absolute_error: 17955.9967\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 17782.66963\n",
      "Epoch 473/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 19105.5381 - mean_absolute_error: 19105.5381 - val_loss: 17925.5574 - val_mean_absolute_error: 17925.5574\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 17782.66963\n",
      "Epoch 474/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 19242.6879 - mean_absolute_error: 19242.6879 - val_loss: 17855.4738 - val_mean_absolute_error: 17855.4738\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 17782.66963\n",
      "Epoch 475/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 19644.7188 - mean_absolute_error: 19644.7188 - val_loss: 18794.9594 - val_mean_absolute_error: 18794.9594\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 17782.66963\n",
      "Epoch 476/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 19292.8432 - mean_absolute_error: 19292.8432 - val_loss: 18294.3960 - val_mean_absolute_error: 18294.3960\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 17782.66963\n",
      "Epoch 477/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 19063.1062 - mean_absolute_error: 19063.1062 - val_loss: 17763.5302 - val_mean_absolute_error: 17763.5302\n",
      "\n",
      "Epoch 00477: val_loss improved from 17782.66963 to 17763.53022, saving model to Weights-477--17763.53022.hdf5\n",
      "Epoch 478/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 19117.4937 - mean_absolute_error: 19117.4937 - val_loss: 18531.6020 - val_mean_absolute_error: 18531.6020\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 17763.53022\n",
      "Epoch 479/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 19125.0916 - mean_absolute_error: 19125.0916 - val_loss: 17870.5512 - val_mean_absolute_error: 17870.5512\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 17763.53022\n",
      "Epoch 480/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 18977.1717 - mean_absolute_error: 18977.1717 - val_loss: 18339.4140 - val_mean_absolute_error: 18339.4140\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 17763.53022\n",
      "Epoch 481/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 19157.0901 - mean_absolute_error: 19157.0901 - val_loss: 18726.3043 - val_mean_absolute_error: 18726.3043\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 17763.53022\n",
      "Epoch 482/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 18807.2155 - mean_absolute_error: 18807.2155 - val_loss: 17754.3044 - val_mean_absolute_error: 17754.3044\n",
      "\n",
      "Epoch 00482: val_loss improved from 17763.53022 to 17754.30435, saving model to Weights-482--17754.30435.hdf5\n",
      "Epoch 483/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 19066.1595 - mean_absolute_error: 19066.1595 - val_loss: 18939.6730 - val_mean_absolute_error: 18939.6730\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 17754.30435\n",
      "Epoch 484/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18954.2309 - mean_absolute_error: 18954.2309 - val_loss: 19705.9288 - val_mean_absolute_error: 19705.9288\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 17754.30435\n",
      "Epoch 485/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 19090.5937 - mean_absolute_error: 19090.5937 - val_loss: 17790.5951 - val_mean_absolute_error: 17790.5951\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 17754.30435\n",
      "Epoch 486/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 19092.2297 - mean_absolute_error: 19092.2297 - val_loss: 18548.0548 - val_mean_absolute_error: 18548.0548\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 17754.30435\n",
      "Epoch 487/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 19405.4871 - mean_absolute_error: 19405.4871 - val_loss: 19477.6526 - val_mean_absolute_error: 19477.6526\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 17754.30435\n",
      "Epoch 488/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 19057.6032 - mean_absolute_error: 19057.6032 - val_loss: 18313.4460 - val_mean_absolute_error: 18313.4460\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 17754.30435\n",
      "Epoch 489/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18854.4631 - mean_absolute_error: 18854.4631 - val_loss: 17851.1728 - val_mean_absolute_error: 17851.1728\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 17754.30435\n",
      "Epoch 490/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18676.7864 - mean_absolute_error: 18676.7864 - val_loss: 18006.2589 - val_mean_absolute_error: 18006.2589\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 17754.30435\n",
      "Epoch 491/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 19358.3947 - mean_absolute_error: 19358.3947 - val_loss: 18667.4933 - val_mean_absolute_error: 18667.4933\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 17754.30435\n",
      "Epoch 492/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18664.4656 - mean_absolute_error: 18664.4656 - val_loss: 18492.4911 - val_mean_absolute_error: 18492.4911\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 17754.30435\n",
      "Epoch 493/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 19045.9125 - mean_absolute_error: 19045.9125 - val_loss: 18490.8456 - val_mean_absolute_error: 18490.8456\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 17754.30435\n",
      "Epoch 494/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 19265.7924 - mean_absolute_error: 19265.7924 - val_loss: 19468.2427 - val_mean_absolute_error: 19468.2427\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 17754.30435\n",
      "Epoch 495/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 19471.0281 - mean_absolute_error: 19471.0281 - val_loss: 17714.5737 - val_mean_absolute_error: 17714.5737\n",
      "\n",
      "Epoch 00495: val_loss improved from 17754.30435 to 17714.57369, saving model to Weights-495--17714.57369.hdf5\n",
      "Epoch 496/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 18900.5066 - mean_absolute_error: 18900.5066 - val_loss: 18211.2133 - val_mean_absolute_error: 18211.2133\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 17714.57369\n",
      "Epoch 497/1500\n",
      "934/934 [==============================] - 0s 225us/step - loss: 19246.1455 - mean_absolute_error: 19246.1455 - val_loss: 20391.3353 - val_mean_absolute_error: 20391.3353\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 17714.57369\n",
      "Epoch 498/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 19089.8455 - mean_absolute_error: 19089.8455 - val_loss: 17764.2623 - val_mean_absolute_error: 17764.2623\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 17714.57369\n",
      "Epoch 499/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 19238.7097 - mean_absolute_error: 19238.7097 - val_loss: 17864.4389 - val_mean_absolute_error: 17864.4389\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 17714.57369\n",
      "Epoch 500/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 19071.6957 - mean_absolute_error: 19071.6957 - val_loss: 18182.9023 - val_mean_absolute_error: 18182.9023\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 17714.57369\n",
      "Epoch 501/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 18786.3413 - mean_absolute_error: 18786.3413 - val_loss: 18293.1227 - val_mean_absolute_error: 18293.1227\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 17714.57369\n",
      "Epoch 502/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 18804.8616 - mean_absolute_error: 18804.8616 - val_loss: 17703.3228 - val_mean_absolute_error: 17703.3228\n",
      "\n",
      "Epoch 00502: val_loss improved from 17714.57369 to 17703.32275, saving model to Weights-502--17703.32275.hdf5\n",
      "Epoch 503/1500\n",
      "934/934 [==============================] - 0s 222us/step - loss: 18776.8800 - mean_absolute_error: 18776.8800 - val_loss: 18196.4337 - val_mean_absolute_error: 18196.4337\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 17703.32275\n",
      "Epoch 504/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 18997.4184 - mean_absolute_error: 18997.4184 - val_loss: 18169.5637 - val_mean_absolute_error: 18169.5637\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 17703.32275\n",
      "Epoch 505/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 18808.8568 - mean_absolute_error: 18808.8568 - val_loss: 17917.4807 - val_mean_absolute_error: 17917.4807\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 17703.32275\n",
      "Epoch 506/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 18769.8487 - mean_absolute_error: 18769.8487 - val_loss: 17980.2433 - val_mean_absolute_error: 17980.2433\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 17703.32275\n",
      "Epoch 507/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 18963.8781 - mean_absolute_error: 18963.8781 - val_loss: 17886.0700 - val_mean_absolute_error: 17886.0700\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 17703.32275\n",
      "Epoch 508/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 19033.9757 - mean_absolute_error: 19033.9757 - val_loss: 18253.4989 - val_mean_absolute_error: 18253.4989\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 17703.32275\n",
      "Epoch 509/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 18999.2551 - mean_absolute_error: 18999.2551 - val_loss: 18334.6888 - val_mean_absolute_error: 18334.6888\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 17703.32275\n",
      "Epoch 510/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 18896.2104 - mean_absolute_error: 18896.2104 - val_loss: 17739.4715 - val_mean_absolute_error: 17739.4715\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 17703.32275\n",
      "Epoch 511/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 18612.0703 - mean_absolute_error: 18612.0703 - val_loss: 17809.4891 - val_mean_absolute_error: 17809.4891\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 17703.32275\n",
      "Epoch 512/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 18658.2584 - mean_absolute_error: 18658.2584 - val_loss: 18179.4571 - val_mean_absolute_error: 18179.4571\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 17703.32275\n",
      "Epoch 513/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 18844.2644 - mean_absolute_error: 18844.2644 - val_loss: 17808.2522 - val_mean_absolute_error: 17808.2522\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 17703.32275\n",
      "Epoch 514/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 18902.9286 - mean_absolute_error: 18902.9286 - val_loss: 17837.0724 - val_mean_absolute_error: 17837.0724\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 17703.32275\n",
      "Epoch 515/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 206us/step - loss: 18923.5134 - mean_absolute_error: 18923.5134 - val_loss: 17768.3324 - val_mean_absolute_error: 17768.3324\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 17703.32275\n",
      "Epoch 516/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18671.7243 - mean_absolute_error: 18671.7243 - val_loss: 18599.8332 - val_mean_absolute_error: 18599.8332\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 17703.32275\n",
      "Epoch 517/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 18936.5257 - mean_absolute_error: 18936.5257 - val_loss: 17987.0521 - val_mean_absolute_error: 17987.0521\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 17703.32275\n",
      "Epoch 518/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 19048.5885 - mean_absolute_error: 19048.5885 - val_loss: 17957.9905 - val_mean_absolute_error: 17957.9905\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 17703.32275\n",
      "Epoch 519/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 18723.5030 - mean_absolute_error: 18723.5030 - val_loss: 17863.9688 - val_mean_absolute_error: 17863.9688\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 17703.32275\n",
      "Epoch 520/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 18571.8058 - mean_absolute_error: 18571.8058 - val_loss: 17889.4783 - val_mean_absolute_error: 17889.4783\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 17703.32275\n",
      "Epoch 521/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 19309.2984 - mean_absolute_error: 19309.2984 - val_loss: 18458.1692 - val_mean_absolute_error: 18458.1692\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 17703.32275\n",
      "Epoch 522/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 19180.6777 - mean_absolute_error: 19180.6777 - val_loss: 17864.2797 - val_mean_absolute_error: 17864.2797\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 17703.32275\n",
      "Epoch 523/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 18575.0190 - mean_absolute_error: 18575.0190 - val_loss: 17819.3597 - val_mean_absolute_error: 17819.3597\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 17703.32275\n",
      "Epoch 524/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 18613.6712 - mean_absolute_error: 18613.6712 - val_loss: 17964.2625 - val_mean_absolute_error: 17964.2625\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 17703.32275\n",
      "Epoch 525/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 19018.7951 - mean_absolute_error: 19018.7951 - val_loss: 18917.6466 - val_mean_absolute_error: 18917.6466\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 17703.32275\n",
      "Epoch 526/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18676.2699 - mean_absolute_error: 18676.2699 - val_loss: 18649.2644 - val_mean_absolute_error: 18649.2644\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 17703.32275\n",
      "Epoch 527/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 18316.3842 - mean_absolute_error: 18316.3842 - val_loss: 18791.0435 - val_mean_absolute_error: 18791.0435\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 17703.32275\n",
      "Epoch 528/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18779.6263 - mean_absolute_error: 18779.6263 - val_loss: 17880.7916 - val_mean_absolute_error: 17880.7916\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 17703.32275\n",
      "Epoch 529/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 18839.4686 - mean_absolute_error: 18839.4686 - val_loss: 18484.3274 - val_mean_absolute_error: 18484.3274\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 17703.32275\n",
      "Epoch 530/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 19281.2326 - mean_absolute_error: 19281.2326 - val_loss: 17912.9026 - val_mean_absolute_error: 17912.9026\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 17703.32275\n",
      "Epoch 531/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 18681.3910 - mean_absolute_error: 18681.3910 - val_loss: 17818.0636 - val_mean_absolute_error: 17818.0636\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 17703.32275\n",
      "Epoch 532/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 18675.6649 - mean_absolute_error: 18675.6649 - val_loss: 17771.3270 - val_mean_absolute_error: 17771.3270\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 17703.32275\n",
      "Epoch 533/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 18736.3877 - mean_absolute_error: 18736.3877 - val_loss: 17696.8143 - val_mean_absolute_error: 17696.8143\n",
      "\n",
      "Epoch 00533: val_loss improved from 17703.32275 to 17696.81425, saving model to Weights-533--17696.81425.hdf5\n",
      "Epoch 534/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 19191.5984 - mean_absolute_error: 19191.5984 - val_loss: 18574.1075 - val_mean_absolute_error: 18574.1075\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 17696.81425\n",
      "Epoch 535/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 18836.6493 - mean_absolute_error: 18836.6493 - val_loss: 18047.8561 - val_mean_absolute_error: 18047.8561\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 17696.81425\n",
      "Epoch 536/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 18379.9058 - mean_absolute_error: 18379.9058 - val_loss: 18572.6592 - val_mean_absolute_error: 18572.6592\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 17696.81425\n",
      "Epoch 537/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18877.9494 - mean_absolute_error: 18877.9494 - val_loss: 17836.0586 - val_mean_absolute_error: 17836.0586\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 17696.81425\n",
      "Epoch 538/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 18770.1541 - mean_absolute_error: 18770.1541 - val_loss: 17775.4951 - val_mean_absolute_error: 17775.4951\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 17696.81425\n",
      "Epoch 539/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 18375.8862 - mean_absolute_error: 18375.8862 - val_loss: 17886.9315 - val_mean_absolute_error: 17886.9315\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 17696.81425\n",
      "Epoch 540/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 18483.5177 - mean_absolute_error: 18483.5177 - val_loss: 19574.4033 - val_mean_absolute_error: 19574.4033\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 17696.81425\n",
      "Epoch 541/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 18674.8035 - mean_absolute_error: 18674.8035 - val_loss: 18100.0746 - val_mean_absolute_error: 18100.0746\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 17696.81425\n",
      "Epoch 542/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18544.7390 - mean_absolute_error: 18544.7390 - val_loss: 18320.1006 - val_mean_absolute_error: 18320.1006\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 17696.81425\n",
      "Epoch 543/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 18632.6844 - mean_absolute_error: 18632.6844 - val_loss: 18236.0990 - val_mean_absolute_error: 18236.0990\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 17696.81425\n",
      "Epoch 544/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 18463.6349 - mean_absolute_error: 18463.6349 - val_loss: 18579.0383 - val_mean_absolute_error: 18579.0383\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 17696.81425\n",
      "Epoch 545/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18486.4152 - mean_absolute_error: 18486.4152 - val_loss: 17663.7375 - val_mean_absolute_error: 17663.7375\n",
      "\n",
      "Epoch 00545: val_loss improved from 17696.81425 to 17663.73749, saving model to Weights-545--17663.73749.hdf5\n",
      "Epoch 546/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 18936.3027 - mean_absolute_error: 18936.3027 - val_loss: 19256.6544 - val_mean_absolute_error: 19256.6544\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 17663.73749\n",
      "Epoch 547/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 18545.6805 - mean_absolute_error: 18545.6805 - val_loss: 19191.7485 - val_mean_absolute_error: 19191.7485\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 17663.73749\n",
      "Epoch 548/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 18558.9699 - mean_absolute_error: 18558.9699 - val_loss: 17920.9403 - val_mean_absolute_error: 17920.9403\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 17663.73749\n",
      "Epoch 549/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 18644.6122 - mean_absolute_error: 18644.6122 - val_loss: 18471.2226 - val_mean_absolute_error: 18471.2226\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 17663.73749\n",
      "Epoch 550/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18358.9772 - mean_absolute_error: 18358.9772 - val_loss: 18220.9717 - val_mean_absolute_error: 18220.9717\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 17663.73749\n",
      "Epoch 551/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 18337.7598 - mean_absolute_error: 18337.7598 - val_loss: 17813.6589 - val_mean_absolute_error: 17813.6589\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 17663.73749\n",
      "Epoch 552/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 18528.0183 - mean_absolute_error: 18528.0183 - val_loss: 17970.6215 - val_mean_absolute_error: 17970.6215\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 17663.73749\n",
      "Epoch 553/1500\n",
      "934/934 [==============================] - 0s 184us/step - loss: 18548.3815 - mean_absolute_error: 18548.3815 - val_loss: 17823.8480 - val_mean_absolute_error: 17823.8480\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 17663.73749\n",
      "Epoch 554/1500\n",
      "934/934 [==============================] - 0s 178us/step - loss: 18800.6534 - mean_absolute_error: 18800.6534 - val_loss: 18354.2717 - val_mean_absolute_error: 18354.2717\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 17663.73749\n",
      "Epoch 555/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 18685.8377 - mean_absolute_error: 18685.8377 - val_loss: 18931.5690 - val_mean_absolute_error: 18931.5690\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 17663.73749\n",
      "Epoch 556/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18369.0673 - mean_absolute_error: 18369.0673 - val_loss: 17869.9694 - val_mean_absolute_error: 17869.9694\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 17663.73749\n",
      "Epoch 557/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 18305.4727 - mean_absolute_error: 18305.4727 - val_loss: 18443.0608 - val_mean_absolute_error: 18443.0608\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 17663.73749\n",
      "Epoch 558/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18450.0095 - mean_absolute_error: 18450.0095 - val_loss: 18107.8640 - val_mean_absolute_error: 18107.8640\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 17663.73749\n",
      "Epoch 559/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 18213.6898 - mean_absolute_error: 18213.6898 - val_loss: 18229.9464 - val_mean_absolute_error: 18229.9464\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 17663.73749\n",
      "Epoch 560/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 18357.1552 - mean_absolute_error: 18357.1552 - val_loss: 18135.0210 - val_mean_absolute_error: 18135.0210\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 17663.73749\n",
      "Epoch 561/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 18433.6415 - mean_absolute_error: 18433.6415 - val_loss: 17810.0925 - val_mean_absolute_error: 17810.0925\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 17663.73749\n",
      "Epoch 562/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18456.7997 - mean_absolute_error: 18456.7997 - val_loss: 18171.2581 - val_mean_absolute_error: 18171.2581\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 17663.73749\n",
      "Epoch 563/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 18855.8485 - mean_absolute_error: 18855.8485 - val_loss: 17778.9972 - val_mean_absolute_error: 17778.9972\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 17663.73749\n",
      "Epoch 564/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 18336.1253 - mean_absolute_error: 18336.1253 - val_loss: 17836.3112 - val_mean_absolute_error: 17836.3112\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 17663.73749\n",
      "Epoch 565/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 18355.3523 - mean_absolute_error: 18355.3523 - val_loss: 18963.1444 - val_mean_absolute_error: 18963.1444\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 17663.73749\n",
      "Epoch 566/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 18304.5821 - mean_absolute_error: 18304.5821 - val_loss: 18967.7947 - val_mean_absolute_error: 18967.7947\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 17663.73749\n",
      "Epoch 567/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 18226.0316 - mean_absolute_error: 18226.0316 - val_loss: 18256.9338 - val_mean_absolute_error: 18256.9338\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 17663.73749\n",
      "Epoch 568/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18319.9107 - mean_absolute_error: 18319.9107 - val_loss: 17862.2400 - val_mean_absolute_error: 17862.2400\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 17663.73749\n",
      "Epoch 569/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 18421.3190 - mean_absolute_error: 18421.3190 - val_loss: 17922.9436 - val_mean_absolute_error: 17922.9436\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 17663.73749\n",
      "Epoch 570/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18685.8910 - mean_absolute_error: 18685.8910 - val_loss: 17891.2294 - val_mean_absolute_error: 17891.2294\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 17663.73749\n",
      "Epoch 571/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18762.2769 - mean_absolute_error: 18762.2769 - val_loss: 18295.7191 - val_mean_absolute_error: 18295.7191\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 17663.73749\n",
      "Epoch 572/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 18338.8982 - mean_absolute_error: 18338.8982 - val_loss: 20448.5258 - val_mean_absolute_error: 20448.5258\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 17663.73749\n",
      "Epoch 573/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18562.1474 - mean_absolute_error: 18562.1474 - val_loss: 18108.3031 - val_mean_absolute_error: 18108.3031\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 17663.73749\n",
      "Epoch 574/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 18639.5734 - mean_absolute_error: 18639.5734 - val_loss: 17894.0990 - val_mean_absolute_error: 17894.0990\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 17663.73749\n",
      "Epoch 575/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 18112.5483 - mean_absolute_error: 18112.5483 - val_loss: 18696.7942 - val_mean_absolute_error: 18696.7942\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 17663.73749\n",
      "Epoch 576/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18244.2342 - mean_absolute_error: 18244.2342 - val_loss: 17962.5040 - val_mean_absolute_error: 17962.5040\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 17663.73749\n",
      "Epoch 577/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 18503.3314 - mean_absolute_error: 18503.3314 - val_loss: 18213.4136 - val_mean_absolute_error: 18213.4136\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 17663.73749\n",
      "Epoch 578/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 18393.9678 - mean_absolute_error: 18393.9678 - val_loss: 17908.6403 - val_mean_absolute_error: 17908.6403\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 17663.73749\n",
      "Epoch 579/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18412.9584 - mean_absolute_error: 18412.9584 - val_loss: 18174.6740 - val_mean_absolute_error: 18174.6740\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 17663.73749\n",
      "Epoch 580/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 18218.8591 - mean_absolute_error: 18218.8591 - val_loss: 18120.9636 - val_mean_absolute_error: 18120.9636\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 17663.73749\n",
      "Epoch 581/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18793.5650 - mean_absolute_error: 18793.5650 - val_loss: 18606.8591 - val_mean_absolute_error: 18606.8591\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 17663.73749\n",
      "Epoch 582/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 195us/step - loss: 18152.4634 - mean_absolute_error: 18152.4634 - val_loss: 18085.6387 - val_mean_absolute_error: 18085.6387\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 17663.73749\n",
      "Epoch 583/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18191.6321 - mean_absolute_error: 18191.6321 - val_loss: 18064.7811 - val_mean_absolute_error: 18064.7811\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 17663.73749\n",
      "Epoch 584/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 18342.9722 - mean_absolute_error: 18342.9722 - val_loss: 17722.3116 - val_mean_absolute_error: 17722.3116\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 17663.73749\n",
      "Epoch 585/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 18334.9599 - mean_absolute_error: 18334.9599 - val_loss: 17919.8120 - val_mean_absolute_error: 17919.8120\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 17663.73749\n",
      "Epoch 586/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 18367.4622 - mean_absolute_error: 18367.4622 - val_loss: 17788.1647 - val_mean_absolute_error: 17788.1647\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 17663.73749\n",
      "Epoch 587/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18136.9910 - mean_absolute_error: 18136.9910 - val_loss: 17897.3441 - val_mean_absolute_error: 17897.3441\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 17663.73749\n",
      "Epoch 588/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18596.4084 - mean_absolute_error: 18596.4084 - val_loss: 18039.4680 - val_mean_absolute_error: 18039.4680\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 17663.73749\n",
      "Epoch 589/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18271.6854 - mean_absolute_error: 18271.6854 - val_loss: 18278.9782 - val_mean_absolute_error: 18278.9782\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 17663.73749\n",
      "Epoch 590/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 18334.9433 - mean_absolute_error: 18334.9433 - val_loss: 18099.1404 - val_mean_absolute_error: 18099.1404\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 17663.73749\n",
      "Epoch 591/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 18244.8183 - mean_absolute_error: 18244.8183 - val_loss: 17916.2280 - val_mean_absolute_error: 17916.2280\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 17663.73749\n",
      "Epoch 592/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 18205.7743 - mean_absolute_error: 18205.7743 - val_loss: 18911.3646 - val_mean_absolute_error: 18911.3646\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 17663.73749\n",
      "Epoch 593/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18133.0251 - mean_absolute_error: 18133.0251 - val_loss: 18062.4719 - val_mean_absolute_error: 18062.4719\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 17663.73749\n",
      "Epoch 594/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 18386.7650 - mean_absolute_error: 18386.7650 - val_loss: 17880.9694 - val_mean_absolute_error: 17880.9694\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 17663.73749\n",
      "Epoch 595/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 18403.4447 - mean_absolute_error: 18403.4447 - val_loss: 17994.2652 - val_mean_absolute_error: 17994.2652\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 17663.73749\n",
      "Epoch 596/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 18445.7663 - mean_absolute_error: 18445.7663 - val_loss: 17672.5373 - val_mean_absolute_error: 17672.5373\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 17663.73749\n",
      "Epoch 597/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 18284.8064 - mean_absolute_error: 18284.8064 - val_loss: 17955.2092 - val_mean_absolute_error: 17955.2092\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 17663.73749\n",
      "Epoch 598/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18163.2868 - mean_absolute_error: 18163.2868 - val_loss: 17846.4713 - val_mean_absolute_error: 17846.4713\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 17663.73749\n",
      "Epoch 599/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18143.8047 - mean_absolute_error: 18143.8047 - val_loss: 20101.9347 - val_mean_absolute_error: 20101.9347\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 17663.73749\n",
      "Epoch 600/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 18059.8809 - mean_absolute_error: 18059.8809 - val_loss: 18353.9004 - val_mean_absolute_error: 18353.9004\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 17663.73749\n",
      "Epoch 601/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18123.9233 - mean_absolute_error: 18123.9233 - val_loss: 17782.3824 - val_mean_absolute_error: 17782.3824\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 17663.73749\n",
      "Epoch 602/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 18381.2215 - mean_absolute_error: 18381.2215 - val_loss: 17853.1403 - val_mean_absolute_error: 17853.1403\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 17663.73749\n",
      "Epoch 603/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18306.5518 - mean_absolute_error: 18306.5518 - val_loss: 18690.2359 - val_mean_absolute_error: 18690.2359\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 17663.73749\n",
      "Epoch 604/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18588.0592 - mean_absolute_error: 18588.0592 - val_loss: 18832.3647 - val_mean_absolute_error: 18832.3647\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 17663.73749\n",
      "Epoch 605/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18573.2657 - mean_absolute_error: 18573.2657 - val_loss: 17970.7925 - val_mean_absolute_error: 17970.7925\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 17663.73749\n",
      "Epoch 606/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 18453.3934 - mean_absolute_error: 18453.3934 - val_loss: 17821.5740 - val_mean_absolute_error: 17821.5740\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 17663.73749\n",
      "Epoch 607/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17930.1111 - mean_absolute_error: 17930.1111 - val_loss: 18023.4588 - val_mean_absolute_error: 18023.4588\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 17663.73749\n",
      "Epoch 608/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18859.8829 - mean_absolute_error: 18859.8829 - val_loss: 18126.7525 - val_mean_absolute_error: 18126.7525\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 17663.73749\n",
      "Epoch 609/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 18110.6000 - mean_absolute_error: 18110.6000 - val_loss: 18103.4060 - val_mean_absolute_error: 18103.4060\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 17663.73749\n",
      "Epoch 610/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18306.5486 - mean_absolute_error: 18306.5486 - val_loss: 18046.9955 - val_mean_absolute_error: 18046.9955\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 17663.73749\n",
      "Epoch 611/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 18035.3287 - mean_absolute_error: 18035.3287 - val_loss: 18401.0777 - val_mean_absolute_error: 18401.0777\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 17663.73749\n",
      "Epoch 612/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 18101.1416 - mean_absolute_error: 18101.1416 - val_loss: 17787.1376 - val_mean_absolute_error: 17787.1376\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 17663.73749\n",
      "Epoch 613/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 18085.4122 - mean_absolute_error: 18085.4122 - val_loss: 17926.0476 - val_mean_absolute_error: 17926.0476\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 17663.73749\n",
      "Epoch 614/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 17982.7086 - mean_absolute_error: 17982.7086 - val_loss: 17928.9289 - val_mean_absolute_error: 17928.9289\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 17663.73749\n",
      "Epoch 615/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18524.2381 - mean_absolute_error: 18524.2381 - val_loss: 18192.5726 - val_mean_absolute_error: 18192.5726\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 17663.73749\n",
      "Epoch 616/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 203us/step - loss: 17938.5209 - mean_absolute_error: 17938.5209 - val_loss: 17779.4591 - val_mean_absolute_error: 17779.4591\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 17663.73749\n",
      "Epoch 617/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 18241.5807 - mean_absolute_error: 18241.5807 - val_loss: 17743.1713 - val_mean_absolute_error: 17743.1713\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 17663.73749\n",
      "Epoch 618/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 17894.5301 - mean_absolute_error: 17894.5301 - val_loss: 18085.1518 - val_mean_absolute_error: 18085.1518\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 17663.73749\n",
      "Epoch 619/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17969.1196 - mean_absolute_error: 17969.1196 - val_loss: 17776.0916 - val_mean_absolute_error: 17776.0916\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 17663.73749\n",
      "Epoch 620/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18070.7690 - mean_absolute_error: 18070.7690 - val_loss: 17880.4045 - val_mean_absolute_error: 17880.4045\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 17663.73749\n",
      "Epoch 621/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 17880.6199 - mean_absolute_error: 17880.6199 - val_loss: 18376.9061 - val_mean_absolute_error: 18376.9061\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 17663.73749\n",
      "Epoch 622/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 17996.7832 - mean_absolute_error: 17996.7832 - val_loss: 18028.4610 - val_mean_absolute_error: 18028.4610\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 17663.73749\n",
      "Epoch 623/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 17950.6911 - mean_absolute_error: 17950.6911 - val_loss: 18142.2774 - val_mean_absolute_error: 18142.2774\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 17663.73749\n",
      "Epoch 624/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18266.8023 - mean_absolute_error: 18266.8023 - val_loss: 17785.6144 - val_mean_absolute_error: 17785.6144\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 17663.73749\n",
      "Epoch 625/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 18154.8776 - mean_absolute_error: 18154.8776 - val_loss: 18023.2555 - val_mean_absolute_error: 18023.2555\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 17663.73749\n",
      "Epoch 626/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18356.7340 - mean_absolute_error: 18356.7340 - val_loss: 18016.9832 - val_mean_absolute_error: 18016.9832\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 17663.73749\n",
      "Epoch 627/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18033.7802 - mean_absolute_error: 18033.7802 - val_loss: 18379.2750 - val_mean_absolute_error: 18379.2750\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 17663.73749\n",
      "Epoch 628/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 18030.0484 - mean_absolute_error: 18030.0484 - val_loss: 18123.2360 - val_mean_absolute_error: 18123.2360\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 17663.73749\n",
      "Epoch 629/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18293.8325 - mean_absolute_error: 18293.8325 - val_loss: 17887.9897 - val_mean_absolute_error: 17887.9897\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 17663.73749\n",
      "Epoch 630/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 17874.1576 - mean_absolute_error: 17874.1576 - val_loss: 18579.0780 - val_mean_absolute_error: 18579.0780\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 17663.73749\n",
      "Epoch 631/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18091.0785 - mean_absolute_error: 18091.0785 - val_loss: 18528.7347 - val_mean_absolute_error: 18528.7347\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 17663.73749\n",
      "Epoch 632/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17772.6992 - mean_absolute_error: 17772.6992 - val_loss: 17750.7187 - val_mean_absolute_error: 17750.7187\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 17663.73749\n",
      "Epoch 633/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17946.8480 - mean_absolute_error: 17946.8480 - val_loss: 19322.7469 - val_mean_absolute_error: 19322.7469\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 17663.73749\n",
      "Epoch 634/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18343.3329 - mean_absolute_error: 18343.3329 - val_loss: 17715.3456 - val_mean_absolute_error: 17715.3456\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 17663.73749\n",
      "Epoch 635/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 17725.0326 - mean_absolute_error: 17725.0326 - val_loss: 18515.6303 - val_mean_absolute_error: 18515.6303\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 17663.73749\n",
      "Epoch 636/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18036.4938 - mean_absolute_error: 18036.4938 - val_loss: 17720.9215 - val_mean_absolute_error: 17720.9215\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 17663.73749\n",
      "Epoch 637/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 18004.5314 - mean_absolute_error: 18004.5314 - val_loss: 18612.8617 - val_mean_absolute_error: 18612.8617\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 17663.73749\n",
      "Epoch 638/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 18147.2770 - mean_absolute_error: 18147.2770 - val_loss: 18099.1576 - val_mean_absolute_error: 18099.1576\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 17663.73749\n",
      "Epoch 639/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 17931.4101 - mean_absolute_error: 17931.4101 - val_loss: 17970.1493 - val_mean_absolute_error: 17970.1493\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 17663.73749\n",
      "Epoch 640/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18109.4063 - mean_absolute_error: 18109.4063 - val_loss: 17883.8021 - val_mean_absolute_error: 17883.8021\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 17663.73749\n",
      "Epoch 641/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18116.6099 - mean_absolute_error: 18116.6099 - val_loss: 18429.0358 - val_mean_absolute_error: 18429.0358\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 17663.73749\n",
      "Epoch 642/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 18325.6164 - mean_absolute_error: 18325.6164 - val_loss: 18146.3171 - val_mean_absolute_error: 18146.3171\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 17663.73749\n",
      "Epoch 643/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 17808.8212 - mean_absolute_error: 17808.8212 - val_loss: 18762.9966 - val_mean_absolute_error: 18762.9966\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 17663.73749\n",
      "Epoch 644/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17897.9324 - mean_absolute_error: 17897.9324 - val_loss: 18224.6483 - val_mean_absolute_error: 18224.6483\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 17663.73749\n",
      "Epoch 645/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17995.6602 - mean_absolute_error: 17995.6602 - val_loss: 20372.8867 - val_mean_absolute_error: 20372.8867\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 17663.73749\n",
      "Epoch 646/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18063.3600 - mean_absolute_error: 18063.3600 - val_loss: 18087.6072 - val_mean_absolute_error: 18087.6072\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 17663.73749\n",
      "Epoch 647/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17936.2882 - mean_absolute_error: 17936.2882 - val_loss: 18131.5005 - val_mean_absolute_error: 18131.5005\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 17663.73749\n",
      "Epoch 648/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 18173.3520 - mean_absolute_error: 18173.3520 - val_loss: 19990.6790 - val_mean_absolute_error: 19990.6790\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 17663.73749\n",
      "Epoch 649/1500\n",
      "934/934 [==============================] - ETA: 0s - loss: 18121.6028 - mean_absolute_error: 18121.602 - 0s 192us/step - loss: 17922.1608 - mean_absolute_error: 17922.1608 - val_loss: 19168.2709 - val_mean_absolute_error: 19168.2709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00649: val_loss did not improve from 17663.73749\n",
      "Epoch 650/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18206.3412 - mean_absolute_error: 18206.3412 - val_loss: 17927.2981 - val_mean_absolute_error: 17927.2981\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 17663.73749\n",
      "Epoch 651/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17954.3407 - mean_absolute_error: 17954.3407 - val_loss: 19414.8185 - val_mean_absolute_error: 19414.8185\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 17663.73749\n",
      "Epoch 652/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18259.5125 - mean_absolute_error: 18259.5125 - val_loss: 17802.6037 - val_mean_absolute_error: 17802.6037\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 17663.73749\n",
      "Epoch 653/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18055.5785 - mean_absolute_error: 18055.5785 - val_loss: 20475.3806 - val_mean_absolute_error: 20475.3806\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 17663.73749\n",
      "Epoch 654/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17994.2015 - mean_absolute_error: 17994.2015 - val_loss: 18720.8494 - val_mean_absolute_error: 18720.8494\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 17663.73749\n",
      "Epoch 655/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17925.8067 - mean_absolute_error: 17925.8067 - val_loss: 17868.7742 - val_mean_absolute_error: 17868.7742\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 17663.73749\n",
      "Epoch 656/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17923.7861 - mean_absolute_error: 17923.7861 - val_loss: 17890.7557 - val_mean_absolute_error: 17890.7557\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 17663.73749\n",
      "Epoch 657/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17788.4271 - mean_absolute_error: 17788.4271 - val_loss: 17926.0909 - val_mean_absolute_error: 17926.0909\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 17663.73749\n",
      "Epoch 658/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17873.3900 - mean_absolute_error: 17873.3900 - val_loss: 19008.6054 - val_mean_absolute_error: 19008.6054\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 17663.73749\n",
      "Epoch 659/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18125.4656 - mean_absolute_error: 18125.4656 - val_loss: 17952.7678 - val_mean_absolute_error: 17952.7678\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 17663.73749\n",
      "Epoch 660/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 18147.5338 - mean_absolute_error: 18147.5338 - val_loss: 18533.3090 - val_mean_absolute_error: 18533.3090\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 17663.73749\n",
      "Epoch 661/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17905.1671 - mean_absolute_error: 17905.1671 - val_loss: 18144.4948 - val_mean_absolute_error: 18144.4948\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 17663.73749\n",
      "Epoch 662/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 18065.3474 - mean_absolute_error: 18065.3474 - val_loss: 18963.4597 - val_mean_absolute_error: 18963.4597\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 17663.73749\n",
      "Epoch 663/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17793.6678 - mean_absolute_error: 17793.6678 - val_loss: 18294.4687 - val_mean_absolute_error: 18294.4687\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 17663.73749\n",
      "Epoch 664/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18286.1810 - mean_absolute_error: 18286.1810 - val_loss: 17842.9270 - val_mean_absolute_error: 17842.9270\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 17663.73749\n",
      "Epoch 665/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 18018.5400 - mean_absolute_error: 18018.5400 - val_loss: 17875.1614 - val_mean_absolute_error: 17875.1614\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 17663.73749\n",
      "Epoch 666/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17889.0174 - mean_absolute_error: 17889.0174 - val_loss: 19713.6096 - val_mean_absolute_error: 19713.6096\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 17663.73749\n",
      "Epoch 667/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18062.0891 - mean_absolute_error: 18062.0891 - val_loss: 18351.4114 - val_mean_absolute_error: 18351.4114\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 17663.73749\n",
      "Epoch 668/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 18096.4676 - mean_absolute_error: 18096.4676 - val_loss: 17854.7720 - val_mean_absolute_error: 17854.7720\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 17663.73749\n",
      "Epoch 669/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 18182.9813 - mean_absolute_error: 18182.9813 - val_loss: 17944.4712 - val_mean_absolute_error: 17944.4712\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 17663.73749\n",
      "Epoch 670/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17992.8231 - mean_absolute_error: 17992.8231 - val_loss: 17784.3341 - val_mean_absolute_error: 17784.3341\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 17663.73749\n",
      "Epoch 671/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 17946.7014 - mean_absolute_error: 17946.7014 - val_loss: 17722.6108 - val_mean_absolute_error: 17722.6108\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 17663.73749\n",
      "Epoch 672/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 17822.5442 - mean_absolute_error: 17822.5442 - val_loss: 17863.7887 - val_mean_absolute_error: 17863.7887\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 17663.73749\n",
      "Epoch 673/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17908.0532 - mean_absolute_error: 17908.0532 - val_loss: 17679.5327 - val_mean_absolute_error: 17679.5327\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 17663.73749\n",
      "Epoch 674/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 18244.9377 - mean_absolute_error: 18244.9377 - val_loss: 18150.1406 - val_mean_absolute_error: 18150.1406\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 17663.73749\n",
      "Epoch 675/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18205.0797 - mean_absolute_error: 18205.0797 - val_loss: 18004.0606 - val_mean_absolute_error: 18004.0606\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 17663.73749\n",
      "Epoch 676/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 18050.8370 - mean_absolute_error: 18050.8370 - val_loss: 18450.9598 - val_mean_absolute_error: 18450.9598\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 17663.73749\n",
      "Epoch 677/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17829.6572 - mean_absolute_error: 17829.6572 - val_loss: 18067.4419 - val_mean_absolute_error: 18067.4419\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 17663.73749\n",
      "Epoch 678/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17757.5453 - mean_absolute_error: 17757.5453 - val_loss: 18548.5568 - val_mean_absolute_error: 18548.5568\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 17663.73749\n",
      "Epoch 679/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 18277.0431 - mean_absolute_error: 18277.0431 - val_loss: 17925.4913 - val_mean_absolute_error: 17925.4913\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 17663.73749\n",
      "Epoch 680/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17765.2615 - mean_absolute_error: 17765.2615 - val_loss: 17899.1298 - val_mean_absolute_error: 17899.1298\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 17663.73749\n",
      "Epoch 681/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17843.3139 - mean_absolute_error: 17843.3139 - val_loss: 17725.0224 - val_mean_absolute_error: 17725.0224\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 17663.73749\n",
      "Epoch 682/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17594.2192 - mean_absolute_error: 17594.2192 - val_loss: 18455.3831 - val_mean_absolute_error: 18455.3831\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 17663.73749\n",
      "Epoch 683/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17994.1566 - mean_absolute_error: 17994.1566 - val_loss: 17728.7659 - val_mean_absolute_error: 17728.7659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00683: val_loss did not improve from 17663.73749\n",
      "Epoch 684/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17898.7281 - mean_absolute_error: 17898.7281 - val_loss: 17582.7298 - val_mean_absolute_error: 17582.7298\n",
      "\n",
      "Epoch 00684: val_loss improved from 17663.73749 to 17582.72983, saving model to Weights-684--17582.72983.hdf5\n",
      "Epoch 685/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 17714.6993 - mean_absolute_error: 17714.6993 - val_loss: 17807.4554 - val_mean_absolute_error: 17807.4554\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 17582.72983\n",
      "Epoch 686/1500\n",
      "934/934 [==============================] - 0s 232us/step - loss: 17918.4844 - mean_absolute_error: 17918.4844 - val_loss: 18320.8800 - val_mean_absolute_error: 18320.8800\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 17582.72983\n",
      "Epoch 687/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 17712.5008 - mean_absolute_error: 17712.5008 - val_loss: 18340.3806 - val_mean_absolute_error: 18340.3806\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 17582.72983\n",
      "Epoch 688/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17884.8725 - mean_absolute_error: 17884.8725 - val_loss: 17980.5494 - val_mean_absolute_error: 17980.5494\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 17582.72983\n",
      "Epoch 689/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 17951.3983 - mean_absolute_error: 17951.3983 - val_loss: 17777.0000 - val_mean_absolute_error: 17777.0000\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 17582.72983\n",
      "Epoch 690/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 17823.0053 - mean_absolute_error: 17823.0053 - val_loss: 18589.6520 - val_mean_absolute_error: 18589.6520\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 17582.72983\n",
      "Epoch 691/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 17986.8200 - mean_absolute_error: 17986.8200 - val_loss: 18074.1915 - val_mean_absolute_error: 18074.1915\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 17582.72983\n",
      "Epoch 692/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 17814.1864 - mean_absolute_error: 17814.1864 - val_loss: 18074.0584 - val_mean_absolute_error: 18074.0584\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 17582.72983\n",
      "Epoch 693/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17527.8312 - mean_absolute_error: 17527.8312 - val_loss: 18119.2427 - val_mean_absolute_error: 18119.2427\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 17582.72983\n",
      "Epoch 694/1500\n",
      "934/934 [==============================] - 0s 227us/step - loss: 18037.0341 - mean_absolute_error: 18037.0341 - val_loss: 17495.2031 - val_mean_absolute_error: 17495.2031\n",
      "\n",
      "Epoch 00694: val_loss improved from 17582.72983 to 17495.20306, saving model to Weights-694--17495.20306.hdf5\n",
      "Epoch 695/1500\n",
      "934/934 [==============================] - 0s 235us/step - loss: 17639.6244 - mean_absolute_error: 17639.6244 - val_loss: 18334.1668 - val_mean_absolute_error: 18334.1668\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 17495.20306\n",
      "Epoch 696/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 18054.2518 - mean_absolute_error: 18054.2518 - val_loss: 18140.3108 - val_mean_absolute_error: 18140.3108\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 17495.20306\n",
      "Epoch 697/1500\n",
      "934/934 [==============================] - 0s 232us/step - loss: 17621.6603 - mean_absolute_error: 17621.6603 - val_loss: 17583.5306 - val_mean_absolute_error: 17583.5306\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 17495.20306\n",
      "Epoch 698/1500\n",
      "934/934 [==============================] - 0s 235us/step - loss: 17691.5101 - mean_absolute_error: 17691.5101 - val_loss: 18665.7620 - val_mean_absolute_error: 18665.7620\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 17495.20306\n",
      "Epoch 699/1500\n",
      "934/934 [==============================] - 0s 234us/step - loss: 18714.2134 - mean_absolute_error: 18714.2134 - val_loss: 17666.4628 - val_mean_absolute_error: 17666.4628\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 17495.20306\n",
      "Epoch 700/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 17725.4790 - mean_absolute_error: 17725.4790 - val_loss: 17596.7718 - val_mean_absolute_error: 17596.7718\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 17495.20306\n",
      "Epoch 701/1500\n",
      "934/934 [==============================] - 0s 229us/step - loss: 17891.2160 - mean_absolute_error: 17891.2160 - val_loss: 17930.4232 - val_mean_absolute_error: 17930.4232\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 17495.20306\n",
      "Epoch 702/1500\n",
      "934/934 [==============================] - 0s 230us/step - loss: 17607.4482 - mean_absolute_error: 17607.4482 - val_loss: 17703.5254 - val_mean_absolute_error: 17703.5254\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 17495.20306\n",
      "Epoch 703/1500\n",
      "934/934 [==============================] - 0s 235us/step - loss: 17628.2658 - mean_absolute_error: 17628.2658 - val_loss: 17925.4142 - val_mean_absolute_error: 17925.4142\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 17495.20306\n",
      "Epoch 704/1500\n",
      "934/934 [==============================] - 0s 225us/step - loss: 17457.0486 - mean_absolute_error: 17457.0486 - val_loss: 17613.5425 - val_mean_absolute_error: 17613.5425\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 17495.20306\n",
      "Epoch 705/1500\n",
      "934/934 [==============================] - 0s 226us/step - loss: 17445.5422 - mean_absolute_error: 17445.5422 - val_loss: 18114.0035 - val_mean_absolute_error: 18114.0035\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 17495.20306\n",
      "Epoch 706/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 17471.6461 - mean_absolute_error: 17471.6461 - val_loss: 18312.1604 - val_mean_absolute_error: 18312.1604\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 17495.20306\n",
      "Epoch 707/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 17662.1556 - mean_absolute_error: 17662.1556 - val_loss: 18285.6139 - val_mean_absolute_error: 18285.6139\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 17495.20306\n",
      "Epoch 708/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 17854.7010 - mean_absolute_error: 17854.7010 - val_loss: 17527.2636 - val_mean_absolute_error: 17527.2636\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 17495.20306\n",
      "Epoch 709/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17924.4299 - mean_absolute_error: 17924.4299 - val_loss: 17590.4014 - val_mean_absolute_error: 17590.4014\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 17495.20306\n",
      "Epoch 710/1500\n",
      "934/934 [==============================] - 0s 217us/step - loss: 17507.4708 - mean_absolute_error: 17507.4708 - val_loss: 17957.1129 - val_mean_absolute_error: 17957.1129\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 17495.20306\n",
      "Epoch 711/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17557.1892 - mean_absolute_error: 17557.1892 - val_loss: 18591.9362 - val_mean_absolute_error: 18591.9362\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 17495.20306\n",
      "Epoch 712/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17512.9123 - mean_absolute_error: 17512.9123 - val_loss: 17949.1188 - val_mean_absolute_error: 17949.1188\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 17495.20306\n",
      "Epoch 713/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17870.9831 - mean_absolute_error: 17870.9831 - val_loss: 21511.1078 - val_mean_absolute_error: 21511.1078\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 17495.20306\n",
      "Epoch 714/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 17775.6442 - mean_absolute_error: 17775.6442 - val_loss: 18471.2151 - val_mean_absolute_error: 18471.2151\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 17495.20306\n",
      "Epoch 715/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 17567.2425 - mean_absolute_error: 17567.2425 - val_loss: 17621.7658 - val_mean_absolute_error: 17621.7658\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 17495.20306\n",
      "Epoch 716/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17565.5692 - mean_absolute_error: 17565.5692 - val_loss: 17799.8700 - val_mean_absolute_error: 17799.8700\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 17495.20306\n",
      "Epoch 717/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17699.1087 - mean_absolute_error: 17699.1087 - val_loss: 17774.3772 - val_mean_absolute_error: 17774.3772\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 17495.20306\n",
      "Epoch 718/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17674.1350 - mean_absolute_error: 17674.1350 - val_loss: 18200.9387 - val_mean_absolute_error: 18200.9387\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 17495.20306\n",
      "Epoch 719/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17790.5146 - mean_absolute_error: 17790.5146 - val_loss: 17827.3601 - val_mean_absolute_error: 17827.3601\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 17495.20306\n",
      "Epoch 720/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17413.0424 - mean_absolute_error: 17413.0424 - val_loss: 17763.6335 - val_mean_absolute_error: 17763.6335\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 17495.20306\n",
      "Epoch 721/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17498.6004 - mean_absolute_error: 17498.6004 - val_loss: 17561.4722 - val_mean_absolute_error: 17561.4722\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 17495.20306\n",
      "Epoch 722/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17759.3449 - mean_absolute_error: 17759.3449 - val_loss: 17674.6157 - val_mean_absolute_error: 17674.6157\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 17495.20306\n",
      "Epoch 723/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17517.3047 - mean_absolute_error: 17517.3047 - val_loss: 17647.7621 - val_mean_absolute_error: 17647.7621\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 17495.20306\n",
      "Epoch 724/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17410.3862 - mean_absolute_error: 17410.3862 - val_loss: 17696.5249 - val_mean_absolute_error: 17696.5249\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 17495.20306\n",
      "Epoch 725/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17542.7433 - mean_absolute_error: 17542.7433 - val_loss: 18036.7354 - val_mean_absolute_error: 18036.7354\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 17495.20306\n",
      "Epoch 726/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 17839.1709 - mean_absolute_error: 17839.1709 - val_loss: 18323.6082 - val_mean_absolute_error: 18323.6082\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 17495.20306\n",
      "Epoch 727/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17317.3042 - mean_absolute_error: 17317.3042 - val_loss: 18337.0299 - val_mean_absolute_error: 18337.0299\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 17495.20306\n",
      "Epoch 728/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17950.7577 - mean_absolute_error: 17950.7577 - val_loss: 17871.2767 - val_mean_absolute_error: 17871.2767\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 17495.20306\n",
      "Epoch 729/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17303.8131 - mean_absolute_error: 17303.8131 - val_loss: 17849.1452 - val_mean_absolute_error: 17849.1452\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 17495.20306\n",
      "Epoch 730/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17945.7814 - mean_absolute_error: 17945.7814 - val_loss: 18049.0119 - val_mean_absolute_error: 18049.0119\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 17495.20306\n",
      "Epoch 731/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17834.4873 - mean_absolute_error: 17834.4873 - val_loss: 17647.0890 - val_mean_absolute_error: 17647.0890\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 17495.20306\n",
      "Epoch 732/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17287.3990 - mean_absolute_error: 17287.3990 - val_loss: 18117.8137 - val_mean_absolute_error: 18117.8137\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 17495.20306\n",
      "Epoch 733/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17738.4401 - mean_absolute_error: 17738.4401 - val_loss: 18108.8461 - val_mean_absolute_error: 18108.8461\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 17495.20306\n",
      "Epoch 734/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17327.0288 - mean_absolute_error: 17327.0288 - val_loss: 18515.3532 - val_mean_absolute_error: 18515.3532\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 17495.20306\n",
      "Epoch 735/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17319.7393 - mean_absolute_error: 17319.7393 - val_loss: 17946.1781 - val_mean_absolute_error: 17946.1781\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 17495.20306\n",
      "Epoch 736/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17516.5441 - mean_absolute_error: 17516.5441 - val_loss: 18264.0857 - val_mean_absolute_error: 18264.0857\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 17495.20306\n",
      "Epoch 737/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17482.4930 - mean_absolute_error: 17482.4930 - val_loss: 18134.1867 - val_mean_absolute_error: 18134.1867\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 17495.20306\n",
      "Epoch 738/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17616.6848 - mean_absolute_error: 17616.6848 - val_loss: 17622.5348 - val_mean_absolute_error: 17622.5348\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 17495.20306\n",
      "Epoch 739/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17341.8879 - mean_absolute_error: 17341.8879 - val_loss: 18002.8731 - val_mean_absolute_error: 18002.8731\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 17495.20306\n",
      "Epoch 740/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 17846.7102 - mean_absolute_error: 17846.7102 - val_loss: 17697.0633 - val_mean_absolute_error: 17697.0633\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 17495.20306\n",
      "Epoch 741/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17829.0484 - mean_absolute_error: 17829.0484 - val_loss: 18778.2307 - val_mean_absolute_error: 18778.2307\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 17495.20306\n",
      "Epoch 742/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17643.2828 - mean_absolute_error: 17643.2828 - val_loss: 17597.6914 - val_mean_absolute_error: 17597.6914\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 17495.20306\n",
      "Epoch 743/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17406.4791 - mean_absolute_error: 17406.4791 - val_loss: 18807.3754 - val_mean_absolute_error: 18807.3754\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 17495.20306\n",
      "Epoch 744/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17606.3080 - mean_absolute_error: 17606.3080 - val_loss: 18047.4556 - val_mean_absolute_error: 18047.4556\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 17495.20306\n",
      "Epoch 745/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 17616.4222 - mean_absolute_error: 17616.4222 - val_loss: 17986.3299 - val_mean_absolute_error: 17986.3299\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 17495.20306\n",
      "Epoch 746/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17814.5181 - mean_absolute_error: 17814.5181 - val_loss: 17659.6024 - val_mean_absolute_error: 17659.6024\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 17495.20306\n",
      "Epoch 747/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17281.0961 - mean_absolute_error: 17281.0961 - val_loss: 17619.7770 - val_mean_absolute_error: 17619.7770\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 17495.20306\n",
      "Epoch 748/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 17395.9038 - mean_absolute_error: 17395.9038 - val_loss: 17786.7770 - val_mean_absolute_error: 17786.7770\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 17495.20306\n",
      "Epoch 749/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17528.7683 - mean_absolute_error: 17528.7683 - val_loss: 17697.8007 - val_mean_absolute_error: 17697.8007\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 17495.20306\n",
      "Epoch 750/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17430.8841 - mean_absolute_error: 17430.8841 - val_loss: 17839.3371 - val_mean_absolute_error: 17839.3371\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 17495.20306\n",
      "Epoch 751/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 194us/step - loss: 17137.7782 - mean_absolute_error: 17137.7782 - val_loss: 17581.6642 - val_mean_absolute_error: 17581.6642\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 17495.20306\n",
      "Epoch 752/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17367.7825 - mean_absolute_error: 17367.7825 - val_loss: 17595.5365 - val_mean_absolute_error: 17595.5365\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 17495.20306\n",
      "Epoch 753/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17481.4518 - mean_absolute_error: 17481.4518 - val_loss: 17583.5588 - val_mean_absolute_error: 17583.5588\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 17495.20306\n",
      "Epoch 754/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17464.8974 - mean_absolute_error: 17464.8974 - val_loss: 17661.1954 - val_mean_absolute_error: 17661.1954\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 17495.20306\n",
      "Epoch 755/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17552.8143 - mean_absolute_error: 17552.8143 - val_loss: 19013.3853 - val_mean_absolute_error: 19013.3853\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 17495.20306\n",
      "Epoch 756/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17556.1969 - mean_absolute_error: 17556.1969 - val_loss: 17425.5716 - val_mean_absolute_error: 17425.5716\n",
      "\n",
      "Epoch 00756: val_loss improved from 17495.20306 to 17425.57160, saving model to Weights-756--17425.57160.hdf5\n",
      "Epoch 757/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17293.4633 - mean_absolute_error: 17293.4633 - val_loss: 18193.9001 - val_mean_absolute_error: 18193.9001\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 17425.57160\n",
      "Epoch 758/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17817.7454 - mean_absolute_error: 17817.7454 - val_loss: 17824.4167 - val_mean_absolute_error: 17824.4167\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 17425.57160\n",
      "Epoch 759/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17283.9664 - mean_absolute_error: 17283.9664 - val_loss: 17889.0084 - val_mean_absolute_error: 17889.0084\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 17425.57160\n",
      "Epoch 760/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17671.4510 - mean_absolute_error: 17671.4510 - val_loss: 17742.4644 - val_mean_absolute_error: 17742.4644\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 17425.57160\n",
      "Epoch 761/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 17265.5224 - mean_absolute_error: 17265.5224 - val_loss: 17804.9105 - val_mean_absolute_error: 17804.9105\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 17425.57160\n",
      "Epoch 762/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 17868.5301 - mean_absolute_error: 17868.5301 - val_loss: 17631.9128 - val_mean_absolute_error: 17631.9128\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 17425.57160\n",
      "Epoch 763/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 17377.4587 - mean_absolute_error: 17377.4587 - val_loss: 17599.3093 - val_mean_absolute_error: 17599.3093\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 17425.57160\n",
      "Epoch 764/1500\n",
      "934/934 [==============================] - 0s 213us/step - loss: 17311.6165 - mean_absolute_error: 17311.6165 - val_loss: 17975.7943 - val_mean_absolute_error: 17975.7943\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 17425.57160\n",
      "Epoch 765/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17732.9400 - mean_absolute_error: 17732.9400 - val_loss: 17615.3278 - val_mean_absolute_error: 17615.3278\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 17425.57160\n",
      "Epoch 766/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 17074.6729 - mean_absolute_error: 17074.6729 - val_loss: 17571.9046 - val_mean_absolute_error: 17571.9046\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 17425.57160\n",
      "Epoch 767/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 17163.3181 - mean_absolute_error: 17163.3181 - val_loss: 17663.8847 - val_mean_absolute_error: 17663.8847\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 17425.57160\n",
      "Epoch 768/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17196.6593 - mean_absolute_error: 17196.6593 - val_loss: 17583.8419 - val_mean_absolute_error: 17583.8419\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 17425.57160\n",
      "Epoch 769/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 17076.6712 - mean_absolute_error: 17076.6712 - val_loss: 17693.8639 - val_mean_absolute_error: 17693.8639\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 17425.57160\n",
      "Epoch 770/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17510.5690 - mean_absolute_error: 17510.5690 - val_loss: 17615.1008 - val_mean_absolute_error: 17615.1008\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 17425.57160\n",
      "Epoch 771/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17409.4338 - mean_absolute_error: 17409.4338 - val_loss: 17611.9490 - val_mean_absolute_error: 17611.9490\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 17425.57160\n",
      "Epoch 772/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 17743.2612 - mean_absolute_error: 17743.2612 - val_loss: 17473.0696 - val_mean_absolute_error: 17473.0696\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 17425.57160\n",
      "Epoch 773/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17256.2757 - mean_absolute_error: 17256.2757 - val_loss: 17907.7936 - val_mean_absolute_error: 17907.7936\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 17425.57160\n",
      "Epoch 774/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17406.1152 - mean_absolute_error: 17406.1152 - val_loss: 18297.0538 - val_mean_absolute_error: 18297.0538\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 17425.57160\n",
      "Epoch 775/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17172.2791 - mean_absolute_error: 17172.2791 - val_loss: 17573.4137 - val_mean_absolute_error: 17573.4137\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 17425.57160\n",
      "Epoch 776/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17498.7056 - mean_absolute_error: 17498.7056 - val_loss: 18265.1150 - val_mean_absolute_error: 18265.1150\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 17425.57160\n",
      "Epoch 777/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 17272.7440 - mean_absolute_error: 17272.7440 - val_loss: 17793.6555 - val_mean_absolute_error: 17793.6555\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 17425.57160\n",
      "Epoch 778/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17788.2035 - mean_absolute_error: 17788.2035 - val_loss: 17792.3995 - val_mean_absolute_error: 17792.3995\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 17425.57160\n",
      "Epoch 779/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17188.4942 - mean_absolute_error: 17188.4942 - val_loss: 17846.9097 - val_mean_absolute_error: 17846.9097\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 17425.57160\n",
      "Epoch 780/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16909.0876 - mean_absolute_error: 16909.0876 - val_loss: 17550.5641 - val_mean_absolute_error: 17550.5641\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 17425.57160\n",
      "Epoch 781/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17236.5224 - mean_absolute_error: 17236.5224 - val_loss: 17706.7637 - val_mean_absolute_error: 17706.7637\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 17425.57160\n",
      "Epoch 782/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17260.5538 - mean_absolute_error: 17260.5538 - val_loss: 19062.7782 - val_mean_absolute_error: 19062.7782\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 17425.57160\n",
      "Epoch 783/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 17112.7807 - mean_absolute_error: 17112.7807 - val_loss: 18308.6966 - val_mean_absolute_error: 18308.6966\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 17425.57160\n",
      "Epoch 784/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17187.4959 - mean_absolute_error: 17187.4959 - val_loss: 19137.0813 - val_mean_absolute_error: 19137.0813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00784: val_loss did not improve from 17425.57160\n",
      "Epoch 785/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17482.8810 - mean_absolute_error: 17482.8810 - val_loss: 17779.0521 - val_mean_absolute_error: 17779.0521\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 17425.57160\n",
      "Epoch 786/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 17430.0740 - mean_absolute_error: 17430.0740 - val_loss: 19464.1110 - val_mean_absolute_error: 19464.1110\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 17425.57160\n",
      "Epoch 787/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17207.4748 - mean_absolute_error: 17207.4748 - val_loss: 18269.1905 - val_mean_absolute_error: 18269.1905\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 17425.57160\n",
      "Epoch 788/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17360.3149 - mean_absolute_error: 17360.3149 - val_loss: 17711.1942 - val_mean_absolute_error: 17711.1942\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 17425.57160\n",
      "Epoch 789/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17113.8721 - mean_absolute_error: 17113.8721 - val_loss: 19603.4342 - val_mean_absolute_error: 19603.4342\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 17425.57160\n",
      "Epoch 790/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17380.7854 - mean_absolute_error: 17380.7854 - val_loss: 17830.0257 - val_mean_absolute_error: 17830.0257\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 17425.57160\n",
      "Epoch 791/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17699.5585 - mean_absolute_error: 17699.5585 - val_loss: 18416.4064 - val_mean_absolute_error: 18416.4064\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 17425.57160\n",
      "Epoch 792/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 17096.5395 - mean_absolute_error: 17096.5395 - val_loss: 17699.1422 - val_mean_absolute_error: 17699.1422\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 17425.57160\n",
      "Epoch 793/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16970.2726 - mean_absolute_error: 16970.2726 - val_loss: 20240.6974 - val_mean_absolute_error: 20240.6974\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 17425.57160\n",
      "Epoch 794/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17079.7578 - mean_absolute_error: 17079.7578 - val_loss: 18139.3559 - val_mean_absolute_error: 18139.3559\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 17425.57160\n",
      "Epoch 795/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16829.7594 - mean_absolute_error: 16829.7594 - val_loss: 17631.5136 - val_mean_absolute_error: 17631.5136\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 17425.57160\n",
      "Epoch 796/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17159.0577 - mean_absolute_error: 17159.0577 - val_loss: 18078.3215 - val_mean_absolute_error: 18078.3215\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 17425.57160\n",
      "Epoch 797/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17433.8052 - mean_absolute_error: 17433.8052 - val_loss: 17758.1922 - val_mean_absolute_error: 17758.1922\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 17425.57160\n",
      "Epoch 798/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 17180.9868 - mean_absolute_error: 17180.9868 - val_loss: 17472.8468 - val_mean_absolute_error: 17472.8468\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 17425.57160\n",
      "Epoch 799/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17329.8079 - mean_absolute_error: 17329.8079 - val_loss: 17890.0412 - val_mean_absolute_error: 17890.0412\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 17425.57160\n",
      "Epoch 800/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17242.2845 - mean_absolute_error: 17242.2845 - val_loss: 18464.2176 - val_mean_absolute_error: 18464.2176\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 17425.57160\n",
      "Epoch 801/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17470.6520 - mean_absolute_error: 17470.6520 - val_loss: 17979.9580 - val_mean_absolute_error: 17979.9580\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 17425.57160\n",
      "Epoch 802/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17227.5751 - mean_absolute_error: 17227.5751 - val_loss: 17796.4631 - val_mean_absolute_error: 17796.4631\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 17425.57160\n",
      "Epoch 803/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17292.3292 - mean_absolute_error: 17292.3292 - val_loss: 18229.4968 - val_mean_absolute_error: 18229.4968\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 17425.57160\n",
      "Epoch 804/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17119.5832 - mean_absolute_error: 17119.5832 - val_loss: 17679.1620 - val_mean_absolute_error: 17679.1620\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 17425.57160\n",
      "Epoch 805/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17350.9548 - mean_absolute_error: 17350.9548 - val_loss: 17628.5115 - val_mean_absolute_error: 17628.5115\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 17425.57160\n",
      "Epoch 806/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17077.6609 - mean_absolute_error: 17077.6609 - val_loss: 19980.2372 - val_mean_absolute_error: 19980.2372\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 17425.57160\n",
      "Epoch 807/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17333.7213 - mean_absolute_error: 17333.7213 - val_loss: 17759.8668 - val_mean_absolute_error: 17759.8668\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 17425.57160\n",
      "Epoch 808/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17213.8535 - mean_absolute_error: 17213.8535 - val_loss: 17692.8748 - val_mean_absolute_error: 17692.8748\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 17425.57160\n",
      "Epoch 809/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17538.6327 - mean_absolute_error: 17538.6327 - val_loss: 18339.3278 - val_mean_absolute_error: 18339.3278\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 17425.57160\n",
      "Epoch 810/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17352.3986 - mean_absolute_error: 17352.3986 - val_loss: 18159.0989 - val_mean_absolute_error: 18159.0989\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 17425.57160\n",
      "Epoch 811/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17185.0948 - mean_absolute_error: 17185.0948 - val_loss: 17664.1787 - val_mean_absolute_error: 17664.1787\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 17425.57160\n",
      "Epoch 812/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17068.6351 - mean_absolute_error: 17068.6351 - val_loss: 18009.8882 - val_mean_absolute_error: 18009.8882\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 17425.57160\n",
      "Epoch 813/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 17298.6882 - mean_absolute_error: 17298.6882 - val_loss: 18750.3339 - val_mean_absolute_error: 18750.3339\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 17425.57160\n",
      "Epoch 814/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 17117.3107 - mean_absolute_error: 17117.3107 - val_loss: 18109.4694 - val_mean_absolute_error: 18109.4694\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 17425.57160\n",
      "Epoch 815/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17320.3513 - mean_absolute_error: 17320.3513 - val_loss: 17670.6047 - val_mean_absolute_error: 17670.6047\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 17425.57160\n",
      "Epoch 816/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16891.5138 - mean_absolute_error: 16891.5138 - val_loss: 17604.6504 - val_mean_absolute_error: 17604.6504\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 17425.57160\n",
      "Epoch 817/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16772.3097 - mean_absolute_error: 16772.3097 - val_loss: 18108.9706 - val_mean_absolute_error: 18108.9706\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 17425.57160\n",
      "Epoch 818/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17155.9065 - mean_absolute_error: 17155.9065 - val_loss: 17868.6870 - val_mean_absolute_error: 17868.6870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00818: val_loss did not improve from 17425.57160\n",
      "Epoch 819/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16902.5292 - mean_absolute_error: 16902.5292 - val_loss: 19247.0924 - val_mean_absolute_error: 19247.0924\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 17425.57160\n",
      "Epoch 820/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17312.3995 - mean_absolute_error: 17312.3995 - val_loss: 17617.3819 - val_mean_absolute_error: 17617.3819\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 17425.57160\n",
      "Epoch 821/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17493.6925 - mean_absolute_error: 17493.6925 - val_loss: 17788.8881 - val_mean_absolute_error: 17788.8881\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 17425.57160\n",
      "Epoch 822/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17226.6563 - mean_absolute_error: 17226.6563 - val_loss: 18389.3257 - val_mean_absolute_error: 18389.3257\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 17425.57160\n",
      "Epoch 823/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17016.2291 - mean_absolute_error: 17016.2291 - val_loss: 17645.9804 - val_mean_absolute_error: 17645.9804\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 17425.57160\n",
      "Epoch 824/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16980.9801 - mean_absolute_error: 16980.9801 - val_loss: 17825.2474 - val_mean_absolute_error: 17825.2474\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 17425.57160\n",
      "Epoch 825/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 17102.1003 - mean_absolute_error: 17102.1003 - val_loss: 17585.4306 - val_mean_absolute_error: 17585.4306\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 17425.57160\n",
      "Epoch 826/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 17119.6485 - mean_absolute_error: 17119.6485 - val_loss: 17829.6891 - val_mean_absolute_error: 17829.6891\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 17425.57160\n",
      "Epoch 827/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16874.3602 - mean_absolute_error: 16874.3602 - val_loss: 17573.9383 - val_mean_absolute_error: 17573.9383\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 17425.57160\n",
      "Epoch 828/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 17251.2191 - mean_absolute_error: 17251.2191 - val_loss: 17646.2911 - val_mean_absolute_error: 17646.2911\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 17425.57160\n",
      "Epoch 829/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 17003.6600 - mean_absolute_error: 17003.6600 - val_loss: 17793.5152 - val_mean_absolute_error: 17793.5152\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 17425.57160\n",
      "Epoch 830/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17126.3860 - mean_absolute_error: 17126.3860 - val_loss: 18392.0143 - val_mean_absolute_error: 18392.0143\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 17425.57160\n",
      "Epoch 831/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16591.5837 - mean_absolute_error: 16591.5837 - val_loss: 18368.1706 - val_mean_absolute_error: 18368.1706\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 17425.57160\n",
      "Epoch 832/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 17246.0636 - mean_absolute_error: 17246.0636 - val_loss: 17556.9015 - val_mean_absolute_error: 17556.9015\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 17425.57160\n",
      "Epoch 833/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17462.4858 - mean_absolute_error: 17462.4858 - val_loss: 17688.9270 - val_mean_absolute_error: 17688.9270\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 17425.57160\n",
      "Epoch 834/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17402.8170 - mean_absolute_error: 17402.8170 - val_loss: 19290.1999 - val_mean_absolute_error: 19290.1999\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 17425.57160\n",
      "Epoch 835/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 16967.3508 - mean_absolute_error: 16967.3508 - val_loss: 17734.4870 - val_mean_absolute_error: 17734.4870\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 17425.57160\n",
      "Epoch 836/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16657.8484 - mean_absolute_error: 16657.8484 - val_loss: 19149.5185 - val_mean_absolute_error: 19149.5185\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 17425.57160\n",
      "Epoch 837/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16940.4056 - mean_absolute_error: 16940.4056 - val_loss: 17500.6516 - val_mean_absolute_error: 17500.6516\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 17425.57160\n",
      "Epoch 838/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 17108.7425 - mean_absolute_error: 17108.7425 - val_loss: 19102.8896 - val_mean_absolute_error: 19102.8896\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 17425.57160\n",
      "Epoch 839/1500\n",
      "934/934 [==============================] - 0s 250us/step - loss: 17302.2891 - mean_absolute_error: 17302.2891 - val_loss: 17411.9577 - val_mean_absolute_error: 17411.9577\n",
      "\n",
      "Epoch 00839: val_loss improved from 17425.57160 to 17411.95768, saving model to Weights-839--17411.95768.hdf5\n",
      "Epoch 840/1500\n",
      "934/934 [==============================] - 0s 234us/step - loss: 16938.3871 - mean_absolute_error: 16938.3871 - val_loss: 20632.8356 - val_mean_absolute_error: 20632.8356\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 17411.95768\n",
      "Epoch 841/1500\n",
      "934/934 [==============================] - 0s 254us/step - loss: 16932.9455 - mean_absolute_error: 16932.9455 - val_loss: 18957.7982 - val_mean_absolute_error: 18957.7982\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 17411.95768\n",
      "Epoch 842/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 17036.8981 - mean_absolute_error: 17036.8981 - val_loss: 18170.8223 - val_mean_absolute_error: 18170.8223\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 17411.95768\n",
      "Epoch 843/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17344.4005 - mean_absolute_error: 17344.4005 - val_loss: 20246.1677 - val_mean_absolute_error: 20246.1677\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 17411.95768\n",
      "Epoch 844/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 17079.1516 - mean_absolute_error: 17079.1516 - val_loss: 17644.3439 - val_mean_absolute_error: 17644.3439\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 17411.95768\n",
      "Epoch 845/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17290.7900 - mean_absolute_error: 17290.7900 - val_loss: 17779.8096 - val_mean_absolute_error: 17779.8096\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 17411.95768\n",
      "Epoch 846/1500\n",
      "934/934 [==============================] - 0s 234us/step - loss: 16908.8517 - mean_absolute_error: 16908.8517 - val_loss: 17734.0677 - val_mean_absolute_error: 17734.0677\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 17411.95768\n",
      "Epoch 847/1500\n",
      "934/934 [==============================] - 0s 224us/step - loss: 17146.5249 - mean_absolute_error: 17146.5249 - val_loss: 17397.8917 - val_mean_absolute_error: 17397.8917\n",
      "\n",
      "Epoch 00847: val_loss improved from 17411.95768 to 17397.89172, saving model to Weights-847--17397.89172.hdf5\n",
      "Epoch 848/1500\n",
      "934/934 [==============================] - 0s 230us/step - loss: 16949.5358 - mean_absolute_error: 16949.5358 - val_loss: 17746.1101 - val_mean_absolute_error: 17746.1101\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 17397.89172\n",
      "Epoch 849/1500\n",
      "934/934 [==============================] - 0s 220us/step - loss: 17041.8684 - mean_absolute_error: 17041.8684 - val_loss: 17439.8702 - val_mean_absolute_error: 17439.8702\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 17397.89172\n",
      "Epoch 850/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 16964.9533 - mean_absolute_error: 16964.9533 - val_loss: 18792.5358 - val_mean_absolute_error: 18792.5358\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 17397.89172\n",
      "Epoch 851/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 17415.5811 - mean_absolute_error: 17415.5811 - val_loss: 17657.2798 - val_mean_absolute_error: 17657.2798\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 17397.89172\n",
      "Epoch 852/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 17214.3532 - mean_absolute_error: 17214.3532 - val_loss: 17435.8257 - val_mean_absolute_error: 17435.8257\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 17397.89172\n",
      "Epoch 853/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 17189.3064 - mean_absolute_error: 17189.3064 - val_loss: 18331.3546 - val_mean_absolute_error: 18331.3546\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 17397.89172\n",
      "Epoch 854/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 16959.7750 - mean_absolute_error: 16959.7750 - val_loss: 18013.4590 - val_mean_absolute_error: 18013.4590\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 17397.89172\n",
      "Epoch 855/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 17057.4839 - mean_absolute_error: 17057.4839 - val_loss: 18651.1611 - val_mean_absolute_error: 18651.1611\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 17397.89172\n",
      "Epoch 856/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16983.1367 - mean_absolute_error: 16983.1367 - val_loss: 17423.6661 - val_mean_absolute_error: 17423.6661\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 17397.89172\n",
      "Epoch 857/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16798.8019 - mean_absolute_error: 16798.8019 - val_loss: 17639.9714 - val_mean_absolute_error: 17639.9714\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 17397.89172\n",
      "Epoch 858/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 16899.7951 - mean_absolute_error: 16899.7951 - val_loss: 17595.0450 - val_mean_absolute_error: 17595.0450\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 17397.89172\n",
      "Epoch 859/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16705.6122 - mean_absolute_error: 16705.6122 - val_loss: 18076.9268 - val_mean_absolute_error: 18076.9268\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 17397.89172\n",
      "Epoch 860/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 17411.2130 - mean_absolute_error: 17411.2130 - val_loss: 17561.7520 - val_mean_absolute_error: 17561.7520\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 17397.89172\n",
      "Epoch 861/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16960.5084 - mean_absolute_error: 16960.5084 - val_loss: 20668.2424 - val_mean_absolute_error: 20668.2424\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 17397.89172\n",
      "Epoch 862/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 16982.1910 - mean_absolute_error: 16982.1910 - val_loss: 18400.5358 - val_mean_absolute_error: 18400.5358\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 17397.89172\n",
      "Epoch 863/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16930.9376 - mean_absolute_error: 16930.9376 - val_loss: 17823.5254 - val_mean_absolute_error: 17823.5254\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 17397.89172\n",
      "Epoch 864/1500\n",
      "934/934 [==============================] - 0s 240us/step - loss: 17054.5626 - mean_absolute_error: 17054.5626 - val_loss: 17981.0536 - val_mean_absolute_error: 17981.0536\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 17397.89172\n",
      "Epoch 865/1500\n",
      "934/934 [==============================] - 0s 237us/step - loss: 17198.5746 - mean_absolute_error: 17198.5746 - val_loss: 17503.6047 - val_mean_absolute_error: 17503.6047\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 17397.89172\n",
      "Epoch 866/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 16795.6711 - mean_absolute_error: 16795.6711 - val_loss: 17475.5521 - val_mean_absolute_error: 17475.5521\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 17397.89172\n",
      "Epoch 867/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16609.1079 - mean_absolute_error: 16609.1079 - val_loss: 17636.8519 - val_mean_absolute_error: 17636.8519\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 17397.89172\n",
      "Epoch 868/1500\n",
      "934/934 [==============================] - 0s 222us/step - loss: 17544.9152 - mean_absolute_error: 17544.9152 - val_loss: 19026.9185 - val_mean_absolute_error: 19026.9185\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 17397.89172\n",
      "Epoch 869/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 17065.0944 - mean_absolute_error: 17065.0944 - val_loss: 17803.3350 - val_mean_absolute_error: 17803.3350\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 17397.89172\n",
      "Epoch 870/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16615.8822 - mean_absolute_error: 16615.8822 - val_loss: 17958.7248 - val_mean_absolute_error: 17958.7248\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 17397.89172\n",
      "Epoch 871/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 16537.9777 - mean_absolute_error: 16537.9777 - val_loss: 18483.8755 - val_mean_absolute_error: 18483.8755\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 17397.89172\n",
      "Epoch 872/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16766.8699 - mean_absolute_error: 16766.8699 - val_loss: 17879.2585 - val_mean_absolute_error: 17879.2585\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 17397.89172\n",
      "Epoch 873/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 17007.6125 - mean_absolute_error: 17007.6125 - val_loss: 17808.1445 - val_mean_absolute_error: 17808.1445\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 17397.89172\n",
      "Epoch 874/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16980.7897 - mean_absolute_error: 16980.7897 - val_loss: 18351.3048 - val_mean_absolute_error: 18351.3048\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 17397.89172\n",
      "Epoch 875/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 17004.5209 - mean_absolute_error: 17004.5209 - val_loss: 18220.8030 - val_mean_absolute_error: 18220.8030\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 17397.89172\n",
      "Epoch 876/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 17117.9584 - mean_absolute_error: 17117.9584 - val_loss: 17606.1469 - val_mean_absolute_error: 17606.1469\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 17397.89172\n",
      "Epoch 877/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 16930.4136 - mean_absolute_error: 16930.4136 - val_loss: 17602.4727 - val_mean_absolute_error: 17602.4727\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 17397.89172\n",
      "Epoch 878/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16454.0642 - mean_absolute_error: 16454.0642 - val_loss: 17416.2008 - val_mean_absolute_error: 17416.2008\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 17397.89172\n",
      "Epoch 879/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16644.7794 - mean_absolute_error: 16644.7794 - val_loss: 17813.7060 - val_mean_absolute_error: 17813.7060\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 17397.89172\n",
      "Epoch 880/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16823.1602 - mean_absolute_error: 16823.1602 - val_loss: 17594.8143 - val_mean_absolute_error: 17594.8143\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 17397.89172\n",
      "Epoch 881/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16556.0959 - mean_absolute_error: 16556.0959 - val_loss: 17763.3450 - val_mean_absolute_error: 17763.3450\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 17397.89172\n",
      "Epoch 882/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 16611.1756 - mean_absolute_error: 16611.1756 - val_loss: 17693.5163 - val_mean_absolute_error: 17693.5163\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 17397.89172\n",
      "Epoch 883/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16637.8358 - mean_absolute_error: 16637.8358 - val_loss: 17659.9112 - val_mean_absolute_error: 17659.9112\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 17397.89172\n",
      "Epoch 884/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 17033.1885 - mean_absolute_error: 17033.1885 - val_loss: 19240.1405 - val_mean_absolute_error: 19240.1405\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 17397.89172\n",
      "Epoch 885/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16787.6057 - mean_absolute_error: 16787.6057 - val_loss: 19056.6414 - val_mean_absolute_error: 19056.6414\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 17397.89172\n",
      "Epoch 886/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 195us/step - loss: 16555.6855 - mean_absolute_error: 16555.6855 - val_loss: 18600.4275 - val_mean_absolute_error: 18600.4275\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 17397.89172\n",
      "Epoch 887/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 17031.7293 - mean_absolute_error: 17031.7293 - val_loss: 17660.4865 - val_mean_absolute_error: 17660.4865\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 17397.89172\n",
      "Epoch 888/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17205.2432 - mean_absolute_error: 17205.2432 - val_loss: 18224.1696 - val_mean_absolute_error: 18224.1696\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 17397.89172\n",
      "Epoch 889/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16608.1825 - mean_absolute_error: 16608.1825 - val_loss: 18055.6871 - val_mean_absolute_error: 18055.6871\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 17397.89172\n",
      "Epoch 890/1500\n",
      "934/934 [==============================] - 0s 219us/step - loss: 17049.3748 - mean_absolute_error: 17049.3748 - val_loss: 18129.1810 - val_mean_absolute_error: 18129.1810\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 17397.89172\n",
      "Epoch 891/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16772.5077 - mean_absolute_error: 16772.5077 - val_loss: 19582.3690 - val_mean_absolute_error: 19582.3690\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 17397.89172\n",
      "Epoch 892/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 16687.0948 - mean_absolute_error: 16687.0948 - val_loss: 17635.8013 - val_mean_absolute_error: 17635.8013\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 17397.89172\n",
      "Epoch 893/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16509.5959 - mean_absolute_error: 16509.5959 - val_loss: 18032.9646 - val_mean_absolute_error: 18032.9646\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 17397.89172\n",
      "Epoch 894/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 16865.0379 - mean_absolute_error: 16865.0379 - val_loss: 18182.3456 - val_mean_absolute_error: 18182.3456\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 17397.89172\n",
      "Epoch 895/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16554.9609 - mean_absolute_error: 16554.9609 - val_loss: 18161.8076 - val_mean_absolute_error: 18161.8076\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 17397.89172\n",
      "Epoch 896/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16943.3609 - mean_absolute_error: 16943.3609 - val_loss: 17696.5370 - val_mean_absolute_error: 17696.5370\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 17397.89172\n",
      "Epoch 897/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16879.1350 - mean_absolute_error: 16879.1350 - val_loss: 17399.5124 - val_mean_absolute_error: 17399.5124\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 17397.89172\n",
      "Epoch 898/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16864.3317 - mean_absolute_error: 16864.3317 - val_loss: 17913.8379 - val_mean_absolute_error: 17913.8379\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 17397.89172\n",
      "Epoch 899/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16955.6146 - mean_absolute_error: 16955.6146 - val_loss: 18569.2901 - val_mean_absolute_error: 18569.2901\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 17397.89172\n",
      "Epoch 900/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 17142.0823 - mean_absolute_error: 17142.0823 - val_loss: 17532.2812 - val_mean_absolute_error: 17532.2812\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 17397.89172\n",
      "Epoch 901/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16722.0088 - mean_absolute_error: 16722.0088 - val_loss: 17480.8430 - val_mean_absolute_error: 17480.8430\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 17397.89172\n",
      "Epoch 902/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16499.7060 - mean_absolute_error: 16499.7060 - val_loss: 17737.4985 - val_mean_absolute_error: 17737.4985\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 17397.89172\n",
      "Epoch 903/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 16791.6793 - mean_absolute_error: 16791.6793 - val_loss: 17684.7725 - val_mean_absolute_error: 17684.7725\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 17397.89172\n",
      "Epoch 904/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 16707.9855 - mean_absolute_error: 16707.9855 - val_loss: 18346.9177 - val_mean_absolute_error: 18346.9177\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 17397.89172\n",
      "Epoch 905/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16554.9040 - mean_absolute_error: 16554.9040 - val_loss: 17621.5634 - val_mean_absolute_error: 17621.5634\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 17397.89172\n",
      "Epoch 906/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16824.7964 - mean_absolute_error: 16824.7964 - val_loss: 17655.4933 - val_mean_absolute_error: 17655.4933\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 17397.89172\n",
      "Epoch 907/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 17214.4008 - mean_absolute_error: 17214.4008 - val_loss: 18787.1537 - val_mean_absolute_error: 18787.1537\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 17397.89172\n",
      "Epoch 908/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 16952.9638 - mean_absolute_error: 16952.9638 - val_loss: 18178.9806 - val_mean_absolute_error: 18178.9806\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 17397.89172\n",
      "Epoch 909/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16426.3983 - mean_absolute_error: 16426.3983 - val_loss: 18570.8921 - val_mean_absolute_error: 18570.8921\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 17397.89172\n",
      "Epoch 910/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 16467.8106 - mean_absolute_error: 16467.8106 - val_loss: 18074.1800 - val_mean_absolute_error: 18074.1800\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 17397.89172\n",
      "Epoch 911/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16738.3073 - mean_absolute_error: 16738.3073 - val_loss: 17494.0899 - val_mean_absolute_error: 17494.0899\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 17397.89172\n",
      "Epoch 912/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16803.2933 - mean_absolute_error: 16803.2933 - val_loss: 17725.0148 - val_mean_absolute_error: 17725.0148\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 17397.89172\n",
      "Epoch 913/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 16667.5178 - mean_absolute_error: 16667.5178 - val_loss: 17822.3752 - val_mean_absolute_error: 17822.3752\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 17397.89172\n",
      "Epoch 914/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16473.9021 - mean_absolute_error: 16473.9021 - val_loss: 18011.3627 - val_mean_absolute_error: 18011.3627\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 17397.89172\n",
      "Epoch 915/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16990.2295 - mean_absolute_error: 16990.2295 - val_loss: 19766.9414 - val_mean_absolute_error: 19766.9414\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 17397.89172\n",
      "Epoch 916/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 16836.7033 - mean_absolute_error: 16836.7033 - val_loss: 17670.4833 - val_mean_absolute_error: 17670.4833\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 17397.89172\n",
      "Epoch 917/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16521.0226 - mean_absolute_error: 16521.0226 - val_loss: 17699.7517 - val_mean_absolute_error: 17699.7517\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 17397.89172\n",
      "Epoch 918/1500\n",
      "934/934 [==============================] - 0s 233us/step - loss: 16694.0525 - mean_absolute_error: 16694.0525 - val_loss: 17427.2892 - val_mean_absolute_error: 17427.2892\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 17397.89172\n",
      "Epoch 919/1500\n",
      "934/934 [==============================] - 0s 187us/step - loss: 16904.8093 - mean_absolute_error: 16904.8093 - val_loss: 17862.6615 - val_mean_absolute_error: 17862.6615\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 17397.89172\n",
      "Epoch 920/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 202us/step - loss: 16982.4013 - mean_absolute_error: 16982.4013 - val_loss: 19405.6293 - val_mean_absolute_error: 19405.6293\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 17397.89172\n",
      "Epoch 921/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16877.2697 - mean_absolute_error: 16877.2697 - val_loss: 17555.1878 - val_mean_absolute_error: 17555.1878\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 17397.89172\n",
      "Epoch 922/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16570.9269 - mean_absolute_error: 16570.9269 - val_loss: 17542.7061 - val_mean_absolute_error: 17542.7061\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 17397.89172\n",
      "Epoch 923/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16390.3366 - mean_absolute_error: 16390.3366 - val_loss: 18362.6348 - val_mean_absolute_error: 18362.6348\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 17397.89172\n",
      "Epoch 924/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16791.3162 - mean_absolute_error: 16791.3162 - val_loss: 18537.1034 - val_mean_absolute_error: 18537.1034\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 17397.89172\n",
      "Epoch 925/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16824.5663 - mean_absolute_error: 16824.5663 - val_loss: 20249.4432 - val_mean_absolute_error: 20249.4432\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 17397.89172\n",
      "Epoch 926/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 17148.9468 - mean_absolute_error: 17148.9468 - val_loss: 17748.9111 - val_mean_absolute_error: 17748.9111\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 17397.89172\n",
      "Epoch 927/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16249.2460 - mean_absolute_error: 16249.2460 - val_loss: 17536.0136 - val_mean_absolute_error: 17536.0136\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 17397.89172\n",
      "Epoch 928/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 16278.1509 - mean_absolute_error: 16278.1509 - val_loss: 18299.6369 - val_mean_absolute_error: 18299.6369\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 17397.89172\n",
      "Epoch 929/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 16625.3547 - mean_absolute_error: 16625.3547 - val_loss: 18252.4861 - val_mean_absolute_error: 18252.4861\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 17397.89172\n",
      "Epoch 930/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 16482.6390 - mean_absolute_error: 16482.6390 - val_loss: 18408.8222 - val_mean_absolute_error: 18408.8222\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 17397.89172\n",
      "Epoch 931/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 16748.9031 - mean_absolute_error: 16748.9031 - val_loss: 17667.7226 - val_mean_absolute_error: 17667.7226\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 17397.89172\n",
      "Epoch 932/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16638.7276 - mean_absolute_error: 16638.7276 - val_loss: 18071.0649 - val_mean_absolute_error: 18071.0649\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 17397.89172\n",
      "Epoch 933/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16531.0915 - mean_absolute_error: 16531.0915 - val_loss: 17656.9132 - val_mean_absolute_error: 17656.9132\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 17397.89172\n",
      "Epoch 934/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 16437.1275 - mean_absolute_error: 16437.1275 - val_loss: 17934.4539 - val_mean_absolute_error: 17934.4539\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 17397.89172\n",
      "Epoch 935/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16751.7075 - mean_absolute_error: 16751.7075 - val_loss: 18806.6901 - val_mean_absolute_error: 18806.6901\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 17397.89172\n",
      "Epoch 936/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16301.4961 - mean_absolute_error: 16301.4961 - val_loss: 17859.6084 - val_mean_absolute_error: 17859.6084\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 17397.89172\n",
      "Epoch 937/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17001.5984 - mean_absolute_error: 17001.5984 - val_loss: 18721.7804 - val_mean_absolute_error: 18721.7804\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 17397.89172\n",
      "Epoch 938/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 16894.2664 - mean_absolute_error: 16894.2664 - val_loss: 17533.0222 - val_mean_absolute_error: 17533.0222\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 17397.89172\n",
      "Epoch 939/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 16376.4559 - mean_absolute_error: 16376.4559 - val_loss: 18424.5296 - val_mean_absolute_error: 18424.5296\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 17397.89172\n",
      "Epoch 940/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16733.4009 - mean_absolute_error: 16733.4009 - val_loss: 17396.9094 - val_mean_absolute_error: 17396.9094\n",
      "\n",
      "Epoch 00940: val_loss improved from 17397.89172 to 17396.90945, saving model to Weights-940--17396.90945.hdf5\n",
      "Epoch 941/1500\n",
      "934/934 [==============================] - 0s 220us/step - loss: 16516.5957 - mean_absolute_error: 16516.5957 - val_loss: 17978.2587 - val_mean_absolute_error: 17978.2587\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 17396.90945\n",
      "Epoch 942/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16439.2648 - mean_absolute_error: 16439.2648 - val_loss: 17875.6815 - val_mean_absolute_error: 17875.6815\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 17396.90945\n",
      "Epoch 943/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 16472.4109 - mean_absolute_error: 16472.4109 - val_loss: 17985.8377 - val_mean_absolute_error: 17985.8377\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 17396.90945\n",
      "Epoch 944/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 16526.8390 - mean_absolute_error: 16526.8390 - val_loss: 17751.1220 - val_mean_absolute_error: 17751.1220\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 17396.90945\n",
      "Epoch 945/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16473.1037 - mean_absolute_error: 16473.1037 - val_loss: 17570.1078 - val_mean_absolute_error: 17570.1078\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 17396.90945\n",
      "Epoch 946/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16694.9080 - mean_absolute_error: 16694.9080 - val_loss: 17588.8687 - val_mean_absolute_error: 17588.8687\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 17396.90945\n",
      "Epoch 947/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 16946.0883 - mean_absolute_error: 16946.0883 - val_loss: 17849.5775 - val_mean_absolute_error: 17849.5775\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 17396.90945\n",
      "Epoch 948/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16292.8341 - mean_absolute_error: 16292.8341 - val_loss: 18710.3121 - val_mean_absolute_error: 18710.3121\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 17396.90945\n",
      "Epoch 949/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16647.7281 - mean_absolute_error: 16647.7281 - val_loss: 17362.5896 - val_mean_absolute_error: 17362.5896\n",
      "\n",
      "Epoch 00949: val_loss improved from 17396.90945 to 17362.58956, saving model to Weights-949--17362.58956.hdf5\n",
      "Epoch 950/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16532.8548 - mean_absolute_error: 16532.8548 - val_loss: 17457.2566 - val_mean_absolute_error: 17457.2566\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 17362.58956\n",
      "Epoch 951/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 16746.2658 - mean_absolute_error: 16746.2658 - val_loss: 17792.3248 - val_mean_absolute_error: 17792.3248\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 17362.58956\n",
      "Epoch 952/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16363.4781 - mean_absolute_error: 16363.4781 - val_loss: 17585.0165 - val_mean_absolute_error: 17585.0165\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 17362.58956\n",
      "Epoch 953/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16367.2419 - mean_absolute_error: 16367.2419 - val_loss: 18244.4353 - val_mean_absolute_error: 18244.4353\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 17362.58956\n",
      "Epoch 954/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16452.9238 - mean_absolute_error: 16452.9238 - val_loss: 17540.9511 - val_mean_absolute_error: 17540.9511\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 17362.58956\n",
      "Epoch 955/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 16201.4480 - mean_absolute_error: 16201.4480 - val_loss: 17582.7025 - val_mean_absolute_error: 17582.7025\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 17362.58956\n",
      "Epoch 956/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16544.0977 - mean_absolute_error: 16544.0977 - val_loss: 17410.7540 - val_mean_absolute_error: 17410.7540\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 17362.58956\n",
      "Epoch 957/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16376.5223 - mean_absolute_error: 16376.5223 - val_loss: 17680.8123 - val_mean_absolute_error: 17680.8123\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 17362.58956\n",
      "Epoch 958/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17068.8261 - mean_absolute_error: 17068.8261 - val_loss: 18254.4853 - val_mean_absolute_error: 18254.4853\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 17362.58956\n",
      "Epoch 959/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 16617.8506 - mean_absolute_error: 16617.8506 - val_loss: 18015.5868 - val_mean_absolute_error: 18015.5868\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 17362.58956\n",
      "Epoch 960/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 16206.2015 - mean_absolute_error: 16206.2015 - val_loss: 17722.5628 - val_mean_absolute_error: 17722.5628\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 17362.58956\n",
      "Epoch 961/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16647.7315 - mean_absolute_error: 16647.7315 - val_loss: 17652.9046 - val_mean_absolute_error: 17652.9046\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 17362.58956\n",
      "Epoch 962/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16485.2426 - mean_absolute_error: 16485.2426 - val_loss: 18613.6122 - val_mean_absolute_error: 18613.6122\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 17362.58956\n",
      "Epoch 963/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 16403.5785 - mean_absolute_error: 16403.5785 - val_loss: 17586.9122 - val_mean_absolute_error: 17586.9122\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 17362.58956\n",
      "Epoch 964/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 16284.7860 - mean_absolute_error: 16284.7860 - val_loss: 18321.4202 - val_mean_absolute_error: 18321.4202\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 17362.58956\n",
      "Epoch 965/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 16640.6721 - mean_absolute_error: 16640.6721 - val_loss: 19067.9098 - val_mean_absolute_error: 19067.9098\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 17362.58956\n",
      "Epoch 966/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 16417.5876 - mean_absolute_error: 16417.5876 - val_loss: 17747.2812 - val_mean_absolute_error: 17747.2812\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 17362.58956\n",
      "Epoch 967/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16171.9935 - mean_absolute_error: 16171.9935 - val_loss: 17939.8155 - val_mean_absolute_error: 17939.8155\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 17362.58956\n",
      "Epoch 968/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 17041.5420 - mean_absolute_error: 17041.5420 - val_loss: 17643.4185 - val_mean_absolute_error: 17643.4185\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 17362.58956\n",
      "Epoch 969/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16172.9669 - mean_absolute_error: 16172.9669 - val_loss: 18259.4375 - val_mean_absolute_error: 18259.4375\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 17362.58956\n",
      "Epoch 970/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16606.3635 - mean_absolute_error: 16606.3635 - val_loss: 19124.7255 - val_mean_absolute_error: 19124.7255\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 17362.58956\n",
      "Epoch 971/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16615.9933 - mean_absolute_error: 16615.9933 - val_loss: 20144.1241 - val_mean_absolute_error: 20144.1241\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 17362.58956\n",
      "Epoch 972/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16532.6509 - mean_absolute_error: 16532.6509 - val_loss: 17556.3168 - val_mean_absolute_error: 17556.3168\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 17362.58956\n",
      "Epoch 973/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16657.2840 - mean_absolute_error: 16657.2840 - val_loss: 17985.0068 - val_mean_absolute_error: 17985.0068\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 17362.58956\n",
      "Epoch 974/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16249.3478 - mean_absolute_error: 16249.3478 - val_loss: 17736.7616 - val_mean_absolute_error: 17736.7616\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 17362.58956\n",
      "Epoch 975/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 16410.3118 - mean_absolute_error: 16410.3118 - val_loss: 17767.1710 - val_mean_absolute_error: 17767.1710\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 17362.58956\n",
      "Epoch 976/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 16306.9424 - mean_absolute_error: 16306.9424 - val_loss: 17513.8104 - val_mean_absolute_error: 17513.8104\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 17362.58956\n",
      "Epoch 977/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 15868.4877 - mean_absolute_error: 15868.4877 - val_loss: 17613.3215 - val_mean_absolute_error: 17613.3215\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 17362.58956\n",
      "Epoch 978/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 16738.4818 - mean_absolute_error: 16738.4818 - val_loss: 18206.3625 - val_mean_absolute_error: 18206.3625\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 17362.58956\n",
      "Epoch 979/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16196.6275 - mean_absolute_error: 16196.6275 - val_loss: 18640.0838 - val_mean_absolute_error: 18640.0838\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 17362.58956\n",
      "Epoch 980/1500\n",
      "934/934 [==============================] - 0s 218us/step - loss: 16139.3805 - mean_absolute_error: 16139.3805 - val_loss: 17875.2001 - val_mean_absolute_error: 17875.2001\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 17362.58956\n",
      "Epoch 981/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16109.4964 - mean_absolute_error: 16109.4964 - val_loss: 18090.4232 - val_mean_absolute_error: 18090.4232\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 17362.58956\n",
      "Epoch 982/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16676.3955 - mean_absolute_error: 16676.3955 - val_loss: 17835.6630 - val_mean_absolute_error: 17835.6630\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 17362.58956\n",
      "Epoch 983/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16491.0644 - mean_absolute_error: 16491.0644 - val_loss: 18782.1175 - val_mean_absolute_error: 18782.1175\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 17362.58956\n",
      "Epoch 984/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 17017.1589 - mean_absolute_error: 17017.1589 - val_loss: 17751.9730 - val_mean_absolute_error: 17751.9730\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 17362.58956\n",
      "Epoch 985/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16221.4385 - mean_absolute_error: 16221.4385 - val_loss: 17632.3973 - val_mean_absolute_error: 17632.3973\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 17362.58956\n",
      "Epoch 986/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16398.0109 - mean_absolute_error: 16398.0109 - val_loss: 18952.5491 - val_mean_absolute_error: 18952.5491\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 17362.58956\n",
      "Epoch 987/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 195us/step - loss: 16185.7304 - mean_absolute_error: 16185.7304 - val_loss: 17954.6306 - val_mean_absolute_error: 17954.6306\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 17362.58956\n",
      "Epoch 988/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 16328.3591 - mean_absolute_error: 16328.3591 - val_loss: 17548.1225 - val_mean_absolute_error: 17548.1225\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 17362.58956\n",
      "Epoch 989/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 16611.3571 - mean_absolute_error: 16611.3571 - val_loss: 18325.3059 - val_mean_absolute_error: 18325.3059\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 17362.58956\n",
      "Epoch 990/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16442.8413 - mean_absolute_error: 16442.8413 - val_loss: 17819.4077 - val_mean_absolute_error: 17819.4077\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 17362.58956\n",
      "Epoch 991/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16673.1369 - mean_absolute_error: 16673.1369 - val_loss: 17639.6810 - val_mean_absolute_error: 17639.6810\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 17362.58956\n",
      "Epoch 992/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16648.8361 - mean_absolute_error: 16648.8361 - val_loss: 17749.7739 - val_mean_absolute_error: 17749.7739\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 17362.58956\n",
      "Epoch 993/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16192.5572 - mean_absolute_error: 16192.5572 - val_loss: 17599.4036 - val_mean_absolute_error: 17599.4036\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 17362.58956\n",
      "Epoch 994/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16073.9610 - mean_absolute_error: 16073.9610 - val_loss: 18102.6955 - val_mean_absolute_error: 18102.6955\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 17362.58956\n",
      "Epoch 995/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16438.6154 - mean_absolute_error: 16438.6154 - val_loss: 18501.9958 - val_mean_absolute_error: 18501.9958\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 17362.58956\n",
      "Epoch 996/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 16253.6794 - mean_absolute_error: 16253.6794 - val_loss: 18039.5806 - val_mean_absolute_error: 18039.5806\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 17362.58956\n",
      "Epoch 997/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16435.8566 - mean_absolute_error: 16435.8566 - val_loss: 17405.3170 - val_mean_absolute_error: 17405.3170\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 17362.58956\n",
      "Epoch 998/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16469.6490 - mean_absolute_error: 16469.6490 - val_loss: 17923.3217 - val_mean_absolute_error: 17923.3217\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 17362.58956\n",
      "Epoch 999/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16029.9780 - mean_absolute_error: 16029.9780 - val_loss: 17909.3353 - val_mean_absolute_error: 17909.3353\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 17362.58956\n",
      "Epoch 1000/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16159.6359 - mean_absolute_error: 16159.6359 - val_loss: 18074.9501 - val_mean_absolute_error: 18074.9501\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 17362.58956\n",
      "Epoch 1001/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16120.7815 - mean_absolute_error: 16120.7815 - val_loss: 17542.4045 - val_mean_absolute_error: 17542.4045\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 17362.58956\n",
      "Epoch 1002/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15970.5977 - mean_absolute_error: 15970.5977 - val_loss: 17992.2216 - val_mean_absolute_error: 17992.2216\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 17362.58956\n",
      "Epoch 1003/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16183.7143 - mean_absolute_error: 16183.7143 - val_loss: 17532.4192 - val_mean_absolute_error: 17532.4192\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 17362.58956\n",
      "Epoch 1004/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16386.5636 - mean_absolute_error: 16386.5636 - val_loss: 18756.3714 - val_mean_absolute_error: 18756.3714\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 17362.58956\n",
      "Epoch 1005/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 16051.6775 - mean_absolute_error: 16051.6775 - val_loss: 18189.1857 - val_mean_absolute_error: 18189.1857\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 17362.58956\n",
      "Epoch 1006/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 15992.5905 - mean_absolute_error: 15992.5905 - val_loss: 17840.9176 - val_mean_absolute_error: 17840.9176\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 17362.58956\n",
      "Epoch 1007/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16546.2540 - mean_absolute_error: 16546.2540 - val_loss: 19237.8154 - val_mean_absolute_error: 19237.8154\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 17362.58956\n",
      "Epoch 1008/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16302.9665 - mean_absolute_error: 16302.9665 - val_loss: 18326.2884 - val_mean_absolute_error: 18326.2884\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 17362.58956\n",
      "Epoch 1009/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15952.0438 - mean_absolute_error: 15952.0438 - val_loss: 17847.7986 - val_mean_absolute_error: 17847.7986\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 17362.58956\n",
      "Epoch 1010/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16032.4859 - mean_absolute_error: 16032.4859 - val_loss: 17691.7363 - val_mean_absolute_error: 17691.7363\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 17362.58956\n",
      "Epoch 1011/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 16002.4674 - mean_absolute_error: 16002.4674 - val_loss: 18474.7991 - val_mean_absolute_error: 18474.7991\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 17362.58956\n",
      "Epoch 1012/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16400.7598 - mean_absolute_error: 16400.7598 - val_loss: 18003.5424 - val_mean_absolute_error: 18003.5424\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 17362.58956\n",
      "Epoch 1013/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16020.5706 - mean_absolute_error: 16020.5706 - val_loss: 18198.4690 - val_mean_absolute_error: 18198.4690\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 17362.58956\n",
      "Epoch 1014/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16565.1458 - mean_absolute_error: 16565.1458 - val_loss: 17724.1830 - val_mean_absolute_error: 17724.1830\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 17362.58956\n",
      "Epoch 1015/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16250.8376 - mean_absolute_error: 16250.8376 - val_loss: 17735.2216 - val_mean_absolute_error: 17735.2216\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 17362.58956\n",
      "Epoch 1016/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16187.6512 - mean_absolute_error: 16187.6512 - val_loss: 17889.4243 - val_mean_absolute_error: 17889.4243\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 17362.58956\n",
      "Epoch 1017/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16023.8046 - mean_absolute_error: 16023.8046 - val_loss: 17795.1617 - val_mean_absolute_error: 17795.1617\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 17362.58956\n",
      "Epoch 1018/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16209.6123 - mean_absolute_error: 16209.6123 - val_loss: 17567.9344 - val_mean_absolute_error: 17567.9344\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 17362.58956\n",
      "Epoch 1019/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16228.6097 - mean_absolute_error: 16228.6097 - val_loss: 17631.5873 - val_mean_absolute_error: 17631.5873\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 17362.58956\n",
      "Epoch 1020/1500\n",
      "934/934 [==============================] - 0s 220us/step - loss: 16108.1345 - mean_absolute_error: 16108.1345 - val_loss: 17993.8073 - val_mean_absolute_error: 17993.8073\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 17362.58956\n",
      "Epoch 1021/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 196us/step - loss: 16125.4102 - mean_absolute_error: 16125.4102 - val_loss: 19552.0321 - val_mean_absolute_error: 19552.0321\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 17362.58956\n",
      "Epoch 1022/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16438.1461 - mean_absolute_error: 16438.1461 - val_loss: 17968.8440 - val_mean_absolute_error: 17968.8440\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 17362.58956\n",
      "Epoch 1023/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16268.2223 - mean_absolute_error: 16268.2223 - val_loss: 17876.1039 - val_mean_absolute_error: 17876.1039\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 17362.58956\n",
      "Epoch 1024/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 16093.6192 - mean_absolute_error: 16093.6192 - val_loss: 18975.8712 - val_mean_absolute_error: 18975.8712\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 17362.58956\n",
      "Epoch 1025/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 16265.1302 - mean_absolute_error: 16265.1302 - val_loss: 17694.3765 - val_mean_absolute_error: 17694.3765\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 17362.58956\n",
      "Epoch 1026/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16309.6496 - mean_absolute_error: 16309.6496 - val_loss: 17438.4765 - val_mean_absolute_error: 17438.4765\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 17362.58956\n",
      "Epoch 1027/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16598.7754 - mean_absolute_error: 16598.7754 - val_loss: 18466.3647 - val_mean_absolute_error: 18466.3647\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 17362.58956\n",
      "Epoch 1028/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15932.7782 - mean_absolute_error: 15932.7782 - val_loss: 18069.8778 - val_mean_absolute_error: 18069.8778\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 17362.58956\n",
      "Epoch 1029/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15838.9577 - mean_absolute_error: 15838.9577 - val_loss: 18056.8931 - val_mean_absolute_error: 18056.8931\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 17362.58956\n",
      "Epoch 1030/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16464.8084 - mean_absolute_error: 16464.8084 - val_loss: 17924.5665 - val_mean_absolute_error: 17924.5665\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 17362.58956\n",
      "Epoch 1031/1500\n",
      "934/934 [==============================] - ETA: 0s - loss: 15750.7456 - mean_absolute_error: 15750.745 - 0s 194us/step - loss: 15829.4538 - mean_absolute_error: 15829.4538 - val_loss: 17646.2565 - val_mean_absolute_error: 17646.2565\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 17362.58956\n",
      "Epoch 1032/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16384.0532 - mean_absolute_error: 16384.0532 - val_loss: 19034.0837 - val_mean_absolute_error: 19034.0837\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 17362.58956\n",
      "Epoch 1033/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16193.7987 - mean_absolute_error: 16193.7987 - val_loss: 17845.3155 - val_mean_absolute_error: 17845.3155\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 17362.58956\n",
      "Epoch 1034/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 16266.1995 - mean_absolute_error: 16266.1995 - val_loss: 18586.8172 - val_mean_absolute_error: 18586.8172\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 17362.58956\n",
      "Epoch 1035/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16298.0403 - mean_absolute_error: 16298.0403 - val_loss: 17902.2606 - val_mean_absolute_error: 17902.2606\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 17362.58956\n",
      "Epoch 1036/1500\n",
      "934/934 [==============================] - ETA: 0s - loss: 15805.3415 - mean_absolute_error: 15805.341 - 0s 200us/step - loss: 16003.4149 - mean_absolute_error: 16003.4149 - val_loss: 18044.4051 - val_mean_absolute_error: 18044.4051\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 17362.58956\n",
      "Epoch 1037/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 16021.2041 - mean_absolute_error: 16021.2041 - val_loss: 18280.2579 - val_mean_absolute_error: 18280.2579\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 17362.58956\n",
      "Epoch 1038/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16009.4482 - mean_absolute_error: 16009.4482 - val_loss: 18785.3022 - val_mean_absolute_error: 18785.3022\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 17362.58956\n",
      "Epoch 1039/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15827.1654 - mean_absolute_error: 15827.1654 - val_loss: 17831.1543 - val_mean_absolute_error: 17831.1543\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 17362.58956\n",
      "Epoch 1040/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15931.6657 - mean_absolute_error: 15931.6657 - val_loss: 17875.8755 - val_mean_absolute_error: 17875.8755\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 17362.58956\n",
      "Epoch 1041/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15593.0387 - mean_absolute_error: 15593.0387 - val_loss: 17890.4987 - val_mean_absolute_error: 17890.4987\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 17362.58956\n",
      "Epoch 1042/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15907.1037 - mean_absolute_error: 15907.1037 - val_loss: 17613.2400 - val_mean_absolute_error: 17613.2400\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 17362.58956\n",
      "Epoch 1043/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16603.8562 - mean_absolute_error: 16603.8562 - val_loss: 17809.1423 - val_mean_absolute_error: 17809.1423\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 17362.58956\n",
      "Epoch 1044/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16012.8173 - mean_absolute_error: 16012.8173 - val_loss: 18396.5900 - val_mean_absolute_error: 18396.5900\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 17362.58956\n",
      "Epoch 1045/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15915.3226 - mean_absolute_error: 15915.3226 - val_loss: 17535.0422 - val_mean_absolute_error: 17535.0422\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 17362.58956\n",
      "Epoch 1046/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 15865.7873 - mean_absolute_error: 15865.7873 - val_loss: 18531.3422 - val_mean_absolute_error: 18531.3422\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 17362.58956\n",
      "Epoch 1047/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 16019.4513 - mean_absolute_error: 16019.4513 - val_loss: 19201.0855 - val_mean_absolute_error: 19201.0855\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 17362.58956\n",
      "Epoch 1048/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15966.4551 - mean_absolute_error: 15966.4551 - val_loss: 19625.3561 - val_mean_absolute_error: 19625.3561\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 17362.58956\n",
      "Epoch 1049/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16075.4824 - mean_absolute_error: 16075.4824 - val_loss: 18101.5284 - val_mean_absolute_error: 18101.5284\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 17362.58956\n",
      "Epoch 1050/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 16245.7821 - mean_absolute_error: 16245.7821 - val_loss: 17805.5722 - val_mean_absolute_error: 17805.5722\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 17362.58956\n",
      "Epoch 1051/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 15722.8878 - mean_absolute_error: 15722.8878 - val_loss: 17856.1590 - val_mean_absolute_error: 17856.1590\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 17362.58956\n",
      "Epoch 1052/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15967.8699 - mean_absolute_error: 15967.8699 - val_loss: 19852.7616 - val_mean_absolute_error: 19852.7616\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 17362.58956\n",
      "Epoch 1053/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15695.4988 - mean_absolute_error: 15695.4988 - val_loss: 17746.5408 - val_mean_absolute_error: 17746.5408\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 17362.58956\n",
      "Epoch 1054/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16259.8860 - mean_absolute_error: 16259.8860 - val_loss: 17810.9928 - val_mean_absolute_error: 17810.9928\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 17362.58956\n",
      "Epoch 1055/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15912.3968 - mean_absolute_error: 15912.3968 - val_loss: 17681.1300 - val_mean_absolute_error: 17681.1300\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 17362.58956\n",
      "Epoch 1056/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16006.2200 - mean_absolute_error: 16006.2200 - val_loss: 17740.7449 - val_mean_absolute_error: 17740.7449\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 17362.58956\n",
      "Epoch 1057/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15994.7085 - mean_absolute_error: 15994.7085 - val_loss: 18440.4043 - val_mean_absolute_error: 18440.4043\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 17362.58956\n",
      "Epoch 1058/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16216.4268 - mean_absolute_error: 16216.4268 - val_loss: 18330.5658 - val_mean_absolute_error: 18330.5658\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 17362.58956\n",
      "Epoch 1059/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16271.3677 - mean_absolute_error: 16271.3677 - val_loss: 18337.8552 - val_mean_absolute_error: 18337.8552\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 17362.58956\n",
      "Epoch 1060/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16121.4661 - mean_absolute_error: 16121.4661 - val_loss: 18240.0820 - val_mean_absolute_error: 18240.0820\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 17362.58956\n",
      "Epoch 1061/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16256.1099 - mean_absolute_error: 16256.1099 - val_loss: 18089.9998 - val_mean_absolute_error: 18089.9998\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 17362.58956\n",
      "Epoch 1062/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 16166.2116 - mean_absolute_error: 16166.2116 - val_loss: 17928.8099 - val_mean_absolute_error: 17928.8099\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 17362.58956\n",
      "Epoch 1063/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15795.5366 - mean_absolute_error: 15795.5366 - val_loss: 18055.2479 - val_mean_absolute_error: 18055.2479\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 17362.58956\n",
      "Epoch 1064/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15762.2387 - mean_absolute_error: 15762.2387 - val_loss: 18312.9992 - val_mean_absolute_error: 18312.9992\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 17362.58956\n",
      "Epoch 1065/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15899.5104 - mean_absolute_error: 15899.5104 - val_loss: 18168.0522 - val_mean_absolute_error: 18168.0522\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 17362.58956\n",
      "Epoch 1066/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 15864.7797 - mean_absolute_error: 15864.7797 - val_loss: 19371.1174 - val_mean_absolute_error: 19371.1174\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 17362.58956\n",
      "Epoch 1067/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15889.3774 - mean_absolute_error: 15889.3774 - val_loss: 18377.9242 - val_mean_absolute_error: 18377.9242\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 17362.58956\n",
      "Epoch 1068/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15891.8980 - mean_absolute_error: 15891.8980 - val_loss: 18055.9101 - val_mean_absolute_error: 18055.9101\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 17362.58956\n",
      "Epoch 1069/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15770.5763 - mean_absolute_error: 15770.5763 - val_loss: 18646.0758 - val_mean_absolute_error: 18646.0758\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 17362.58956\n",
      "Epoch 1070/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15671.2519 - mean_absolute_error: 15671.2519 - val_loss: 19361.8905 - val_mean_absolute_error: 19361.8905\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 17362.58956\n",
      "Epoch 1071/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15753.1063 - mean_absolute_error: 15753.1063 - val_loss: 18343.6877 - val_mean_absolute_error: 18343.6877\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 17362.58956\n",
      "Epoch 1072/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15771.9689 - mean_absolute_error: 15771.9689 - val_loss: 18191.1812 - val_mean_absolute_error: 18191.1812\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 17362.58956\n",
      "Epoch 1073/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15700.5949 - mean_absolute_error: 15700.5949 - val_loss: 17698.4504 - val_mean_absolute_error: 17698.4504\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 17362.58956\n",
      "Epoch 1074/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15612.8605 - mean_absolute_error: 15612.8605 - val_loss: 18198.1171 - val_mean_absolute_error: 18198.1171\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 17362.58956\n",
      "Epoch 1075/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15771.6664 - mean_absolute_error: 15771.6664 - val_loss: 17917.5562 - val_mean_absolute_error: 17917.5562\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 17362.58956\n",
      "Epoch 1076/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15953.1503 - mean_absolute_error: 15953.1503 - val_loss: 18067.4600 - val_mean_absolute_error: 18067.4600\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 17362.58956\n",
      "Epoch 1077/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16176.1037 - mean_absolute_error: 16176.1037 - val_loss: 18012.6985 - val_mean_absolute_error: 18012.6985\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 17362.58956\n",
      "Epoch 1078/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15689.2517 - mean_absolute_error: 15689.2517 - val_loss: 18637.5041 - val_mean_absolute_error: 18637.5041\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 17362.58956\n",
      "Epoch 1079/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16278.8755 - mean_absolute_error: 16278.8755 - val_loss: 17986.8020 - val_mean_absolute_error: 17986.8020\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 17362.58956\n",
      "Epoch 1080/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 15938.3968 - mean_absolute_error: 15938.3968 - val_loss: 20309.5204 - val_mean_absolute_error: 20309.5204\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 17362.58956\n",
      "Epoch 1081/1500\n",
      "934/934 [==============================] - 0s 223us/step - loss: 16188.9460 - mean_absolute_error: 16188.9460 - val_loss: 18004.2004 - val_mean_absolute_error: 18004.2004\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 17362.58956\n",
      "Epoch 1082/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 15804.5887 - mean_absolute_error: 15804.5887 - val_loss: 18153.6318 - val_mean_absolute_error: 18153.6318\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 17362.58956\n",
      "Epoch 1083/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16056.9196 - mean_absolute_error: 16056.9196 - val_loss: 17797.1303 - val_mean_absolute_error: 17797.1303\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 17362.58956\n",
      "Epoch 1084/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15680.0840 - mean_absolute_error: 15680.0840 - val_loss: 18289.6073 - val_mean_absolute_error: 18289.6073\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 17362.58956\n",
      "Epoch 1085/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15853.5422 - mean_absolute_error: 15853.5422 - val_loss: 19770.8531 - val_mean_absolute_error: 19770.8531\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 17362.58956\n",
      "Epoch 1086/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 16062.3083 - mean_absolute_error: 16062.3083 - val_loss: 19652.7595 - val_mean_absolute_error: 19652.7595\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 17362.58956\n",
      "Epoch 1087/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 15888.5466 - mean_absolute_error: 15888.5466 - val_loss: 18269.6011 - val_mean_absolute_error: 18269.6011\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 17362.58956\n",
      "Epoch 1088/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 192us/step - loss: 15384.8337 - mean_absolute_error: 15384.8337 - val_loss: 18369.7499 - val_mean_absolute_error: 18369.7499\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 17362.58956\n",
      "Epoch 1089/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15845.0286 - mean_absolute_error: 15845.0286 - val_loss: 18206.6406 - val_mean_absolute_error: 18206.6406\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 17362.58956\n",
      "Epoch 1090/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 15530.2649 - mean_absolute_error: 15530.2649 - val_loss: 18207.8234 - val_mean_absolute_error: 18207.8234\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 17362.58956\n",
      "Epoch 1091/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 15570.7654 - mean_absolute_error: 15570.7654 - val_loss: 18717.1536 - val_mean_absolute_error: 18717.1536\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 17362.58956\n",
      "Epoch 1092/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15638.1106 - mean_absolute_error: 15638.1106 - val_loss: 18294.4713 - val_mean_absolute_error: 18294.4713\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 17362.58956\n",
      "Epoch 1093/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 16014.5955 - mean_absolute_error: 16014.5955 - val_loss: 18358.0589 - val_mean_absolute_error: 18358.0589\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 17362.58956\n",
      "Epoch 1094/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15770.5258 - mean_absolute_error: 15770.5258 - val_loss: 18568.5440 - val_mean_absolute_error: 18568.5440\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 17362.58956\n",
      "Epoch 1095/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 15655.3827 - mean_absolute_error: 15655.3827 - val_loss: 18076.6664 - val_mean_absolute_error: 18076.6664\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 17362.58956\n",
      "Epoch 1096/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 16169.9913 - mean_absolute_error: 16169.9913 - val_loss: 18149.7920 - val_mean_absolute_error: 18149.7920\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 17362.58956\n",
      "Epoch 1097/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 15388.6280 - mean_absolute_error: 15388.6280 - val_loss: 18113.7519 - val_mean_absolute_error: 18113.7519\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 17362.58956\n",
      "Epoch 1098/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 16028.4940 - mean_absolute_error: 16028.4940 - val_loss: 18159.8651 - val_mean_absolute_error: 18159.8651\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 17362.58956\n",
      "Epoch 1099/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15485.0822 - mean_absolute_error: 15485.0822 - val_loss: 18025.6743 - val_mean_absolute_error: 18025.6743\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 17362.58956\n",
      "Epoch 1100/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15499.4309 - mean_absolute_error: 15499.4309 - val_loss: 18572.9619 - val_mean_absolute_error: 18572.9619\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 17362.58956\n",
      "Epoch 1101/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15578.8173 - mean_absolute_error: 15578.8173 - val_loss: 17939.2022 - val_mean_absolute_error: 17939.2022\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 17362.58956\n",
      "Epoch 1102/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15622.1651 - mean_absolute_error: 15622.1651 - val_loss: 18050.5239 - val_mean_absolute_error: 18050.5239\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 17362.58956\n",
      "Epoch 1103/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15617.7530 - mean_absolute_error: 15617.7530 - val_loss: 18094.0344 - val_mean_absolute_error: 18094.0344\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 17362.58956\n",
      "Epoch 1104/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15844.0189 - mean_absolute_error: 15844.0189 - val_loss: 17918.3589 - val_mean_absolute_error: 17918.3589\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 17362.58956\n",
      "Epoch 1105/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15814.0406 - mean_absolute_error: 15814.0406 - val_loss: 18437.6687 - val_mean_absolute_error: 18437.6687\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 17362.58956\n",
      "Epoch 1106/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 16110.9723 - mean_absolute_error: 16110.9723 - val_loss: 17922.4184 - val_mean_absolute_error: 17922.4184\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 17362.58956\n",
      "Epoch 1107/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 16037.4022 - mean_absolute_error: 16037.4022 - val_loss: 17847.4005 - val_mean_absolute_error: 17847.4005\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 17362.58956\n",
      "Epoch 1108/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 16044.7270 - mean_absolute_error: 16044.7270 - val_loss: 18134.0315 - val_mean_absolute_error: 18134.0315\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 17362.58956\n",
      "Epoch 1109/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15282.7384 - mean_absolute_error: 15282.7384 - val_loss: 18019.1890 - val_mean_absolute_error: 18019.1890\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 17362.58956\n",
      "Epoch 1110/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15425.3509 - mean_absolute_error: 15425.3509 - val_loss: 18193.9559 - val_mean_absolute_error: 18193.9559\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 17362.58956\n",
      "Epoch 1111/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15679.4138 - mean_absolute_error: 15679.4138 - val_loss: 18078.4934 - val_mean_absolute_error: 18078.4934\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 17362.58956\n",
      "Epoch 1112/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 15522.6856 - mean_absolute_error: 15522.6856 - val_loss: 18387.0965 - val_mean_absolute_error: 18387.0965\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 17362.58956\n",
      "Epoch 1113/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15602.4481 - mean_absolute_error: 15602.4481 - val_loss: 18832.3826 - val_mean_absolute_error: 18832.3826\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 17362.58956\n",
      "Epoch 1114/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15912.8466 - mean_absolute_error: 15912.8466 - val_loss: 19050.8726 - val_mean_absolute_error: 19050.8726\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 17362.58956\n",
      "Epoch 1115/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15640.4983 - mean_absolute_error: 15640.4983 - val_loss: 18607.1702 - val_mean_absolute_error: 18607.1702\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 17362.58956\n",
      "Epoch 1116/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15522.0765 - mean_absolute_error: 15522.0765 - val_loss: 18094.4507 - val_mean_absolute_error: 18094.4507\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 17362.58956\n",
      "Epoch 1117/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15586.4004 - mean_absolute_error: 15586.4004 - val_loss: 18317.0296 - val_mean_absolute_error: 18317.0296\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 17362.58956\n",
      "Epoch 1118/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15368.4104 - mean_absolute_error: 15368.4104 - val_loss: 18254.9219 - val_mean_absolute_error: 18254.9219\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 17362.58956\n",
      "Epoch 1119/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15587.3516 - mean_absolute_error: 15587.3516 - val_loss: 18243.4941 - val_mean_absolute_error: 18243.4941\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 17362.58956\n",
      "Epoch 1120/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15408.2723 - mean_absolute_error: 15408.2723 - val_loss: 18108.1595 - val_mean_absolute_error: 18108.1595\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 17362.58956\n",
      "Epoch 1121/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15429.2415 - mean_absolute_error: 15429.2415 - val_loss: 18513.6356 - val_mean_absolute_error: 18513.6356\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 17362.58956\n",
      "Epoch 1122/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 193us/step - loss: 15913.1163 - mean_absolute_error: 15913.1163 - val_loss: 17798.0586 - val_mean_absolute_error: 17798.0586\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 17362.58956\n",
      "Epoch 1123/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15320.6340 - mean_absolute_error: 15320.6340 - val_loss: 18368.0245 - val_mean_absolute_error: 18368.0245\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 17362.58956\n",
      "Epoch 1124/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15461.7128 - mean_absolute_error: 15461.7128 - val_loss: 18569.2488 - val_mean_absolute_error: 18569.2488\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 17362.58956\n",
      "Epoch 1125/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 15879.0557 - mean_absolute_error: 15879.0557 - val_loss: 18487.2387 - val_mean_absolute_error: 18487.2387\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 17362.58956\n",
      "Epoch 1126/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15905.0577 - mean_absolute_error: 15905.0577 - val_loss: 18171.3363 - val_mean_absolute_error: 18171.3363\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 17362.58956\n",
      "Epoch 1127/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15507.2220 - mean_absolute_error: 15507.2220 - val_loss: 20126.6773 - val_mean_absolute_error: 20126.6773\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 17362.58956\n",
      "Epoch 1128/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15628.3130 - mean_absolute_error: 15628.3130 - val_loss: 18205.4244 - val_mean_absolute_error: 18205.4244\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 17362.58956\n",
      "Epoch 1129/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15336.9418 - mean_absolute_error: 15336.9418 - val_loss: 18608.3561 - val_mean_absolute_error: 18608.3561\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 17362.58956\n",
      "Epoch 1130/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15786.5783 - mean_absolute_error: 15786.5783 - val_loss: 19238.6141 - val_mean_absolute_error: 19238.6141\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 17362.58956\n",
      "Epoch 1131/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15620.9850 - mean_absolute_error: 15620.9850 - val_loss: 19563.2587 - val_mean_absolute_error: 19563.2587\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 17362.58956\n",
      "Epoch 1132/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15426.9229 - mean_absolute_error: 15426.9229 - val_loss: 18279.8886 - val_mean_absolute_error: 18279.8886\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 17362.58956\n",
      "Epoch 1133/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 16188.3533 - mean_absolute_error: 16188.3533 - val_loss: 18693.7401 - val_mean_absolute_error: 18693.7401\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 17362.58956\n",
      "Epoch 1134/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 15361.1288 - mean_absolute_error: 15361.1288 - val_loss: 19076.8688 - val_mean_absolute_error: 19076.8688\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 17362.58956\n",
      "Epoch 1135/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15523.4798 - mean_absolute_error: 15523.4798 - val_loss: 18246.0051 - val_mean_absolute_error: 18246.0051\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 17362.58956\n",
      "Epoch 1136/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14986.8881 - mean_absolute_error: 14986.8881 - val_loss: 18075.7988 - val_mean_absolute_error: 18075.7988\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 17362.58956\n",
      "Epoch 1137/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15320.7472 - mean_absolute_error: 15320.7472 - val_loss: 18954.5080 - val_mean_absolute_error: 18954.5080\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 17362.58956\n",
      "Epoch 1138/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14988.9665 - mean_absolute_error: 14988.9665 - val_loss: 19840.9173 - val_mean_absolute_error: 19840.9173\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 17362.58956\n",
      "Epoch 1139/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15417.5298 - mean_absolute_error: 15417.5298 - val_loss: 18158.5598 - val_mean_absolute_error: 18158.5598\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 17362.58956\n",
      "Epoch 1140/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 15387.8689 - mean_absolute_error: 15387.8689 - val_loss: 18703.4718 - val_mean_absolute_error: 18703.4718\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 17362.58956\n",
      "Epoch 1141/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15522.7806 - mean_absolute_error: 15522.7806 - val_loss: 18096.7434 - val_mean_absolute_error: 18096.7434\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 17362.58956\n",
      "Epoch 1142/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 15215.3164 - mean_absolute_error: 15215.3164 - val_loss: 18796.2016 - val_mean_absolute_error: 18796.2016\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 17362.58956\n",
      "Epoch 1143/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 15828.3565 - mean_absolute_error: 15828.3565 - val_loss: 18177.7621 - val_mean_absolute_error: 18177.7621\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 17362.58956\n",
      "Epoch 1144/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15298.1051 - mean_absolute_error: 15298.1051 - val_loss: 18337.9464 - val_mean_absolute_error: 18337.9464\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 17362.58956\n",
      "Epoch 1145/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15233.6405 - mean_absolute_error: 15233.6405 - val_loss: 18470.2780 - val_mean_absolute_error: 18470.2780\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 17362.58956\n",
      "Epoch 1146/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15902.2083 - mean_absolute_error: 15902.2083 - val_loss: 17883.0891 - val_mean_absolute_error: 17883.0891\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 17362.58956\n",
      "Epoch 1147/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15192.0161 - mean_absolute_error: 15192.0161 - val_loss: 18407.6738 - val_mean_absolute_error: 18407.6738\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 17362.58956\n",
      "Epoch 1148/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15514.5162 - mean_absolute_error: 15514.5162 - val_loss: 18030.8784 - val_mean_absolute_error: 18030.8784\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 17362.58956\n",
      "Epoch 1149/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15255.7564 - mean_absolute_error: 15255.7564 - val_loss: 17959.8880 - val_mean_absolute_error: 17959.8880\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 17362.58956\n",
      "Epoch 1150/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15166.5588 - mean_absolute_error: 15166.5588 - val_loss: 18484.3094 - val_mean_absolute_error: 18484.3094\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 17362.58956\n",
      "Epoch 1151/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 15275.8159 - mean_absolute_error: 15275.8159 - val_loss: 17973.9502 - val_mean_absolute_error: 17973.9502\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 17362.58956\n",
      "Epoch 1152/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 15418.1459 - mean_absolute_error: 15418.1459 - val_loss: 18327.0230 - val_mean_absolute_error: 18327.0230\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 17362.58956\n",
      "Epoch 1153/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15290.1629 - mean_absolute_error: 15290.1629 - val_loss: 18164.7930 - val_mean_absolute_error: 18164.7930\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 17362.58956\n",
      "Epoch 1154/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 15561.5650 - mean_absolute_error: 15561.5650 - val_loss: 18080.0989 - val_mean_absolute_error: 18080.0989\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 17362.58956\n",
      "Epoch 1155/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 15558.6049 - mean_absolute_error: 15558.6049 - val_loss: 17735.6333 - val_mean_absolute_error: 17735.6333\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 17362.58956\n",
      "Epoch 1156/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 195us/step - loss: 15581.1477 - mean_absolute_error: 15581.1477 - val_loss: 17779.3447 - val_mean_absolute_error: 17779.3447\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 17362.58956\n",
      "Epoch 1157/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15656.3081 - mean_absolute_error: 15656.3081 - val_loss: 18643.2402 - val_mean_absolute_error: 18643.2402\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 17362.58956\n",
      "Epoch 1158/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15565.8167 - mean_absolute_error: 15565.8167 - val_loss: 17853.5871 - val_mean_absolute_error: 17853.5871\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 17362.58956\n",
      "Epoch 1159/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 15298.4101 - mean_absolute_error: 15298.4101 - val_loss: 19218.2951 - val_mean_absolute_error: 19218.2951\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 17362.58956\n",
      "Epoch 1160/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15222.5906 - mean_absolute_error: 15222.5906 - val_loss: 17678.8310 - val_mean_absolute_error: 17678.8310\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 17362.58956\n",
      "Epoch 1161/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15682.5900 - mean_absolute_error: 15682.5900 - val_loss: 17784.4386 - val_mean_absolute_error: 17784.4386\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 17362.58956\n",
      "Epoch 1162/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15350.4774 - mean_absolute_error: 15350.4774 - val_loss: 19020.0459 - val_mean_absolute_error: 19020.0459\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 17362.58956\n",
      "Epoch 1163/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15188.9591 - mean_absolute_error: 15188.9591 - val_loss: 18425.8019 - val_mean_absolute_error: 18425.8019\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 17362.58956\n",
      "Epoch 1164/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 15537.4026 - mean_absolute_error: 15537.4026 - val_loss: 18453.8854 - val_mean_absolute_error: 18453.8854\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 17362.58956\n",
      "Epoch 1165/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14919.1491 - mean_absolute_error: 14919.1491 - val_loss: 18083.0149 - val_mean_absolute_error: 18083.0149\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 17362.58956\n",
      "Epoch 1166/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14903.7408 - mean_absolute_error: 14903.7408 - val_loss: 18350.7343 - val_mean_absolute_error: 18350.7343\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 17362.58956\n",
      "Epoch 1167/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15831.3405 - mean_absolute_error: 15831.3405 - val_loss: 18619.2786 - val_mean_absolute_error: 18619.2786\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 17362.58956\n",
      "Epoch 1168/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15353.4519 - mean_absolute_error: 15353.4519 - val_loss: 18227.5064 - val_mean_absolute_error: 18227.5064\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 17362.58956\n",
      "Epoch 1169/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 15288.6724 - mean_absolute_error: 15288.6724 - val_loss: 18524.3100 - val_mean_absolute_error: 18524.3100\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 17362.58956\n",
      "Epoch 1170/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15355.6066 - mean_absolute_error: 15355.6066 - val_loss: 18695.3100 - val_mean_absolute_error: 18695.3100\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 17362.58956\n",
      "Epoch 1171/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15526.5242 - mean_absolute_error: 15526.5242 - val_loss: 18356.2289 - val_mean_absolute_error: 18356.2289\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 17362.58956\n",
      "Epoch 1172/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15589.6760 - mean_absolute_error: 15589.6760 - val_loss: 17871.6165 - val_mean_absolute_error: 17871.6165\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 17362.58956\n",
      "Epoch 1173/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15638.1580 - mean_absolute_error: 15638.1580 - val_loss: 19744.7704 - val_mean_absolute_error: 19744.7704\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 17362.58956\n",
      "Epoch 1174/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15583.0535 - mean_absolute_error: 15583.0535 - val_loss: 17858.6127 - val_mean_absolute_error: 17858.6127\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 17362.58956\n",
      "Epoch 1175/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15860.1401 - mean_absolute_error: 15860.1401 - val_loss: 19418.4010 - val_mean_absolute_error: 19418.4010\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 17362.58956\n",
      "Epoch 1176/1500\n",
      "934/934 [==============================] - 0s 242us/step - loss: 15365.9711 - mean_absolute_error: 15365.9711 - val_loss: 19088.4546 - val_mean_absolute_error: 19088.4546\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 17362.58956\n",
      "Epoch 1177/1500\n",
      "934/934 [==============================] - 0s 220us/step - loss: 15247.9352 - mean_absolute_error: 15247.9352 - val_loss: 18106.6278 - val_mean_absolute_error: 18106.6278\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 17362.58956\n",
      "Epoch 1178/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 15247.2372 - mean_absolute_error: 15247.2372 - val_loss: 17901.2760 - val_mean_absolute_error: 17901.2760\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 17362.58956\n",
      "Epoch 1179/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15194.7575 - mean_absolute_error: 15194.7575 - val_loss: 17890.7720 - val_mean_absolute_error: 17890.7720\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 17362.58956\n",
      "Epoch 1180/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 15499.5807 - mean_absolute_error: 15499.5807 - val_loss: 18324.3103 - val_mean_absolute_error: 18324.3103\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 17362.58956\n",
      "Epoch 1181/1500\n",
      "934/934 [==============================] - 0s 215us/step - loss: 14979.7504 - mean_absolute_error: 14979.7504 - val_loss: 18924.7886 - val_mean_absolute_error: 18924.7886\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 17362.58956\n",
      "Epoch 1182/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15377.7476 - mean_absolute_error: 15377.7476 - val_loss: 17788.7690 - val_mean_absolute_error: 17788.7690\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 17362.58956\n",
      "Epoch 1183/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 15539.2982 - mean_absolute_error: 15539.2982 - val_loss: 19055.5451 - val_mean_absolute_error: 19055.5451\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 17362.58956\n",
      "Epoch 1184/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15554.0578 - mean_absolute_error: 15554.0578 - val_loss: 18214.1241 - val_mean_absolute_error: 18214.1241\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 17362.58956\n",
      "Epoch 1185/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14992.4135 - mean_absolute_error: 14992.4135 - val_loss: 17584.8060 - val_mean_absolute_error: 17584.8060\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 17362.58956\n",
      "Epoch 1186/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15357.8642 - mean_absolute_error: 15357.8642 - val_loss: 18657.8576 - val_mean_absolute_error: 18657.8576\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 17362.58956\n",
      "Epoch 1187/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15242.0205 - mean_absolute_error: 15242.0205 - val_loss: 18780.2662 - val_mean_absolute_error: 18780.2662\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 17362.58956\n",
      "Epoch 1188/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15055.1972 - mean_absolute_error: 15055.1972 - val_loss: 17974.8586 - val_mean_absolute_error: 17974.8586\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 17362.58956\n",
      "Epoch 1189/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 15194.6977 - mean_absolute_error: 15194.6977 - val_loss: 18017.2385 - val_mean_absolute_error: 18017.2385\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 17362.58956\n",
      "Epoch 1190/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 192us/step - loss: 14859.1353 - mean_absolute_error: 14859.1353 - val_loss: 18262.8764 - val_mean_absolute_error: 18262.8764\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 17362.58956\n",
      "Epoch 1191/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 15168.2881 - mean_absolute_error: 15168.2881 - val_loss: 18009.0603 - val_mean_absolute_error: 18009.0603\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 17362.58956\n",
      "Epoch 1192/1500\n",
      "934/934 [==============================] - 0s 186us/step - loss: 14955.6453 - mean_absolute_error: 14955.6453 - val_loss: 17648.8833 - val_mean_absolute_error: 17648.8833\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 17362.58956\n",
      "Epoch 1193/1500\n",
      "934/934 [==============================] - 0s 179us/step - loss: 14972.4066 - mean_absolute_error: 14972.4066 - val_loss: 18130.7594 - val_mean_absolute_error: 18130.7594\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 17362.58956\n",
      "Epoch 1194/1500\n",
      "934/934 [==============================] - 0s 179us/step - loss: 15541.0679 - mean_absolute_error: 15541.0679 - val_loss: 18016.7221 - val_mean_absolute_error: 18016.7221\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 17362.58956\n",
      "Epoch 1195/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 14935.2134 - mean_absolute_error: 14935.2134 - val_loss: 18776.7260 - val_mean_absolute_error: 18776.7260\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 17362.58956\n",
      "Epoch 1196/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 15042.8875 - mean_absolute_error: 15042.8875 - val_loss: 17924.1682 - val_mean_absolute_error: 17924.1682\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 17362.58956\n",
      "Epoch 1197/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15238.8619 - mean_absolute_error: 15238.8619 - val_loss: 18626.8235 - val_mean_absolute_error: 18626.8235\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 17362.58956\n",
      "Epoch 1198/1500\n",
      "934/934 [==============================] - 0s 221us/step - loss: 15555.8343 - mean_absolute_error: 15555.8343 - val_loss: 21966.5249 - val_mean_absolute_error: 21966.5249\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 17362.58956\n",
      "Epoch 1199/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 15776.3824 - mean_absolute_error: 15776.3824 - val_loss: 18053.9998 - val_mean_absolute_error: 18053.9998\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 17362.58956\n",
      "Epoch 1200/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15558.8477 - mean_absolute_error: 15558.8477 - val_loss: 18230.3094 - val_mean_absolute_error: 18230.3094\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 17362.58956\n",
      "Epoch 1201/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 15122.0153 - mean_absolute_error: 15122.0153 - val_loss: 18539.8494 - val_mean_absolute_error: 18539.8494\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 17362.58956\n",
      "Epoch 1202/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15516.7354 - mean_absolute_error: 15516.7354 - val_loss: 18495.0678 - val_mean_absolute_error: 18495.0678\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 17362.58956\n",
      "Epoch 1203/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15022.1880 - mean_absolute_error: 15022.1880 - val_loss: 18017.6148 - val_mean_absolute_error: 18017.6148\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 17362.58956\n",
      "Epoch 1204/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15823.0989 - mean_absolute_error: 15823.0989 - val_loss: 18528.3494 - val_mean_absolute_error: 18528.3494\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 17362.58956\n",
      "Epoch 1205/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15025.7721 - mean_absolute_error: 15025.7721 - val_loss: 18220.8511 - val_mean_absolute_error: 18220.8511\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 17362.58956\n",
      "Epoch 1206/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15288.1679 - mean_absolute_error: 15288.1679 - val_loss: 18591.7229 - val_mean_absolute_error: 18591.7229\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 17362.58956\n",
      "Epoch 1207/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15344.3212 - mean_absolute_error: 15344.3212 - val_loss: 18616.8681 - val_mean_absolute_error: 18616.8681\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 17362.58956\n",
      "Epoch 1208/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15209.9714 - mean_absolute_error: 15209.9714 - val_loss: 18449.1339 - val_mean_absolute_error: 18449.1339\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 17362.58956\n",
      "Epoch 1209/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15534.4419 - mean_absolute_error: 15534.4419 - val_loss: 18484.3332 - val_mean_absolute_error: 18484.3332\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 17362.58956\n",
      "Epoch 1210/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15267.7615 - mean_absolute_error: 15267.7615 - val_loss: 18219.2993 - val_mean_absolute_error: 18219.2993\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 17362.58956\n",
      "Epoch 1211/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 15192.1536 - mean_absolute_error: 15192.1536 - val_loss: 19060.4149 - val_mean_absolute_error: 19060.4149\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 17362.58956\n",
      "Epoch 1212/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15188.0775 - mean_absolute_error: 15188.0775 - val_loss: 17968.9006 - val_mean_absolute_error: 17968.9006\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 17362.58956\n",
      "Epoch 1213/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 15213.5357 - mean_absolute_error: 15213.5357 - val_loss: 18485.4835 - val_mean_absolute_error: 18485.4835\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 17362.58956\n",
      "Epoch 1214/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15100.9052 - mean_absolute_error: 15100.9052 - val_loss: 20087.0994 - val_mean_absolute_error: 20087.0994\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 17362.58956\n",
      "Epoch 1215/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15139.2191 - mean_absolute_error: 15139.2191 - val_loss: 18267.3290 - val_mean_absolute_error: 18267.3290\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 17362.58956\n",
      "Epoch 1216/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 15388.7484 - mean_absolute_error: 15388.7484 - val_loss: 18927.5220 - val_mean_absolute_error: 18927.5220\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 17362.58956\n",
      "Epoch 1217/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14844.4936 - mean_absolute_error: 14844.4936 - val_loss: 18108.1922 - val_mean_absolute_error: 18108.1922\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 17362.58956\n",
      "Epoch 1218/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 14854.5025 - mean_absolute_error: 14854.5025 - val_loss: 18209.8923 - val_mean_absolute_error: 18209.8923\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 17362.58956\n",
      "Epoch 1219/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 15093.1992 - mean_absolute_error: 15093.1992 - val_loss: 18574.5634 - val_mean_absolute_error: 18574.5634\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 17362.58956\n",
      "Epoch 1220/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15348.9568 - mean_absolute_error: 15348.9568 - val_loss: 18176.1563 - val_mean_absolute_error: 18176.1563\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 17362.58956\n",
      "Epoch 1221/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15224.8134 - mean_absolute_error: 15224.8134 - val_loss: 18329.9342 - val_mean_absolute_error: 18329.9342\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 17362.58956\n",
      "Epoch 1222/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15081.0755 - mean_absolute_error: 15081.0755 - val_loss: 18880.2486 - val_mean_absolute_error: 18880.2486\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 17362.58956\n",
      "Epoch 1223/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15251.9107 - mean_absolute_error: 15251.9107 - val_loss: 18623.3509 - val_mean_absolute_error: 18623.3509\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 17362.58956\n",
      "Epoch 1224/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 199us/step - loss: 15053.7378 - mean_absolute_error: 15053.7378 - val_loss: 18127.3093 - val_mean_absolute_error: 18127.3093\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 17362.58956\n",
      "Epoch 1225/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15452.2835 - mean_absolute_error: 15452.2835 - val_loss: 18102.6785 - val_mean_absolute_error: 18102.6785\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 17362.58956\n",
      "Epoch 1226/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15473.9975 - mean_absolute_error: 15473.9975 - val_loss: 17885.9552 - val_mean_absolute_error: 17885.9552\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 17362.58956\n",
      "Epoch 1227/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 14972.3548 - mean_absolute_error: 14972.3548 - val_loss: 18176.4014 - val_mean_absolute_error: 18176.4014\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 17362.58956\n",
      "Epoch 1228/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15154.5375 - mean_absolute_error: 15154.5375 - val_loss: 18206.3858 - val_mean_absolute_error: 18206.3858\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 17362.58956\n",
      "Epoch 1229/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14906.1424 - mean_absolute_error: 14906.1424 - val_loss: 18149.1665 - val_mean_absolute_error: 18149.1665\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 17362.58956\n",
      "Epoch 1230/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15032.2647 - mean_absolute_error: 15032.2647 - val_loss: 18500.4808 - val_mean_absolute_error: 18500.4808\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 17362.58956\n",
      "Epoch 1231/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14858.3835 - mean_absolute_error: 14858.3835 - val_loss: 19286.1545 - val_mean_absolute_error: 19286.1545\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 17362.58956\n",
      "Epoch 1232/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15016.1186 - mean_absolute_error: 15016.1186 - val_loss: 18106.2813 - val_mean_absolute_error: 18106.2813\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 17362.58956\n",
      "Epoch 1233/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14964.0914 - mean_absolute_error: 14964.0914 - val_loss: 18635.8302 - val_mean_absolute_error: 18635.8302\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 17362.58956\n",
      "Epoch 1234/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15479.4104 - mean_absolute_error: 15479.4104 - val_loss: 18886.9557 - val_mean_absolute_error: 18886.9557\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 17362.58956\n",
      "Epoch 1235/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14905.7295 - mean_absolute_error: 14905.7295 - val_loss: 18679.4581 - val_mean_absolute_error: 18679.4581\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 17362.58956\n",
      "Epoch 1236/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14648.0957 - mean_absolute_error: 14648.0957 - val_loss: 19436.7036 - val_mean_absolute_error: 19436.7036\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 17362.58956\n",
      "Epoch 1237/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15018.0337 - mean_absolute_error: 15018.0337 - val_loss: 18603.4940 - val_mean_absolute_error: 18603.4940\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 17362.58956\n",
      "Epoch 1238/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15043.2269 - mean_absolute_error: 15043.2269 - val_loss: 19482.9011 - val_mean_absolute_error: 19482.9011\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 17362.58956\n",
      "Epoch 1239/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15246.1337 - mean_absolute_error: 15246.1337 - val_loss: 18009.6096 - val_mean_absolute_error: 18009.6096\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 17362.58956\n",
      "Epoch 1240/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 15208.1852 - mean_absolute_error: 15208.1852 - val_loss: 17883.9641 - val_mean_absolute_error: 17883.9641\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 17362.58956\n",
      "Epoch 1241/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 15061.1469 - mean_absolute_error: 15061.1469 - val_loss: 18768.6647 - val_mean_absolute_error: 18768.6647\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 17362.58956\n",
      "Epoch 1242/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15238.6714 - mean_absolute_error: 15238.6714 - val_loss: 18141.3913 - val_mean_absolute_error: 18141.3913\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 17362.58956\n",
      "Epoch 1243/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14984.1441 - mean_absolute_error: 14984.1441 - val_loss: 18178.8826 - val_mean_absolute_error: 18178.8826\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 17362.58956\n",
      "Epoch 1244/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 15102.4184 - mean_absolute_error: 15102.4184 - val_loss: 18257.7836 - val_mean_absolute_error: 18257.7836\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 17362.58956\n",
      "Epoch 1245/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 15148.9669 - mean_absolute_error: 15148.9669 - val_loss: 18229.4365 - val_mean_absolute_error: 18229.4365\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 17362.58956\n",
      "Epoch 1246/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 15376.1041 - mean_absolute_error: 15376.1041 - val_loss: 18234.5484 - val_mean_absolute_error: 18234.5484\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 17362.58956\n",
      "Epoch 1247/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 15780.9577 - mean_absolute_error: 15780.9577 - val_loss: 18281.2775 - val_mean_absolute_error: 18281.2775\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 17362.58956\n",
      "Epoch 1248/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 15031.8339 - mean_absolute_error: 15031.8339 - val_loss: 18001.9636 - val_mean_absolute_error: 18001.9636\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 17362.58956\n",
      "Epoch 1249/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14738.5246 - mean_absolute_error: 14738.5246 - val_loss: 18028.9330 - val_mean_absolute_error: 18028.9330\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 17362.58956\n",
      "Epoch 1250/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14815.0326 - mean_absolute_error: 14815.0326 - val_loss: 17787.5086 - val_mean_absolute_error: 17787.5086\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 17362.58956\n",
      "Epoch 1251/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14885.7806 - mean_absolute_error: 14885.7806 - val_loss: 17959.9365 - val_mean_absolute_error: 17959.9365\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 17362.58956\n",
      "Epoch 1252/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 15444.1753 - mean_absolute_error: 15444.1753 - val_loss: 18290.2272 - val_mean_absolute_error: 18290.2272\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 17362.58956\n",
      "Epoch 1253/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 15009.3339 - mean_absolute_error: 15009.3339 - val_loss: 18312.8352 - val_mean_absolute_error: 18312.8352\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 17362.58956\n",
      "Epoch 1254/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14447.7352 - mean_absolute_error: 14447.7352 - val_loss: 18251.0780 - val_mean_absolute_error: 18251.0780\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 17362.58956\n",
      "Epoch 1255/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14627.7328 - mean_absolute_error: 14627.7328 - val_loss: 18094.9556 - val_mean_absolute_error: 18094.9556\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 17362.58956\n",
      "Epoch 1256/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14618.6700 - mean_absolute_error: 14618.6700 - val_loss: 18601.0964 - val_mean_absolute_error: 18601.0964\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 17362.58956\n",
      "Epoch 1257/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14628.3016 - mean_absolute_error: 14628.3016 - val_loss: 18450.0031 - val_mean_absolute_error: 18450.0031\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 17362.58956\n",
      "Epoch 1258/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 202us/step - loss: 14950.7041 - mean_absolute_error: 14950.7041 - val_loss: 18588.0181 - val_mean_absolute_error: 18588.0181\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 17362.58956\n",
      "Epoch 1259/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 15122.5467 - mean_absolute_error: 15122.5467 - val_loss: 18219.9457 - val_mean_absolute_error: 18219.9457\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 17362.58956\n",
      "Epoch 1260/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 15055.1496 - mean_absolute_error: 15055.1496 - val_loss: 19408.0244 - val_mean_absolute_error: 19408.0244\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 17362.58956\n",
      "Epoch 1261/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14799.8583 - mean_absolute_error: 14799.8583 - val_loss: 18921.3634 - val_mean_absolute_error: 18921.3634\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 17362.58956\n",
      "Epoch 1262/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 15294.7312 - mean_absolute_error: 15294.7312 - val_loss: 19636.7118 - val_mean_absolute_error: 19636.7118\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 17362.58956\n",
      "Epoch 1263/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 14761.2618 - mean_absolute_error: 14761.2618 - val_loss: 18423.4742 - val_mean_absolute_error: 18423.4742\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 17362.58956\n",
      "Epoch 1264/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14769.3925 - mean_absolute_error: 14769.3925 - val_loss: 18163.2257 - val_mean_absolute_error: 18163.2257\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 17362.58956\n",
      "Epoch 1265/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14565.4941 - mean_absolute_error: 14565.4941 - val_loss: 19187.7883 - val_mean_absolute_error: 19187.7883\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 17362.58956\n",
      "Epoch 1266/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14815.5324 - mean_absolute_error: 14815.5324 - val_loss: 18394.1768 - val_mean_absolute_error: 18394.1768\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 17362.58956\n",
      "Epoch 1267/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15166.8347 - mean_absolute_error: 15166.8347 - val_loss: 17990.1314 - val_mean_absolute_error: 17990.1314\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 17362.58956\n",
      "Epoch 1268/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14955.9280 - mean_absolute_error: 14955.9280 - val_loss: 19487.6980 - val_mean_absolute_error: 19487.6980\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 17362.58956\n",
      "Epoch 1269/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14651.2591 - mean_absolute_error: 14651.2591 - val_loss: 19206.0817 - val_mean_absolute_error: 19206.0817\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 17362.58956\n",
      "Epoch 1270/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14678.7283 - mean_absolute_error: 14678.7283 - val_loss: 18657.6746 - val_mean_absolute_error: 18657.6746\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 17362.58956\n",
      "Epoch 1271/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14827.8595 - mean_absolute_error: 14827.8595 - val_loss: 18608.7156 - val_mean_absolute_error: 18608.7156\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 17362.58956\n",
      "Epoch 1272/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14772.2554 - mean_absolute_error: 14772.2554 - val_loss: 17958.2788 - val_mean_absolute_error: 17958.2788\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 17362.58956\n",
      "Epoch 1273/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14846.5128 - mean_absolute_error: 14846.5128 - val_loss: 18419.4915 - val_mean_absolute_error: 18419.4915\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 17362.58956\n",
      "Epoch 1274/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14783.1238 - mean_absolute_error: 14783.1238 - val_loss: 18993.7742 - val_mean_absolute_error: 18993.7742\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 17362.58956\n",
      "Epoch 1275/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14784.2437 - mean_absolute_error: 14784.2437 - val_loss: 18071.0740 - val_mean_absolute_error: 18071.0740\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 17362.58956\n",
      "Epoch 1276/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 15184.4716 - mean_absolute_error: 15184.4716 - val_loss: 18344.0874 - val_mean_absolute_error: 18344.0874\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 17362.58956\n",
      "Epoch 1277/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 14991.6655 - mean_absolute_error: 14991.6655 - val_loss: 17731.1487 - val_mean_absolute_error: 17731.1487\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 17362.58956\n",
      "Epoch 1278/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14736.3805 - mean_absolute_error: 14736.3805 - val_loss: 17947.0199 - val_mean_absolute_error: 17947.0199\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 17362.58956\n",
      "Epoch 1279/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 15171.4001 - mean_absolute_error: 15171.4001 - val_loss: 18400.5173 - val_mean_absolute_error: 18400.5173\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 17362.58956\n",
      "Epoch 1280/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14723.3708 - mean_absolute_error: 14723.3708 - val_loss: 18079.6705 - val_mean_absolute_error: 18079.6705\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 17362.58956\n",
      "Epoch 1281/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14761.3354 - mean_absolute_error: 14761.3354 - val_loss: 18474.0325 - val_mean_absolute_error: 18474.0325\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 17362.58956\n",
      "Epoch 1282/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14720.1987 - mean_absolute_error: 14720.1987 - val_loss: 20318.3593 - val_mean_absolute_error: 20318.3593\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 17362.58956\n",
      "Epoch 1283/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15081.0841 - mean_absolute_error: 15081.0841 - val_loss: 18179.3632 - val_mean_absolute_error: 18179.3632\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 17362.58956\n",
      "Epoch 1284/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 15475.0106 - mean_absolute_error: 15475.0106 - val_loss: 19058.6173 - val_mean_absolute_error: 19058.6173\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 17362.58956\n",
      "Epoch 1285/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14744.9317 - mean_absolute_error: 14744.9317 - val_loss: 18091.3900 - val_mean_absolute_error: 18091.3900\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 17362.58956\n",
      "Epoch 1286/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14707.2512 - mean_absolute_error: 14707.2512 - val_loss: 18655.0108 - val_mean_absolute_error: 18655.0108\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 17362.58956\n",
      "Epoch 1287/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15177.2874 - mean_absolute_error: 15177.2874 - val_loss: 17911.2109 - val_mean_absolute_error: 17911.2109\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 17362.58956\n",
      "Epoch 1288/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15006.0753 - mean_absolute_error: 15006.0753 - val_loss: 18938.6418 - val_mean_absolute_error: 18938.6418\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 17362.58956\n",
      "Epoch 1289/1500\n",
      "934/934 [==============================] - 0s 188us/step - loss: 14880.9822 - mean_absolute_error: 14880.9822 - val_loss: 17906.2185 - val_mean_absolute_error: 17906.2185\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 17362.58956\n",
      "Epoch 1290/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15134.1528 - mean_absolute_error: 15134.1528 - val_loss: 18321.4111 - val_mean_absolute_error: 18321.4111\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 17362.58956\n",
      "Epoch 1291/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14756.7342 - mean_absolute_error: 14756.7342 - val_loss: 17869.5030 - val_mean_absolute_error: 17869.5030\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 17362.58956\n",
      "Epoch 1292/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 193us/step - loss: 15795.3716 - mean_absolute_error: 15795.3716 - val_loss: 18176.1279 - val_mean_absolute_error: 18176.1279\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 17362.58956\n",
      "Epoch 1293/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 15302.3104 - mean_absolute_error: 15302.3104 - val_loss: 17886.1898 - val_mean_absolute_error: 17886.1898\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 17362.58956\n",
      "Epoch 1294/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14823.5655 - mean_absolute_error: 14823.5655 - val_loss: 18297.2094 - val_mean_absolute_error: 18297.2094\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 17362.58956\n",
      "Epoch 1295/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14763.3631 - mean_absolute_error: 14763.3631 - val_loss: 17768.9780 - val_mean_absolute_error: 17768.9780\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 17362.58956\n",
      "Epoch 1296/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14976.4304 - mean_absolute_error: 14976.4304 - val_loss: 18174.4857 - val_mean_absolute_error: 18174.4857\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 17362.58956\n",
      "Epoch 1297/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14953.9658 - mean_absolute_error: 14953.9658 - val_loss: 18477.6033 - val_mean_absolute_error: 18477.6033\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 17362.58956\n",
      "Epoch 1298/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14851.3828 - mean_absolute_error: 14851.3828 - val_loss: 17767.8703 - val_mean_absolute_error: 17767.8703\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 17362.58956\n",
      "Epoch 1299/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14442.4202 - mean_absolute_error: 14442.4202 - val_loss: 19799.9775 - val_mean_absolute_error: 19799.9775\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 17362.58956\n",
      "Epoch 1300/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 15138.0767 - mean_absolute_error: 15138.0767 - val_loss: 17982.1671 - val_mean_absolute_error: 17982.1671\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 17362.58956\n",
      "Epoch 1301/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14861.8741 - mean_absolute_error: 14861.8741 - val_loss: 18386.3768 - val_mean_absolute_error: 18386.3768\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 17362.58956\n",
      "Epoch 1302/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15152.1501 - mean_absolute_error: 15152.1501 - val_loss: 18260.2772 - val_mean_absolute_error: 18260.2772\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 17362.58956\n",
      "Epoch 1303/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14594.1087 - mean_absolute_error: 14594.1087 - val_loss: 18057.6270 - val_mean_absolute_error: 18057.6270\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 17362.58956\n",
      "Epoch 1304/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14534.7471 - mean_absolute_error: 14534.7471 - val_loss: 18510.3657 - val_mean_absolute_error: 18510.3657\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 17362.58956\n",
      "Epoch 1305/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14616.7774 - mean_absolute_error: 14616.7774 - val_loss: 18042.5362 - val_mean_absolute_error: 18042.5362\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 17362.58956\n",
      "Epoch 1306/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 14562.4552 - mean_absolute_error: 14562.4552 - val_loss: 18244.3021 - val_mean_absolute_error: 18244.3021\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 17362.58956\n",
      "Epoch 1307/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14679.6462 - mean_absolute_error: 14679.6462 - val_loss: 18246.5536 - val_mean_absolute_error: 18246.5536\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 17362.58956\n",
      "Epoch 1308/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15021.9066 - mean_absolute_error: 15021.9066 - val_loss: 17896.0654 - val_mean_absolute_error: 17896.0654\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 17362.58956\n",
      "Epoch 1309/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15225.2761 - mean_absolute_error: 15225.2761 - val_loss: 17815.5519 - val_mean_absolute_error: 17815.5519\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 17362.58956\n",
      "Epoch 1310/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14865.2481 - mean_absolute_error: 14865.2481 - val_loss: 18659.5854 - val_mean_absolute_error: 18659.5854\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 17362.58956\n",
      "Epoch 1311/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14968.4580 - mean_absolute_error: 14968.4580 - val_loss: 18691.7001 - val_mean_absolute_error: 18691.7001\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 17362.58956\n",
      "Epoch 1312/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14727.7815 - mean_absolute_error: 14727.7815 - val_loss: 17843.4966 - val_mean_absolute_error: 17843.4966\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 17362.58956\n",
      "Epoch 1313/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14929.1232 - mean_absolute_error: 14929.1232 - val_loss: 18816.4288 - val_mean_absolute_error: 18816.4288\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 17362.58956\n",
      "Epoch 1314/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14848.6087 - mean_absolute_error: 14848.6087 - val_loss: 17971.1787 - val_mean_absolute_error: 17971.1787\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 17362.58956\n",
      "Epoch 1315/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14495.8211 - mean_absolute_error: 14495.8211 - val_loss: 17679.2091 - val_mean_absolute_error: 17679.2091\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 17362.58956\n",
      "Epoch 1316/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14878.6746 - mean_absolute_error: 14878.6746 - val_loss: 17928.4411 - val_mean_absolute_error: 17928.4411\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 17362.58956\n",
      "Epoch 1317/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14579.1626 - mean_absolute_error: 14579.1626 - val_loss: 18411.8239 - val_mean_absolute_error: 18411.8239\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 17362.58956\n",
      "Epoch 1318/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15042.0849 - mean_absolute_error: 15042.0849 - val_loss: 18012.9343 - val_mean_absolute_error: 18012.9343\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 17362.58956\n",
      "Epoch 1319/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14288.9666 - mean_absolute_error: 14288.9666 - val_loss: 18054.7265 - val_mean_absolute_error: 18054.7265\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 17362.58956\n",
      "Epoch 1320/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 15286.2405 - mean_absolute_error: 15286.2405 - val_loss: 23300.3610 - val_mean_absolute_error: 23300.3610\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 17362.58956\n",
      "Epoch 1321/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14768.3140 - mean_absolute_error: 14768.3140 - val_loss: 17855.6059 - val_mean_absolute_error: 17855.6059\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 17362.58956\n",
      "Epoch 1322/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14769.5071 - mean_absolute_error: 14769.5071 - val_loss: 17977.1688 - val_mean_absolute_error: 17977.1688\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 17362.58956\n",
      "Epoch 1323/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14911.9352 - mean_absolute_error: 14911.9352 - val_loss: 18534.9228 - val_mean_absolute_error: 18534.9228\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 17362.58956\n",
      "Epoch 1324/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 14707.8900 - mean_absolute_error: 14707.8900 - val_loss: 18381.8695 - val_mean_absolute_error: 18381.8695\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 17362.58956\n",
      "Epoch 1325/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 15216.8038 - mean_absolute_error: 15216.8038 - val_loss: 18021.4981 - val_mean_absolute_error: 18021.4981\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 17362.58956\n",
      "Epoch 1326/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 198us/step - loss: 14992.1031 - mean_absolute_error: 14992.1031 - val_loss: 19541.5640 - val_mean_absolute_error: 19541.5640\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 17362.58956\n",
      "Epoch 1327/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14589.7717 - mean_absolute_error: 14589.7717 - val_loss: 18856.2871 - val_mean_absolute_error: 18856.2871\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 17362.58956\n",
      "Epoch 1328/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 14373.6111 - mean_absolute_error: 14373.6111 - val_loss: 18189.8625 - val_mean_absolute_error: 18189.8625\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 17362.58956\n",
      "Epoch 1329/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14262.2537 - mean_absolute_error: 14262.2537 - val_loss: 18195.2158 - val_mean_absolute_error: 18195.2158\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 17362.58956\n",
      "Epoch 1330/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14985.9539 - mean_absolute_error: 14985.9539 - val_loss: 18305.0173 - val_mean_absolute_error: 18305.0173\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 17362.58956\n",
      "Epoch 1331/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14813.8068 - mean_absolute_error: 14813.8068 - val_loss: 18104.7450 - val_mean_absolute_error: 18104.7450\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 17362.58956\n",
      "Epoch 1332/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14308.2591 - mean_absolute_error: 14308.2591 - val_loss: 18734.4571 - val_mean_absolute_error: 18734.4571\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 17362.58956\n",
      "Epoch 1333/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 14942.1132 - mean_absolute_error: 14942.1132 - val_loss: 18270.2240 - val_mean_absolute_error: 18270.2240\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 17362.58956\n",
      "Epoch 1334/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14689.4359 - mean_absolute_error: 14689.4359 - val_loss: 17983.4359 - val_mean_absolute_error: 17983.4359\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 17362.58956\n",
      "Epoch 1335/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14971.3199 - mean_absolute_error: 14971.3199 - val_loss: 18328.8272 - val_mean_absolute_error: 18328.8272\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 17362.58956\n",
      "Epoch 1336/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15123.0093 - mean_absolute_error: 15123.0093 - val_loss: 18922.3360 - val_mean_absolute_error: 18922.3360\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 17362.58956\n",
      "Epoch 1337/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14344.0191 - mean_absolute_error: 14344.0191 - val_loss: 18537.9628 - val_mean_absolute_error: 18537.9628\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 17362.58956\n",
      "Epoch 1338/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14515.1138 - mean_absolute_error: 14515.1138 - val_loss: 18126.4205 - val_mean_absolute_error: 18126.4205\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 17362.58956\n",
      "Epoch 1339/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14377.4002 - mean_absolute_error: 14377.4002 - val_loss: 18152.1185 - val_mean_absolute_error: 18152.1185\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 17362.58956\n",
      "Epoch 1340/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14628.1369 - mean_absolute_error: 14628.1369 - val_loss: 17980.8310 - val_mean_absolute_error: 17980.8310\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 17362.58956\n",
      "Epoch 1341/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14530.0293 - mean_absolute_error: 14530.0293 - val_loss: 18154.2459 - val_mean_absolute_error: 18154.2459\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 17362.58956\n",
      "Epoch 1342/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14465.6596 - mean_absolute_error: 14465.6596 - val_loss: 18361.6669 - val_mean_absolute_error: 18361.6669\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 17362.58956\n",
      "Epoch 1343/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14499.3055 - mean_absolute_error: 14499.3055 - val_loss: 18367.4180 - val_mean_absolute_error: 18367.4180\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 17362.58956\n",
      "Epoch 1344/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15007.8428 - mean_absolute_error: 15007.8428 - val_loss: 17949.8703 - val_mean_absolute_error: 17949.8703\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 17362.58956\n",
      "Epoch 1345/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14556.2403 - mean_absolute_error: 14556.2403 - val_loss: 18296.1147 - val_mean_absolute_error: 18296.1147\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 17362.58956\n",
      "Epoch 1346/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14563.6936 - mean_absolute_error: 14563.6936 - val_loss: 18474.5547 - val_mean_absolute_error: 18474.5547\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 17362.58956\n",
      "Epoch 1347/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14912.8130 - mean_absolute_error: 14912.8130 - val_loss: 18409.4013 - val_mean_absolute_error: 18409.4013\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 17362.58956\n",
      "Epoch 1348/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14416.7007 - mean_absolute_error: 14416.7007 - val_loss: 18056.0065 - val_mean_absolute_error: 18056.0065\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 17362.58956\n",
      "Epoch 1349/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 15067.8818 - mean_absolute_error: 15067.8818 - val_loss: 17995.3970 - val_mean_absolute_error: 17995.3970\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 17362.58956\n",
      "Epoch 1350/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14644.3542 - mean_absolute_error: 14644.3542 - val_loss: 18124.0255 - val_mean_absolute_error: 18124.0255\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 17362.58956\n",
      "Epoch 1351/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14429.5348 - mean_absolute_error: 14429.5348 - val_loss: 18079.3233 - val_mean_absolute_error: 18079.3233\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 17362.58956\n",
      "Epoch 1352/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14474.3116 - mean_absolute_error: 14474.3116 - val_loss: 17974.6963 - val_mean_absolute_error: 17974.6963\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 17362.58956\n",
      "Epoch 1353/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 14354.5474 - mean_absolute_error: 14354.5474 - val_loss: 20186.2807 - val_mean_absolute_error: 20186.2807\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 17362.58956\n",
      "Epoch 1354/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 14410.2399 - mean_absolute_error: 14410.2399 - val_loss: 18157.0660 - val_mean_absolute_error: 18157.0660\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 17362.58956\n",
      "Epoch 1355/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14695.4012 - mean_absolute_error: 14695.4012 - val_loss: 18485.7671 - val_mean_absolute_error: 18485.7671\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 17362.58956\n",
      "Epoch 1356/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14629.3517 - mean_absolute_error: 14629.3517 - val_loss: 18295.2375 - val_mean_absolute_error: 18295.2375\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 17362.58956\n",
      "Epoch 1357/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 14633.9501 - mean_absolute_error: 14633.9501 - val_loss: 18312.1805 - val_mean_absolute_error: 18312.1805\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 17362.58956\n",
      "Epoch 1358/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14920.9652 - mean_absolute_error: 14920.9652 - val_loss: 18278.6001 - val_mean_absolute_error: 18278.6001\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 17362.58956\n",
      "Epoch 1359/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14308.6915 - mean_absolute_error: 14308.6915 - val_loss: 20356.6053 - val_mean_absolute_error: 20356.6053\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 17362.58956\n",
      "Epoch 1360/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 198us/step - loss: 14691.1933 - mean_absolute_error: 14691.1933 - val_loss: 17789.9064 - val_mean_absolute_error: 17789.9064\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 17362.58956\n",
      "Epoch 1361/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14843.6401 - mean_absolute_error: 14843.6401 - val_loss: 19298.5804 - val_mean_absolute_error: 19298.5804\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 17362.58956\n",
      "Epoch 1362/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14723.5363 - mean_absolute_error: 14723.5363 - val_loss: 19284.5918 - val_mean_absolute_error: 19284.5918\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 17362.58956\n",
      "Epoch 1363/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 15260.2582 - mean_absolute_error: 15260.2582 - val_loss: 18042.1929 - val_mean_absolute_error: 18042.1929\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 17362.58956\n",
      "Epoch 1364/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14997.5250 - mean_absolute_error: 14997.5250 - val_loss: 21566.5311 - val_mean_absolute_error: 21566.5311\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 17362.58956\n",
      "Epoch 1365/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14741.6478 - mean_absolute_error: 14741.6478 - val_loss: 18007.0461 - val_mean_absolute_error: 18007.0461\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 17362.58956\n",
      "Epoch 1366/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14715.8004 - mean_absolute_error: 14715.8004 - val_loss: 19859.3006 - val_mean_absolute_error: 19859.3006\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 17362.58956\n",
      "Epoch 1367/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14246.2383 - mean_absolute_error: 14246.2383 - val_loss: 18512.8792 - val_mean_absolute_error: 18512.8792\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 17362.58956\n",
      "Epoch 1368/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14608.7357 - mean_absolute_error: 14608.7357 - val_loss: 18416.3217 - val_mean_absolute_error: 18416.3217\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 17362.58956\n",
      "Epoch 1369/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14402.6117 - mean_absolute_error: 14402.6117 - val_loss: 18172.0694 - val_mean_absolute_error: 18172.0694\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 17362.58956\n",
      "Epoch 1370/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14411.3285 - mean_absolute_error: 14411.3285 - val_loss: 17906.2084 - val_mean_absolute_error: 17906.2084\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 17362.58956\n",
      "Epoch 1371/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 14493.4594 - mean_absolute_error: 14493.4594 - val_loss: 18152.5721 - val_mean_absolute_error: 18152.5721\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 17362.58956\n",
      "Epoch 1372/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 14630.1559 - mean_absolute_error: 14630.1559 - val_loss: 17962.0842 - val_mean_absolute_error: 17962.0842\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 17362.58956\n",
      "Epoch 1373/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14656.0613 - mean_absolute_error: 14656.0613 - val_loss: 18948.1573 - val_mean_absolute_error: 18948.1573\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 17362.58956\n",
      "Epoch 1374/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14700.9150 - mean_absolute_error: 14700.9150 - val_loss: 18184.0098 - val_mean_absolute_error: 18184.0098\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 17362.58956\n",
      "Epoch 1375/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14259.0158 - mean_absolute_error: 14259.0158 - val_loss: 18060.7891 - val_mean_absolute_error: 18060.7891\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 17362.58956\n",
      "Epoch 1376/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14805.4263 - mean_absolute_error: 14805.4263 - val_loss: 18330.6117 - val_mean_absolute_error: 18330.6117\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 17362.58956\n",
      "Epoch 1377/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14125.7554 - mean_absolute_error: 14125.7554 - val_loss: 18266.0508 - val_mean_absolute_error: 18266.0508\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 17362.58956\n",
      "Epoch 1378/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14782.6720 - mean_absolute_error: 14782.6720 - val_loss: 17827.0246 - val_mean_absolute_error: 17827.0246\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 17362.58956\n",
      "Epoch 1379/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14745.7868 - mean_absolute_error: 14745.7868 - val_loss: 17617.3875 - val_mean_absolute_error: 17617.3875\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 17362.58956\n",
      "Epoch 1380/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14637.6884 - mean_absolute_error: 14637.6884 - val_loss: 17940.3558 - val_mean_absolute_error: 17940.3558\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 17362.58956\n",
      "Epoch 1381/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14348.7144 - mean_absolute_error: 14348.7144 - val_loss: 17668.6686 - val_mean_absolute_error: 17668.6686\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 17362.58956\n",
      "Epoch 1382/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 14445.4931 - mean_absolute_error: 14445.4931 - val_loss: 17943.1360 - val_mean_absolute_error: 17943.1360\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 17362.58956\n",
      "Epoch 1383/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14364.3107 - mean_absolute_error: 14364.3107 - val_loss: 18038.4741 - val_mean_absolute_error: 18038.4741\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 17362.58956\n",
      "Epoch 1384/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14519.8310 - mean_absolute_error: 14519.8310 - val_loss: 17959.3280 - val_mean_absolute_error: 17959.3280\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 17362.58956\n",
      "Epoch 1385/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14172.5788 - mean_absolute_error: 14172.5788 - val_loss: 18363.7229 - val_mean_absolute_error: 18363.7229\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 17362.58956\n",
      "Epoch 1386/1500\n",
      "934/934 [==============================] - 0s 203us/step - loss: 14799.9208 - mean_absolute_error: 14799.9208 - val_loss: 18214.8113 - val_mean_absolute_error: 18214.8113\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 17362.58956\n",
      "Epoch 1387/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 14522.0790 - mean_absolute_error: 14522.0790 - val_loss: 18949.9671 - val_mean_absolute_error: 18949.9671\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 17362.58956\n",
      "Epoch 1388/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14967.1978 - mean_absolute_error: 14967.1978 - val_loss: 18026.2183 - val_mean_absolute_error: 18026.2183\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 17362.58956\n",
      "Epoch 1389/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 14579.6433 - mean_absolute_error: 14579.6433 - val_loss: 18381.0965 - val_mean_absolute_error: 18381.0965\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 17362.58956\n",
      "Epoch 1390/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14512.2985 - mean_absolute_error: 14512.2985 - val_loss: 18087.5331 - val_mean_absolute_error: 18087.5331\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 17362.58956\n",
      "Epoch 1391/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14780.1345 - mean_absolute_error: 14780.1345 - val_loss: 18150.4384 - val_mean_absolute_error: 18150.4384\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 17362.58956\n",
      "Epoch 1392/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 14637.0130 - mean_absolute_error: 14637.0130 - val_loss: 18215.3533 - val_mean_absolute_error: 18215.3533\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 17362.58956\n",
      "Epoch 1393/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14405.2601 - mean_absolute_error: 14405.2601 - val_loss: 18760.1019 - val_mean_absolute_error: 18760.1019\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 17362.58956\n",
      "Epoch 1394/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 199us/step - loss: 14391.4184 - mean_absolute_error: 14391.4184 - val_loss: 18066.9288 - val_mean_absolute_error: 18066.9288\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 17362.58956\n",
      "Epoch 1395/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14399.1908 - mean_absolute_error: 14399.1908 - val_loss: 17921.2886 - val_mean_absolute_error: 17921.2886\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 17362.58956\n",
      "Epoch 1396/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14311.2162 - mean_absolute_error: 14311.2162 - val_loss: 17678.1292 - val_mean_absolute_error: 17678.1292\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 17362.58956\n",
      "Epoch 1397/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14557.1574 - mean_absolute_error: 14557.1574 - val_loss: 18040.4903 - val_mean_absolute_error: 18040.4903\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 17362.58956\n",
      "Epoch 1398/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 14306.9451 - mean_absolute_error: 14306.9451 - val_loss: 21976.8781 - val_mean_absolute_error: 21976.8781\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 17362.58956\n",
      "Epoch 1399/1500\n",
      "934/934 [==============================] - 0s 212us/step - loss: 14614.4606 - mean_absolute_error: 14614.4606 - val_loss: 18254.3265 - val_mean_absolute_error: 18254.3265\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 17362.58956\n",
      "Epoch 1400/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 14770.2927 - mean_absolute_error: 14770.2927 - val_loss: 18018.6226 - val_mean_absolute_error: 18018.6226\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 17362.58956\n",
      "Epoch 1401/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 14488.7406 - mean_absolute_error: 14488.7406 - val_loss: 18089.9986 - val_mean_absolute_error: 18089.9986\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 17362.58956\n",
      "Epoch 1402/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 14762.2382 - mean_absolute_error: 14762.2382 - val_loss: 18549.4284 - val_mean_absolute_error: 18549.4284\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 17362.58956\n",
      "Epoch 1403/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 14726.6629 - mean_absolute_error: 14726.6629 - val_loss: 18195.0533 - val_mean_absolute_error: 18195.0533\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 17362.58956\n",
      "Epoch 1404/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 14512.9626 - mean_absolute_error: 14512.9626 - val_loss: 18237.5411 - val_mean_absolute_error: 18237.5411\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 17362.58956\n",
      "Epoch 1405/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 14376.7074 - mean_absolute_error: 14376.7074 - val_loss: 18484.4210 - val_mean_absolute_error: 18484.4210\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 17362.58956\n",
      "Epoch 1406/1500\n",
      "934/934 [==============================] - 0s 210us/step - loss: 14507.8114 - mean_absolute_error: 14507.8114 - val_loss: 17671.2001 - val_mean_absolute_error: 17671.2001\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 17362.58956\n",
      "Epoch 1407/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 14393.6030 - mean_absolute_error: 14393.6030 - val_loss: 18387.6000 - val_mean_absolute_error: 18387.6000\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 17362.58956\n",
      "Epoch 1408/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 14212.3042 - mean_absolute_error: 14212.3042 - val_loss: 18589.9718 - val_mean_absolute_error: 18589.9718\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 17362.58956\n",
      "Epoch 1409/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14334.4574 - mean_absolute_error: 14334.4574 - val_loss: 19745.3593 - val_mean_absolute_error: 19745.3593\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 17362.58956\n",
      "Epoch 1410/1500\n",
      "934/934 [==============================] - 0s 207us/step - loss: 14413.7764 - mean_absolute_error: 14413.7764 - val_loss: 17921.2337 - val_mean_absolute_error: 17921.2337\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 17362.58956\n",
      "Epoch 1411/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14752.8800 - mean_absolute_error: 14752.8800 - val_loss: 19635.6591 - val_mean_absolute_error: 19635.6591\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 17362.58956\n",
      "Epoch 1412/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 14195.1802 - mean_absolute_error: 14195.1802 - val_loss: 20790.9279 - val_mean_absolute_error: 20790.9279\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 17362.58956\n",
      "Epoch 1413/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 14361.0853 - mean_absolute_error: 14361.0853 - val_loss: 18157.2709 - val_mean_absolute_error: 18157.2709\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 17362.58956\n",
      "Epoch 1414/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14800.7876 - mean_absolute_error: 14800.7876 - val_loss: 18052.9449 - val_mean_absolute_error: 18052.9449\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 17362.58956\n",
      "Epoch 1415/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14421.4545 - mean_absolute_error: 14421.4545 - val_loss: 17818.0399 - val_mean_absolute_error: 17818.0399\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 17362.58956\n",
      "Epoch 1416/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14306.8326 - mean_absolute_error: 14306.8326 - val_loss: 18239.4627 - val_mean_absolute_error: 18239.4627\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 17362.58956\n",
      "Epoch 1417/1500\n",
      "934/934 [==============================] - 0s 189us/step - loss: 14397.0973 - mean_absolute_error: 14397.0973 - val_loss: 18470.1094 - val_mean_absolute_error: 18470.1094\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 17362.58956\n",
      "Epoch 1418/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14198.1833 - mean_absolute_error: 14198.1833 - val_loss: 18256.5024 - val_mean_absolute_error: 18256.5024\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 17362.58956\n",
      "Epoch 1419/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14331.6471 - mean_absolute_error: 14331.6471 - val_loss: 17707.2799 - val_mean_absolute_error: 17707.2799\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 17362.58956\n",
      "Epoch 1420/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14757.5866 - mean_absolute_error: 14757.5866 - val_loss: 18034.7512 - val_mean_absolute_error: 18034.7512\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 17362.58956\n",
      "Epoch 1421/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14503.0983 - mean_absolute_error: 14503.0983 - val_loss: 17718.9542 - val_mean_absolute_error: 17718.9542\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 17362.58956\n",
      "Epoch 1422/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14593.1973 - mean_absolute_error: 14593.1973 - val_loss: 18209.9930 - val_mean_absolute_error: 18209.9930\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 17362.58956\n",
      "Epoch 1423/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14469.3952 - mean_absolute_error: 14469.3952 - val_loss: 17722.3503 - val_mean_absolute_error: 17722.3503\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 17362.58956\n",
      "Epoch 1424/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14506.6449 - mean_absolute_error: 14506.6449 - val_loss: 17868.8241 - val_mean_absolute_error: 17868.8241\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 17362.58956\n",
      "Epoch 1425/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14657.6331 - mean_absolute_error: 14657.6331 - val_loss: 18064.0861 - val_mean_absolute_error: 18064.0861\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 17362.58956\n",
      "Epoch 1426/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 13983.5012 - mean_absolute_error: 13983.5012 - val_loss: 19114.1715 - val_mean_absolute_error: 19114.1715\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 17362.58956\n",
      "Epoch 1427/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14513.3720 - mean_absolute_error: 14513.3720 - val_loss: 17947.4674 - val_mean_absolute_error: 17947.4674\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 17362.58956\n",
      "Epoch 1428/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 204us/step - loss: 14305.4909 - mean_absolute_error: 14305.4909 - val_loss: 17869.2288 - val_mean_absolute_error: 17869.2288\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 17362.58956\n",
      "Epoch 1429/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14190.0369 - mean_absolute_error: 14190.0369 - val_loss: 18826.2100 - val_mean_absolute_error: 18826.2100\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 17362.58956\n",
      "Epoch 1430/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14442.5281 - mean_absolute_error: 14442.5281 - val_loss: 18696.5167 - val_mean_absolute_error: 18696.5167\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 17362.58956\n",
      "Epoch 1431/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14628.0331 - mean_absolute_error: 14628.0331 - val_loss: 17814.7849 - val_mean_absolute_error: 17814.7849\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 17362.58956\n",
      "Epoch 1432/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14418.8728 - mean_absolute_error: 14418.8728 - val_loss: 18968.3502 - val_mean_absolute_error: 18968.3502\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 17362.58956\n",
      "Epoch 1433/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 15252.0132 - mean_absolute_error: 15252.0132 - val_loss: 20272.7747 - val_mean_absolute_error: 20272.7747\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 17362.58956\n",
      "Epoch 1434/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14617.6341 - mean_absolute_error: 14617.6341 - val_loss: 17903.9935 - val_mean_absolute_error: 17903.9935\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 17362.58956\n",
      "Epoch 1435/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14058.3300 - mean_absolute_error: 14058.3300 - val_loss: 18110.5280 - val_mean_absolute_error: 18110.5280\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 17362.58956\n",
      "Epoch 1436/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 14352.4837 - mean_absolute_error: 14352.4837 - val_loss: 18712.8867 - val_mean_absolute_error: 18712.8867\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 17362.58956\n",
      "Epoch 1437/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14494.9122 - mean_absolute_error: 14494.9122 - val_loss: 17973.6592 - val_mean_absolute_error: 17973.6592\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 17362.58956\n",
      "Epoch 1438/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14426.9070 - mean_absolute_error: 14426.9070 - val_loss: 17795.4409 - val_mean_absolute_error: 17795.4409\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 17362.58956\n",
      "Epoch 1439/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14300.2795 - mean_absolute_error: 14300.2795 - val_loss: 18393.6754 - val_mean_absolute_error: 18393.6754\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 17362.58956\n",
      "Epoch 1440/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14004.1060 - mean_absolute_error: 14004.1060 - val_loss: 18564.6752 - val_mean_absolute_error: 18564.6752\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 17362.58956\n",
      "Epoch 1441/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14068.8341 - mean_absolute_error: 14068.8341 - val_loss: 17994.1089 - val_mean_absolute_error: 17994.1089\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 17362.58956\n",
      "Epoch 1442/1500\n",
      "934/934 [==============================] - 0s 208us/step - loss: 14290.4387 - mean_absolute_error: 14290.4387 - val_loss: 18042.1842 - val_mean_absolute_error: 18042.1842\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 17362.58956\n",
      "Epoch 1443/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14242.7847 - mean_absolute_error: 14242.7847 - val_loss: 18446.6421 - val_mean_absolute_error: 18446.6421\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 17362.58956\n",
      "Epoch 1444/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14460.0524 - mean_absolute_error: 14460.0524 - val_loss: 18269.2864 - val_mean_absolute_error: 18269.2864\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 17362.58956\n",
      "Epoch 1445/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14018.4610 - mean_absolute_error: 14018.4610 - val_loss: 17991.9419 - val_mean_absolute_error: 17991.9419\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 17362.58956\n",
      "Epoch 1446/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14280.5609 - mean_absolute_error: 14280.5609 - val_loss: 17844.9249 - val_mean_absolute_error: 17844.9249\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 17362.58956\n",
      "Epoch 1447/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 13998.6950 - mean_absolute_error: 13998.6950 - val_loss: 18175.1003 - val_mean_absolute_error: 18175.1003\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 17362.58956\n",
      "Epoch 1448/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14206.0180 - mean_absolute_error: 14206.0180 - val_loss: 18103.5438 - val_mean_absolute_error: 18103.5438\n",
      "\n",
      "Epoch 01448: val_loss did not improve from 17362.58956\n",
      "Epoch 1449/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14063.8559 - mean_absolute_error: 14063.8559 - val_loss: 18104.2706 - val_mean_absolute_error: 18104.2706\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 17362.58956\n",
      "Epoch 1450/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14443.6297 - mean_absolute_error: 14443.6297 - val_loss: 20211.5351 - val_mean_absolute_error: 20211.5351\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 17362.58956\n",
      "Epoch 1451/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 14173.3964 - mean_absolute_error: 14173.3964 - val_loss: 17784.0558 - val_mean_absolute_error: 17784.0558\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 17362.58956\n",
      "Epoch 1452/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14175.1953 - mean_absolute_error: 14175.1953 - val_loss: 18108.2963 - val_mean_absolute_error: 18108.2963\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 17362.58956\n",
      "Epoch 1453/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14468.7063 - mean_absolute_error: 14468.7063 - val_loss: 18028.5007 - val_mean_absolute_error: 18028.5007\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 17362.58956\n",
      "Epoch 1454/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 15045.1925 - mean_absolute_error: 15045.1925 - val_loss: 18118.4132 - val_mean_absolute_error: 18118.4132\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 17362.58956\n",
      "Epoch 1455/1500\n",
      "934/934 [==============================] - 0s 211us/step - loss: 14115.3706 - mean_absolute_error: 14115.3706 - val_loss: 17904.8875 - val_mean_absolute_error: 17904.8875\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 17362.58956\n",
      "Epoch 1456/1500\n",
      "934/934 [==============================] - 0s 238us/step - loss: 14000.7997 - mean_absolute_error: 14000.7997 - val_loss: 17987.9701 - val_mean_absolute_error: 17987.9701\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 17362.58956\n",
      "Epoch 1457/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14293.2502 - mean_absolute_error: 14293.2502 - val_loss: 17969.4297 - val_mean_absolute_error: 17969.4297\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 17362.58956\n",
      "Epoch 1458/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14347.7167 - mean_absolute_error: 14347.7167 - val_loss: 18393.4533 - val_mean_absolute_error: 18393.4533\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 17362.58956\n",
      "Epoch 1459/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14321.0536 - mean_absolute_error: 14321.0536 - val_loss: 17688.9973 - val_mean_absolute_error: 17688.9973\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 17362.58956\n",
      "Epoch 1460/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 13947.5030 - mean_absolute_error: 13947.5030 - val_loss: 18654.5002 - val_mean_absolute_error: 18654.5002\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 17362.58956\n",
      "Epoch 1461/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 14358.4445 - mean_absolute_error: 14358.4445 - val_loss: 19329.8750 - val_mean_absolute_error: 19329.8750\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 17362.58956\n",
      "Epoch 1462/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 192us/step - loss: 14683.6471 - mean_absolute_error: 14683.6471 - val_loss: 17789.7188 - val_mean_absolute_error: 17789.7188\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 17362.58956\n",
      "Epoch 1463/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 14354.3004 - mean_absolute_error: 14354.3004 - val_loss: 17943.9916 - val_mean_absolute_error: 17943.9916\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 17362.58956\n",
      "Epoch 1464/1500\n",
      "934/934 [==============================] - 0s 192us/step - loss: 13812.1452 - mean_absolute_error: 13812.1452 - val_loss: 18306.4771 - val_mean_absolute_error: 18306.4771\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 17362.58956\n",
      "Epoch 1465/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14014.0611 - mean_absolute_error: 14014.0611 - val_loss: 17921.5271 - val_mean_absolute_error: 17921.5271\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 17362.58956\n",
      "Epoch 1466/1500\n",
      "934/934 [==============================] - 0s 191us/step - loss: 14021.9455 - mean_absolute_error: 14021.9455 - val_loss: 18168.2425 - val_mean_absolute_error: 18168.2425\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 17362.58956\n",
      "Epoch 1467/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14212.2064 - mean_absolute_error: 14212.2064 - val_loss: 19650.2591 - val_mean_absolute_error: 19650.2591\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 17362.58956\n",
      "Epoch 1468/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14549.1037 - mean_absolute_error: 14549.1037 - val_loss: 18233.4120 - val_mean_absolute_error: 18233.4120\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 17362.58956\n",
      "Epoch 1469/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14103.5190 - mean_absolute_error: 14103.5190 - val_loss: 17855.9985 - val_mean_absolute_error: 17855.9985\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 17362.58956\n",
      "Epoch 1470/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 14204.4511 - mean_absolute_error: 14204.4511 - val_loss: 17966.6489 - val_mean_absolute_error: 17966.6489\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 17362.58956\n",
      "Epoch 1471/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 13873.2126 - mean_absolute_error: 13873.2126 - val_loss: 18414.4974 - val_mean_absolute_error: 18414.4974\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 17362.58956\n",
      "Epoch 1472/1500\n",
      "934/934 [==============================] - 0s 190us/step - loss: 13978.3597 - mean_absolute_error: 13978.3597 - val_loss: 18064.0690 - val_mean_absolute_error: 18064.0690\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 17362.58956\n",
      "Epoch 1473/1500\n",
      "934/934 [==============================] - 0s 193us/step - loss: 13989.1512 - mean_absolute_error: 13989.1512 - val_loss: 18809.5073 - val_mean_absolute_error: 18809.5073\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 17362.58956\n",
      "Epoch 1474/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14098.8062 - mean_absolute_error: 14098.8062 - val_loss: 18496.2256 - val_mean_absolute_error: 18496.2256\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 17362.58956\n",
      "Epoch 1475/1500\n",
      "934/934 [==============================] - 0s 205us/step - loss: 13965.4813 - mean_absolute_error: 13965.4813 - val_loss: 18194.7577 - val_mean_absolute_error: 18194.7577\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 17362.58956\n",
      "Epoch 1476/1500\n",
      "934/934 [==============================] - 0s 194us/step - loss: 14360.4926 - mean_absolute_error: 14360.4926 - val_loss: 17725.0176 - val_mean_absolute_error: 17725.0176\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 17362.58956\n",
      "Epoch 1477/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 13970.4474 - mean_absolute_error: 13970.4474 - val_loss: 18606.2129 - val_mean_absolute_error: 18606.2129\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 17362.58956\n",
      "Epoch 1478/1500\n",
      "934/934 [==============================] - 0s 209us/step - loss: 14173.1032 - mean_absolute_error: 14173.1032 - val_loss: 17934.9984 - val_mean_absolute_error: 17934.9984\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 17362.58956\n",
      "Epoch 1479/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 14203.5597 - mean_absolute_error: 14203.5597 - val_loss: 18732.0482 - val_mean_absolute_error: 18732.0482\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 17362.58956\n",
      "Epoch 1480/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14214.5750 - mean_absolute_error: 14214.5750 - val_loss: 17957.7710 - val_mean_absolute_error: 17957.7710\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 17362.58956\n",
      "Epoch 1481/1500\n",
      "934/934 [==============================] - 0s 195us/step - loss: 13913.0223 - mean_absolute_error: 13913.0223 - val_loss: 18366.1197 - val_mean_absolute_error: 18366.1197\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 17362.58956\n",
      "Epoch 1482/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 13922.2060 - mean_absolute_error: 13922.2060 - val_loss: 17959.7616 - val_mean_absolute_error: 17959.7616\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 17362.58956\n",
      "Epoch 1483/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 13772.1338 - mean_absolute_error: 13772.1338 - val_loss: 18578.5892 - val_mean_absolute_error: 18578.5892\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 17362.58956\n",
      "Epoch 1484/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 13890.2096 - mean_absolute_error: 13890.2096 - val_loss: 19140.8886 - val_mean_absolute_error: 19140.8886\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 17362.58956\n",
      "Epoch 1485/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14318.2891 - mean_absolute_error: 14318.2891 - val_loss: 18212.2918 - val_mean_absolute_error: 18212.2918\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 17362.58956\n",
      "Epoch 1486/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 13759.3876 - mean_absolute_error: 13759.3876 - val_loss: 18615.8524 - val_mean_absolute_error: 18615.8524\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 17362.58956\n",
      "Epoch 1487/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 14644.5888 - mean_absolute_error: 14644.5888 - val_loss: 18114.8198 - val_mean_absolute_error: 18114.8198\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 17362.58956\n",
      "Epoch 1488/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 13811.3057 - mean_absolute_error: 13811.3057 - val_loss: 18041.6292 - val_mean_absolute_error: 18041.6292\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 17362.58956\n",
      "Epoch 1489/1500\n",
      "934/934 [==============================] - 0s 200us/step - loss: 14208.2967 - mean_absolute_error: 14208.2967 - val_loss: 20601.7325 - val_mean_absolute_error: 20601.7325\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 17362.58956\n",
      "Epoch 1490/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 14206.1111 - mean_absolute_error: 14206.1111 - val_loss: 17955.5960 - val_mean_absolute_error: 17955.5960\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 17362.58956\n",
      "Epoch 1491/1500\n",
      "934/934 [==============================] - 0s 196us/step - loss: 13969.2765 - mean_absolute_error: 13969.2765 - val_loss: 19863.2275 - val_mean_absolute_error: 19863.2275\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 17362.58956\n",
      "Epoch 1492/1500\n",
      "934/934 [==============================] - 0s 202us/step - loss: 14016.0327 - mean_absolute_error: 14016.0327 - val_loss: 18431.1700 - val_mean_absolute_error: 18431.1700\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 17362.58956\n",
      "Epoch 1493/1500\n",
      "934/934 [==============================] - 0s 201us/step - loss: 14165.1438 - mean_absolute_error: 14165.1438 - val_loss: 18766.1870 - val_mean_absolute_error: 18766.1870\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 17362.58956\n",
      "Epoch 1494/1500\n",
      "934/934 [==============================] - 0s 198us/step - loss: 14312.3871 - mean_absolute_error: 14312.3871 - val_loss: 18522.9039 - val_mean_absolute_error: 18522.9039\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 17362.58956\n",
      "Epoch 1495/1500\n",
      "934/934 [==============================] - 0s 214us/step - loss: 14774.8534 - mean_absolute_error: 14774.8534 - val_loss: 18252.3926 - val_mean_absolute_error: 18252.3926\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 17362.58956\n",
      "Epoch 1496/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934/934 [==============================] - 0s 201us/step - loss: 14082.4177 - mean_absolute_error: 14082.4177 - val_loss: 18452.9886 - val_mean_absolute_error: 18452.9886\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 17362.58956\n",
      "Epoch 1497/1500\n",
      "934/934 [==============================] - 0s 199us/step - loss: 13867.2288 - mean_absolute_error: 13867.2288 - val_loss: 18079.4514 - val_mean_absolute_error: 18079.4514\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 17362.58956\n",
      "Epoch 1498/1500\n",
      "934/934 [==============================] - 0s 216us/step - loss: 13804.2990 - mean_absolute_error: 13804.2990 - val_loss: 18392.6888 - val_mean_absolute_error: 18392.6888\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 17362.58956\n",
      "Epoch 1499/1500\n",
      "934/934 [==============================] - 0s 206us/step - loss: 13781.3987 - mean_absolute_error: 13781.3987 - val_loss: 18145.3808 - val_mean_absolute_error: 18145.3808\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 17362.58956\n",
      "Epoch 1500/1500\n",
      "934/934 [==============================] - 0s 204us/step - loss: 14029.1036 - mean_absolute_error: 14029.1036 - val_loss: 18068.0973 - val_mean_absolute_error: 18068.0973\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 17362.58956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x174a4abda20>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nmodel.fit(vxtrain, vytrain, epochs=1500, batch_size=5, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 18403.49591984161\n",
      "Mean Squared Error: 991444532.5245295\n",
      "Root Mean Squared Error: 31487.21220629939\n"
     ]
    }
   ],
   "source": [
    "y_pred=Nmodel.predict(vxtest)\n",
    "vytest= ytest.to_numpy()\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(vytest, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(vytest, y_pred))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(vytest, y_pred))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3)\n",
    "axs[0].set_title('Actual value')\n",
    "axs[0].plot(vytest)\n",
    "axs[1].set_title('Predicted value')\n",
    "axs[1].plot(y_pred)\n",
    "axs[2].set_title('Compared values')\n",
    "axs[2].plot(vytest)\n",
    "axs[2].plot(y_pred)\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmodel(\n",
      "  (fc1): Linear(in_features=75, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class tmodel(nn.Module):\n",
    "    def __init__(self,begin_size):\n",
    "        super(tmodel,self).__init__()\n",
    "        self.fc1 = nn.Linear(begin_size,32)\n",
    "        init.normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        init.normal_(self.fc2.weight)\n",
    "        self.fc3 = nn.Linear(16,1)\n",
    "        init.normal_(self.fc3.weight)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred = F.relu(self.fc1(x))\n",
    "        y_pred = F.relu(self.fc2(y_pred))\n",
    "        y_pred = self.fc3(y_pred)\n",
    "        return y_pred\n",
    "ntmodel = tmodel(vxtrain.shape[1])\n",
    "print(ntmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2977"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(ntmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size_fvytrain = np.resize(vytrain,(len(vytrain),1))\n",
    "fvxtrain = vxtrain.astype(np.float32)\n",
    "fvytrain = new_size_fvytrain.astype(np.float32)\n",
    "tytrain = torch.from_numpy(fvytrain)\n",
    "txtrain = torch.from_numpy(fvxtrain)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(ntmodel.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain = torch.utils.data.TensorDataset(txtrain,tytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1df0470b390>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata=torch.utils.data.DataLoader(datatrain, batch_size=5, shuffle= True)\n",
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 302431.688\n",
      "[1,    10] loss: 325006.581\n",
      "[1,    15] loss: 327268.456\n",
      "[1,    20] loss: 378571.612\n",
      "[1,    25] loss: 343916.625\n",
      "[1,    30] loss: 321093.441\n",
      "[1,    35] loss: 308498.869\n",
      "[1,    40] loss: 307248.881\n",
      "[1,    45] loss: 323450.575\n",
      "[1,    50] loss: 305739.731\n",
      "[1,    55] loss: 317540.269\n",
      "[1,    60] loss: 303218.856\n",
      "[1,    65] loss: 326371.494\n",
      "[1,    70] loss: 322308.612\n",
      "[1,    75] loss: 322570.997\n",
      "[1,    80] loss: 321693.066\n",
      "[1,    85] loss: 349677.287\n",
      "[1,    90] loss: 287463.300\n",
      "[1,    95] loss: 291607.716\n",
      "[1,   100] loss: 326606.956\n",
      "[1,   105] loss: 355820.612\n",
      "[1,   110] loss: 303786.291\n",
      "[1,   115] loss: 310258.694\n",
      "[1,   120] loss: 278722.791\n",
      "[1,   125] loss: 309130.431\n",
      "[1,   130] loss: 262698.416\n",
      "[1,   135] loss: 318082.631\n",
      "[1,   140] loss: 282002.544\n",
      "[1,   145] loss: 314601.300\n",
      "[1,   150] loss: 269959.378\n",
      "[1,   155] loss: 295711.138\n",
      "[1,   160] loss: 345128.112\n",
      "[1,   165] loss: 296907.531\n",
      "[1,   170] loss: 359560.081\n",
      "[1,   175] loss: 263363.975\n",
      "[1,   180] loss: 249846.466\n",
      "[1,   185] loss: 279634.016\n",
      "[1,   190] loss: 286911.294\n",
      "[1,   195] loss: 245872.656\n",
      "[1,   200] loss: 258611.612\n",
      "[1,   205] loss: 254314.231\n",
      "[1,   210] loss: 263616.025\n",
      "[1,   215] loss: 262451.031\n",
      "[1,   220] loss: 273586.122\n",
      "[1,   225] loss: 242653.691\n",
      "[1,   230] loss: 270034.875\n",
      "[2,     5] loss: 248516.306\n",
      "[2,    10] loss: 235785.803\n",
      "[2,    15] loss: 253788.925\n",
      "[2,    20] loss: 299032.184\n",
      "[2,    25] loss: 246109.356\n",
      "[2,    30] loss: 251703.081\n",
      "[2,    35] loss: 268681.906\n",
      "[2,    40] loss: 215385.294\n",
      "[2,    45] loss: 225442.975\n",
      "[2,    50] loss: 316099.091\n",
      "[2,    55] loss: 225092.047\n",
      "[2,    60] loss: 263545.356\n",
      "[2,    65] loss: 277204.919\n",
      "[2,    70] loss: 251195.506\n",
      "[2,    75] loss: 244939.394\n",
      "[2,    80] loss: 240283.712\n",
      "[2,    85] loss: 266529.466\n",
      "[2,    90] loss: 210785.128\n",
      "[2,    95] loss: 254494.356\n",
      "[2,   100] loss: 239951.291\n",
      "[2,   105] loss: 211301.328\n",
      "[2,   110] loss: 232962.778\n",
      "[2,   115] loss: 227491.084\n",
      "[2,   120] loss: 230621.413\n",
      "[2,   125] loss: 200490.616\n",
      "[2,   130] loss: 199951.075\n",
      "[2,   135] loss: 200320.191\n",
      "[2,   140] loss: 208251.803\n",
      "[2,   145] loss: 225551.144\n",
      "[2,   150] loss: 219264.388\n",
      "[2,   155] loss: 220359.359\n",
      "[2,   160] loss: 253512.987\n",
      "[2,   165] loss: 212692.831\n",
      "[2,   170] loss: 212939.044\n",
      "[2,   175] loss: 220047.444\n",
      "[2,   180] loss: 200386.381\n",
      "[2,   185] loss: 217918.288\n",
      "[2,   190] loss: 226875.091\n",
      "[2,   195] loss: 201271.803\n",
      "[2,   200] loss: 206039.959\n",
      "[2,   205] loss: 232031.031\n",
      "[2,   210] loss: 179043.516\n",
      "[2,   215] loss: 196218.806\n",
      "[2,   220] loss: 207863.528\n",
      "[2,   225] loss: 216726.325\n",
      "[2,   230] loss: 214625.212\n",
      "[3,     5] loss: 216951.356\n",
      "[3,    10] loss: 185431.684\n",
      "[3,    15] loss: 198949.109\n",
      "[3,    20] loss: 183280.438\n",
      "[3,    25] loss: 195495.309\n",
      "[3,    30] loss: 199325.791\n",
      "[3,    35] loss: 177770.130\n",
      "[3,    40] loss: 176183.716\n",
      "[3,    45] loss: 206762.575\n",
      "[3,    50] loss: 211413.772\n",
      "[3,    55] loss: 174891.478\n",
      "[3,    60] loss: 188429.506\n",
      "[3,    65] loss: 189223.053\n",
      "[3,    70] loss: 165392.609\n",
      "[3,    75] loss: 175134.206\n",
      "[3,    80] loss: 177126.600\n",
      "[3,    85] loss: 155494.075\n",
      "[3,    90] loss: 174097.913\n",
      "[3,    95] loss: 163686.972\n",
      "[3,   100] loss: 144164.661\n",
      "[3,   105] loss: 156766.603\n",
      "[3,   110] loss: 152739.138\n",
      "[3,   115] loss: 152149.966\n",
      "[3,   120] loss: 182964.581\n",
      "[3,   125] loss: 141086.977\n",
      "[3,   130] loss: 142639.564\n",
      "[3,   135] loss: 163034.369\n",
      "[3,   140] loss: 165186.411\n",
      "[3,   145] loss: 134658.534\n",
      "[3,   150] loss: 127855.180\n",
      "[3,   155] loss: 139496.797\n",
      "[3,   160] loss: 128175.778\n",
      "[3,   165] loss: 139341.175\n",
      "[3,   170] loss: 138934.448\n",
      "[3,   175] loss: 117478.661\n",
      "[3,   180] loss: 127421.309\n",
      "[3,   185] loss: 143317.600\n",
      "[3,   190] loss: 145681.781\n",
      "[3,   195] loss: 123798.644\n",
      "[3,   200] loss: 145699.788\n",
      "[3,   205] loss: 126642.231\n",
      "[3,   210] loss: 116968.814\n",
      "[3,   215] loss: 131710.062\n",
      "[3,   220] loss: 112206.372\n",
      "[3,   225] loss: 128672.325\n",
      "[3,   230] loss: 146913.353\n",
      "[4,     5] loss: 127107.639\n",
      "[4,    10] loss: 127843.286\n",
      "[4,    15] loss: 109459.912\n",
      "[4,    20] loss: 116179.233\n",
      "[4,    25] loss: 108897.358\n",
      "[4,    30] loss: 97373.464\n",
      "[4,    35] loss: 134218.522\n",
      "[4,    40] loss: 121304.205\n",
      "[4,    45] loss: 115164.230\n",
      "[4,    50] loss: 106107.984\n",
      "[4,    55] loss: 114408.295\n",
      "[4,    60] loss: 89848.816\n",
      "[4,    65] loss: 102683.284\n",
      "[4,    70] loss: 97778.119\n",
      "[4,    75] loss: 85633.949\n",
      "[4,    80] loss: 92030.820\n",
      "[4,    85] loss: 107747.428\n",
      "[4,    90] loss: 90633.391\n",
      "[4,    95] loss: 88833.620\n",
      "[4,   100] loss: 72217.781\n",
      "[4,   105] loss: 90677.894\n",
      "[4,   110] loss: 75869.723\n",
      "[4,   115] loss: 89248.319\n",
      "[4,   120] loss: 68370.761\n",
      "[4,   125] loss: 112607.866\n",
      "[4,   130] loss: 77994.852\n",
      "[4,   135] loss: 84755.977\n",
      "[4,   140] loss: 79272.109\n",
      "[4,   145] loss: 77922.655\n",
      "[4,   150] loss: 67695.587\n",
      "[4,   155] loss: 91099.326\n",
      "[4,   160] loss: 83981.934\n",
      "[4,   165] loss: 84154.807\n",
      "[4,   170] loss: 85087.175\n",
      "[4,   175] loss: 74506.455\n",
      "[4,   180] loss: 69391.881\n",
      "[4,   185] loss: 60516.629\n",
      "[4,   190] loss: 74435.829\n",
      "[4,   195] loss: 94398.619\n",
      "[4,   200] loss: 55134.417\n",
      "[4,   205] loss: 64636.873\n",
      "[4,   210] loss: 87472.543\n",
      "[4,   215] loss: 76928.171\n",
      "[4,   220] loss: 84371.842\n",
      "[4,   225] loss: 69192.664\n",
      "[4,   230] loss: 51671.128\n",
      "[5,     5] loss: 76828.009\n",
      "[5,    10] loss: 64865.204\n",
      "[5,    15] loss: 54649.067\n",
      "[5,    20] loss: 81280.120\n",
      "[5,    25] loss: 49007.716\n",
      "[5,    30] loss: 56925.555\n",
      "[5,    35] loss: 71530.602\n",
      "[5,    40] loss: 56564.864\n",
      "[5,    45] loss: 55132.271\n",
      "[5,    50] loss: 64625.156\n",
      "[5,    55] loss: 55815.752\n",
      "[5,    60] loss: 61319.876\n",
      "[5,    65] loss: 46622.184\n",
      "[5,    70] loss: 64150.110\n",
      "[5,    75] loss: 61832.814\n",
      "[5,    80] loss: 57235.609\n",
      "[5,    85] loss: 70093.887\n",
      "[5,    90] loss: 70336.544\n",
      "[5,    95] loss: 56707.920\n",
      "[5,   100] loss: 52496.228\n",
      "[5,   105] loss: 68749.914\n",
      "[5,   110] loss: 45500.216\n",
      "[5,   115] loss: 54449.202\n",
      "[5,   120] loss: 54579.202\n",
      "[5,   125] loss: 45752.130\n",
      "[5,   130] loss: 51732.668\n",
      "[5,   135] loss: 61007.973\n",
      "[5,   140] loss: 47760.473\n",
      "[5,   145] loss: 47846.333\n",
      "[5,   150] loss: 66185.441\n",
      "[5,   155] loss: 68255.940\n",
      "[5,   160] loss: 53239.373\n",
      "[5,   165] loss: 89251.380\n",
      "[5,   170] loss: 56061.850\n",
      "[5,   175] loss: 33679.303\n",
      "[5,   180] loss: 42051.423\n",
      "[5,   185] loss: 39159.570\n",
      "[5,   190] loss: 69016.575\n",
      "[5,   195] loss: 41264.139\n",
      "[5,   200] loss: 67714.902\n",
      "[5,   205] loss: 47110.059\n",
      "[5,   210] loss: 68728.916\n",
      "[5,   215] loss: 61269.990\n",
      "[5,   220] loss: 49732.586\n",
      "[5,   225] loss: 51275.089\n",
      "[5,   230] loss: 53746.469\n",
      "[6,     5] loss: 63780.734\n",
      "[6,    10] loss: 46867.463\n",
      "[6,    15] loss: 45800.268\n",
      "[6,    20] loss: 66327.867\n",
      "[6,    25] loss: 50120.543\n",
      "[6,    30] loss: 42430.632\n",
      "[6,    35] loss: 45208.170\n",
      "[6,    40] loss: 51244.763\n",
      "[6,    45] loss: 56497.582\n",
      "[6,    50] loss: 52192.807\n",
      "[6,    55] loss: 51706.480\n",
      "[6,    60] loss: 60912.406\n",
      "[6,    65] loss: 65575.706\n",
      "[6,    70] loss: 41864.782\n",
      "[6,    75] loss: 37379.129\n",
      "[6,    80] loss: 42165.269\n",
      "[6,    85] loss: 54642.098\n",
      "[6,    90] loss: 41858.348\n",
      "[6,    95] loss: 80147.034\n",
      "[6,   100] loss: 50798.653\n",
      "[6,   105] loss: 57607.018\n",
      "[6,   110] loss: 85242.453\n",
      "[6,   115] loss: 54605.492\n",
      "[6,   120] loss: 44722.563\n",
      "[6,   125] loss: 41921.934\n",
      "[6,   130] loss: 43589.490\n",
      "[6,   135] loss: 51976.453\n",
      "[6,   140] loss: 43029.021\n",
      "[6,   145] loss: 62181.694\n",
      "[6,   150] loss: 44597.291\n",
      "[6,   155] loss: 56463.792\n",
      "[6,   160] loss: 44207.360\n",
      "[6,   165] loss: 51239.056\n",
      "[6,   170] loss: 52410.978\n",
      "[6,   175] loss: 47454.810\n",
      "[6,   180] loss: 55750.446\n",
      "[6,   185] loss: 43903.356\n",
      "[6,   190] loss: 25153.249\n",
      "[6,   195] loss: 87164.023\n",
      "[6,   200] loss: 51062.460\n",
      "[6,   205] loss: 51071.877\n",
      "[6,   210] loss: 52868.211\n",
      "[6,   215] loss: 61099.937\n",
      "[6,   220] loss: 48607.369\n",
      "[6,   225] loss: 43647.319\n",
      "[6,   230] loss: 47215.952\n",
      "[7,     5] loss: 45427.494\n",
      "[7,    10] loss: 32980.998\n",
      "[7,    15] loss: 61139.577\n",
      "[7,    20] loss: 66104.775\n",
      "[7,    25] loss: 45053.571\n",
      "[7,    30] loss: 67278.031\n",
      "[7,    35] loss: 66343.247\n",
      "[7,    40] loss: 53414.677\n",
      "[7,    45] loss: 37790.316\n",
      "[7,    50] loss: 47397.309\n",
      "[7,    55] loss: 38875.765\n",
      "[7,    60] loss: 48842.616\n",
      "[7,    65] loss: 40776.261\n",
      "[7,    70] loss: 51117.832\n",
      "[7,    75] loss: 47637.015\n",
      "[7,    80] loss: 56433.889\n",
      "[7,    85] loss: 57322.104\n",
      "[7,    90] loss: 49297.520\n",
      "[7,    95] loss: 72236.893\n",
      "[7,   100] loss: 94751.621\n",
      "[7,   105] loss: 58686.090\n",
      "[7,   110] loss: 49955.142\n",
      "[7,   115] loss: 39971.904\n",
      "[7,   120] loss: 54602.035\n",
      "[7,   125] loss: 46724.623\n",
      "[7,   130] loss: 70128.841\n",
      "[7,   135] loss: 50723.759\n",
      "[7,   140] loss: 48009.293\n",
      "[7,   145] loss: 46661.754\n",
      "[7,   150] loss: 43736.927\n",
      "[7,   155] loss: 47547.803\n",
      "[7,   160] loss: 45773.307\n",
      "[7,   165] loss: 39374.550\n",
      "[7,   170] loss: 55037.310\n",
      "[7,   175] loss: 55848.131\n",
      "[7,   180] loss: 64509.343\n",
      "[7,   185] loss: 45908.027\n",
      "[7,   190] loss: 65074.137\n",
      "[7,   195] loss: 42526.944\n",
      "[7,   200] loss: 39139.261\n",
      "[7,   205] loss: 39808.275\n",
      "[7,   210] loss: 52984.990\n",
      "[7,   215] loss: 25241.431\n",
      "[7,   220] loss: 41645.050\n",
      "[7,   225] loss: 36685.589\n",
      "[7,   230] loss: 42174.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,     5] loss: 38529.490\n",
      "[8,    10] loss: 59402.863\n",
      "[8,    15] loss: 67580.952\n",
      "[8,    20] loss: 61949.078\n",
      "[8,    25] loss: 74067.317\n",
      "[8,    30] loss: 38716.395\n",
      "[8,    35] loss: 47584.982\n",
      "[8,    40] loss: 32282.993\n",
      "[8,    45] loss: 52860.812\n",
      "[8,    50] loss: 70421.885\n",
      "[8,    55] loss: 37803.438\n",
      "[8,    60] loss: 54445.538\n",
      "[8,    65] loss: 46915.732\n",
      "[8,    70] loss: 34611.946\n",
      "[8,    75] loss: 45330.779\n",
      "[8,    80] loss: 59258.227\n",
      "[8,    85] loss: 50395.728\n",
      "[8,    90] loss: 77820.816\n",
      "[8,    95] loss: 40341.695\n",
      "[8,   100] loss: 42529.293\n",
      "[8,   105] loss: 31484.521\n",
      "[8,   110] loss: 53227.306\n",
      "[8,   115] loss: 45322.289\n",
      "[8,   120] loss: 48515.482\n",
      "[8,   125] loss: 51339.731\n",
      "[8,   130] loss: 40177.139\n",
      "[8,   135] loss: 53387.088\n",
      "[8,   140] loss: 31296.705\n",
      "[8,   145] loss: 34195.280\n",
      "[8,   150] loss: 58982.348\n",
      "[8,   155] loss: 50516.983\n",
      "[8,   160] loss: 41900.250\n",
      "[8,   165] loss: 42154.250\n",
      "[8,   170] loss: 73159.114\n",
      "[8,   175] loss: 50566.242\n",
      "[8,   180] loss: 60044.422\n",
      "[8,   185] loss: 39125.346\n",
      "[8,   190] loss: 49196.874\n",
      "[8,   195] loss: 53887.223\n",
      "[8,   200] loss: 51825.268\n",
      "[8,   205] loss: 50846.159\n",
      "[8,   210] loss: 40824.815\n",
      "[8,   215] loss: 33717.482\n",
      "[8,   220] loss: 73072.411\n",
      "[8,   225] loss: 35074.580\n",
      "[8,   230] loss: 45931.536\n",
      "[9,     5] loss: 39659.351\n",
      "[9,    10] loss: 36999.507\n",
      "[9,    15] loss: 36722.314\n",
      "[9,    20] loss: 68579.973\n",
      "[9,    25] loss: 64347.749\n",
      "[9,    30] loss: 37205.273\n",
      "[9,    35] loss: 50966.205\n",
      "[9,    40] loss: 66581.665\n",
      "[9,    45] loss: 43778.765\n",
      "[9,    50] loss: 42934.401\n",
      "[9,    55] loss: 44128.647\n",
      "[9,    60] loss: 48809.842\n",
      "[9,    65] loss: 38117.281\n",
      "[9,    70] loss: 44573.527\n",
      "[9,    75] loss: 53017.715\n",
      "[9,    80] loss: 40493.671\n",
      "[9,    85] loss: 40960.170\n",
      "[9,    90] loss: 42627.973\n",
      "[9,    95] loss: 36820.585\n",
      "[9,   100] loss: 36202.314\n",
      "[9,   105] loss: 39077.720\n",
      "[9,   110] loss: 66457.269\n",
      "[9,   115] loss: 46276.688\n",
      "[9,   120] loss: 62496.539\n",
      "[9,   125] loss: 42389.393\n",
      "[9,   130] loss: 48310.878\n",
      "[9,   135] loss: 54438.911\n",
      "[9,   140] loss: 52415.409\n",
      "[9,   145] loss: 47274.371\n",
      "[9,   150] loss: 83964.437\n",
      "[9,   155] loss: 43434.615\n",
      "[9,   160] loss: 56816.872\n",
      "[9,   165] loss: 54747.433\n",
      "[9,   170] loss: 58915.347\n",
      "[9,   175] loss: 57400.420\n",
      "[9,   180] loss: 29524.304\n",
      "[9,   185] loss: 51222.466\n",
      "[9,   190] loss: 54831.604\n",
      "[9,   195] loss: 48426.748\n",
      "[9,   200] loss: 42434.494\n",
      "[9,   205] loss: 30698.178\n",
      "[9,   210] loss: 51046.833\n",
      "[9,   215] loss: 52661.707\n",
      "[9,   220] loss: 47233.103\n",
      "[9,   225] loss: 55931.430\n",
      "[9,   230] loss: 44912.771\n",
      "[10,     5] loss: 37211.608\n",
      "[10,    10] loss: 36141.646\n",
      "[10,    15] loss: 53983.338\n",
      "[10,    20] loss: 39362.148\n",
      "[10,    25] loss: 52404.509\n",
      "[10,    30] loss: 45132.877\n",
      "[10,    35] loss: 43104.361\n",
      "[10,    40] loss: 53020.535\n",
      "[10,    45] loss: 76184.415\n",
      "[10,    50] loss: 32799.620\n",
      "[10,    55] loss: 36987.554\n",
      "[10,    60] loss: 43879.875\n",
      "[10,    65] loss: 39593.522\n",
      "[10,    70] loss: 43163.133\n",
      "[10,    75] loss: 45048.766\n",
      "[10,    80] loss: 58988.274\n",
      "[10,    85] loss: 51892.784\n",
      "[10,    90] loss: 44731.982\n",
      "[10,    95] loss: 52079.764\n",
      "[10,   100] loss: 58316.870\n",
      "[10,   105] loss: 38528.463\n",
      "[10,   110] loss: 42795.342\n",
      "[10,   115] loss: 34745.013\n",
      "[10,   120] loss: 44625.245\n",
      "[10,   125] loss: 42941.034\n",
      "[10,   130] loss: 33304.739\n",
      "[10,   135] loss: 47535.525\n",
      "[10,   140] loss: 40726.027\n",
      "[10,   145] loss: 44191.863\n",
      "[10,   150] loss: 41293.204\n",
      "[10,   155] loss: 76889.733\n",
      "[10,   160] loss: 47042.563\n",
      "[10,   165] loss: 45778.041\n",
      "[10,   170] loss: 34999.566\n",
      "[10,   175] loss: 47457.512\n",
      "[10,   180] loss: 63997.027\n",
      "[10,   185] loss: 43255.382\n",
      "[10,   190] loss: 84016.927\n",
      "[10,   195] loss: 45367.400\n",
      "[10,   200] loss: 32243.903\n",
      "[10,   205] loss: 47023.441\n",
      "[10,   210] loss: 47128.068\n",
      "[10,   215] loss: 42207.541\n",
      "[10,   220] loss: 46198.495\n",
      "[10,   225] loss: 61684.377\n",
      "[10,   230] loss: 57719.498\n",
      "[11,     5] loss: 53600.688\n",
      "[11,    10] loss: 47286.639\n",
      "[11,    15] loss: 55350.168\n",
      "[11,    20] loss: 73062.785\n",
      "[11,    25] loss: 51580.431\n",
      "[11,    30] loss: 44249.887\n",
      "[11,    35] loss: 30536.398\n",
      "[11,    40] loss: 42355.163\n",
      "[11,    45] loss: 42240.783\n",
      "[11,    50] loss: 38326.621\n",
      "[11,    55] loss: 47217.562\n",
      "[11,    60] loss: 48776.151\n",
      "[11,    65] loss: 38069.595\n",
      "[11,    70] loss: 52567.066\n",
      "[11,    75] loss: 39305.203\n",
      "[11,    80] loss: 46106.830\n",
      "[11,    85] loss: 42233.114\n",
      "[11,    90] loss: 42994.106\n",
      "[11,    95] loss: 70929.193\n",
      "[11,   100] loss: 39906.220\n",
      "[11,   105] loss: 58616.400\n",
      "[11,   110] loss: 48372.044\n",
      "[11,   115] loss: 48857.187\n",
      "[11,   120] loss: 52435.987\n",
      "[11,   125] loss: 22860.073\n",
      "[11,   130] loss: 30320.889\n",
      "[11,   135] loss: 60747.804\n",
      "[11,   140] loss: 42135.744\n",
      "[11,   145] loss: 40104.095\n",
      "[11,   150] loss: 59574.217\n",
      "[11,   155] loss: 34695.568\n",
      "[11,   160] loss: 50172.404\n",
      "[11,   165] loss: 43189.102\n",
      "[11,   170] loss: 53479.259\n",
      "[11,   175] loss: 64264.638\n",
      "[11,   180] loss: 40485.220\n",
      "[11,   185] loss: 41023.909\n",
      "[11,   190] loss: 38963.853\n",
      "[11,   195] loss: 38166.463\n",
      "[11,   200] loss: 47205.441\n",
      "[11,   205] loss: 38098.502\n",
      "[11,   210] loss: 45687.460\n",
      "[11,   215] loss: 42143.054\n",
      "[11,   220] loss: 41432.964\n",
      "[11,   225] loss: 44488.189\n",
      "[11,   230] loss: 48765.228\n",
      "[12,     5] loss: 47076.494\n",
      "[12,    10] loss: 32588.894\n",
      "[12,    15] loss: 36489.144\n",
      "[12,    20] loss: 36552.818\n",
      "[12,    25] loss: 32329.349\n",
      "[12,    30] loss: 81124.080\n",
      "[12,    35] loss: 44149.162\n",
      "[12,    40] loss: 48654.636\n",
      "[12,    45] loss: 45706.901\n",
      "[12,    50] loss: 44246.144\n",
      "[12,    55] loss: 35329.889\n",
      "[12,    60] loss: 46694.191\n",
      "[12,    65] loss: 58591.828\n",
      "[12,    70] loss: 60175.207\n",
      "[12,    75] loss: 35291.863\n",
      "[12,    80] loss: 50113.281\n",
      "[12,    85] loss: 40112.307\n",
      "[12,    90] loss: 42167.826\n",
      "[12,    95] loss: 62208.505\n",
      "[12,   100] loss: 59669.705\n",
      "[12,   105] loss: 44845.286\n",
      "[12,   110] loss: 49592.592\n",
      "[12,   115] loss: 32054.989\n",
      "[12,   120] loss: 48160.871\n",
      "[12,   125] loss: 65980.221\n",
      "[12,   130] loss: 35636.395\n",
      "[12,   135] loss: 47837.447\n",
      "[12,   140] loss: 31159.161\n",
      "[12,   145] loss: 35330.891\n",
      "[12,   150] loss: 34877.422\n",
      "[12,   155] loss: 39510.995\n",
      "[12,   160] loss: 62398.602\n",
      "[12,   165] loss: 40205.043\n",
      "[12,   170] loss: 28481.926\n",
      "[12,   175] loss: 33512.416\n",
      "[12,   180] loss: 42065.709\n",
      "[12,   185] loss: 43378.977\n",
      "[12,   190] loss: 42160.967\n",
      "[12,   195] loss: 42618.648\n",
      "[12,   200] loss: 40869.199\n",
      "[12,   205] loss: 43602.519\n",
      "[12,   210] loss: 43349.517\n",
      "[12,   215] loss: 37369.092\n",
      "[12,   220] loss: 49824.674\n",
      "[12,   225] loss: 51719.214\n",
      "[12,   230] loss: 55613.070\n",
      "[13,     5] loss: 52421.525\n",
      "[13,    10] loss: 45175.225\n",
      "[13,    15] loss: 48961.075\n",
      "[13,    20] loss: 36531.404\n",
      "[13,    25] loss: 35683.057\n",
      "[13,    30] loss: 37766.543\n",
      "[13,    35] loss: 32807.833\n",
      "[13,    40] loss: 48407.132\n",
      "[13,    45] loss: 33808.734\n",
      "[13,    50] loss: 43265.596\n",
      "[13,    55] loss: 33634.064\n",
      "[13,    60] loss: 63932.520\n",
      "[13,    65] loss: 49774.120\n",
      "[13,    70] loss: 51916.762\n",
      "[13,    75] loss: 40351.860\n",
      "[13,    80] loss: 29135.800\n",
      "[13,    85] loss: 48035.927\n",
      "[13,    90] loss: 32834.470\n",
      "[13,    95] loss: 44549.981\n",
      "[13,   100] loss: 34425.930\n",
      "[13,   105] loss: 38418.663\n",
      "[13,   110] loss: 48631.484\n",
      "[13,   115] loss: 60949.319\n",
      "[13,   120] loss: 56850.236\n",
      "[13,   125] loss: 54422.277\n",
      "[13,   130] loss: 33979.512\n",
      "[13,   135] loss: 35972.441\n",
      "[13,   140] loss: 34160.785\n",
      "[13,   145] loss: 45333.755\n",
      "[13,   150] loss: 35403.071\n",
      "[13,   155] loss: 53496.900\n",
      "[13,   160] loss: 37472.235\n",
      "[13,   165] loss: 45901.241\n",
      "[13,   170] loss: 53100.811\n",
      "[13,   175] loss: 50809.677\n",
      "[13,   180] loss: 33493.209\n",
      "[13,   185] loss: 48371.393\n",
      "[13,   190] loss: 51822.198\n",
      "[13,   195] loss: 43559.309\n",
      "[13,   200] loss: 46998.127\n",
      "[13,   205] loss: 48328.361\n",
      "[13,   210] loss: 44541.891\n",
      "[13,   215] loss: 60932.459\n",
      "[13,   220] loss: 35194.235\n",
      "[13,   225] loss: 46737.222\n",
      "[13,   230] loss: 35423.972\n",
      "[14,     5] loss: 34233.875\n",
      "[14,    10] loss: 30177.161\n",
      "[14,    15] loss: 35418.430\n",
      "[14,    20] loss: 45605.810\n",
      "[14,    25] loss: 57717.969\n",
      "[14,    30] loss: 44900.747\n",
      "[14,    35] loss: 40293.178\n",
      "[14,    40] loss: 39745.394\n",
      "[14,    45] loss: 53478.466\n",
      "[14,    50] loss: 43904.727\n",
      "[14,    55] loss: 57911.672\n",
      "[14,    60] loss: 39373.652\n",
      "[14,    65] loss: 33338.145\n",
      "[14,    70] loss: 54976.387\n",
      "[14,    75] loss: 46991.244\n",
      "[14,    80] loss: 46101.084\n",
      "[14,    85] loss: 40775.477\n",
      "[14,    90] loss: 31002.939\n",
      "[14,    95] loss: 52122.446\n",
      "[14,   100] loss: 35988.832\n",
      "[14,   105] loss: 35141.921\n",
      "[14,   110] loss: 34398.619\n",
      "[14,   115] loss: 50817.012\n",
      "[14,   120] loss: 48037.493\n",
      "[14,   125] loss: 54893.305\n",
      "[14,   130] loss: 38323.494\n",
      "[14,   135] loss: 53824.726\n",
      "[14,   140] loss: 43483.120\n",
      "[14,   145] loss: 40058.698\n",
      "[14,   150] loss: 26922.294\n",
      "[14,   155] loss: 38777.912\n",
      "[14,   160] loss: 23139.413\n",
      "[14,   165] loss: 54733.298\n",
      "[14,   170] loss: 38940.188\n",
      "[14,   175] loss: 59301.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14,   180] loss: 43038.183\n",
      "[14,   185] loss: 41412.987\n",
      "[14,   190] loss: 36260.313\n",
      "[14,   195] loss: 46065.561\n",
      "[14,   200] loss: 46565.481\n",
      "[14,   205] loss: 58010.753\n",
      "[14,   210] loss: 46411.715\n",
      "[14,   215] loss: 45468.027\n",
      "[14,   220] loss: 47642.479\n",
      "[14,   225] loss: 32630.967\n",
      "[14,   230] loss: 32366.423\n",
      "[15,     5] loss: 55314.541\n",
      "[15,    10] loss: 39206.695\n",
      "[15,    15] loss: 34072.953\n",
      "[15,    20] loss: 41686.237\n",
      "[15,    25] loss: 33005.177\n",
      "[15,    30] loss: 41876.689\n",
      "[15,    35] loss: 42916.934\n",
      "[15,    40] loss: 44250.510\n",
      "[15,    45] loss: 35477.893\n",
      "[15,    50] loss: 49592.527\n",
      "[15,    55] loss: 64915.695\n",
      "[15,    60] loss: 32339.166\n",
      "[15,    65] loss: 33606.839\n",
      "[15,    70] loss: 45095.581\n",
      "[15,    75] loss: 43158.746\n",
      "[15,    80] loss: 72253.490\n",
      "[15,    85] loss: 42053.069\n",
      "[15,    90] loss: 45455.107\n",
      "[15,    95] loss: 43421.326\n",
      "[15,   100] loss: 35247.631\n",
      "[15,   105] loss: 54329.957\n",
      "[15,   110] loss: 36557.073\n",
      "[15,   115] loss: 36373.920\n",
      "[15,   120] loss: 60403.389\n",
      "[15,   125] loss: 52983.364\n",
      "[15,   130] loss: 32292.752\n",
      "[15,   135] loss: 36173.717\n",
      "[15,   140] loss: 34591.463\n",
      "[15,   145] loss: 33951.253\n",
      "[15,   150] loss: 39048.243\n",
      "[15,   155] loss: 32269.228\n",
      "[15,   160] loss: 60198.152\n",
      "[15,   165] loss: 46798.391\n",
      "[15,   170] loss: 35921.334\n",
      "[15,   175] loss: 50179.147\n",
      "[15,   180] loss: 45112.887\n",
      "[15,   185] loss: 46308.657\n",
      "[15,   190] loss: 33212.009\n",
      "[15,   195] loss: 37061.787\n",
      "[15,   200] loss: 33140.676\n",
      "[15,   205] loss: 33053.928\n",
      "[15,   210] loss: 35554.444\n",
      "[15,   215] loss: 41050.602\n",
      "[15,   220] loss: 42594.030\n",
      "[15,   225] loss: 43336.563\n",
      "[15,   230] loss: 36470.477\n",
      "[16,     5] loss: 36247.536\n",
      "[16,    10] loss: 28302.386\n",
      "[16,    15] loss: 38761.179\n",
      "[16,    20] loss: 34947.477\n",
      "[16,    25] loss: 49284.312\n",
      "[16,    30] loss: 43712.860\n",
      "[16,    35] loss: 41307.686\n",
      "[16,    40] loss: 51660.168\n",
      "[16,    45] loss: 33340.373\n",
      "[16,    50] loss: 46219.663\n",
      "[16,    55] loss: 33933.977\n",
      "[16,    60] loss: 35716.473\n",
      "[16,    65] loss: 42861.029\n",
      "[16,    70] loss: 30309.285\n",
      "[16,    75] loss: 28473.198\n",
      "[16,    80] loss: 43286.479\n",
      "[16,    85] loss: 57974.053\n",
      "[16,    90] loss: 31362.298\n",
      "[16,    95] loss: 34756.364\n",
      "[16,   100] loss: 39852.501\n",
      "[16,   105] loss: 39432.607\n",
      "[16,   110] loss: 39291.954\n",
      "[16,   115] loss: 42919.115\n",
      "[16,   120] loss: 29446.875\n",
      "[16,   125] loss: 36617.642\n",
      "[16,   130] loss: 40889.931\n",
      "[16,   135] loss: 41706.657\n",
      "[16,   140] loss: 43373.273\n",
      "[16,   145] loss: 42063.261\n",
      "[16,   150] loss: 39483.289\n",
      "[16,   155] loss: 51702.713\n",
      "[16,   160] loss: 50867.063\n",
      "[16,   165] loss: 39782.292\n",
      "[16,   170] loss: 49533.205\n",
      "[16,   175] loss: 48632.661\n",
      "[16,   180] loss: 36551.291\n",
      "[16,   185] loss: 41979.884\n",
      "[16,   190] loss: 40073.861\n",
      "[16,   195] loss: 37742.830\n",
      "[16,   200] loss: 42729.356\n",
      "[16,   205] loss: 30814.104\n",
      "[16,   210] loss: 61977.471\n",
      "[16,   215] loss: 35566.540\n",
      "[16,   220] loss: 37650.007\n",
      "[16,   225] loss: 63647.484\n",
      "[16,   230] loss: 44014.454\n",
      "[17,     5] loss: 39396.613\n",
      "[17,    10] loss: 47891.556\n",
      "[17,    15] loss: 30236.063\n",
      "[17,    20] loss: 35660.900\n",
      "[17,    25] loss: 26753.896\n",
      "[17,    30] loss: 41686.593\n",
      "[17,    35] loss: 53413.440\n",
      "[17,    40] loss: 39789.995\n",
      "[17,    45] loss: 47918.146\n",
      "[17,    50] loss: 51769.210\n",
      "[17,    55] loss: 34362.605\n",
      "[17,    60] loss: 50563.455\n",
      "[17,    65] loss: 35665.403\n",
      "[17,    70] loss: 36759.615\n",
      "[17,    75] loss: 42712.789\n",
      "[17,    80] loss: 43182.848\n",
      "[17,    85] loss: 42477.381\n",
      "[17,    90] loss: 34937.745\n",
      "[17,    95] loss: 35330.836\n",
      "[17,   100] loss: 41207.934\n",
      "[17,   105] loss: 42106.380\n",
      "[17,   110] loss: 37769.310\n",
      "[17,   115] loss: 40846.482\n",
      "[17,   120] loss: 37241.079\n",
      "[17,   125] loss: 38824.497\n",
      "[17,   130] loss: 41670.267\n",
      "[17,   135] loss: 39170.697\n",
      "[17,   140] loss: 26840.536\n",
      "[17,   145] loss: 58911.427\n",
      "[17,   150] loss: 35426.079\n",
      "[17,   155] loss: 38025.752\n",
      "[17,   160] loss: 47504.148\n",
      "[17,   165] loss: 31537.746\n",
      "[17,   170] loss: 28831.411\n",
      "[17,   175] loss: 30956.743\n",
      "[17,   180] loss: 44422.027\n",
      "[17,   185] loss: 46876.558\n",
      "[17,   190] loss: 30574.611\n",
      "[17,   195] loss: 37089.769\n",
      "[17,   200] loss: 46934.554\n",
      "[17,   205] loss: 41324.426\n",
      "[17,   210] loss: 41475.827\n",
      "[17,   215] loss: 40810.217\n",
      "[17,   220] loss: 57364.955\n",
      "[17,   225] loss: 42425.854\n",
      "[17,   230] loss: 39891.852\n",
      "[18,     5] loss: 42872.597\n",
      "[18,    10] loss: 31864.412\n",
      "[18,    15] loss: 35397.111\n",
      "[18,    20] loss: 35202.216\n",
      "[18,    25] loss: 42717.328\n",
      "[18,    30] loss: 40266.809\n",
      "[18,    35] loss: 34759.608\n",
      "[18,    40] loss: 34025.862\n",
      "[18,    45] loss: 26842.154\n",
      "[18,    50] loss: 35361.004\n",
      "[18,    55] loss: 48024.745\n",
      "[18,    60] loss: 45298.614\n",
      "[18,    65] loss: 32966.299\n",
      "[18,    70] loss: 36446.145\n",
      "[18,    75] loss: 36130.290\n",
      "[18,    80] loss: 30206.261\n",
      "[18,    85] loss: 29239.271\n",
      "[18,    90] loss: 26665.870\n",
      "[18,    95] loss: 73545.732\n",
      "[18,   100] loss: 38420.235\n",
      "[18,   105] loss: 36400.557\n",
      "[18,   110] loss: 41194.018\n",
      "[18,   115] loss: 47277.612\n",
      "[18,   120] loss: 43149.410\n",
      "[18,   125] loss: 39320.400\n",
      "[18,   130] loss: 52105.891\n",
      "[18,   135] loss: 34020.829\n",
      "[18,   140] loss: 63219.298\n",
      "[18,   145] loss: 43725.155\n",
      "[18,   150] loss: 46258.508\n",
      "[18,   155] loss: 51609.271\n",
      "[18,   160] loss: 33938.148\n",
      "[18,   165] loss: 44775.139\n",
      "[18,   170] loss: 36568.684\n",
      "[18,   175] loss: 41943.808\n",
      "[18,   180] loss: 43662.545\n",
      "[18,   185] loss: 34632.898\n",
      "[18,   190] loss: 40732.115\n",
      "[18,   195] loss: 40489.173\n",
      "[18,   200] loss: 35338.373\n",
      "[18,   205] loss: 37373.887\n",
      "[18,   210] loss: 30824.792\n",
      "[18,   215] loss: 37133.964\n",
      "[18,   220] loss: 38232.810\n",
      "[18,   225] loss: 41824.443\n",
      "[18,   230] loss: 27262.573\n",
      "[19,     5] loss: 52100.021\n",
      "[19,    10] loss: 50464.780\n",
      "[19,    15] loss: 41268.963\n",
      "[19,    20] loss: 35321.180\n",
      "[19,    25] loss: 37156.280\n",
      "[19,    30] loss: 47325.243\n",
      "[19,    35] loss: 39406.936\n",
      "[19,    40] loss: 41613.800\n",
      "[19,    45] loss: 27708.059\n",
      "[19,    50] loss: 49450.427\n",
      "[19,    55] loss: 37018.471\n",
      "[19,    60] loss: 29654.972\n",
      "[19,    65] loss: 39587.437\n",
      "[19,    70] loss: 33774.224\n",
      "[19,    75] loss: 33847.220\n",
      "[19,    80] loss: 45476.600\n",
      "[19,    85] loss: 46604.239\n",
      "[19,    90] loss: 29778.465\n",
      "[19,    95] loss: 32824.116\n",
      "[19,   100] loss: 36085.990\n",
      "[19,   105] loss: 48690.746\n",
      "[19,   110] loss: 25431.521\n",
      "[19,   115] loss: 20005.551\n",
      "[19,   120] loss: 57054.614\n",
      "[19,   125] loss: 51903.669\n",
      "[19,   130] loss: 29697.085\n",
      "[19,   135] loss: 27024.517\n",
      "[19,   140] loss: 35603.718\n",
      "[19,   145] loss: 37149.273\n",
      "[19,   150] loss: 28138.061\n",
      "[19,   155] loss: 44201.148\n",
      "[19,   160] loss: 41751.446\n",
      "[19,   165] loss: 43136.670\n",
      "[19,   170] loss: 31667.386\n",
      "[19,   175] loss: 55939.925\n",
      "[19,   180] loss: 61820.570\n",
      "[19,   185] loss: 34597.375\n",
      "[19,   190] loss: 27785.720\n",
      "[19,   195] loss: 28761.584\n",
      "[19,   200] loss: 32797.002\n",
      "[19,   205] loss: 37660.671\n",
      "[19,   210] loss: 39421.608\n",
      "[19,   215] loss: 36465.075\n",
      "[19,   220] loss: 43965.906\n",
      "[19,   225] loss: 39222.162\n",
      "[19,   230] loss: 52817.966\n",
      "[20,     5] loss: 35113.527\n",
      "[20,    10] loss: 36773.260\n",
      "[20,    15] loss: 40881.193\n",
      "[20,    20] loss: 47536.285\n",
      "[20,    25] loss: 24487.829\n",
      "[20,    30] loss: 36478.517\n",
      "[20,    35] loss: 54865.385\n",
      "[20,    40] loss: 36853.398\n",
      "[20,    45] loss: 37595.776\n",
      "[20,    50] loss: 35102.454\n",
      "[20,    55] loss: 36335.123\n",
      "[20,    60] loss: 45915.436\n",
      "[20,    65] loss: 36830.782\n",
      "[20,    70] loss: 35152.646\n",
      "[20,    75] loss: 33235.031\n",
      "[20,    80] loss: 48992.621\n",
      "[20,    85] loss: 48263.798\n",
      "[20,    90] loss: 43267.133\n",
      "[20,    95] loss: 42514.738\n",
      "[20,   100] loss: 37236.513\n",
      "[20,   105] loss: 47247.143\n",
      "[20,   110] loss: 27997.157\n",
      "[20,   115] loss: 30891.442\n",
      "[20,   120] loss: 33267.374\n",
      "[20,   125] loss: 44290.968\n",
      "[20,   130] loss: 48022.031\n",
      "[20,   135] loss: 30558.609\n",
      "[20,   140] loss: 36331.311\n",
      "[20,   145] loss: 26248.787\n",
      "[20,   150] loss: 35867.000\n",
      "[20,   155] loss: 24770.779\n",
      "[20,   160] loss: 25610.913\n",
      "[20,   165] loss: 52955.821\n",
      "[20,   170] loss: 37882.507\n",
      "[20,   175] loss: 48107.314\n",
      "[20,   180] loss: 43276.688\n",
      "[20,   185] loss: 41594.128\n",
      "[20,   190] loss: 23211.736\n",
      "[20,   195] loss: 44540.277\n",
      "[20,   200] loss: 33735.168\n",
      "[20,   205] loss: 46863.357\n",
      "[20,   210] loss: 31673.285\n",
      "[20,   215] loss: 51726.262\n",
      "[20,   220] loss: 55111.379\n",
      "[20,   225] loss: 32969.436\n",
      "[20,   230] loss: 29073.951\n",
      "[21,     5] loss: 35510.914\n",
      "[21,    10] loss: 31315.711\n",
      "[21,    15] loss: 37722.345\n",
      "[21,    20] loss: 33265.825\n",
      "[21,    25] loss: 41181.399\n",
      "[21,    30] loss: 32922.755\n",
      "[21,    35] loss: 41327.383\n",
      "[21,    40] loss: 61406.162\n",
      "[21,    45] loss: 34627.659\n",
      "[21,    50] loss: 36222.902\n",
      "[21,    55] loss: 37544.666\n",
      "[21,    60] loss: 41544.102\n",
      "[21,    65] loss: 41283.054\n",
      "[21,    70] loss: 42934.936\n",
      "[21,    75] loss: 31099.391\n",
      "[21,    80] loss: 51757.033\n",
      "[21,    85] loss: 37677.844\n",
      "[21,    90] loss: 50519.225\n",
      "[21,    95] loss: 34204.622\n",
      "[21,   100] loss: 39041.170\n",
      "[21,   105] loss: 44160.813\n",
      "[21,   110] loss: 49205.961\n",
      "[21,   115] loss: 36581.798\n",
      "[21,   120] loss: 29791.177\n",
      "[21,   125] loss: 34226.824\n",
      "[21,   130] loss: 43747.898\n",
      "[21,   135] loss: 39673.933\n",
      "[21,   140] loss: 25225.341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21,   145] loss: 31946.324\n",
      "[21,   150] loss: 40999.861\n",
      "[21,   155] loss: 41332.935\n",
      "[21,   160] loss: 39090.664\n",
      "[21,   165] loss: 30573.324\n",
      "[21,   170] loss: 32407.520\n",
      "[21,   175] loss: 28235.217\n",
      "[21,   180] loss: 51399.956\n",
      "[21,   185] loss: 37896.845\n",
      "[21,   190] loss: 43149.404\n",
      "[21,   195] loss: 36850.372\n",
      "[21,   200] loss: 33049.880\n",
      "[21,   205] loss: 21799.756\n",
      "[21,   210] loss: 29505.797\n",
      "[21,   215] loss: 35568.318\n",
      "[21,   220] loss: 25145.159\n",
      "[21,   225] loss: 37463.527\n",
      "[21,   230] loss: 52063.292\n",
      "[22,     5] loss: 38482.213\n",
      "[22,    10] loss: 27672.718\n",
      "[22,    15] loss: 44010.842\n",
      "[22,    20] loss: 32614.368\n",
      "[22,    25] loss: 37473.883\n",
      "[22,    30] loss: 41752.629\n",
      "[22,    35] loss: 28275.724\n",
      "[22,    40] loss: 41051.395\n",
      "[22,    45] loss: 37982.180\n",
      "[22,    50] loss: 36054.248\n",
      "[22,    55] loss: 25309.370\n",
      "[22,    60] loss: 25847.744\n",
      "[22,    65] loss: 42550.523\n",
      "[22,    70] loss: 34262.299\n",
      "[22,    75] loss: 44871.290\n",
      "[22,    80] loss: 29684.396\n",
      "[22,    85] loss: 37559.406\n",
      "[22,    90] loss: 28555.616\n",
      "[22,    95] loss: 23754.308\n",
      "[22,   100] loss: 32989.336\n",
      "[22,   105] loss: 44078.294\n",
      "[22,   110] loss: 31743.616\n",
      "[22,   115] loss: 32862.039\n",
      "[22,   120] loss: 43139.374\n",
      "[22,   125] loss: 37125.809\n",
      "[22,   130] loss: 44979.617\n",
      "[22,   135] loss: 38333.591\n",
      "[22,   140] loss: 30073.551\n",
      "[22,   145] loss: 35498.622\n",
      "[22,   150] loss: 24236.389\n",
      "[22,   155] loss: 48601.017\n",
      "[22,   160] loss: 42061.097\n",
      "[22,   165] loss: 44744.307\n",
      "[22,   170] loss: 33908.328\n",
      "[22,   175] loss: 48354.631\n",
      "[22,   180] loss: 39328.334\n",
      "[22,   185] loss: 39477.540\n",
      "[22,   190] loss: 35255.236\n",
      "[22,   195] loss: 35939.856\n",
      "[22,   200] loss: 51370.665\n",
      "[22,   205] loss: 49339.257\n",
      "[22,   210] loss: 44709.721\n",
      "[22,   215] loss: 23850.960\n",
      "[22,   220] loss: 30975.461\n",
      "[22,   225] loss: 54746.075\n",
      "[22,   230] loss: 52400.617\n",
      "[23,     5] loss: 35531.732\n",
      "[23,    10] loss: 32419.168\n",
      "[23,    15] loss: 53577.948\n",
      "[23,    20] loss: 31422.845\n",
      "[23,    25] loss: 26600.325\n",
      "[23,    30] loss: 63626.180\n",
      "[23,    35] loss: 37270.729\n",
      "[23,    40] loss: 33256.054\n",
      "[23,    45] loss: 27581.083\n",
      "[23,    50] loss: 36223.076\n",
      "[23,    55] loss: 41748.007\n",
      "[23,    60] loss: 30137.877\n",
      "[23,    65] loss: 52027.118\n",
      "[23,    70] loss: 36518.314\n",
      "[23,    75] loss: 36577.425\n",
      "[23,    80] loss: 40761.309\n",
      "[23,    85] loss: 28958.196\n",
      "[23,    90] loss: 37731.331\n",
      "[23,    95] loss: 37580.596\n",
      "[23,   100] loss: 28827.014\n",
      "[23,   105] loss: 29296.213\n",
      "[23,   110] loss: 44553.002\n",
      "[23,   115] loss: 40598.714\n",
      "[23,   120] loss: 33211.421\n",
      "[23,   125] loss: 46337.524\n",
      "[23,   130] loss: 28718.396\n",
      "[23,   135] loss: 34591.662\n",
      "[23,   140] loss: 31336.238\n",
      "[23,   145] loss: 37677.124\n",
      "[23,   150] loss: 33462.024\n",
      "[23,   155] loss: 31159.616\n",
      "[23,   160] loss: 40639.405\n",
      "[23,   165] loss: 31602.361\n",
      "[23,   170] loss: 41448.435\n",
      "[23,   175] loss: 55634.504\n",
      "[23,   180] loss: 51447.094\n",
      "[23,   185] loss: 50153.721\n",
      "[23,   190] loss: 30509.059\n",
      "[23,   195] loss: 26352.345\n",
      "[23,   200] loss: 42287.764\n",
      "[23,   205] loss: 35474.098\n",
      "[23,   210] loss: 35399.836\n",
      "[23,   215] loss: 27738.852\n",
      "[23,   220] loss: 28197.550\n",
      "[23,   225] loss: 43502.971\n",
      "[23,   230] loss: 25576.840\n",
      "[24,     5] loss: 34163.290\n",
      "[24,    10] loss: 29968.654\n",
      "[24,    15] loss: 43178.871\n",
      "[24,    20] loss: 39746.525\n",
      "[24,    25] loss: 33910.622\n",
      "[24,    30] loss: 34287.122\n",
      "[24,    35] loss: 49416.052\n",
      "[24,    40] loss: 36029.855\n",
      "[24,    45] loss: 50338.480\n",
      "[24,    50] loss: 34241.313\n",
      "[24,    55] loss: 32619.417\n",
      "[24,    60] loss: 40155.971\n",
      "[24,    65] loss: 34658.045\n",
      "[24,    70] loss: 46586.784\n",
      "[24,    75] loss: 39169.475\n",
      "[24,    80] loss: 37334.286\n",
      "[24,    85] loss: 36205.993\n",
      "[24,    90] loss: 30053.606\n",
      "[24,    95] loss: 28339.089\n",
      "[24,   100] loss: 32013.420\n",
      "[24,   105] loss: 35497.741\n",
      "[24,   110] loss: 47971.650\n",
      "[24,   115] loss: 42009.021\n",
      "[24,   120] loss: 38802.307\n",
      "[24,   125] loss: 29772.986\n",
      "[24,   130] loss: 33333.779\n",
      "[24,   135] loss: 34658.907\n",
      "[24,   140] loss: 48466.523\n",
      "[24,   145] loss: 37565.040\n",
      "[24,   150] loss: 34556.926\n",
      "[24,   155] loss: 33021.709\n",
      "[24,   160] loss: 45109.933\n",
      "[24,   165] loss: 54815.783\n",
      "[24,   170] loss: 34880.347\n",
      "[24,   175] loss: 28736.084\n",
      "[24,   180] loss: 43177.803\n",
      "[24,   185] loss: 35420.223\n",
      "[24,   190] loss: 36016.786\n",
      "[24,   195] loss: 28416.741\n",
      "[24,   200] loss: 38637.721\n",
      "[24,   205] loss: 35595.064\n",
      "[24,   210] loss: 44862.707\n",
      "[24,   215] loss: 26007.316\n",
      "[24,   220] loss: 32359.562\n",
      "[24,   225] loss: 30462.468\n",
      "[24,   230] loss: 31679.237\n",
      "[25,     5] loss: 44326.060\n",
      "[25,    10] loss: 37363.188\n",
      "[25,    15] loss: 26393.622\n",
      "[25,    20] loss: 28057.878\n",
      "[25,    25] loss: 42487.316\n",
      "[25,    30] loss: 39761.300\n",
      "[25,    35] loss: 22892.364\n",
      "[25,    40] loss: 47673.713\n",
      "[25,    45] loss: 25444.415\n",
      "[25,    50] loss: 35461.345\n",
      "[25,    55] loss: 26425.815\n",
      "[25,    60] loss: 18667.978\n",
      "[25,    65] loss: 47039.796\n",
      "[25,    70] loss: 32224.729\n",
      "[25,    75] loss: 26777.129\n",
      "[25,    80] loss: 29852.584\n",
      "[25,    85] loss: 53275.636\n",
      "[25,    90] loss: 34244.570\n",
      "[25,    95] loss: 22935.471\n",
      "[25,   100] loss: 37056.312\n",
      "[25,   105] loss: 34468.546\n",
      "[25,   110] loss: 36322.931\n",
      "[25,   115] loss: 36768.912\n",
      "[25,   120] loss: 32946.802\n",
      "[25,   125] loss: 44226.321\n",
      "[25,   130] loss: 32663.345\n",
      "[25,   135] loss: 39812.007\n",
      "[25,   140] loss: 26044.933\n",
      "[25,   145] loss: 40171.318\n",
      "[25,   150] loss: 50951.415\n",
      "[25,   155] loss: 42473.247\n",
      "[25,   160] loss: 32930.197\n",
      "[25,   165] loss: 44625.698\n",
      "[25,   170] loss: 27476.245\n",
      "[25,   175] loss: 32265.308\n",
      "[25,   180] loss: 47898.688\n",
      "[25,   185] loss: 35577.963\n",
      "[25,   190] loss: 25857.161\n",
      "[25,   195] loss: 37845.941\n",
      "[25,   200] loss: 41724.945\n",
      "[25,   205] loss: 37352.996\n",
      "[25,   210] loss: 37080.778\n",
      "[25,   215] loss: 40276.487\n",
      "[25,   220] loss: 42231.941\n",
      "[25,   225] loss: 71462.506\n",
      "[25,   230] loss: 29734.404\n",
      "[26,     5] loss: 27662.350\n",
      "[26,    10] loss: 31358.036\n",
      "[26,    15] loss: 37782.934\n",
      "[26,    20] loss: 32392.183\n",
      "[26,    25] loss: 42914.088\n",
      "[26,    30] loss: 37010.898\n",
      "[26,    35] loss: 22773.534\n",
      "[26,    40] loss: 35808.938\n",
      "[26,    45] loss: 27136.978\n",
      "[26,    50] loss: 50966.225\n",
      "[26,    55] loss: 31982.750\n",
      "[26,    60] loss: 29257.803\n",
      "[26,    65] loss: 32175.436\n",
      "[26,    70] loss: 49331.192\n",
      "[26,    75] loss: 27166.163\n",
      "[26,    80] loss: 29205.702\n",
      "[26,    85] loss: 42047.440\n",
      "[26,    90] loss: 43946.019\n",
      "[26,    95] loss: 34794.057\n",
      "[26,   100] loss: 33703.109\n",
      "[26,   105] loss: 45115.640\n",
      "[26,   110] loss: 34234.527\n",
      "[26,   115] loss: 39187.889\n",
      "[26,   120] loss: 22377.000\n",
      "[26,   125] loss: 40450.059\n",
      "[26,   130] loss: 42058.599\n",
      "[26,   135] loss: 33000.251\n",
      "[26,   140] loss: 39105.607\n",
      "[26,   145] loss: 40486.101\n",
      "[26,   150] loss: 49418.746\n",
      "[26,   155] loss: 31414.760\n",
      "[26,   160] loss: 50054.056\n",
      "[26,   165] loss: 31203.700\n",
      "[26,   170] loss: 41415.412\n",
      "[26,   175] loss: 30003.131\n",
      "[26,   180] loss: 35853.242\n",
      "[26,   185] loss: 27056.969\n",
      "[26,   190] loss: 39453.240\n",
      "[26,   195] loss: 28184.805\n",
      "[26,   200] loss: 45113.945\n",
      "[26,   205] loss: 23359.858\n",
      "[26,   210] loss: 41507.238\n",
      "[26,   215] loss: 41970.548\n",
      "[26,   220] loss: 37301.612\n",
      "[26,   225] loss: 35633.095\n",
      "[26,   230] loss: 36254.117\n",
      "[27,     5] loss: 31769.075\n",
      "[27,    10] loss: 23316.534\n",
      "[27,    15] loss: 34751.735\n",
      "[27,    20] loss: 27901.040\n",
      "[27,    25] loss: 32834.562\n",
      "[27,    30] loss: 42302.121\n",
      "[27,    35] loss: 42387.748\n",
      "[27,    40] loss: 32838.927\n",
      "[27,    45] loss: 34998.099\n",
      "[27,    50] loss: 24412.534\n",
      "[27,    55] loss: 39579.914\n",
      "[27,    60] loss: 54178.542\n",
      "[27,    65] loss: 29131.170\n",
      "[27,    70] loss: 24883.671\n",
      "[27,    75] loss: 48094.227\n",
      "[27,    80] loss: 36914.709\n",
      "[27,    85] loss: 36572.355\n",
      "[27,    90] loss: 47340.908\n",
      "[27,    95] loss: 42164.812\n",
      "[27,   100] loss: 41041.296\n",
      "[27,   105] loss: 40425.750\n",
      "[27,   110] loss: 26576.995\n",
      "[27,   115] loss: 39324.185\n",
      "[27,   120] loss: 29819.291\n",
      "[27,   125] loss: 51383.367\n",
      "[27,   130] loss: 31928.686\n",
      "[27,   135] loss: 60273.648\n",
      "[27,   140] loss: 31599.361\n",
      "[27,   145] loss: 38674.381\n",
      "[27,   150] loss: 28774.492\n",
      "[27,   155] loss: 30979.959\n",
      "[27,   160] loss: 35525.854\n",
      "[27,   165] loss: 30512.374\n",
      "[27,   170] loss: 34606.273\n",
      "[27,   175] loss: 28865.294\n",
      "[27,   180] loss: 34169.840\n",
      "[27,   185] loss: 35629.959\n",
      "[27,   190] loss: 38549.956\n",
      "[27,   195] loss: 29994.577\n",
      "[27,   200] loss: 35465.879\n",
      "[27,   205] loss: 30325.028\n",
      "[27,   210] loss: 32497.193\n",
      "[27,   215] loss: 35160.120\n",
      "[27,   220] loss: 35655.776\n",
      "[27,   225] loss: 27606.770\n",
      "[27,   230] loss: 38259.195\n",
      "[28,     5] loss: 26597.004\n",
      "[28,    10] loss: 32444.267\n",
      "[28,    15] loss: 41261.386\n",
      "[28,    20] loss: 56739.350\n",
      "[28,    25] loss: 36501.328\n",
      "[28,    30] loss: 26693.105\n",
      "[28,    35] loss: 28896.136\n",
      "[28,    40] loss: 36683.495\n",
      "[28,    45] loss: 34498.154\n",
      "[28,    50] loss: 46546.255\n",
      "[28,    55] loss: 32376.884\n",
      "[28,    60] loss: 40064.288\n",
      "[28,    65] loss: 35297.048\n",
      "[28,    70] loss: 26732.536\n",
      "[28,    75] loss: 44624.766\n",
      "[28,    80] loss: 29424.270\n",
      "[28,    85] loss: 40059.540\n",
      "[28,    90] loss: 35404.393\n",
      "[28,    95] loss: 32526.769\n",
      "[28,   100] loss: 33454.812\n",
      "[28,   105] loss: 33373.711\n",
      "[28,   110] loss: 29303.187\n",
      "[28,   115] loss: 25459.936\n",
      "[28,   120] loss: 27981.225\n",
      "[28,   125] loss: 55520.662\n",
      "[28,   130] loss: 25095.617\n",
      "[28,   135] loss: 33644.501\n",
      "[28,   140] loss: 28939.274\n",
      "[28,   145] loss: 54557.177\n",
      "[28,   150] loss: 26574.966\n",
      "[28,   155] loss: 28720.150\n",
      "[28,   160] loss: 40872.725\n",
      "[28,   165] loss: 21443.316\n",
      "[28,   170] loss: 36757.580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28,   175] loss: 27734.196\n",
      "[28,   180] loss: 37273.591\n",
      "[28,   185] loss: 50315.718\n",
      "[28,   190] loss: 36214.043\n",
      "[28,   195] loss: 35584.120\n",
      "[28,   200] loss: 31908.869\n",
      "[28,   205] loss: 31125.578\n",
      "[28,   210] loss: 35256.746\n",
      "[28,   215] loss: 42340.693\n",
      "[28,   220] loss: 33773.304\n",
      "[28,   225] loss: 26505.222\n",
      "[28,   230] loss: 34710.288\n",
      "[29,     5] loss: 32372.105\n",
      "[29,    10] loss: 34053.012\n",
      "[29,    15] loss: 24667.892\n",
      "[29,    20] loss: 26651.885\n",
      "[29,    25] loss: 37786.178\n",
      "[29,    30] loss: 39505.205\n",
      "[29,    35] loss: 28305.799\n",
      "[29,    40] loss: 26804.166\n",
      "[29,    45] loss: 51901.547\n",
      "[29,    50] loss: 52720.732\n",
      "[29,    55] loss: 29329.673\n",
      "[29,    60] loss: 35180.380\n",
      "[29,    65] loss: 44556.084\n",
      "[29,    70] loss: 33195.306\n",
      "[29,    75] loss: 20825.320\n",
      "[29,    80] loss: 25983.787\n",
      "[29,    85] loss: 25564.256\n",
      "[29,    90] loss: 32880.248\n",
      "[29,    95] loss: 37689.796\n",
      "[29,   100] loss: 30231.493\n",
      "[29,   105] loss: 23612.262\n",
      "[29,   110] loss: 30622.319\n",
      "[29,   115] loss: 27922.422\n",
      "[29,   120] loss: 31265.839\n",
      "[29,   125] loss: 56953.166\n",
      "[29,   130] loss: 22352.625\n",
      "[29,   135] loss: 34424.345\n",
      "[29,   140] loss: 62966.848\n",
      "[29,   145] loss: 38405.859\n",
      "[29,   150] loss: 33577.355\n",
      "[29,   155] loss: 37203.473\n",
      "[29,   160] loss: 35523.691\n",
      "[29,   165] loss: 31147.749\n",
      "[29,   170] loss: 38537.345\n",
      "[29,   175] loss: 28856.878\n",
      "[29,   180] loss: 30655.107\n",
      "[29,   185] loss: 53389.524\n",
      "[29,   190] loss: 34089.009\n",
      "[29,   195] loss: 40736.366\n",
      "[29,   200] loss: 47374.820\n",
      "[29,   205] loss: 24131.139\n",
      "[29,   210] loss: 33789.670\n",
      "[29,   215] loss: 40353.309\n",
      "[29,   220] loss: 36084.698\n",
      "[29,   225] loss: 27535.010\n",
      "[29,   230] loss: 29306.767\n",
      "[30,     5] loss: 40202.152\n",
      "[30,    10] loss: 27376.358\n",
      "[30,    15] loss: 43056.999\n",
      "[30,    20] loss: 36596.498\n",
      "[30,    25] loss: 26941.687\n",
      "[30,    30] loss: 49772.613\n",
      "[30,    35] loss: 36783.218\n",
      "[30,    40] loss: 35796.982\n",
      "[30,    45] loss: 43832.992\n",
      "[30,    50] loss: 36340.519\n",
      "[30,    55] loss: 33928.158\n",
      "[30,    60] loss: 29691.947\n",
      "[30,    65] loss: 26863.242\n",
      "[30,    70] loss: 27440.093\n",
      "[30,    75] loss: 29408.590\n",
      "[30,    80] loss: 25912.368\n",
      "[30,    85] loss: 51414.926\n",
      "[30,    90] loss: 31580.120\n",
      "[30,    95] loss: 21610.325\n",
      "[30,   100] loss: 36497.585\n",
      "[30,   105] loss: 28773.379\n",
      "[30,   110] loss: 36329.039\n",
      "[30,   115] loss: 29632.455\n",
      "[30,   120] loss: 34813.383\n",
      "[30,   125] loss: 40707.514\n",
      "[30,   130] loss: 30968.907\n",
      "[30,   135] loss: 36941.579\n",
      "[30,   140] loss: 36857.691\n",
      "[30,   145] loss: 36305.728\n",
      "[30,   150] loss: 30388.614\n",
      "[30,   155] loss: 37749.048\n",
      "[30,   160] loss: 33408.679\n",
      "[30,   165] loss: 35439.486\n",
      "[30,   170] loss: 48265.903\n",
      "[30,   175] loss: 31436.325\n",
      "[30,   180] loss: 37379.250\n",
      "[30,   185] loss: 35838.681\n",
      "[30,   190] loss: 29012.217\n",
      "[30,   195] loss: 35378.446\n",
      "[30,   200] loss: 53882.937\n",
      "[30,   205] loss: 35939.551\n",
      "[30,   210] loss: 33896.746\n",
      "[30,   215] loss: 29849.000\n",
      "[30,   220] loss: 28685.271\n",
      "[30,   225] loss: 28748.562\n",
      "[30,   230] loss: 21515.023\n",
      "[31,     5] loss: 28844.804\n",
      "[31,    10] loss: 46305.687\n",
      "[31,    15] loss: 37901.318\n",
      "[31,    20] loss: 20322.314\n",
      "[31,    25] loss: 56189.173\n",
      "[31,    30] loss: 36963.115\n",
      "[31,    35] loss: 25989.985\n",
      "[31,    40] loss: 28975.130\n",
      "[31,    45] loss: 26352.482\n",
      "[31,    50] loss: 40547.054\n",
      "[31,    55] loss: 24647.574\n",
      "[31,    60] loss: 34202.562\n",
      "[31,    65] loss: 30498.196\n",
      "[31,    70] loss: 43169.245\n",
      "[31,    75] loss: 48241.252\n",
      "[31,    80] loss: 37464.523\n",
      "[31,    85] loss: 47632.434\n",
      "[31,    90] loss: 27941.960\n",
      "[31,    95] loss: 27824.830\n",
      "[31,   100] loss: 37671.204\n",
      "[31,   105] loss: 41060.514\n",
      "[31,   110] loss: 48844.496\n",
      "[31,   115] loss: 43456.971\n",
      "[31,   120] loss: 29392.736\n",
      "[31,   125] loss: 37565.714\n",
      "[31,   130] loss: 39988.311\n",
      "[31,   135] loss: 30089.711\n",
      "[31,   140] loss: 39128.101\n",
      "[31,   145] loss: 18108.919\n",
      "[31,   150] loss: 28952.022\n",
      "[31,   155] loss: 34718.694\n",
      "[31,   160] loss: 25095.967\n",
      "[31,   165] loss: 28689.714\n",
      "[31,   170] loss: 28085.566\n",
      "[31,   175] loss: 28790.099\n",
      "[31,   180] loss: 27455.570\n",
      "[31,   185] loss: 24652.193\n",
      "[31,   190] loss: 33359.901\n",
      "[31,   195] loss: 32508.329\n",
      "[31,   200] loss: 36627.986\n",
      "[31,   205] loss: 32097.916\n",
      "[31,   210] loss: 38795.850\n",
      "[31,   215] loss: 26620.390\n",
      "[31,   220] loss: 36522.691\n",
      "[31,   225] loss: 27679.118\n",
      "[31,   230] loss: 39531.174\n",
      "[32,     5] loss: 28280.755\n",
      "[32,    10] loss: 25069.418\n",
      "[32,    15] loss: 39413.901\n",
      "[32,    20] loss: 27031.053\n",
      "[32,    25] loss: 28597.120\n",
      "[32,    30] loss: 25347.961\n",
      "[32,    35] loss: 27539.746\n",
      "[32,    40] loss: 45410.333\n",
      "[32,    45] loss: 25915.816\n",
      "[32,    50] loss: 28613.035\n",
      "[32,    55] loss: 31956.121\n",
      "[32,    60] loss: 38883.320\n",
      "[32,    65] loss: 35729.869\n",
      "[32,    70] loss: 31858.162\n",
      "[32,    75] loss: 39908.458\n",
      "[32,    80] loss: 30182.479\n",
      "[32,    85] loss: 43337.684\n",
      "[32,    90] loss: 29579.594\n",
      "[32,    95] loss: 38805.559\n",
      "[32,   100] loss: 28974.376\n",
      "[32,   105] loss: 19245.556\n",
      "[32,   110] loss: 33518.646\n",
      "[32,   115] loss: 30417.618\n",
      "[32,   120] loss: 17658.269\n",
      "[32,   125] loss: 34150.340\n",
      "[32,   130] loss: 32820.633\n",
      "[32,   135] loss: 30707.321\n",
      "[32,   140] loss: 55177.318\n",
      "[32,   145] loss: 42513.631\n",
      "[32,   150] loss: 30815.992\n",
      "[32,   155] loss: 33929.421\n",
      "[32,   160] loss: 43600.650\n",
      "[32,   165] loss: 31511.896\n",
      "[32,   170] loss: 25663.132\n",
      "[32,   175] loss: 45007.267\n",
      "[32,   180] loss: 36872.379\n",
      "[32,   185] loss: 33952.385\n",
      "[32,   190] loss: 32018.111\n",
      "[32,   195] loss: 32398.405\n",
      "[32,   200] loss: 30635.009\n",
      "[32,   205] loss: 37980.682\n",
      "[32,   210] loss: 39710.549\n",
      "[32,   215] loss: 25690.979\n",
      "[32,   220] loss: 43544.366\n",
      "[32,   225] loss: 58514.528\n",
      "[32,   230] loss: 25299.952\n",
      "[33,     5] loss: 35957.823\n",
      "[33,    10] loss: 57957.175\n",
      "[33,    15] loss: 30879.226\n",
      "[33,    20] loss: 33059.539\n",
      "[33,    25] loss: 37304.677\n",
      "[33,    30] loss: 33714.731\n",
      "[33,    35] loss: 36414.035\n",
      "[33,    40] loss: 26940.771\n",
      "[33,    45] loss: 37441.250\n",
      "[33,    50] loss: 36236.638\n",
      "[33,    55] loss: 37087.753\n",
      "[33,    60] loss: 28157.939\n",
      "[33,    65] loss: 28620.500\n",
      "[33,    70] loss: 30348.109\n",
      "[33,    75] loss: 28760.102\n",
      "[33,    80] loss: 31820.817\n",
      "[33,    85] loss: 19544.599\n",
      "[33,    90] loss: 27915.392\n",
      "[33,    95] loss: 27956.855\n",
      "[33,   100] loss: 26891.689\n",
      "[33,   105] loss: 40753.498\n",
      "[33,   110] loss: 31925.110\n",
      "[33,   115] loss: 41137.551\n",
      "[33,   120] loss: 35102.109\n",
      "[33,   125] loss: 30861.158\n",
      "[33,   130] loss: 25824.017\n",
      "[33,   135] loss: 28229.209\n",
      "[33,   140] loss: 27192.497\n",
      "[33,   145] loss: 41810.137\n",
      "[33,   150] loss: 30758.575\n",
      "[33,   155] loss: 34380.799\n",
      "[33,   160] loss: 45926.552\n",
      "[33,   165] loss: 32965.404\n",
      "[33,   170] loss: 26389.370\n",
      "[33,   175] loss: 35742.884\n",
      "[33,   180] loss: 30095.647\n",
      "[33,   185] loss: 31552.129\n",
      "[33,   190] loss: 40878.268\n",
      "[33,   195] loss: 40729.938\n",
      "[33,   200] loss: 27300.059\n",
      "[33,   205] loss: 29665.323\n",
      "[33,   210] loss: 37514.440\n",
      "[33,   215] loss: 30220.960\n",
      "[33,   220] loss: 25683.800\n",
      "[33,   225] loss: 33404.302\n",
      "[33,   230] loss: 36975.689\n",
      "[34,     5] loss: 34323.224\n",
      "[34,    10] loss: 41947.592\n",
      "[34,    15] loss: 33745.221\n",
      "[34,    20] loss: 24066.837\n",
      "[34,    25] loss: 28731.854\n",
      "[34,    30] loss: 34830.961\n",
      "[34,    35] loss: 40692.852\n",
      "[34,    40] loss: 30618.869\n",
      "[34,    45] loss: 25701.278\n",
      "[34,    50] loss: 34217.515\n",
      "[34,    55] loss: 38386.083\n",
      "[34,    60] loss: 24853.851\n",
      "[34,    65] loss: 43096.312\n",
      "[34,    70] loss: 29436.280\n",
      "[34,    75] loss: 34926.822\n",
      "[34,    80] loss: 28474.155\n",
      "[34,    85] loss: 41485.507\n",
      "[34,    90] loss: 39438.281\n",
      "[34,    95] loss: 24595.203\n",
      "[34,   100] loss: 31689.337\n",
      "[34,   105] loss: 47287.101\n",
      "[34,   110] loss: 34182.851\n",
      "[34,   115] loss: 24193.835\n",
      "[34,   120] loss: 50112.734\n",
      "[34,   125] loss: 32538.644\n",
      "[34,   130] loss: 32380.522\n",
      "[34,   135] loss: 31723.188\n",
      "[34,   140] loss: 34388.908\n",
      "[34,   145] loss: 48369.118\n",
      "[34,   150] loss: 27278.365\n",
      "[34,   155] loss: 41661.363\n",
      "[34,   160] loss: 28894.301\n",
      "[34,   165] loss: 21456.947\n",
      "[34,   170] loss: 27120.025\n",
      "[34,   175] loss: 29656.515\n",
      "[34,   180] loss: 28443.743\n",
      "[34,   185] loss: 19348.693\n",
      "[34,   190] loss: 22451.907\n",
      "[34,   195] loss: 22950.429\n",
      "[34,   200] loss: 37383.891\n",
      "[34,   205] loss: 31788.112\n",
      "[34,   210] loss: 33409.814\n",
      "[34,   215] loss: 34435.705\n",
      "[34,   220] loss: 44112.890\n",
      "[34,   225] loss: 32386.163\n",
      "[34,   230] loss: 37690.888\n",
      "[35,     5] loss: 36066.513\n",
      "[35,    10] loss: 29987.850\n",
      "[35,    15] loss: 23641.090\n",
      "[35,    20] loss: 38223.656\n",
      "[35,    25] loss: 33713.872\n",
      "[35,    30] loss: 25368.018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,    35] loss: 36244.258\n",
      "[35,    40] loss: 22784.409\n",
      "[35,    45] loss: 23535.491\n",
      "[35,    50] loss: 21859.543\n",
      "[35,    55] loss: 55408.605\n",
      "[35,    60] loss: 28284.173\n",
      "[35,    65] loss: 24124.325\n",
      "[35,    70] loss: 32649.136\n",
      "[35,    75] loss: 26389.218\n",
      "[35,    80] loss: 27322.848\n",
      "[35,    85] loss: 27454.554\n",
      "[35,    90] loss: 40793.611\n",
      "[35,    95] loss: 28595.162\n",
      "[35,   100] loss: 39255.688\n",
      "[35,   105] loss: 30848.807\n",
      "[35,   110] loss: 24133.930\n",
      "[35,   115] loss: 32988.062\n",
      "[35,   120] loss: 47618.776\n",
      "[35,   125] loss: 49298.885\n",
      "[35,   130] loss: 28199.362\n",
      "[35,   135] loss: 31185.000\n",
      "[35,   140] loss: 34059.745\n",
      "[35,   145] loss: 42431.289\n",
      "[35,   150] loss: 32458.439\n",
      "[35,   155] loss: 22626.396\n",
      "[35,   160] loss: 42522.531\n",
      "[35,   165] loss: 38662.375\n",
      "[35,   170] loss: 42649.600\n",
      "[35,   175] loss: 37722.623\n",
      "[35,   180] loss: 33525.495\n",
      "[35,   185] loss: 30757.196\n",
      "[35,   190] loss: 28857.694\n",
      "[35,   195] loss: 36256.865\n",
      "[35,   200] loss: 31871.705\n",
      "[35,   205] loss: 37970.232\n",
      "[35,   210] loss: 29693.246\n",
      "[35,   215] loss: 22753.670\n",
      "[35,   220] loss: 36472.150\n",
      "[35,   225] loss: 23146.953\n",
      "[35,   230] loss: 31376.988\n",
      "[36,     5] loss: 40192.861\n",
      "[36,    10] loss: 41716.574\n",
      "[36,    15] loss: 23630.450\n",
      "[36,    20] loss: 27005.613\n",
      "[36,    25] loss: 39688.893\n",
      "[36,    30] loss: 44039.704\n",
      "[36,    35] loss: 33006.705\n",
      "[36,    40] loss: 23946.441\n",
      "[36,    45] loss: 20521.150\n",
      "[36,    50] loss: 31640.068\n",
      "[36,    55] loss: 26494.420\n",
      "[36,    60] loss: 33399.463\n",
      "[36,    65] loss: 41348.163\n",
      "[36,    70] loss: 41240.143\n",
      "[36,    75] loss: 36806.334\n",
      "[36,    80] loss: 36044.296\n",
      "[36,    85] loss: 32228.770\n",
      "[36,    90] loss: 26526.793\n",
      "[36,    95] loss: 22739.724\n",
      "[36,   100] loss: 71562.465\n",
      "[36,   105] loss: 41724.749\n",
      "[36,   110] loss: 23733.853\n",
      "[36,   115] loss: 29451.390\n",
      "[36,   120] loss: 19358.094\n",
      "[36,   125] loss: 35250.136\n",
      "[36,   130] loss: 24935.743\n",
      "[36,   135] loss: 41645.480\n",
      "[36,   140] loss: 33440.853\n",
      "[36,   145] loss: 22007.434\n",
      "[36,   150] loss: 31589.496\n",
      "[36,   155] loss: 42238.447\n",
      "[36,   160] loss: 20932.777\n",
      "[36,   165] loss: 31789.141\n",
      "[36,   170] loss: 45987.949\n",
      "[36,   175] loss: 43306.713\n",
      "[36,   180] loss: 24492.119\n",
      "[36,   185] loss: 31536.304\n",
      "[36,   190] loss: 29107.621\n",
      "[36,   195] loss: 25795.665\n",
      "[36,   200] loss: 25871.161\n",
      "[36,   205] loss: 22699.924\n",
      "[36,   210] loss: 38922.265\n",
      "[36,   215] loss: 25651.578\n",
      "[36,   220] loss: 25211.897\n",
      "[36,   225] loss: 28127.217\n",
      "[36,   230] loss: 33583.287\n",
      "[37,     5] loss: 29234.047\n",
      "[37,    10] loss: 43188.664\n",
      "[37,    15] loss: 26204.134\n",
      "[37,    20] loss: 46752.525\n",
      "[37,    25] loss: 40937.946\n",
      "[37,    30] loss: 29242.175\n",
      "[37,    35] loss: 36126.634\n",
      "[37,    40] loss: 29510.788\n",
      "[37,    45] loss: 42600.555\n",
      "[37,    50] loss: 37606.931\n",
      "[37,    55] loss: 25784.919\n",
      "[37,    60] loss: 35877.928\n",
      "[37,    65] loss: 36135.594\n",
      "[37,    70] loss: 29392.162\n",
      "[37,    75] loss: 36107.562\n",
      "[37,    80] loss: 34790.878\n",
      "[37,    85] loss: 22176.308\n",
      "[37,    90] loss: 24064.355\n",
      "[37,    95] loss: 34171.866\n",
      "[37,   100] loss: 28155.523\n",
      "[37,   105] loss: 34149.202\n",
      "[37,   110] loss: 37924.896\n",
      "[37,   115] loss: 22653.926\n",
      "[37,   120] loss: 42449.826\n",
      "[37,   125] loss: 27608.832\n",
      "[37,   130] loss: 23128.927\n",
      "[37,   135] loss: 26506.132\n",
      "[37,   140] loss: 40525.708\n",
      "[37,   145] loss: 38456.560\n",
      "[37,   150] loss: 32595.623\n",
      "[37,   155] loss: 32463.525\n",
      "[37,   160] loss: 30174.995\n",
      "[37,   165] loss: 22139.376\n",
      "[37,   170] loss: 39461.093\n",
      "[37,   175] loss: 24795.366\n",
      "[37,   180] loss: 28052.693\n",
      "[37,   185] loss: 38714.078\n",
      "[37,   190] loss: 24001.410\n",
      "[37,   195] loss: 34666.268\n",
      "[37,   200] loss: 47472.005\n",
      "[37,   205] loss: 31461.133\n",
      "[37,   210] loss: 18901.649\n",
      "[37,   215] loss: 30884.220\n",
      "[37,   220] loss: 21705.070\n",
      "[37,   225] loss: 32354.173\n",
      "[37,   230] loss: 19966.170\n",
      "[38,     5] loss: 20669.331\n",
      "[38,    10] loss: 22724.577\n",
      "[38,    15] loss: 37452.087\n",
      "[38,    20] loss: 39816.491\n",
      "[38,    25] loss: 36827.211\n",
      "[38,    30] loss: 32054.818\n",
      "[38,    35] loss: 24171.190\n",
      "[38,    40] loss: 18920.772\n",
      "[38,    45] loss: 28070.299\n",
      "[38,    50] loss: 37928.379\n",
      "[38,    55] loss: 24387.980\n",
      "[38,    60] loss: 32313.371\n",
      "[38,    65] loss: 34631.784\n",
      "[38,    70] loss: 28056.624\n",
      "[38,    75] loss: 28760.602\n",
      "[38,    80] loss: 31894.428\n",
      "[38,    85] loss: 35979.924\n",
      "[38,    90] loss: 40721.040\n",
      "[38,    95] loss: 24994.272\n",
      "[38,   100] loss: 26163.063\n",
      "[38,   105] loss: 27264.582\n",
      "[38,   110] loss: 17739.974\n",
      "[38,   115] loss: 30175.325\n",
      "[38,   120] loss: 32964.793\n",
      "[38,   125] loss: 35478.958\n",
      "[38,   130] loss: 38085.274\n",
      "[38,   135] loss: 39350.901\n",
      "[38,   140] loss: 26413.566\n",
      "[38,   145] loss: 27181.891\n",
      "[38,   150] loss: 30542.396\n",
      "[38,   155] loss: 50997.293\n",
      "[38,   160] loss: 24508.695\n",
      "[38,   165] loss: 41592.907\n",
      "[38,   170] loss: 27343.821\n",
      "[38,   175] loss: 39249.503\n",
      "[38,   180] loss: 38493.736\n",
      "[38,   185] loss: 25216.118\n",
      "[38,   190] loss: 26545.124\n",
      "[38,   195] loss: 18045.685\n",
      "[38,   200] loss: 30530.284\n",
      "[38,   205] loss: 27243.483\n",
      "[38,   210] loss: 23736.408\n",
      "[38,   215] loss: 40543.138\n",
      "[38,   220] loss: 43554.397\n",
      "[38,   225] loss: 33669.802\n",
      "[38,   230] loss: 48695.576\n",
      "[39,     5] loss: 30365.169\n",
      "[39,    10] loss: 25517.116\n",
      "[39,    15] loss: 18423.821\n",
      "[39,    20] loss: 30336.755\n",
      "[39,    25] loss: 27981.622\n",
      "[39,    30] loss: 21584.598\n",
      "[39,    35] loss: 34314.765\n",
      "[39,    40] loss: 20973.489\n",
      "[39,    45] loss: 50925.365\n",
      "[39,    50] loss: 40244.597\n",
      "[39,    55] loss: 38040.173\n",
      "[39,    60] loss: 34645.719\n",
      "[39,    65] loss: 41921.050\n",
      "[39,    70] loss: 31655.591\n",
      "[39,    75] loss: 23006.796\n",
      "[39,    80] loss: 18343.084\n",
      "[39,    85] loss: 39914.377\n",
      "[39,    90] loss: 22502.325\n",
      "[39,    95] loss: 36978.011\n",
      "[39,   100] loss: 36249.830\n",
      "[39,   105] loss: 33968.079\n",
      "[39,   110] loss: 36337.737\n",
      "[39,   115] loss: 28925.559\n",
      "[39,   120] loss: 39558.057\n",
      "[39,   125] loss: 40121.466\n",
      "[39,   130] loss: 35825.153\n",
      "[39,   135] loss: 33639.246\n",
      "[39,   140] loss: 54118.802\n",
      "[39,   145] loss: 24247.779\n",
      "[39,   150] loss: 22291.448\n",
      "[39,   155] loss: 20374.530\n",
      "[39,   160] loss: 37685.853\n",
      "[39,   165] loss: 32427.289\n",
      "[39,   170] loss: 23080.044\n",
      "[39,   175] loss: 24311.098\n",
      "[39,   180] loss: 43540.657\n",
      "[39,   185] loss: 35969.620\n",
      "[39,   190] loss: 34778.648\n",
      "[39,   195] loss: 35299.754\n",
      "[39,   200] loss: 37694.148\n",
      "[39,   205] loss: 24000.679\n",
      "[39,   210] loss: 20935.433\n",
      "[39,   215] loss: 30089.598\n",
      "[39,   220] loss: 26558.621\n",
      "[39,   225] loss: 19492.344\n",
      "[39,   230] loss: 33797.999\n",
      "[40,     5] loss: 24953.771\n",
      "[40,    10] loss: 25207.513\n",
      "[40,    15] loss: 21382.323\n",
      "[40,    20] loss: 32730.904\n",
      "[40,    25] loss: 28661.225\n",
      "[40,    30] loss: 27888.169\n",
      "[40,    35] loss: 25778.113\n",
      "[40,    40] loss: 43195.224\n",
      "[40,    45] loss: 23044.192\n",
      "[40,    50] loss: 54713.271\n",
      "[40,    55] loss: 31693.521\n",
      "[40,    60] loss: 33539.350\n",
      "[40,    65] loss: 34652.353\n",
      "[40,    70] loss: 28075.126\n",
      "[40,    75] loss: 28615.091\n",
      "[40,    80] loss: 30791.108\n",
      "[40,    85] loss: 20545.300\n",
      "[40,    90] loss: 26466.326\n",
      "[40,    95] loss: 34810.741\n",
      "[40,   100] loss: 20711.451\n",
      "[40,   105] loss: 20232.144\n",
      "[40,   110] loss: 31198.608\n",
      "[40,   115] loss: 24212.780\n",
      "[40,   120] loss: 39107.474\n",
      "[40,   125] loss: 30028.896\n",
      "[40,   130] loss: 27469.716\n",
      "[40,   135] loss: 37446.665\n",
      "[40,   140] loss: 55982.253\n",
      "[40,   145] loss: 37922.142\n",
      "[40,   150] loss: 32102.397\n",
      "[40,   155] loss: 41342.285\n",
      "[40,   160] loss: 20214.029\n",
      "[40,   165] loss: 37071.520\n",
      "[40,   170] loss: 25832.061\n",
      "[40,   175] loss: 31597.626\n",
      "[40,   180] loss: 25960.863\n",
      "[40,   185] loss: 21799.204\n",
      "[40,   190] loss: 25490.228\n",
      "[40,   195] loss: 32011.964\n",
      "[40,   200] loss: 41664.224\n",
      "[40,   205] loss: 47384.357\n",
      "[40,   210] loss: 26310.707\n",
      "[40,   215] loss: 27939.168\n",
      "[40,   220] loss: 29779.235\n",
      "[40,   225] loss: 22809.779\n",
      "[40,   230] loss: 26305.503\n",
      "[41,     5] loss: 24049.080\n",
      "[41,    10] loss: 31968.997\n",
      "[41,    15] loss: 28106.721\n",
      "[41,    20] loss: 46598.280\n",
      "[41,    25] loss: 39735.834\n",
      "[41,    30] loss: 32526.855\n",
      "[41,    35] loss: 35615.673\n",
      "[41,    40] loss: 24606.549\n",
      "[41,    45] loss: 39018.732\n",
      "[41,    50] loss: 31486.762\n",
      "[41,    55] loss: 38386.590\n",
      "[41,    60] loss: 28720.917\n",
      "[41,    65] loss: 26693.930\n",
      "[41,    70] loss: 48448.304\n",
      "[41,    75] loss: 38798.512\n",
      "[41,    80] loss: 34849.387\n",
      "[41,    85] loss: 21290.302\n",
      "[41,    90] loss: 19893.602\n",
      "[41,    95] loss: 26731.772\n",
      "[41,   100] loss: 28670.275\n",
      "[41,   105] loss: 28510.815\n",
      "[41,   110] loss: 44785.105\n",
      "[41,   115] loss: 24973.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41,   120] loss: 38676.591\n",
      "[41,   125] loss: 30468.478\n",
      "[41,   130] loss: 32020.884\n",
      "[41,   135] loss: 20779.633\n",
      "[41,   140] loss: 25480.553\n",
      "[41,   145] loss: 35844.085\n",
      "[41,   150] loss: 29555.586\n",
      "[41,   155] loss: 19678.831\n",
      "[41,   160] loss: 30740.026\n",
      "[41,   165] loss: 25046.248\n",
      "[41,   170] loss: 27097.273\n",
      "[41,   175] loss: 23840.723\n",
      "[41,   180] loss: 28035.136\n",
      "[41,   185] loss: 25180.858\n",
      "[41,   190] loss: 32399.180\n",
      "[41,   195] loss: 39525.560\n",
      "[41,   200] loss: 32885.843\n",
      "[41,   205] loss: 24539.981\n",
      "[41,   210] loss: 25404.341\n",
      "[41,   215] loss: 34135.478\n",
      "[41,   220] loss: 27742.334\n",
      "[41,   225] loss: 31238.115\n",
      "[41,   230] loss: 36634.733\n",
      "[42,     5] loss: 46972.023\n",
      "[42,    10] loss: 28769.566\n",
      "[42,    15] loss: 33708.307\n",
      "[42,    20] loss: 31385.507\n",
      "[42,    25] loss: 39602.933\n",
      "[42,    30] loss: 23946.218\n",
      "[42,    35] loss: 36969.051\n",
      "[42,    40] loss: 29491.300\n",
      "[42,    45] loss: 33126.284\n",
      "[42,    50] loss: 33758.932\n",
      "[42,    55] loss: 27821.346\n",
      "[42,    60] loss: 29639.957\n",
      "[42,    65] loss: 23485.072\n",
      "[42,    70] loss: 32119.245\n",
      "[42,    75] loss: 32829.855\n",
      "[42,    80] loss: 25931.434\n",
      "[42,    85] loss: 28981.970\n",
      "[42,    90] loss: 25897.308\n",
      "[42,    95] loss: 38182.388\n",
      "[42,   100] loss: 18711.854\n",
      "[42,   105] loss: 30820.800\n",
      "[42,   110] loss: 20052.182\n",
      "[42,   115] loss: 36422.745\n",
      "[42,   120] loss: 27829.221\n",
      "[42,   125] loss: 30585.548\n",
      "[42,   130] loss: 22362.202\n",
      "[42,   135] loss: 28967.767\n",
      "[42,   140] loss: 25640.369\n",
      "[42,   145] loss: 42372.066\n",
      "[42,   150] loss: 27158.373\n",
      "[42,   155] loss: 43764.708\n",
      "[42,   160] loss: 28947.189\n",
      "[42,   165] loss: 39404.222\n",
      "[42,   170] loss: 31820.887\n",
      "[42,   175] loss: 29148.723\n",
      "[42,   180] loss: 47607.787\n",
      "[42,   185] loss: 38438.444\n",
      "[42,   190] loss: 26050.511\n",
      "[42,   195] loss: 22906.584\n",
      "[42,   200] loss: 29486.433\n",
      "[42,   205] loss: 30082.237\n",
      "[42,   210] loss: 17130.927\n",
      "[42,   215] loss: 27152.878\n",
      "[42,   220] loss: 20172.497\n",
      "[42,   225] loss: 29196.083\n",
      "[42,   230] loss: 40497.179\n",
      "[43,     5] loss: 23979.424\n",
      "[43,    10] loss: 27679.027\n",
      "[43,    15] loss: 35899.130\n",
      "[43,    20] loss: 28100.593\n",
      "[43,    25] loss: 30705.660\n",
      "[43,    30] loss: 32707.886\n",
      "[43,    35] loss: 32690.633\n",
      "[43,    40] loss: 29190.243\n",
      "[43,    45] loss: 27269.763\n",
      "[43,    50] loss: 18201.429\n",
      "[43,    55] loss: 36056.819\n",
      "[43,    60] loss: 31696.845\n",
      "[43,    65] loss: 23641.796\n",
      "[43,    70] loss: 32652.589\n",
      "[43,    75] loss: 33669.835\n",
      "[43,    80] loss: 34868.746\n",
      "[43,    85] loss: 28291.048\n",
      "[43,    90] loss: 28527.522\n",
      "[43,    95] loss: 33798.462\n",
      "[43,   100] loss: 58915.463\n",
      "[43,   105] loss: 25921.586\n",
      "[43,   110] loss: 34750.682\n",
      "[43,   115] loss: 26830.743\n",
      "[43,   120] loss: 31575.217\n",
      "[43,   125] loss: 38297.174\n",
      "[43,   130] loss: 30194.796\n",
      "[43,   135] loss: 35704.778\n",
      "[43,   140] loss: 24731.660\n",
      "[43,   145] loss: 45600.550\n",
      "[43,   150] loss: 20188.472\n",
      "[43,   155] loss: 31849.936\n",
      "[43,   160] loss: 19288.769\n",
      "[43,   165] loss: 24896.669\n",
      "[43,   170] loss: 32801.635\n",
      "[43,   175] loss: 26354.746\n",
      "[43,   180] loss: 29534.054\n",
      "[43,   185] loss: 26876.532\n",
      "[43,   190] loss: 35619.068\n",
      "[43,   195] loss: 34096.295\n",
      "[43,   200] loss: 37913.795\n",
      "[43,   205] loss: 25795.119\n",
      "[43,   210] loss: 25883.372\n",
      "[43,   215] loss: 41263.062\n",
      "[43,   220] loss: 32331.560\n",
      "[43,   225] loss: 19949.257\n",
      "[43,   230] loss: 23625.470\n",
      "[44,     5] loss: 22725.104\n",
      "[44,    10] loss: 20026.270\n",
      "[44,    15] loss: 43989.629\n",
      "[44,    20] loss: 26456.106\n",
      "[44,    25] loss: 18231.251\n",
      "[44,    30] loss: 21403.615\n",
      "[44,    35] loss: 26632.017\n",
      "[44,    40] loss: 22947.915\n",
      "[44,    45] loss: 25452.558\n",
      "[44,    50] loss: 31029.899\n",
      "[44,    55] loss: 26108.271\n",
      "[44,    60] loss: 28480.467\n",
      "[44,    65] loss: 31151.961\n",
      "[44,    70] loss: 22678.628\n",
      "[44,    75] loss: 34330.241\n",
      "[44,    80] loss: 41409.109\n",
      "[44,    85] loss: 28670.511\n",
      "[44,    90] loss: 37574.095\n",
      "[44,    95] loss: 29247.405\n",
      "[44,   100] loss: 26069.786\n",
      "[44,   105] loss: 19145.135\n",
      "[44,   110] loss: 27171.467\n",
      "[44,   115] loss: 27714.107\n",
      "[44,   120] loss: 25116.018\n",
      "[44,   125] loss: 25202.770\n",
      "[44,   130] loss: 26288.989\n",
      "[44,   135] loss: 32324.695\n",
      "[44,   140] loss: 34942.742\n",
      "[44,   145] loss: 42636.030\n",
      "[44,   150] loss: 24496.085\n",
      "[44,   155] loss: 36074.778\n",
      "[44,   160] loss: 23675.311\n",
      "[44,   165] loss: 35399.252\n",
      "[44,   170] loss: 56058.214\n",
      "[44,   175] loss: 33044.290\n",
      "[44,   180] loss: 39762.540\n",
      "[44,   185] loss: 32726.785\n",
      "[44,   190] loss: 38705.637\n",
      "[44,   195] loss: 47518.659\n",
      "[44,   200] loss: 30319.038\n",
      "[44,   205] loss: 34048.812\n",
      "[44,   210] loss: 28126.603\n",
      "[44,   215] loss: 27565.463\n",
      "[44,   220] loss: 33009.495\n",
      "[44,   225] loss: 25740.807\n",
      "[44,   230] loss: 33106.332\n",
      "[45,     5] loss: 30817.407\n",
      "[45,    10] loss: 21250.093\n",
      "[45,    15] loss: 31678.097\n",
      "[45,    20] loss: 30010.032\n",
      "[45,    25] loss: 23880.507\n",
      "[45,    30] loss: 41062.190\n",
      "[45,    35] loss: 21948.345\n",
      "[45,    40] loss: 32928.438\n",
      "[45,    45] loss: 23564.521\n",
      "[45,    50] loss: 28739.995\n",
      "[45,    55] loss: 32357.173\n",
      "[45,    60] loss: 30356.542\n",
      "[45,    65] loss: 50505.404\n",
      "[45,    70] loss: 20804.394\n",
      "[45,    75] loss: 34873.499\n",
      "[45,    80] loss: 35640.260\n",
      "[45,    85] loss: 38176.354\n",
      "[45,    90] loss: 51249.771\n",
      "[45,    95] loss: 29972.890\n",
      "[45,   100] loss: 25061.696\n",
      "[45,   105] loss: 34115.679\n",
      "[45,   110] loss: 41855.684\n",
      "[45,   115] loss: 33456.209\n",
      "[45,   120] loss: 24855.488\n",
      "[45,   125] loss: 32788.727\n",
      "[45,   130] loss: 15078.147\n",
      "[45,   135] loss: 38987.804\n",
      "[45,   140] loss: 46734.325\n",
      "[45,   145] loss: 27230.321\n",
      "[45,   150] loss: 22724.784\n",
      "[45,   155] loss: 24135.567\n",
      "[45,   160] loss: 19484.091\n",
      "[45,   165] loss: 29844.253\n",
      "[45,   170] loss: 31200.735\n",
      "[45,   175] loss: 32097.786\n",
      "[45,   180] loss: 18991.142\n",
      "[45,   185] loss: 32948.644\n",
      "[45,   190] loss: 24710.975\n",
      "[45,   195] loss: 23745.786\n",
      "[45,   200] loss: 19884.341\n",
      "[45,   205] loss: 23032.906\n",
      "[45,   210] loss: 29854.745\n",
      "[45,   215] loss: 30180.062\n",
      "[45,   220] loss: 32290.280\n",
      "[45,   225] loss: 27638.025\n",
      "[45,   230] loss: 30771.183\n",
      "[46,     5] loss: 32916.399\n",
      "[46,    10] loss: 30207.893\n",
      "[46,    15] loss: 30547.556\n",
      "[46,    20] loss: 30507.305\n",
      "[46,    25] loss: 43118.661\n",
      "[46,    30] loss: 29128.067\n",
      "[46,    35] loss: 22766.601\n",
      "[46,    40] loss: 29454.841\n",
      "[46,    45] loss: 36367.484\n",
      "[46,    50] loss: 29363.379\n",
      "[46,    55] loss: 31439.763\n",
      "[46,    60] loss: 26175.808\n",
      "[46,    65] loss: 24552.116\n",
      "[46,    70] loss: 30444.316\n",
      "[46,    75] loss: 29517.561\n",
      "[46,    80] loss: 31080.136\n",
      "[46,    85] loss: 30873.214\n",
      "[46,    90] loss: 21967.217\n",
      "[46,    95] loss: 22723.939\n",
      "[46,   100] loss: 26452.203\n",
      "[46,   105] loss: 39082.037\n",
      "[46,   110] loss: 31920.116\n",
      "[46,   115] loss: 20094.815\n",
      "[46,   120] loss: 23393.275\n",
      "[46,   125] loss: 25940.155\n",
      "[46,   130] loss: 27543.833\n",
      "[46,   135] loss: 20975.739\n",
      "[46,   140] loss: 20652.980\n",
      "[46,   145] loss: 19923.460\n",
      "[46,   150] loss: 17589.323\n",
      "[46,   155] loss: 33634.098\n",
      "[46,   160] loss: 31736.298\n",
      "[46,   165] loss: 29925.280\n",
      "[46,   170] loss: 36016.769\n",
      "[46,   175] loss: 33262.019\n",
      "[46,   180] loss: 49957.241\n",
      "[46,   185] loss: 25173.669\n",
      "[46,   190] loss: 24548.694\n",
      "[46,   195] loss: 24788.386\n",
      "[46,   200] loss: 26396.746\n",
      "[46,   205] loss: 41980.484\n",
      "[46,   210] loss: 33235.789\n",
      "[46,   215] loss: 44334.539\n",
      "[46,   220] loss: 28149.603\n",
      "[46,   225] loss: 27671.615\n",
      "[46,   230] loss: 28982.698\n",
      "[47,     5] loss: 25870.952\n",
      "[47,    10] loss: 31638.429\n",
      "[47,    15] loss: 30490.355\n",
      "[47,    20] loss: 26426.204\n",
      "[47,    25] loss: 54046.043\n",
      "[47,    30] loss: 28846.053\n",
      "[47,    35] loss: 22833.569\n",
      "[47,    40] loss: 28697.586\n",
      "[47,    45] loss: 18611.100\n",
      "[47,    50] loss: 26281.830\n",
      "[47,    55] loss: 33492.983\n",
      "[47,    60] loss: 38998.731\n",
      "[47,    65] loss: 32715.611\n",
      "[47,    70] loss: 28028.700\n",
      "[47,    75] loss: 26670.930\n",
      "[47,    80] loss: 25155.462\n",
      "[47,    85] loss: 38292.441\n",
      "[47,    90] loss: 25666.867\n",
      "[47,    95] loss: 40748.751\n",
      "[47,   100] loss: 40852.390\n",
      "[47,   105] loss: 28308.081\n",
      "[47,   110] loss: 25817.911\n",
      "[47,   115] loss: 28238.959\n",
      "[47,   120] loss: 29056.112\n",
      "[47,   125] loss: 30699.543\n",
      "[47,   130] loss: 23551.621\n",
      "[47,   135] loss: 22613.535\n",
      "[47,   140] loss: 43535.637\n",
      "[47,   145] loss: 22338.283\n",
      "[47,   150] loss: 29122.254\n",
      "[47,   155] loss: 22357.570\n",
      "[47,   160] loss: 30874.745\n",
      "[47,   165] loss: 22369.069\n",
      "[47,   170] loss: 22096.722\n",
      "[47,   175] loss: 35882.985\n",
      "[47,   180] loss: 29407.269\n",
      "[47,   185] loss: 26160.449\n",
      "[47,   190] loss: 25721.490\n",
      "[47,   195] loss: 33553.104\n",
      "[47,   200] loss: 25421.418\n",
      "[47,   205] loss: 30036.640\n",
      "[47,   210] loss: 30930.919\n",
      "[47,   215] loss: 35393.111\n",
      "[47,   220] loss: 26852.176\n",
      "[47,   225] loss: 34221.098\n",
      "[47,   230] loss: 28241.990\n",
      "[48,     5] loss: 23544.042\n",
      "[48,    10] loss: 33687.680\n",
      "[48,    15] loss: 54950.990\n",
      "[48,    20] loss: 25145.321\n",
      "[48,    25] loss: 24823.757\n",
      "[48,    30] loss: 30666.011\n",
      "[48,    35] loss: 26061.678\n",
      "[48,    40] loss: 37901.762\n",
      "[48,    45] loss: 28548.893\n",
      "[48,    50] loss: 26022.193\n",
      "[48,    55] loss: 30911.238\n",
      "[48,    60] loss: 22623.210\n",
      "[48,    65] loss: 27223.687\n",
      "[48,    70] loss: 23737.717\n",
      "[48,    75] loss: 29706.493\n",
      "[48,    80] loss: 36210.841\n",
      "[48,    85] loss: 23584.806\n",
      "[48,    90] loss: 21833.771\n",
      "[48,    95] loss: 31894.088\n",
      "[48,   100] loss: 34671.945\n",
      "[48,   105] loss: 25363.925\n",
      "[48,   110] loss: 21186.278\n",
      "[48,   115] loss: 26260.137\n",
      "[48,   120] loss: 34527.941\n",
      "[48,   125] loss: 40644.011\n",
      "[48,   130] loss: 26862.637\n",
      "[48,   135] loss: 40306.291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48,   140] loss: 34459.643\n",
      "[48,   145] loss: 35102.370\n",
      "[48,   150] loss: 18022.303\n",
      "[48,   155] loss: 19754.085\n",
      "[48,   160] loss: 24520.896\n",
      "[48,   165] loss: 25018.270\n",
      "[48,   170] loss: 34852.466\n",
      "[48,   175] loss: 32075.869\n",
      "[48,   180] loss: 25667.081\n",
      "[48,   185] loss: 23348.705\n",
      "[48,   190] loss: 28205.784\n",
      "[48,   195] loss: 35093.489\n",
      "[48,   200] loss: 33529.836\n",
      "[48,   205] loss: 31126.902\n",
      "[48,   210] loss: 29526.125\n",
      "[48,   215] loss: 36784.830\n",
      "[48,   220] loss: 40528.770\n",
      "[48,   225] loss: 25320.234\n",
      "[48,   230] loss: 33715.380\n",
      "[49,     5] loss: 35847.696\n",
      "[49,    10] loss: 18205.990\n",
      "[49,    15] loss: 27170.077\n",
      "[49,    20] loss: 36965.458\n",
      "[49,    25] loss: 25388.979\n",
      "[49,    30] loss: 22920.716\n",
      "[49,    35] loss: 22421.657\n",
      "[49,    40] loss: 36913.492\n",
      "[49,    45] loss: 19792.090\n",
      "[49,    50] loss: 32789.496\n",
      "[49,    55] loss: 33324.349\n",
      "[49,    60] loss: 50248.589\n",
      "[49,    65] loss: 23740.297\n",
      "[49,    70] loss: 29355.005\n",
      "[49,    75] loss: 39940.404\n",
      "[49,    80] loss: 24992.382\n",
      "[49,    85] loss: 28831.342\n",
      "[49,    90] loss: 39946.016\n",
      "[49,    95] loss: 19304.353\n",
      "[49,   100] loss: 29022.435\n",
      "[49,   105] loss: 22254.157\n",
      "[49,   110] loss: 33363.825\n",
      "[49,   115] loss: 21175.599\n",
      "[49,   120] loss: 28627.854\n",
      "[49,   125] loss: 33810.623\n",
      "[49,   130] loss: 21449.067\n",
      "[49,   135] loss: 23078.629\n",
      "[49,   140] loss: 39031.317\n",
      "[49,   145] loss: 26007.320\n",
      "[49,   150] loss: 25523.304\n",
      "[49,   155] loss: 21562.160\n",
      "[49,   160] loss: 26255.068\n",
      "[49,   165] loss: 32255.459\n",
      "[49,   170] loss: 36623.494\n",
      "[49,   175] loss: 29769.916\n",
      "[49,   180] loss: 31344.447\n",
      "[49,   185] loss: 46301.700\n",
      "[49,   190] loss: 42563.145\n",
      "[49,   195] loss: 23202.215\n",
      "[49,   200] loss: 25355.301\n",
      "[49,   205] loss: 20203.861\n",
      "[49,   210] loss: 32854.519\n",
      "[49,   215] loss: 20901.425\n",
      "[49,   220] loss: 43319.564\n",
      "[49,   225] loss: 28015.810\n",
      "[49,   230] loss: 20017.222\n",
      "[50,     5] loss: 37746.196\n",
      "[50,    10] loss: 23873.407\n",
      "[50,    15] loss: 32747.119\n",
      "[50,    20] loss: 33520.565\n",
      "[50,    25] loss: 31612.661\n",
      "[50,    30] loss: 23695.858\n",
      "[50,    35] loss: 40535.975\n",
      "[50,    40] loss: 20306.491\n",
      "[50,    45] loss: 34232.991\n",
      "[50,    50] loss: 20872.989\n",
      "[50,    55] loss: 30798.843\n",
      "[50,    60] loss: 20740.501\n",
      "[50,    65] loss: 27990.074\n",
      "[50,    70] loss: 27451.015\n",
      "[50,    75] loss: 23438.833\n",
      "[50,    80] loss: 33661.171\n",
      "[50,    85] loss: 28402.735\n",
      "[50,    90] loss: 32328.930\n",
      "[50,    95] loss: 30513.377\n",
      "[50,   100] loss: 26523.273\n",
      "[50,   105] loss: 24273.538\n",
      "[50,   110] loss: 32084.264\n",
      "[50,   115] loss: 22307.090\n",
      "[50,   120] loss: 22286.460\n",
      "[50,   125] loss: 35801.558\n",
      "[50,   130] loss: 50598.859\n",
      "[50,   135] loss: 19058.232\n",
      "[50,   140] loss: 31931.434\n",
      "[50,   145] loss: 25997.879\n",
      "[50,   150] loss: 46718.102\n",
      "[50,   155] loss: 18353.270\n",
      "[50,   160] loss: 26792.014\n",
      "[50,   165] loss: 22052.143\n",
      "[50,   170] loss: 35042.052\n",
      "[50,   175] loss: 23965.790\n",
      "[50,   180] loss: 30374.400\n",
      "[50,   185] loss: 34293.896\n",
      "[50,   190] loss: 27458.667\n",
      "[50,   195] loss: 25294.726\n",
      "[50,   200] loss: 23291.715\n",
      "[50,   205] loss: 31439.813\n",
      "[50,   210] loss: 38538.372\n",
      "[50,   215] loss: 28137.130\n",
      "[50,   220] loss: 37606.868\n",
      "[50,   225] loss: 23062.348\n",
      "[50,   230] loss: 38373.041\n",
      "[51,     5] loss: 30287.015\n",
      "[51,    10] loss: 24805.145\n",
      "[51,    15] loss: 29238.240\n",
      "[51,    20] loss: 27265.312\n",
      "[51,    25] loss: 29288.652\n",
      "[51,    30] loss: 26015.405\n",
      "[51,    35] loss: 45714.103\n",
      "[51,    40] loss: 22489.630\n",
      "[51,    45] loss: 35956.731\n",
      "[51,    50] loss: 40817.246\n",
      "[51,    55] loss: 47916.080\n",
      "[51,    60] loss: 23415.398\n",
      "[51,    65] loss: 21032.742\n",
      "[51,    70] loss: 25712.129\n",
      "[51,    75] loss: 24055.023\n",
      "[51,    80] loss: 24670.832\n",
      "[51,    85] loss: 27731.616\n",
      "[51,    90] loss: 40070.229\n",
      "[51,    95] loss: 37262.894\n",
      "[51,   100] loss: 28585.536\n",
      "[51,   105] loss: 31346.097\n",
      "[51,   110] loss: 16901.148\n",
      "[51,   115] loss: 28374.702\n",
      "[51,   120] loss: 24334.235\n",
      "[51,   125] loss: 32236.067\n",
      "[51,   130] loss: 42187.248\n",
      "[51,   135] loss: 31558.919\n",
      "[51,   140] loss: 23642.811\n",
      "[51,   145] loss: 31986.633\n",
      "[51,   150] loss: 24980.985\n",
      "[51,   155] loss: 25617.203\n",
      "[51,   160] loss: 28227.215\n",
      "[51,   165] loss: 31116.150\n",
      "[51,   170] loss: 19615.243\n",
      "[51,   175] loss: 27525.863\n",
      "[51,   180] loss: 44530.932\n",
      "[51,   185] loss: 28709.073\n",
      "[51,   190] loss: 28482.179\n",
      "[51,   195] loss: 19759.487\n",
      "[51,   200] loss: 56646.749\n",
      "[51,   205] loss: 19290.453\n",
      "[51,   210] loss: 20788.323\n",
      "[51,   215] loss: 27418.641\n",
      "[51,   220] loss: 23957.284\n",
      "[51,   225] loss: 33757.400\n",
      "[51,   230] loss: 23378.978\n",
      "[52,     5] loss: 55021.225\n",
      "[52,    10] loss: 23027.063\n",
      "[52,    15] loss: 40424.264\n",
      "[52,    20] loss: 43189.710\n",
      "[52,    25] loss: 33488.309\n",
      "[52,    30] loss: 31814.955\n",
      "[52,    35] loss: 24016.475\n",
      "[52,    40] loss: 29310.809\n",
      "[52,    45] loss: 45910.385\n",
      "[52,    50] loss: 24621.194\n",
      "[52,    55] loss: 35956.744\n",
      "[52,    60] loss: 31388.302\n",
      "[52,    65] loss: 26470.939\n",
      "[52,    70] loss: 27674.757\n",
      "[52,    75] loss: 28148.789\n",
      "[52,    80] loss: 21041.031\n",
      "[52,    85] loss: 27357.910\n",
      "[52,    90] loss: 27545.198\n",
      "[52,    95] loss: 39117.820\n",
      "[52,   100] loss: 23114.612\n",
      "[52,   105] loss: 37285.826\n",
      "[52,   110] loss: 27768.288\n",
      "[52,   115] loss: 26788.143\n",
      "[52,   120] loss: 29025.645\n",
      "[52,   125] loss: 29416.843\n",
      "[52,   130] loss: 21087.180\n",
      "[52,   135] loss: 21713.580\n",
      "[52,   140] loss: 34870.198\n",
      "[52,   145] loss: 20345.651\n",
      "[52,   150] loss: 20444.267\n",
      "[52,   155] loss: 30438.133\n",
      "[52,   160] loss: 43282.115\n",
      "[52,   165] loss: 20390.631\n",
      "[52,   170] loss: 23265.448\n",
      "[52,   175] loss: 21974.719\n",
      "[52,   180] loss: 29002.582\n",
      "[52,   185] loss: 18648.316\n",
      "[52,   190] loss: 27521.688\n",
      "[52,   195] loss: 33825.577\n",
      "[52,   200] loss: 27939.916\n",
      "[52,   205] loss: 26766.973\n",
      "[52,   210] loss: 20500.516\n",
      "[52,   215] loss: 35738.410\n",
      "[52,   220] loss: 20453.366\n",
      "[52,   225] loss: 29780.102\n",
      "[52,   230] loss: 23929.015\n",
      "[53,     5] loss: 30880.939\n",
      "[53,    10] loss: 53636.134\n",
      "[53,    15] loss: 33026.481\n",
      "[53,    20] loss: 29132.245\n",
      "[53,    25] loss: 24597.932\n",
      "[53,    30] loss: 26852.114\n",
      "[53,    35] loss: 26819.662\n",
      "[53,    40] loss: 22196.452\n",
      "[53,    45] loss: 25643.666\n",
      "[53,    50] loss: 42859.536\n",
      "[53,    55] loss: 44305.643\n",
      "[53,    60] loss: 23661.083\n",
      "[53,    65] loss: 22941.128\n",
      "[53,    70] loss: 13143.359\n",
      "[53,    75] loss: 32511.017\n",
      "[53,    80] loss: 26659.008\n",
      "[53,    85] loss: 36083.476\n",
      "[53,    90] loss: 26030.394\n",
      "[53,    95] loss: 34976.688\n",
      "[53,   100] loss: 21370.379\n",
      "[53,   105] loss: 24259.424\n",
      "[53,   110] loss: 29094.725\n",
      "[53,   115] loss: 27820.009\n",
      "[53,   120] loss: 21152.451\n",
      "[53,   125] loss: 25789.439\n",
      "[53,   130] loss: 33752.641\n",
      "[53,   135] loss: 29762.973\n",
      "[53,   140] loss: 23662.699\n",
      "[53,   145] loss: 34951.022\n",
      "[53,   150] loss: 29307.479\n",
      "[53,   155] loss: 26648.880\n",
      "[53,   160] loss: 25392.082\n",
      "[53,   165] loss: 21763.989\n",
      "[53,   170] loss: 24999.218\n",
      "[53,   175] loss: 17941.847\n",
      "[53,   180] loss: 28535.062\n",
      "[53,   185] loss: 31029.909\n",
      "[53,   190] loss: 34964.089\n",
      "[53,   195] loss: 30736.657\n",
      "[53,   200] loss: 38944.525\n",
      "[53,   205] loss: 20238.171\n",
      "[53,   210] loss: 22148.135\n",
      "[53,   215] loss: 24359.463\n",
      "[53,   220] loss: 45079.594\n",
      "[53,   225] loss: 25013.383\n",
      "[53,   230] loss: 32722.780\n",
      "[54,     5] loss: 31206.029\n",
      "[54,    10] loss: 24755.746\n",
      "[54,    15] loss: 34500.707\n",
      "[54,    20] loss: 25340.884\n",
      "[54,    25] loss: 24222.300\n",
      "[54,    30] loss: 42134.151\n",
      "[54,    35] loss: 28185.882\n",
      "[54,    40] loss: 30751.444\n",
      "[54,    45] loss: 29867.052\n",
      "[54,    50] loss: 55623.274\n",
      "[54,    55] loss: 24021.150\n",
      "[54,    60] loss: 26105.783\n",
      "[54,    65] loss: 39973.936\n",
      "[54,    70] loss: 43128.799\n",
      "[54,    75] loss: 30256.658\n",
      "[54,    80] loss: 26873.540\n",
      "[54,    85] loss: 27044.778\n",
      "[54,    90] loss: 23678.781\n",
      "[54,    95] loss: 23633.164\n",
      "[54,   100] loss: 29147.954\n",
      "[54,   105] loss: 41079.106\n",
      "[54,   110] loss: 15859.080\n",
      "[54,   115] loss: 24271.963\n",
      "[54,   120] loss: 23176.211\n",
      "[54,   125] loss: 20152.732\n",
      "[54,   130] loss: 27773.457\n",
      "[54,   135] loss: 32185.918\n",
      "[54,   140] loss: 31592.154\n",
      "[54,   145] loss: 24095.049\n",
      "[54,   150] loss: 26874.100\n",
      "[54,   155] loss: 26052.912\n",
      "[54,   160] loss: 33656.699\n",
      "[54,   165] loss: 25789.211\n",
      "[54,   170] loss: 29565.808\n",
      "[54,   175] loss: 35106.979\n",
      "[54,   180] loss: 40228.164\n",
      "[54,   185] loss: 19550.905\n",
      "[54,   190] loss: 32995.976\n",
      "[54,   195] loss: 29886.386\n",
      "[54,   200] loss: 25140.208\n",
      "[54,   205] loss: 25439.750\n",
      "[54,   210] loss: 19810.633\n",
      "[54,   215] loss: 26344.274\n",
      "[54,   220] loss: 26714.846\n",
      "[54,   225] loss: 24786.948\n",
      "[54,   230] loss: 18320.403\n",
      "[55,     5] loss: 34971.352\n",
      "[55,    10] loss: 46284.622\n",
      "[55,    15] loss: 27873.399\n",
      "[55,    20] loss: 24654.820\n",
      "[55,    25] loss: 33514.216\n",
      "[55,    30] loss: 24524.717\n",
      "[55,    35] loss: 41048.162\n",
      "[55,    40] loss: 26952.924\n",
      "[55,    45] loss: 20218.300\n",
      "[55,    50] loss: 26329.286\n",
      "[55,    55] loss: 28712.423\n",
      "[55,    60] loss: 31628.604\n",
      "[55,    65] loss: 24490.084\n",
      "[55,    70] loss: 24535.928\n",
      "[55,    75] loss: 27825.344\n",
      "[55,    80] loss: 28567.762\n",
      "[55,    85] loss: 26624.514\n",
      "[55,    90] loss: 31153.885\n",
      "[55,    95] loss: 29986.751\n",
      "[55,   100] loss: 46369.949\n",
      "[55,   105] loss: 21596.902\n",
      "[55,   110] loss: 28229.315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55,   115] loss: 19079.039\n",
      "[55,   120] loss: 42089.085\n",
      "[55,   125] loss: 30638.932\n",
      "[55,   130] loss: 26305.984\n",
      "[55,   135] loss: 34547.702\n",
      "[55,   140] loss: 21496.750\n",
      "[55,   145] loss: 25335.025\n",
      "[55,   150] loss: 35606.363\n",
      "[55,   155] loss: 23995.807\n",
      "[55,   160] loss: 22620.172\n",
      "[55,   165] loss: 29285.540\n",
      "[55,   170] loss: 22744.471\n",
      "[55,   175] loss: 25263.518\n",
      "[55,   180] loss: 27838.441\n",
      "[55,   185] loss: 31762.527\n",
      "[55,   190] loss: 25019.514\n",
      "[55,   195] loss: 24329.475\n",
      "[55,   200] loss: 25823.324\n",
      "[55,   205] loss: 28418.785\n",
      "[55,   210] loss: 30512.380\n",
      "[55,   215] loss: 30992.722\n",
      "[55,   220] loss: 37882.332\n",
      "[55,   225] loss: 25385.889\n",
      "[55,   230] loss: 23804.279\n",
      "[56,     5] loss: 28566.811\n",
      "[56,    10] loss: 23624.971\n",
      "[56,    15] loss: 33448.580\n",
      "[56,    20] loss: 19248.110\n",
      "[56,    25] loss: 22899.982\n",
      "[56,    30] loss: 35379.872\n",
      "[56,    35] loss: 47325.073\n",
      "[56,    40] loss: 37555.534\n",
      "[56,    45] loss: 31157.544\n",
      "[56,    50] loss: 32226.211\n",
      "[56,    55] loss: 21901.141\n",
      "[56,    60] loss: 32575.311\n",
      "[56,    65] loss: 30698.689\n",
      "[56,    70] loss: 18714.695\n",
      "[56,    75] loss: 21547.963\n",
      "[56,    80] loss: 34152.052\n",
      "[56,    85] loss: 27048.096\n",
      "[56,    90] loss: 26339.739\n",
      "[56,    95] loss: 28131.313\n",
      "[56,   100] loss: 30266.995\n",
      "[56,   105] loss: 33599.560\n",
      "[56,   110] loss: 21995.096\n",
      "[56,   115] loss: 37486.353\n",
      "[56,   120] loss: 33017.958\n",
      "[56,   125] loss: 36637.759\n",
      "[56,   130] loss: 31926.683\n",
      "[56,   135] loss: 20559.003\n",
      "[56,   140] loss: 28532.502\n",
      "[56,   145] loss: 30009.298\n",
      "[56,   150] loss: 26227.773\n",
      "[56,   155] loss: 23125.988\n",
      "[56,   160] loss: 27310.427\n",
      "[56,   165] loss: 22633.258\n",
      "[56,   170] loss: 34684.820\n",
      "[56,   175] loss: 26446.450\n",
      "[56,   180] loss: 25059.303\n",
      "[56,   185] loss: 40286.211\n",
      "[56,   190] loss: 35391.958\n",
      "[56,   195] loss: 25525.980\n",
      "[56,   200] loss: 20071.003\n",
      "[56,   205] loss: 19872.142\n",
      "[56,   210] loss: 25664.056\n",
      "[56,   215] loss: 27051.683\n",
      "[56,   220] loss: 19867.657\n",
      "[56,   225] loss: 33498.361\n",
      "[56,   230] loss: 17565.713\n",
      "[57,     5] loss: 24478.080\n",
      "[57,    10] loss: 27143.780\n",
      "[57,    15] loss: 45735.601\n",
      "[57,    20] loss: 38394.236\n",
      "[57,    25] loss: 25008.975\n",
      "[57,    30] loss: 22775.704\n",
      "[57,    35] loss: 22841.020\n",
      "[57,    40] loss: 25397.352\n",
      "[57,    45] loss: 26076.285\n",
      "[57,    50] loss: 17386.522\n",
      "[57,    55] loss: 40216.494\n",
      "[57,    60] loss: 28623.729\n",
      "[57,    65] loss: 25105.089\n",
      "[57,    70] loss: 36734.697\n",
      "[57,    75] loss: 32166.239\n",
      "[57,    80] loss: 31055.448\n",
      "[57,    85] loss: 28503.377\n",
      "[57,    90] loss: 26314.706\n",
      "[57,    95] loss: 28669.766\n",
      "[57,   100] loss: 18919.727\n",
      "[57,   105] loss: 32366.095\n",
      "[57,   110] loss: 14881.493\n",
      "[57,   115] loss: 22457.288\n",
      "[57,   120] loss: 36356.407\n",
      "[57,   125] loss: 35632.000\n",
      "[57,   130] loss: 26898.365\n",
      "[57,   135] loss: 23015.328\n",
      "[57,   140] loss: 48331.657\n",
      "[57,   145] loss: 27873.771\n",
      "[57,   150] loss: 25859.545\n",
      "[57,   155] loss: 22119.686\n",
      "[57,   160] loss: 26494.080\n",
      "[57,   165] loss: 24383.919\n",
      "[57,   170] loss: 46655.277\n",
      "[57,   175] loss: 32844.826\n",
      "[57,   180] loss: 30780.654\n",
      "[57,   185] loss: 37507.865\n",
      "[57,   190] loss: 23874.836\n",
      "[57,   195] loss: 41146.849\n",
      "[57,   200] loss: 17179.886\n",
      "[57,   205] loss: 24601.836\n",
      "[57,   210] loss: 25415.906\n",
      "[57,   215] loss: 25031.370\n",
      "[57,   220] loss: 24204.305\n",
      "[57,   225] loss: 26517.634\n",
      "[57,   230] loss: 29950.554\n",
      "[58,     5] loss: 23280.040\n",
      "[58,    10] loss: 28616.105\n",
      "[58,    15] loss: 19403.770\n",
      "[58,    20] loss: 15975.715\n",
      "[58,    25] loss: 19177.069\n",
      "[58,    30] loss: 32582.089\n",
      "[58,    35] loss: 33225.121\n",
      "[58,    40] loss: 29846.215\n",
      "[58,    45] loss: 29799.805\n",
      "[58,    50] loss: 33904.718\n",
      "[58,    55] loss: 33643.834\n",
      "[58,    60] loss: 31711.378\n",
      "[58,    65] loss: 24729.443\n",
      "[58,    70] loss: 22333.900\n",
      "[58,    75] loss: 26163.491\n",
      "[58,    80] loss: 25855.420\n",
      "[58,    85] loss: 39753.887\n",
      "[58,    90] loss: 23587.711\n",
      "[58,    95] loss: 36162.620\n",
      "[58,   100] loss: 18144.474\n",
      "[58,   105] loss: 18276.056\n",
      "[58,   110] loss: 32013.604\n",
      "[58,   115] loss: 30246.836\n",
      "[58,   120] loss: 17892.566\n",
      "[58,   125] loss: 20806.706\n",
      "[58,   130] loss: 32104.753\n",
      "[58,   135] loss: 34695.653\n",
      "[58,   140] loss: 19458.771\n",
      "[58,   145] loss: 20312.342\n",
      "[58,   150] loss: 21594.129\n",
      "[58,   155] loss: 32569.106\n",
      "[58,   160] loss: 29701.146\n",
      "[58,   165] loss: 32220.785\n",
      "[58,   170] loss: 24146.147\n",
      "[58,   175] loss: 20135.768\n",
      "[58,   180] loss: 35666.766\n",
      "[58,   185] loss: 34351.683\n",
      "[58,   190] loss: 29370.750\n",
      "[58,   195] loss: 25223.176\n",
      "[58,   200] loss: 23620.258\n",
      "[58,   205] loss: 31084.497\n",
      "[58,   210] loss: 41821.535\n",
      "[58,   215] loss: 24183.342\n",
      "[58,   220] loss: 67857.196\n",
      "[58,   225] loss: 29272.717\n",
      "[58,   230] loss: 27285.660\n",
      "[59,     5] loss: 36390.948\n",
      "[59,    10] loss: 30555.857\n",
      "[59,    15] loss: 33546.066\n",
      "[59,    20] loss: 18251.668\n",
      "[59,    25] loss: 32890.286\n",
      "[59,    30] loss: 23302.875\n",
      "[59,    35] loss: 29594.373\n",
      "[59,    40] loss: 49086.509\n",
      "[59,    45] loss: 27991.519\n",
      "[59,    50] loss: 25160.707\n",
      "[59,    55] loss: 43282.295\n",
      "[59,    60] loss: 14742.612\n",
      "[59,    65] loss: 27701.935\n",
      "[59,    70] loss: 27786.689\n",
      "[59,    75] loss: 33263.436\n",
      "[59,    80] loss: 26811.938\n",
      "[59,    85] loss: 35277.954\n",
      "[59,    90] loss: 24648.626\n",
      "[59,    95] loss: 38111.677\n",
      "[59,   100] loss: 28087.984\n",
      "[59,   105] loss: 30304.363\n",
      "[59,   110] loss: 30986.126\n",
      "[59,   115] loss: 26831.464\n",
      "[59,   120] loss: 16756.362\n",
      "[59,   125] loss: 20259.670\n",
      "[59,   130] loss: 30299.360\n",
      "[59,   135] loss: 25259.227\n",
      "[59,   140] loss: 29992.868\n",
      "[59,   145] loss: 33329.245\n",
      "[59,   150] loss: 30319.625\n",
      "[59,   155] loss: 23103.165\n",
      "[59,   160] loss: 33277.048\n",
      "[59,   165] loss: 28986.477\n",
      "[59,   170] loss: 22667.315\n",
      "[59,   175] loss: 19832.678\n",
      "[59,   180] loss: 28024.397\n",
      "[59,   185] loss: 23960.788\n",
      "[59,   190] loss: 30317.916\n",
      "[59,   195] loss: 24675.370\n",
      "[59,   200] loss: 29915.723\n",
      "[59,   205] loss: 27928.756\n",
      "[59,   210] loss: 35053.071\n",
      "[59,   215] loss: 32171.464\n",
      "[59,   220] loss: 17453.723\n",
      "[59,   225] loss: 26834.568\n",
      "[59,   230] loss: 25254.116\n",
      "[60,     5] loss: 26106.820\n",
      "[60,    10] loss: 33627.988\n",
      "[60,    15] loss: 26059.005\n",
      "[60,    20] loss: 23441.627\n",
      "[60,    25] loss: 24503.998\n",
      "[60,    30] loss: 24357.160\n",
      "[60,    35] loss: 33365.213\n",
      "[60,    40] loss: 25817.280\n",
      "[60,    45] loss: 28971.389\n",
      "[60,    50] loss: 32085.629\n",
      "[60,    55] loss: 29001.991\n",
      "[60,    60] loss: 26088.429\n",
      "[60,    65] loss: 21762.016\n",
      "[60,    70] loss: 29521.690\n",
      "[60,    75] loss: 32573.138\n",
      "[60,    80] loss: 32582.170\n",
      "[60,    85] loss: 20403.656\n",
      "[60,    90] loss: 43774.923\n",
      "[60,    95] loss: 16763.919\n",
      "[60,   100] loss: 27196.902\n",
      "[60,   105] loss: 17574.446\n",
      "[60,   110] loss: 34189.995\n",
      "[60,   115] loss: 47998.689\n",
      "[60,   120] loss: 28653.085\n",
      "[60,   125] loss: 26675.981\n",
      "[60,   130] loss: 19097.002\n",
      "[60,   135] loss: 22521.736\n",
      "[60,   140] loss: 29249.124\n",
      "[60,   145] loss: 34126.045\n",
      "[60,   150] loss: 23985.789\n",
      "[60,   155] loss: 29097.426\n",
      "[60,   160] loss: 33920.821\n",
      "[60,   165] loss: 28299.615\n",
      "[60,   170] loss: 23547.763\n",
      "[60,   175] loss: 51373.874\n",
      "[60,   180] loss: 19678.216\n",
      "[60,   185] loss: 28400.138\n",
      "[60,   190] loss: 31645.146\n",
      "[60,   195] loss: 45725.953\n",
      "[60,   200] loss: 26217.919\n",
      "[60,   205] loss: 26630.687\n",
      "[60,   210] loss: 16508.834\n",
      "[60,   215] loss: 27062.647\n",
      "[60,   220] loss: 32955.067\n",
      "[60,   225] loss: 18760.723\n",
      "[60,   230] loss: 26478.580\n",
      "[61,     5] loss: 19056.163\n",
      "[61,    10] loss: 29475.396\n",
      "[61,    15] loss: 24058.512\n",
      "[61,    20] loss: 45047.282\n",
      "[61,    25] loss: 25460.370\n",
      "[61,    30] loss: 20248.308\n",
      "[61,    35] loss: 29011.599\n",
      "[61,    40] loss: 28637.330\n",
      "[61,    45] loss: 30499.827\n",
      "[61,    50] loss: 23270.547\n",
      "[61,    55] loss: 38405.343\n",
      "[61,    60] loss: 46387.188\n",
      "[61,    65] loss: 29668.881\n",
      "[61,    70] loss: 41314.804\n",
      "[61,    75] loss: 33300.273\n",
      "[61,    80] loss: 26169.031\n",
      "[61,    85] loss: 29255.293\n",
      "[61,    90] loss: 27364.330\n",
      "[61,    95] loss: 25633.713\n",
      "[61,   100] loss: 24610.861\n",
      "[61,   105] loss: 33049.989\n",
      "[61,   110] loss: 30281.673\n",
      "[61,   115] loss: 31570.579\n",
      "[61,   120] loss: 44474.496\n",
      "[61,   125] loss: 35143.942\n",
      "[61,   130] loss: 25713.701\n",
      "[61,   135] loss: 28678.485\n",
      "[61,   140] loss: 29766.579\n",
      "[61,   145] loss: 31227.866\n",
      "[61,   150] loss: 33964.212\n",
      "[61,   155] loss: 25557.692\n",
      "[61,   160] loss: 23679.435\n",
      "[61,   165] loss: 24370.889\n",
      "[61,   170] loss: 23978.127\n",
      "[61,   175] loss: 39510.680\n",
      "[61,   180] loss: 15921.311\n",
      "[61,   185] loss: 26863.926\n",
      "[61,   190] loss: 23945.884\n",
      "[61,   195] loss: 21776.640\n",
      "[61,   200] loss: 25754.526\n",
      "[61,   205] loss: 21858.102\n",
      "[61,   210] loss: 19848.602\n",
      "[61,   215] loss: 24679.211\n",
      "[61,   220] loss: 21866.478\n",
      "[61,   225] loss: 18966.334\n",
      "[61,   230] loss: 22038.399\n",
      "[62,     5] loss: 24921.893\n",
      "[62,    10] loss: 27584.372\n",
      "[62,    15] loss: 33167.334\n",
      "[62,    20] loss: 39940.477\n",
      "[62,    25] loss: 27996.120\n",
      "[62,    30] loss: 21985.788\n",
      "[62,    35] loss: 28358.717\n",
      "[62,    40] loss: 21026.214\n",
      "[62,    45] loss: 26888.620\n",
      "[62,    50] loss: 54943.745\n",
      "[62,    55] loss: 31650.816\n",
      "[62,    60] loss: 24484.718\n",
      "[62,    65] loss: 18907.930\n",
      "[62,    70] loss: 26891.515\n",
      "[62,    75] loss: 20268.744\n",
      "[62,    80] loss: 21083.125\n",
      "[62,    85] loss: 22691.612\n",
      "[62,    90] loss: 27040.271\n",
      "[62,    95] loss: 37384.124\n",
      "[62,   100] loss: 24958.781\n",
      "[62,   105] loss: 34364.518\n",
      "[62,   110] loss: 20464.621\n",
      "[62,   115] loss: 28460.954\n",
      "[62,   120] loss: 27901.666\n",
      "[62,   125] loss: 27478.388\n",
      "[62,   130] loss: 22962.004\n",
      "[62,   135] loss: 27848.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62,   140] loss: 36224.256\n",
      "[62,   145] loss: 30059.603\n",
      "[62,   150] loss: 35722.764\n",
      "[62,   155] loss: 29969.570\n",
      "[62,   160] loss: 26420.110\n",
      "[62,   165] loss: 24258.991\n",
      "[62,   170] loss: 26527.344\n",
      "[62,   175] loss: 28756.212\n",
      "[62,   180] loss: 29939.811\n",
      "[62,   185] loss: 34486.238\n",
      "[62,   190] loss: 36950.725\n",
      "[62,   195] loss: 22482.529\n",
      "[62,   200] loss: 16168.913\n",
      "[62,   205] loss: 41124.088\n",
      "[62,   210] loss: 26043.094\n",
      "[62,   215] loss: 23196.451\n",
      "[62,   220] loss: 23562.635\n",
      "[62,   225] loss: 29073.374\n",
      "[62,   230] loss: 30843.362\n",
      "[63,     5] loss: 34916.041\n",
      "[63,    10] loss: 36370.088\n",
      "[63,    15] loss: 23758.554\n",
      "[63,    20] loss: 30389.536\n",
      "[63,    25] loss: 26576.457\n",
      "[63,    30] loss: 33019.026\n",
      "[63,    35] loss: 23996.300\n",
      "[63,    40] loss: 27470.711\n",
      "[63,    45] loss: 32894.579\n",
      "[63,    50] loss: 34281.412\n",
      "[63,    55] loss: 24658.524\n",
      "[63,    60] loss: 33955.508\n",
      "[63,    65] loss: 20186.264\n",
      "[63,    70] loss: 30464.631\n",
      "[63,    75] loss: 34013.516\n",
      "[63,    80] loss: 41250.170\n",
      "[63,    85] loss: 24239.623\n",
      "[63,    90] loss: 26669.130\n",
      "[63,    95] loss: 41219.933\n",
      "[63,   100] loss: 32313.761\n",
      "[63,   105] loss: 26546.020\n",
      "[63,   110] loss: 19857.936\n",
      "[63,   115] loss: 19584.118\n",
      "[63,   120] loss: 27093.583\n",
      "[63,   125] loss: 22400.547\n",
      "[63,   130] loss: 60832.707\n",
      "[63,   135] loss: 31515.196\n",
      "[63,   140] loss: 26062.160\n",
      "[63,   145] loss: 20588.939\n",
      "[63,   150] loss: 24823.455\n",
      "[63,   155] loss: 22858.352\n",
      "[63,   160] loss: 26783.619\n",
      "[63,   165] loss: 24900.888\n",
      "[63,   170] loss: 30288.746\n",
      "[63,   175] loss: 22467.934\n",
      "[63,   180] loss: 20636.558\n",
      "[63,   185] loss: 38200.947\n",
      "[63,   190] loss: 21528.354\n",
      "[63,   195] loss: 21537.728\n",
      "[63,   200] loss: 24045.849\n",
      "[63,   205] loss: 24879.241\n",
      "[63,   210] loss: 34041.393\n",
      "[63,   215] loss: 27981.608\n",
      "[63,   220] loss: 21540.482\n",
      "[63,   225] loss: 21352.302\n",
      "[63,   230] loss: 27096.355\n",
      "[64,     5] loss: 31337.476\n",
      "[64,    10] loss: 18780.014\n",
      "[64,    15] loss: 21215.926\n",
      "[64,    20] loss: 33847.711\n",
      "[64,    25] loss: 29136.582\n",
      "[64,    30] loss: 21760.633\n",
      "[64,    35] loss: 32148.825\n",
      "[64,    40] loss: 25775.740\n",
      "[64,    45] loss: 33457.239\n",
      "[64,    50] loss: 26258.713\n",
      "[64,    55] loss: 24658.480\n",
      "[64,    60] loss: 19322.878\n",
      "[64,    65] loss: 24198.370\n",
      "[64,    70] loss: 19223.020\n",
      "[64,    75] loss: 24056.225\n",
      "[64,    80] loss: 30481.273\n",
      "[64,    85] loss: 29728.364\n",
      "[64,    90] loss: 27304.986\n",
      "[64,    95] loss: 33082.218\n",
      "[64,   100] loss: 25200.146\n",
      "[64,   105] loss: 24784.264\n",
      "[64,   110] loss: 31132.458\n",
      "[64,   115] loss: 27879.086\n",
      "[64,   120] loss: 23612.501\n",
      "[64,   125] loss: 29317.996\n",
      "[64,   130] loss: 27309.458\n",
      "[64,   135] loss: 24623.361\n",
      "[64,   140] loss: 20717.564\n",
      "[64,   145] loss: 34147.987\n",
      "[64,   150] loss: 28514.791\n",
      "[64,   155] loss: 47254.056\n",
      "[64,   160] loss: 19852.708\n",
      "[64,   165] loss: 42361.171\n",
      "[64,   170] loss: 29873.323\n",
      "[64,   175] loss: 38670.912\n",
      "[64,   180] loss: 31943.433\n",
      "[64,   185] loss: 34523.279\n",
      "[64,   190] loss: 18587.092\n",
      "[64,   195] loss: 34568.338\n",
      "[64,   200] loss: 27745.586\n",
      "[64,   205] loss: 31341.306\n",
      "[64,   210] loss: 29901.109\n",
      "[64,   215] loss: 21487.015\n",
      "[64,   220] loss: 16215.146\n",
      "[64,   225] loss: 38691.185\n",
      "[64,   230] loss: 23441.670\n",
      "[65,     5] loss: 21767.802\n",
      "[65,    10] loss: 29390.261\n",
      "[65,    15] loss: 25181.777\n",
      "[65,    20] loss: 15289.117\n",
      "[65,    25] loss: 22079.461\n",
      "[65,    30] loss: 33507.267\n",
      "[65,    35] loss: 25797.055\n",
      "[65,    40] loss: 40248.879\n",
      "[65,    45] loss: 48941.238\n",
      "[65,    50] loss: 28682.144\n",
      "[65,    55] loss: 31690.371\n",
      "[65,    60] loss: 19496.572\n",
      "[65,    65] loss: 39146.360\n",
      "[65,    70] loss: 21372.496\n",
      "[65,    75] loss: 29514.104\n",
      "[65,    80] loss: 16860.402\n",
      "[65,    85] loss: 25085.612\n",
      "[65,    90] loss: 31737.080\n",
      "[65,    95] loss: 27489.229\n",
      "[65,   100] loss: 35879.075\n",
      "[65,   105] loss: 27738.579\n",
      "[65,   110] loss: 35855.841\n",
      "[65,   115] loss: 20722.625\n",
      "[65,   120] loss: 33652.759\n",
      "[65,   125] loss: 27191.354\n",
      "[65,   130] loss: 20983.959\n",
      "[65,   135] loss: 25668.523\n",
      "[65,   140] loss: 27052.377\n",
      "[65,   145] loss: 19284.954\n",
      "[65,   150] loss: 30689.440\n",
      "[65,   155] loss: 22355.807\n",
      "[65,   160] loss: 37424.812\n",
      "[65,   165] loss: 26897.140\n",
      "[65,   170] loss: 34945.879\n",
      "[65,   175] loss: 30208.886\n",
      "[65,   180] loss: 22620.869\n",
      "[65,   185] loss: 19809.672\n",
      "[65,   190] loss: 33726.534\n",
      "[65,   195] loss: 31269.344\n",
      "[65,   200] loss: 28615.771\n",
      "[65,   205] loss: 25462.480\n",
      "[65,   210] loss: 32464.743\n",
      "[65,   215] loss: 30735.335\n",
      "[65,   220] loss: 25527.247\n",
      "[65,   225] loss: 22960.562\n",
      "[65,   230] loss: 27394.084\n",
      "[66,     5] loss: 34683.409\n",
      "[66,    10] loss: 35127.578\n",
      "[66,    15] loss: 21577.118\n",
      "[66,    20] loss: 25433.049\n",
      "[66,    25] loss: 29799.061\n",
      "[66,    30] loss: 48461.516\n",
      "[66,    35] loss: 34379.983\n",
      "[66,    40] loss: 17705.410\n",
      "[66,    45] loss: 25792.170\n",
      "[66,    50] loss: 23019.347\n",
      "[66,    55] loss: 26800.195\n",
      "[66,    60] loss: 30784.156\n",
      "[66,    65] loss: 26026.146\n",
      "[66,    70] loss: 30491.506\n",
      "[66,    75] loss: 34818.105\n",
      "[66,    80] loss: 35327.831\n",
      "[66,    85] loss: 49667.819\n",
      "[66,    90] loss: 27042.654\n",
      "[66,    95] loss: 17220.052\n",
      "[66,   100] loss: 23624.836\n",
      "[66,   105] loss: 33849.025\n",
      "[66,   110] loss: 27981.629\n",
      "[66,   115] loss: 33922.800\n",
      "[66,   120] loss: 29902.024\n",
      "[66,   125] loss: 31868.471\n",
      "[66,   130] loss: 25326.844\n",
      "[66,   135] loss: 28389.989\n",
      "[66,   140] loss: 27956.934\n",
      "[66,   145] loss: 22044.540\n",
      "[66,   150] loss: 29928.371\n",
      "[66,   155] loss: 28363.108\n",
      "[66,   160] loss: 22411.917\n",
      "[66,   165] loss: 24735.255\n",
      "[66,   170] loss: 22938.158\n",
      "[66,   175] loss: 35335.448\n",
      "[66,   180] loss: 27046.818\n",
      "[66,   185] loss: 20297.508\n",
      "[66,   190] loss: 19949.726\n",
      "[66,   195] loss: 19544.051\n",
      "[66,   200] loss: 27581.536\n",
      "[66,   205] loss: 28432.402\n",
      "[66,   210] loss: 25126.196\n",
      "[66,   215] loss: 26800.005\n",
      "[66,   220] loss: 23024.070\n",
      "[66,   225] loss: 25417.632\n",
      "[66,   230] loss: 24756.984\n",
      "[67,     5] loss: 27630.510\n",
      "[67,    10] loss: 25080.975\n",
      "[67,    15] loss: 24449.247\n",
      "[67,    20] loss: 24207.108\n",
      "[67,    25] loss: 23830.209\n",
      "[67,    30] loss: 28898.262\n",
      "[67,    35] loss: 22342.077\n",
      "[67,    40] loss: 23644.764\n",
      "[67,    45] loss: 23962.731\n",
      "[67,    50] loss: 43676.418\n",
      "[67,    55] loss: 22619.846\n",
      "[67,    60] loss: 17915.745\n",
      "[67,    65] loss: 30833.098\n",
      "[67,    70] loss: 29534.781\n",
      "[67,    75] loss: 22131.011\n",
      "[67,    80] loss: 17350.882\n",
      "[67,    85] loss: 58286.989\n",
      "[67,    90] loss: 25411.896\n",
      "[67,    95] loss: 23314.202\n",
      "[67,   100] loss: 32632.071\n",
      "[67,   105] loss: 31266.388\n",
      "[67,   110] loss: 35255.918\n",
      "[67,   115] loss: 25763.837\n",
      "[67,   120] loss: 25882.408\n",
      "[67,   125] loss: 29447.767\n",
      "[67,   130] loss: 23163.173\n",
      "[67,   135] loss: 29501.989\n",
      "[67,   140] loss: 23705.006\n",
      "[67,   145] loss: 22640.332\n",
      "[67,   150] loss: 33344.271\n",
      "[67,   155] loss: 22020.694\n",
      "[67,   160] loss: 42674.233\n",
      "[67,   165] loss: 21914.808\n",
      "[67,   170] loss: 25912.046\n",
      "[67,   175] loss: 26400.413\n",
      "[67,   180] loss: 33518.718\n",
      "[67,   185] loss: 35570.381\n",
      "[67,   190] loss: 24025.440\n",
      "[67,   195] loss: 28629.395\n",
      "[67,   200] loss: 25541.238\n",
      "[67,   205] loss: 22290.566\n",
      "[67,   210] loss: 32250.052\n",
      "[67,   215] loss: 31381.549\n",
      "[67,   220] loss: 31910.867\n",
      "[67,   225] loss: 26268.055\n",
      "[67,   230] loss: 33931.619\n",
      "[68,     5] loss: 30309.644\n",
      "[68,    10] loss: 30880.645\n",
      "[68,    15] loss: 30268.365\n",
      "[68,    20] loss: 26007.228\n",
      "[68,    25] loss: 25302.965\n",
      "[68,    30] loss: 19313.393\n",
      "[68,    35] loss: 25399.459\n",
      "[68,    40] loss: 30006.409\n",
      "[68,    45] loss: 20353.893\n",
      "[68,    50] loss: 29310.921\n",
      "[68,    55] loss: 23856.759\n",
      "[68,    60] loss: 22133.411\n",
      "[68,    65] loss: 27165.509\n",
      "[68,    70] loss: 33943.601\n",
      "[68,    75] loss: 26305.593\n",
      "[68,    80] loss: 26297.811\n",
      "[68,    85] loss: 21643.951\n",
      "[68,    90] loss: 31784.334\n",
      "[68,    95] loss: 25201.711\n",
      "[68,   100] loss: 23534.595\n",
      "[68,   105] loss: 29703.261\n",
      "[68,   110] loss: 26991.215\n",
      "[68,   115] loss: 54152.145\n",
      "[68,   120] loss: 28361.863\n",
      "[68,   125] loss: 15585.225\n",
      "[68,   130] loss: 34339.850\n",
      "[68,   135] loss: 30583.818\n",
      "[68,   140] loss: 26709.891\n",
      "[68,   145] loss: 29419.885\n",
      "[68,   150] loss: 22833.097\n",
      "[68,   155] loss: 27176.091\n",
      "[68,   160] loss: 16335.450\n",
      "[68,   165] loss: 25978.127\n",
      "[68,   170] loss: 32768.685\n",
      "[68,   175] loss: 34794.071\n",
      "[68,   180] loss: 28463.077\n",
      "[68,   185] loss: 35179.315\n",
      "[68,   190] loss: 21888.260\n",
      "[68,   195] loss: 27008.664\n",
      "[68,   200] loss: 31492.826\n",
      "[68,   205] loss: 20875.203\n",
      "[68,   210] loss: 31361.054\n",
      "[68,   215] loss: 22202.646\n",
      "[68,   220] loss: 23640.988\n",
      "[68,   225] loss: 47634.325\n",
      "[68,   230] loss: 34408.523\n",
      "[69,     5] loss: 35112.905\n",
      "[69,    10] loss: 26830.839\n",
      "[69,    15] loss: 31925.831\n",
      "[69,    20] loss: 22911.148\n",
      "[69,    25] loss: 51761.457\n",
      "[69,    30] loss: 22256.638\n",
      "[69,    35] loss: 26245.857\n",
      "[69,    40] loss: 24425.276\n",
      "[69,    45] loss: 20689.208\n",
      "[69,    50] loss: 18987.645\n",
      "[69,    55] loss: 25274.528\n",
      "[69,    60] loss: 30558.139\n",
      "[69,    65] loss: 25356.860\n",
      "[69,    70] loss: 24838.415\n",
      "[69,    75] loss: 38508.606\n",
      "[69,    80] loss: 27420.580\n",
      "[69,    85] loss: 23045.991\n",
      "[69,    90] loss: 27423.185\n",
      "[69,    95] loss: 24061.523\n",
      "[69,   100] loss: 29833.097\n",
      "[69,   105] loss: 32513.819\n",
      "[69,   110] loss: 31458.261\n",
      "[69,   115] loss: 36695.537\n",
      "[69,   120] loss: 35001.067\n",
      "[69,   125] loss: 18309.659\n",
      "[69,   130] loss: 15010.832\n",
      "[69,   135] loss: 16650.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69,   140] loss: 21093.200\n",
      "[69,   145] loss: 41193.752\n",
      "[69,   150] loss: 23067.564\n",
      "[69,   155] loss: 22288.424\n",
      "[69,   160] loss: 26864.143\n",
      "[69,   165] loss: 41183.481\n",
      "[69,   170] loss: 36409.027\n",
      "[69,   175] loss: 35451.439\n",
      "[69,   180] loss: 26449.301\n",
      "[69,   185] loss: 32568.222\n",
      "[69,   190] loss: 27474.024\n",
      "[69,   195] loss: 23292.737\n",
      "[69,   200] loss: 24346.173\n",
      "[69,   205] loss: 39451.032\n",
      "[69,   210] loss: 19557.240\n",
      "[69,   215] loss: 22826.545\n",
      "[69,   220] loss: 40487.340\n",
      "[69,   225] loss: 21971.479\n",
      "[69,   230] loss: 16892.708\n",
      "[70,     5] loss: 25884.092\n",
      "[70,    10] loss: 33232.822\n",
      "[70,    15] loss: 27099.443\n",
      "[70,    20] loss: 20968.245\n",
      "[70,    25] loss: 30713.395\n",
      "[70,    30] loss: 27056.271\n",
      "[70,    35] loss: 27374.525\n",
      "[70,    40] loss: 23836.569\n",
      "[70,    45] loss: 26755.131\n",
      "[70,    50] loss: 27575.470\n",
      "[70,    55] loss: 39782.532\n",
      "[70,    60] loss: 25632.029\n",
      "[70,    65] loss: 29293.375\n",
      "[70,    70] loss: 24321.344\n",
      "[70,    75] loss: 33803.786\n",
      "[70,    80] loss: 20341.426\n",
      "[70,    85] loss: 22790.091\n",
      "[70,    90] loss: 22059.924\n",
      "[70,    95] loss: 41532.709\n",
      "[70,   100] loss: 29005.273\n",
      "[70,   105] loss: 20675.943\n",
      "[70,   110] loss: 36787.873\n",
      "[70,   115] loss: 25593.859\n",
      "[70,   120] loss: 23782.495\n",
      "[70,   125] loss: 28727.717\n",
      "[70,   130] loss: 21230.462\n",
      "[70,   135] loss: 28964.279\n",
      "[70,   140] loss: 24104.478\n",
      "[70,   145] loss: 26005.895\n",
      "[70,   150] loss: 42301.818\n",
      "[70,   155] loss: 47342.764\n",
      "[70,   160] loss: 37418.596\n",
      "[70,   165] loss: 29688.030\n",
      "[70,   170] loss: 20171.952\n",
      "[70,   175] loss: 19198.904\n",
      "[70,   180] loss: 20886.682\n",
      "[70,   185] loss: 31379.906\n",
      "[70,   190] loss: 22815.173\n",
      "[70,   195] loss: 26667.487\n",
      "[70,   200] loss: 22251.203\n",
      "[70,   205] loss: 29428.930\n",
      "[70,   210] loss: 28727.311\n",
      "[70,   215] loss: 18504.467\n",
      "[70,   220] loss: 37652.238\n",
      "[70,   225] loss: 24510.887\n",
      "[70,   230] loss: 27865.953\n",
      "[71,     5] loss: 20738.577\n",
      "[71,    10] loss: 43176.257\n",
      "[71,    15] loss: 44544.311\n",
      "[71,    20] loss: 23159.088\n",
      "[71,    25] loss: 27276.840\n",
      "[71,    30] loss: 29968.966\n",
      "[71,    35] loss: 25666.511\n",
      "[71,    40] loss: 25199.456\n",
      "[71,    45] loss: 26516.418\n",
      "[71,    50] loss: 49010.207\n",
      "[71,    55] loss: 30222.059\n",
      "[71,    60] loss: 21318.394\n",
      "[71,    65] loss: 25266.923\n",
      "[71,    70] loss: 21560.078\n",
      "[71,    75] loss: 28611.682\n",
      "[71,    80] loss: 25055.742\n",
      "[71,    85] loss: 27058.359\n",
      "[71,    90] loss: 26324.185\n",
      "[71,    95] loss: 28367.984\n",
      "[71,   100] loss: 27262.456\n",
      "[71,   105] loss: 23410.758\n",
      "[71,   110] loss: 34419.459\n",
      "[71,   115] loss: 23046.617\n",
      "[71,   120] loss: 30321.123\n",
      "[71,   125] loss: 34215.582\n",
      "[71,   130] loss: 25269.125\n",
      "[71,   135] loss: 27597.786\n",
      "[71,   140] loss: 26269.129\n",
      "[71,   145] loss: 18350.954\n",
      "[71,   150] loss: 25424.570\n",
      "[71,   155] loss: 21024.095\n",
      "[71,   160] loss: 26996.070\n",
      "[71,   165] loss: 30649.660\n",
      "[71,   170] loss: 17091.911\n",
      "[71,   175] loss: 20540.281\n",
      "[71,   180] loss: 20707.378\n",
      "[71,   185] loss: 43860.060\n",
      "[71,   190] loss: 21661.243\n",
      "[71,   195] loss: 32234.182\n",
      "[71,   200] loss: 28904.641\n",
      "[71,   205] loss: 25912.754\n",
      "[71,   210] loss: 30650.645\n",
      "[71,   215] loss: 33779.010\n",
      "[71,   220] loss: 31053.569\n",
      "[71,   225] loss: 24636.386\n",
      "[71,   230] loss: 27061.031\n",
      "[72,     5] loss: 28878.223\n",
      "[72,    10] loss: 27403.420\n",
      "[72,    15] loss: 22523.605\n",
      "[72,    20] loss: 18507.286\n",
      "[72,    25] loss: 47306.734\n",
      "[72,    30] loss: 40590.796\n",
      "[72,    35] loss: 30971.310\n",
      "[72,    40] loss: 22877.899\n",
      "[72,    45] loss: 25562.512\n",
      "[72,    50] loss: 25649.393\n",
      "[72,    55] loss: 24626.882\n",
      "[72,    60] loss: 20658.219\n",
      "[72,    65] loss: 27199.770\n",
      "[72,    70] loss: 42248.244\n",
      "[72,    75] loss: 28311.382\n",
      "[72,    80] loss: 27862.926\n",
      "[72,    85] loss: 19643.747\n",
      "[72,    90] loss: 25040.569\n",
      "[72,    95] loss: 23339.660\n",
      "[72,   100] loss: 28934.967\n",
      "[72,   105] loss: 26262.031\n",
      "[72,   110] loss: 25183.983\n",
      "[72,   115] loss: 39444.410\n",
      "[72,   120] loss: 24470.649\n",
      "[72,   125] loss: 26238.010\n",
      "[72,   130] loss: 18146.157\n",
      "[72,   135] loss: 27931.726\n",
      "[72,   140] loss: 29113.707\n",
      "[72,   145] loss: 17080.767\n",
      "[72,   150] loss: 20024.119\n",
      "[72,   155] loss: 34754.718\n",
      "[72,   160] loss: 30758.672\n",
      "[72,   165] loss: 33368.716\n",
      "[72,   170] loss: 33390.476\n",
      "[72,   175] loss: 25490.039\n",
      "[72,   180] loss: 27599.471\n",
      "[72,   185] loss: 28623.784\n",
      "[72,   190] loss: 27000.834\n",
      "[72,   195] loss: 20972.994\n",
      "[72,   200] loss: 20584.467\n",
      "[72,   205] loss: 31347.089\n",
      "[72,   210] loss: 23152.184\n",
      "[72,   215] loss: 19919.141\n",
      "[72,   220] loss: 39523.749\n",
      "[72,   225] loss: 26706.025\n",
      "[72,   230] loss: 42573.982\n",
      "[73,     5] loss: 26864.438\n",
      "[73,    10] loss: 21365.540\n",
      "[73,    15] loss: 21440.219\n",
      "[73,    20] loss: 32972.459\n",
      "[73,    25] loss: 18307.104\n",
      "[73,    30] loss: 38351.352\n",
      "[73,    35] loss: 26043.629\n",
      "[73,    40] loss: 31656.473\n",
      "[73,    45] loss: 24914.157\n",
      "[73,    50] loss: 17168.028\n",
      "[73,    55] loss: 30463.716\n",
      "[73,    60] loss: 32026.263\n",
      "[73,    65] loss: 27600.403\n",
      "[73,    70] loss: 45700.650\n",
      "[73,    75] loss: 28034.471\n",
      "[73,    80] loss: 16895.965\n",
      "[73,    85] loss: 31140.078\n",
      "[73,    90] loss: 25967.162\n",
      "[73,    95] loss: 30799.499\n",
      "[73,   100] loss: 37650.385\n",
      "[73,   105] loss: 29322.196\n",
      "[73,   110] loss: 21223.672\n",
      "[73,   115] loss: 25377.580\n",
      "[73,   120] loss: 16890.602\n",
      "[73,   125] loss: 21785.922\n",
      "[73,   130] loss: 29522.155\n",
      "[73,   135] loss: 18583.257\n",
      "[73,   140] loss: 33904.180\n",
      "[73,   145] loss: 23803.009\n",
      "[73,   150] loss: 31839.168\n",
      "[73,   155] loss: 28284.366\n",
      "[73,   160] loss: 34489.974\n",
      "[73,   165] loss: 30950.236\n",
      "[73,   170] loss: 36029.107\n",
      "[73,   175] loss: 29137.462\n",
      "[73,   180] loss: 26768.753\n",
      "[73,   185] loss: 25977.354\n",
      "[73,   190] loss: 42272.682\n",
      "[73,   195] loss: 32117.536\n",
      "[73,   200] loss: 26177.959\n",
      "[73,   205] loss: 28219.947\n",
      "[73,   210] loss: 29734.478\n",
      "[73,   215] loss: 23391.052\n",
      "[73,   220] loss: 31297.140\n",
      "[73,   225] loss: 17849.635\n",
      "[73,   230] loss: 24171.558\n",
      "[74,     5] loss: 24360.777\n",
      "[74,    10] loss: 30117.334\n",
      "[74,    15] loss: 23066.589\n",
      "[74,    20] loss: 26043.757\n",
      "[74,    25] loss: 20615.857\n",
      "[74,    30] loss: 28158.039\n",
      "[74,    35] loss: 28477.021\n",
      "[74,    40] loss: 21231.133\n",
      "[74,    45] loss: 21551.456\n",
      "[74,    50] loss: 46383.936\n",
      "[74,    55] loss: 19000.243\n",
      "[74,    60] loss: 23462.338\n",
      "[74,    65] loss: 25858.884\n",
      "[74,    70] loss: 27499.785\n",
      "[74,    75] loss: 21408.150\n",
      "[74,    80] loss: 13651.259\n",
      "[74,    85] loss: 24132.445\n",
      "[74,    90] loss: 25000.820\n",
      "[74,    95] loss: 23764.021\n",
      "[74,   100] loss: 33611.826\n",
      "[74,   105] loss: 39064.002\n",
      "[74,   110] loss: 25661.484\n",
      "[74,   115] loss: 26994.596\n",
      "[74,   120] loss: 23479.199\n",
      "[74,   125] loss: 27045.975\n",
      "[74,   130] loss: 34301.637\n",
      "[74,   135] loss: 27299.392\n",
      "[74,   140] loss: 35223.433\n",
      "[74,   145] loss: 22315.991\n",
      "[74,   150] loss: 32112.400\n",
      "[74,   155] loss: 30046.519\n",
      "[74,   160] loss: 64697.458\n",
      "[74,   165] loss: 19423.611\n",
      "[74,   170] loss: 21704.803\n",
      "[74,   175] loss: 36725.061\n",
      "[74,   180] loss: 29775.102\n",
      "[74,   185] loss: 35790.832\n",
      "[74,   190] loss: 22881.595\n",
      "[74,   195] loss: 28482.561\n",
      "[74,   200] loss: 23871.870\n",
      "[74,   205] loss: 25672.040\n",
      "[74,   210] loss: 28859.360\n",
      "[74,   215] loss: 22644.581\n",
      "[74,   220] loss: 25995.573\n",
      "[74,   225] loss: 27962.453\n",
      "[74,   230] loss: 26071.091\n",
      "[75,     5] loss: 29786.019\n",
      "[75,    10] loss: 22071.134\n",
      "[75,    15] loss: 57978.502\n",
      "[75,    20] loss: 24905.820\n",
      "[75,    25] loss: 37649.220\n",
      "[75,    30] loss: 31782.595\n",
      "[75,    35] loss: 35386.073\n",
      "[75,    40] loss: 21932.067\n",
      "[75,    45] loss: 20497.164\n",
      "[75,    50] loss: 22082.493\n",
      "[75,    55] loss: 33332.114\n",
      "[75,    60] loss: 27621.121\n",
      "[75,    65] loss: 28005.834\n",
      "[75,    70] loss: 18936.644\n",
      "[75,    75] loss: 33733.500\n",
      "[75,    80] loss: 39213.594\n",
      "[75,    85] loss: 26782.137\n",
      "[75,    90] loss: 33506.279\n",
      "[75,    95] loss: 25343.276\n",
      "[75,   100] loss: 34721.888\n",
      "[75,   105] loss: 20442.688\n",
      "[75,   110] loss: 33368.784\n",
      "[75,   115] loss: 26011.530\n",
      "[75,   120] loss: 24300.913\n",
      "[75,   125] loss: 27237.182\n",
      "[75,   130] loss: 26332.177\n",
      "[75,   135] loss: 33886.940\n",
      "[75,   140] loss: 23917.114\n",
      "[75,   145] loss: 28032.482\n",
      "[75,   150] loss: 33841.376\n",
      "[75,   155] loss: 19388.302\n",
      "[75,   160] loss: 30801.672\n",
      "[75,   165] loss: 20256.190\n",
      "[75,   170] loss: 28969.790\n",
      "[75,   175] loss: 27325.700\n",
      "[75,   180] loss: 25099.355\n",
      "[75,   185] loss: 23668.660\n",
      "[75,   190] loss: 25712.536\n",
      "[75,   195] loss: 26584.733\n",
      "[75,   200] loss: 29011.576\n",
      "[75,   205] loss: 23862.995\n",
      "[75,   210] loss: 33443.859\n",
      "[75,   215] loss: 17656.401\n",
      "[75,   220] loss: 23880.480\n",
      "[75,   225] loss: 23532.821\n",
      "[75,   230] loss: 14757.064\n",
      "[76,     5] loss: 23462.463\n",
      "[76,    10] loss: 28988.675\n",
      "[76,    15] loss: 48026.469\n",
      "[76,    20] loss: 29227.135\n",
      "[76,    25] loss: 27847.557\n",
      "[76,    30] loss: 27975.786\n",
      "[76,    35] loss: 21510.384\n",
      "[76,    40] loss: 24886.652\n",
      "[76,    45] loss: 22936.301\n",
      "[76,    50] loss: 19681.086\n",
      "[76,    55] loss: 20769.176\n",
      "[76,    60] loss: 16180.091\n",
      "[76,    65] loss: 31682.129\n",
      "[76,    70] loss: 22013.451\n",
      "[76,    75] loss: 28659.890\n",
      "[76,    80] loss: 27244.112\n",
      "[76,    85] loss: 19009.241\n",
      "[76,    90] loss: 27482.037\n",
      "[76,    95] loss: 33412.341\n",
      "[76,   100] loss: 21441.226\n",
      "[76,   105] loss: 26401.319\n",
      "[76,   110] loss: 16910.588\n",
      "[76,   115] loss: 25396.766\n",
      "[76,   120] loss: 32017.680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76,   125] loss: 30969.062\n",
      "[76,   130] loss: 27599.979\n",
      "[76,   135] loss: 14966.586\n",
      "[76,   140] loss: 32594.762\n",
      "[76,   145] loss: 27211.812\n",
      "[76,   150] loss: 36954.064\n",
      "[76,   155] loss: 24770.615\n",
      "[76,   160] loss: 21229.500\n",
      "[76,   165] loss: 28798.879\n",
      "[76,   170] loss: 37999.216\n",
      "[76,   175] loss: 34858.778\n",
      "[76,   180] loss: 18949.941\n",
      "[76,   185] loss: 41645.542\n",
      "[76,   190] loss: 33354.672\n",
      "[76,   195] loss: 35187.841\n",
      "[76,   200] loss: 28850.559\n",
      "[76,   205] loss: 32053.249\n",
      "[76,   210] loss: 22864.041\n",
      "[76,   215] loss: 23849.725\n",
      "[76,   220] loss: 34658.653\n",
      "[76,   225] loss: 21457.940\n",
      "[76,   230] loss: 35479.896\n",
      "[77,     5] loss: 29336.025\n",
      "[77,    10] loss: 20468.816\n",
      "[77,    15] loss: 21879.296\n",
      "[77,    20] loss: 30707.580\n",
      "[77,    25] loss: 47327.924\n",
      "[77,    30] loss: 18946.538\n",
      "[77,    35] loss: 25030.174\n",
      "[77,    40] loss: 19946.174\n",
      "[77,    45] loss: 30415.746\n",
      "[77,    50] loss: 19565.443\n",
      "[77,    55] loss: 39606.217\n",
      "[77,    60] loss: 27425.375\n",
      "[77,    65] loss: 24138.457\n",
      "[77,    70] loss: 38399.577\n",
      "[77,    75] loss: 18073.987\n",
      "[77,    80] loss: 22546.135\n",
      "[77,    85] loss: 29694.820\n",
      "[77,    90] loss: 29978.526\n",
      "[77,    95] loss: 23605.525\n",
      "[77,   100] loss: 32912.937\n",
      "[77,   105] loss: 26878.466\n",
      "[77,   110] loss: 31823.957\n",
      "[77,   115] loss: 21295.375\n",
      "[77,   120] loss: 37365.464\n",
      "[77,   125] loss: 26906.073\n",
      "[77,   130] loss: 29708.843\n",
      "[77,   135] loss: 20810.637\n",
      "[77,   140] loss: 24604.414\n",
      "[77,   145] loss: 34403.917\n",
      "[77,   150] loss: 32311.963\n",
      "[77,   155] loss: 16443.491\n",
      "[77,   160] loss: 59951.123\n",
      "[77,   165] loss: 25779.167\n",
      "[77,   170] loss: 19645.709\n",
      "[77,   175] loss: 24898.402\n",
      "[77,   180] loss: 40952.463\n",
      "[77,   185] loss: 24173.218\n",
      "[77,   190] loss: 21980.058\n",
      "[77,   195] loss: 23451.691\n",
      "[77,   200] loss: 25132.358\n",
      "[77,   205] loss: 19918.663\n",
      "[77,   210] loss: 27535.407\n",
      "[77,   215] loss: 26363.354\n",
      "[77,   220] loss: 15895.046\n",
      "[77,   225] loss: 37565.851\n",
      "[77,   230] loss: 26107.573\n",
      "[78,     5] loss: 27161.795\n",
      "[78,    10] loss: 26828.317\n",
      "[78,    15] loss: 21084.590\n",
      "[78,    20] loss: 27115.232\n",
      "[78,    25] loss: 26014.927\n",
      "[78,    30] loss: 45044.510\n",
      "[78,    35] loss: 25477.058\n",
      "[78,    40] loss: 19961.647\n",
      "[78,    45] loss: 20864.095\n",
      "[78,    50] loss: 24422.566\n",
      "[78,    55] loss: 28064.665\n",
      "[78,    60] loss: 32937.843\n",
      "[78,    65] loss: 28884.746\n",
      "[78,    70] loss: 32781.473\n",
      "[78,    75] loss: 25729.624\n",
      "[78,    80] loss: 28733.584\n",
      "[78,    85] loss: 21987.611\n",
      "[78,    90] loss: 26954.201\n",
      "[78,    95] loss: 23645.825\n",
      "[78,   100] loss: 33730.633\n",
      "[78,   105] loss: 25211.997\n",
      "[78,   110] loss: 21141.225\n",
      "[78,   115] loss: 16699.124\n",
      "[78,   120] loss: 28396.639\n",
      "[78,   125] loss: 27449.853\n",
      "[78,   130] loss: 24195.724\n",
      "[78,   135] loss: 25615.525\n",
      "[78,   140] loss: 29252.537\n",
      "[78,   145] loss: 32675.827\n",
      "[78,   150] loss: 22717.381\n",
      "[78,   155] loss: 12862.913\n",
      "[78,   160] loss: 42012.634\n",
      "[78,   165] loss: 26661.382\n",
      "[78,   170] loss: 47946.277\n",
      "[78,   175] loss: 21356.909\n",
      "[78,   180] loss: 28136.698\n",
      "[78,   185] loss: 34418.176\n",
      "[78,   190] loss: 23602.243\n",
      "[78,   195] loss: 30209.264\n",
      "[78,   200] loss: 30693.275\n",
      "[78,   205] loss: 24137.020\n",
      "[78,   210] loss: 29546.268\n",
      "[78,   215] loss: 35387.001\n",
      "[78,   220] loss: 30426.904\n",
      "[78,   225] loss: 27283.038\n",
      "[78,   230] loss: 19678.071\n",
      "[79,     5] loss: 24719.955\n",
      "[79,    10] loss: 29032.277\n",
      "[79,    15] loss: 18879.630\n",
      "[79,    20] loss: 31120.886\n",
      "[79,    25] loss: 34985.161\n",
      "[79,    30] loss: 40818.312\n",
      "[79,    35] loss: 25281.398\n",
      "[79,    40] loss: 27291.361\n",
      "[79,    45] loss: 21200.594\n",
      "[79,    50] loss: 21885.288\n",
      "[79,    55] loss: 23123.285\n",
      "[79,    60] loss: 22061.237\n",
      "[79,    65] loss: 22153.481\n",
      "[79,    70] loss: 21414.942\n",
      "[79,    75] loss: 44013.027\n",
      "[79,    80] loss: 29611.152\n",
      "[79,    85] loss: 27275.452\n",
      "[79,    90] loss: 23623.568\n",
      "[79,    95] loss: 28389.482\n",
      "[79,   100] loss: 30592.231\n",
      "[79,   105] loss: 25543.726\n",
      "[79,   110] loss: 19230.821\n",
      "[79,   115] loss: 41198.681\n",
      "[79,   120] loss: 23838.730\n",
      "[79,   125] loss: 21207.377\n",
      "[79,   130] loss: 25370.088\n",
      "[79,   135] loss: 28062.064\n",
      "[79,   140] loss: 28420.157\n",
      "[79,   145] loss: 28172.077\n",
      "[79,   150] loss: 18052.260\n",
      "[79,   155] loss: 38971.983\n",
      "[79,   160] loss: 24248.354\n",
      "[79,   165] loss: 32827.491\n",
      "[79,   170] loss: 28084.303\n",
      "[79,   175] loss: 12527.747\n",
      "[79,   180] loss: 22065.690\n",
      "[79,   185] loss: 26921.820\n",
      "[79,   190] loss: 31401.681\n",
      "[79,   195] loss: 22765.800\n",
      "[79,   200] loss: 22907.999\n",
      "[79,   205] loss: 29656.753\n",
      "[79,   210] loss: 34020.384\n",
      "[79,   215] loss: 26226.986\n",
      "[79,   220] loss: 28520.295\n",
      "[79,   225] loss: 49956.557\n",
      "[79,   230] loss: 23744.470\n",
      "[80,     5] loss: 34019.034\n",
      "[80,    10] loss: 28832.414\n",
      "[80,    15] loss: 21724.410\n",
      "[80,    20] loss: 27245.747\n",
      "[80,    25] loss: 28062.218\n",
      "[80,    30] loss: 33103.585\n",
      "[80,    35] loss: 23623.149\n",
      "[80,    40] loss: 21584.904\n",
      "[80,    45] loss: 25105.771\n",
      "[80,    50] loss: 26325.597\n",
      "[80,    55] loss: 29435.097\n",
      "[80,    60] loss: 18022.284\n",
      "[80,    65] loss: 19659.463\n",
      "[80,    70] loss: 25132.214\n",
      "[80,    75] loss: 26775.576\n",
      "[80,    80] loss: 19290.838\n",
      "[80,    85] loss: 31835.718\n",
      "[80,    90] loss: 29695.155\n",
      "[80,    95] loss: 17423.931\n",
      "[80,   100] loss: 19316.285\n",
      "[80,   105] loss: 22762.358\n",
      "[80,   110] loss: 16288.692\n",
      "[80,   115] loss: 19411.261\n",
      "[80,   120] loss: 44248.743\n",
      "[80,   125] loss: 28303.805\n",
      "[80,   130] loss: 26001.750\n",
      "[80,   135] loss: 31261.432\n",
      "[80,   140] loss: 20767.815\n",
      "[80,   145] loss: 17304.009\n",
      "[80,   150] loss: 37787.589\n",
      "[80,   155] loss: 35974.126\n",
      "[80,   160] loss: 31216.234\n",
      "[80,   165] loss: 18926.161\n",
      "[80,   170] loss: 21491.076\n",
      "[80,   175] loss: 30859.304\n",
      "[80,   180] loss: 61464.210\n",
      "[80,   185] loss: 30349.183\n",
      "[80,   190] loss: 30240.969\n",
      "[80,   195] loss: 28259.746\n",
      "[80,   200] loss: 21836.413\n",
      "[80,   205] loss: 23619.686\n",
      "[80,   210] loss: 25152.705\n",
      "[80,   215] loss: 32532.795\n",
      "[80,   220] loss: 27204.009\n",
      "[80,   225] loss: 39359.642\n",
      "[80,   230] loss: 36339.344\n",
      "[81,     5] loss: 36726.260\n",
      "[81,    10] loss: 36841.061\n",
      "[81,    15] loss: 31751.363\n",
      "[81,    20] loss: 29154.496\n",
      "[81,    25] loss: 25525.176\n",
      "[81,    30] loss: 19347.399\n",
      "[81,    35] loss: 28096.014\n",
      "[81,    40] loss: 27502.563\n",
      "[81,    45] loss: 32208.349\n",
      "[81,    50] loss: 31991.769\n",
      "[81,    55] loss: 25703.902\n",
      "[81,    60] loss: 25301.970\n",
      "[81,    65] loss: 21171.753\n",
      "[81,    70] loss: 18614.319\n",
      "[81,    75] loss: 19156.823\n",
      "[81,    80] loss: 27032.819\n",
      "[81,    85] loss: 25058.788\n",
      "[81,    90] loss: 27734.355\n",
      "[81,    95] loss: 31226.504\n",
      "[81,   100] loss: 22344.602\n",
      "[81,   105] loss: 27052.320\n",
      "[81,   110] loss: 25241.279\n",
      "[81,   115] loss: 27171.285\n",
      "[81,   120] loss: 23727.284\n",
      "[81,   125] loss: 26048.581\n",
      "[81,   130] loss: 36702.638\n",
      "[81,   135] loss: 34182.484\n",
      "[81,   140] loss: 50096.962\n",
      "[81,   145] loss: 24054.849\n",
      "[81,   150] loss: 34363.472\n",
      "[81,   155] loss: 16079.783\n",
      "[81,   160] loss: 21892.419\n",
      "[81,   165] loss: 28181.031\n",
      "[81,   170] loss: 30569.206\n",
      "[81,   175] loss: 25885.205\n",
      "[81,   180] loss: 45549.427\n",
      "[81,   185] loss: 30609.970\n",
      "[81,   190] loss: 20807.974\n",
      "[81,   195] loss: 26478.310\n",
      "[81,   200] loss: 27677.118\n",
      "[81,   205] loss: 23212.175\n",
      "[81,   210] loss: 27166.336\n",
      "[81,   215] loss: 32413.151\n",
      "[81,   220] loss: 21960.649\n",
      "[81,   225] loss: 17467.763\n",
      "[81,   230] loss: 23170.398\n",
      "[82,     5] loss: 20624.811\n",
      "[82,    10] loss: 30760.966\n",
      "[82,    15] loss: 20800.781\n",
      "[82,    20] loss: 25704.843\n",
      "[82,    25] loss: 26434.254\n",
      "[82,    30] loss: 16744.286\n",
      "[82,    35] loss: 25774.395\n",
      "[82,    40] loss: 28012.089\n",
      "[82,    45] loss: 22361.937\n",
      "[82,    50] loss: 18295.437\n",
      "[82,    55] loss: 30560.642\n",
      "[82,    60] loss: 26689.630\n",
      "[82,    65] loss: 32274.708\n",
      "[82,    70] loss: 22226.446\n",
      "[82,    75] loss: 51684.325\n",
      "[82,    80] loss: 30901.076\n",
      "[82,    85] loss: 23432.403\n",
      "[82,    90] loss: 26241.102\n",
      "[82,    95] loss: 30709.719\n",
      "[82,   100] loss: 38968.986\n",
      "[82,   105] loss: 25119.889\n",
      "[82,   110] loss: 26003.699\n",
      "[82,   115] loss: 23101.088\n",
      "[82,   120] loss: 25129.423\n",
      "[82,   125] loss: 32834.562\n",
      "[82,   130] loss: 31340.387\n",
      "[82,   135] loss: 26059.137\n",
      "[82,   140] loss: 25798.734\n",
      "[82,   145] loss: 29707.354\n",
      "[82,   150] loss: 28130.107\n",
      "[82,   155] loss: 27390.556\n",
      "[82,   160] loss: 34686.502\n",
      "[82,   165] loss: 35130.312\n",
      "[82,   170] loss: 19765.534\n",
      "[82,   175] loss: 27253.913\n",
      "[82,   180] loss: 30292.348\n",
      "[82,   185] loss: 34453.025\n",
      "[82,   190] loss: 36953.374\n",
      "[82,   195] loss: 24572.825\n",
      "[82,   200] loss: 33004.285\n",
      "[82,   205] loss: 25275.541\n",
      "[82,   210] loss: 16174.509\n",
      "[82,   215] loss: 22391.896\n",
      "[82,   220] loss: 34657.303\n",
      "[82,   225] loss: 21492.075\n",
      "[82,   230] loss: 23403.621\n",
      "[83,     5] loss: 21688.456\n",
      "[83,    10] loss: 28346.506\n",
      "[83,    15] loss: 33677.569\n",
      "[83,    20] loss: 38429.870\n",
      "[83,    25] loss: 21050.965\n",
      "[83,    30] loss: 27542.616\n",
      "[83,    35] loss: 20088.088\n",
      "[83,    40] loss: 22826.084\n",
      "[83,    45] loss: 24593.232\n",
      "[83,    50] loss: 18156.890\n",
      "[83,    55] loss: 18616.532\n",
      "[83,    60] loss: 29789.354\n",
      "[83,    65] loss: 36975.543\n",
      "[83,    70] loss: 37164.897\n",
      "[83,    75] loss: 41742.620\n",
      "[83,    80] loss: 30156.259\n",
      "[83,    85] loss: 25840.438\n",
      "[83,    90] loss: 28286.877\n",
      "[83,    95] loss: 30494.554\n",
      "[83,   100] loss: 26617.446\n",
      "[83,   105] loss: 23184.802\n",
      "[83,   110] loss: 28007.033\n",
      "[83,   115] loss: 32737.648\n",
      "[83,   120] loss: 35856.561\n",
      "[83,   125] loss: 16039.554\n",
      "[83,   130] loss: 17639.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83,   135] loss: 31439.554\n",
      "[83,   140] loss: 21059.019\n",
      "[83,   145] loss: 29058.998\n",
      "[83,   150] loss: 23772.873\n",
      "[83,   155] loss: 17753.851\n",
      "[83,   160] loss: 29038.654\n",
      "[83,   165] loss: 26813.118\n",
      "[83,   170] loss: 30326.957\n",
      "[83,   175] loss: 18844.118\n",
      "[83,   180] loss: 20697.445\n",
      "[83,   185] loss: 30990.263\n",
      "[83,   190] loss: 25126.346\n",
      "[83,   195] loss: 22779.327\n",
      "[83,   200] loss: 22779.352\n",
      "[83,   205] loss: 54219.896\n",
      "[83,   210] loss: 36218.395\n",
      "[83,   215] loss: 25243.338\n",
      "[83,   220] loss: 27133.973\n",
      "[83,   225] loss: 24712.858\n",
      "[83,   230] loss: 23459.334\n",
      "[84,     5] loss: 24027.691\n",
      "[84,    10] loss: 30721.737\n",
      "[84,    15] loss: 21563.778\n",
      "[84,    20] loss: 21698.481\n",
      "[84,    25] loss: 25127.934\n",
      "[84,    30] loss: 24771.712\n",
      "[84,    35] loss: 21263.385\n",
      "[84,    40] loss: 32932.790\n",
      "[84,    45] loss: 27917.819\n",
      "[84,    50] loss: 26149.423\n",
      "[84,    55] loss: 34106.856\n",
      "[84,    60] loss: 22000.076\n",
      "[84,    65] loss: 34886.606\n",
      "[84,    70] loss: 29489.187\n",
      "[84,    75] loss: 30784.002\n",
      "[84,    80] loss: 28579.985\n",
      "[84,    85] loss: 21972.008\n",
      "[84,    90] loss: 25556.948\n",
      "[84,    95] loss: 17805.924\n",
      "[84,   100] loss: 21200.804\n",
      "[84,   105] loss: 52871.224\n",
      "[84,   110] loss: 38550.080\n",
      "[84,   115] loss: 25729.886\n",
      "[84,   120] loss: 15718.031\n",
      "[84,   125] loss: 23545.205\n",
      "[84,   130] loss: 45175.349\n",
      "[84,   135] loss: 24656.504\n",
      "[84,   140] loss: 22830.797\n",
      "[84,   145] loss: 36058.256\n",
      "[84,   150] loss: 17529.972\n",
      "[84,   155] loss: 21914.248\n",
      "[84,   160] loss: 24492.283\n",
      "[84,   165] loss: 44850.892\n",
      "[84,   170] loss: 33393.821\n",
      "[84,   175] loss: 21422.232\n",
      "[84,   180] loss: 33780.434\n",
      "[84,   185] loss: 24723.112\n",
      "[84,   190] loss: 25454.746\n",
      "[84,   195] loss: 20381.390\n",
      "[84,   200] loss: 21390.235\n",
      "[84,   205] loss: 18435.372\n",
      "[84,   210] loss: 36201.172\n",
      "[84,   215] loss: 27073.615\n",
      "[84,   220] loss: 37611.530\n",
      "[84,   225] loss: 29525.477\n",
      "[84,   230] loss: 20813.859\n",
      "[85,     5] loss: 24561.366\n",
      "[85,    10] loss: 24066.299\n",
      "[85,    15] loss: 19515.762\n",
      "[85,    20] loss: 30372.145\n",
      "[85,    25] loss: 20946.913\n",
      "[85,    30] loss: 35334.405\n",
      "[85,    35] loss: 26742.514\n",
      "[85,    40] loss: 25815.353\n",
      "[85,    45] loss: 26318.295\n",
      "[85,    50] loss: 24576.744\n",
      "[85,    55] loss: 21875.369\n",
      "[85,    60] loss: 28934.218\n",
      "[85,    65] loss: 30818.765\n",
      "[85,    70] loss: 28819.157\n",
      "[85,    75] loss: 29349.715\n",
      "[85,    80] loss: 42871.874\n",
      "[85,    85] loss: 26956.985\n",
      "[85,    90] loss: 24243.256\n",
      "[85,    95] loss: 33105.497\n",
      "[85,   100] loss: 25889.185\n",
      "[85,   105] loss: 56530.943\n",
      "[85,   110] loss: 28288.730\n",
      "[85,   115] loss: 30431.474\n",
      "[85,   120] loss: 20344.465\n",
      "[85,   125] loss: 21814.288\n",
      "[85,   130] loss: 19237.489\n",
      "[85,   135] loss: 26403.526\n",
      "[85,   140] loss: 31414.241\n",
      "[85,   145] loss: 35581.813\n",
      "[85,   150] loss: 32422.164\n",
      "[85,   155] loss: 15309.159\n",
      "[85,   160] loss: 25542.510\n",
      "[85,   165] loss: 26431.548\n",
      "[85,   170] loss: 20626.248\n",
      "[85,   175] loss: 29503.541\n",
      "[85,   180] loss: 31520.427\n",
      "[85,   185] loss: 31028.521\n",
      "[85,   190] loss: 29931.813\n",
      "[85,   195] loss: 16901.630\n",
      "[85,   200] loss: 25586.242\n",
      "[85,   205] loss: 20479.418\n",
      "[85,   210] loss: 16068.920\n",
      "[85,   215] loss: 39903.654\n",
      "[85,   220] loss: 19880.993\n",
      "[85,   225] loss: 31511.113\n",
      "[85,   230] loss: 32547.811\n",
      "[86,     5] loss: 28113.924\n",
      "[86,    10] loss: 35280.571\n",
      "[86,    15] loss: 32712.417\n",
      "[86,    20] loss: 25834.935\n",
      "[86,    25] loss: 37789.735\n",
      "[86,    30] loss: 25453.209\n",
      "[86,    35] loss: 24833.772\n",
      "[86,    40] loss: 32327.106\n",
      "[86,    45] loss: 20391.350\n",
      "[86,    50] loss: 48055.560\n",
      "[86,    55] loss: 32555.741\n",
      "[86,    60] loss: 27470.622\n",
      "[86,    65] loss: 25516.129\n",
      "[86,    70] loss: 21879.941\n",
      "[86,    75] loss: 20938.232\n",
      "[86,    80] loss: 25519.083\n",
      "[86,    85] loss: 26632.629\n",
      "[86,    90] loss: 25063.403\n",
      "[86,    95] loss: 20609.793\n",
      "[86,   100] loss: 49461.610\n",
      "[86,   105] loss: 28040.002\n",
      "[86,   110] loss: 20111.820\n",
      "[86,   115] loss: 27544.011\n",
      "[86,   120] loss: 32820.955\n",
      "[86,   125] loss: 26262.274\n",
      "[86,   130] loss: 35030.772\n",
      "[86,   135] loss: 27605.562\n",
      "[86,   140] loss: 25420.230\n",
      "[86,   145] loss: 19896.579\n",
      "[86,   150] loss: 21337.764\n",
      "[86,   155] loss: 25177.292\n",
      "[86,   160] loss: 30431.176\n",
      "[86,   165] loss: 29145.282\n",
      "[86,   170] loss: 28231.464\n",
      "[86,   175] loss: 19717.125\n",
      "[86,   180] loss: 31714.661\n",
      "[86,   185] loss: 19370.288\n",
      "[86,   190] loss: 33773.359\n",
      "[86,   195] loss: 25105.810\n",
      "[86,   200] loss: 26799.491\n",
      "[86,   205] loss: 28337.541\n",
      "[86,   210] loss: 24769.221\n",
      "[86,   215] loss: 26084.853\n",
      "[86,   220] loss: 25369.209\n",
      "[86,   225] loss: 18954.963\n",
      "[86,   230] loss: 22092.309\n",
      "[87,     5] loss: 34653.719\n",
      "[87,    10] loss: 25864.443\n",
      "[87,    15] loss: 36966.577\n",
      "[87,    20] loss: 21681.993\n",
      "[87,    25] loss: 21895.750\n",
      "[87,    30] loss: 25729.896\n",
      "[87,    35] loss: 28115.608\n",
      "[87,    40] loss: 47079.424\n",
      "[87,    45] loss: 21659.102\n",
      "[87,    50] loss: 37751.105\n",
      "[87,    55] loss: 35135.712\n",
      "[87,    60] loss: 34873.316\n",
      "[87,    65] loss: 24920.218\n",
      "[87,    70] loss: 26227.617\n",
      "[87,    75] loss: 20185.271\n",
      "[87,    80] loss: 28466.242\n",
      "[87,    85] loss: 23110.783\n",
      "[87,    90] loss: 27795.509\n",
      "[87,    95] loss: 24184.680\n",
      "[87,   100] loss: 30888.306\n",
      "[87,   105] loss: 36410.805\n",
      "[87,   110] loss: 33853.815\n",
      "[87,   115] loss: 25199.880\n",
      "[87,   120] loss: 29237.987\n",
      "[87,   125] loss: 20336.252\n",
      "[87,   130] loss: 25334.634\n",
      "[87,   135] loss: 22087.692\n",
      "[87,   140] loss: 27701.158\n",
      "[87,   145] loss: 23426.425\n",
      "[87,   150] loss: 22227.714\n",
      "[87,   155] loss: 19078.565\n",
      "[87,   160] loss: 35419.010\n",
      "[87,   165] loss: 25206.478\n",
      "[87,   170] loss: 22593.714\n",
      "[87,   175] loss: 29117.533\n",
      "[87,   180] loss: 20844.254\n",
      "[87,   185] loss: 13460.456\n",
      "[87,   190] loss: 24363.864\n",
      "[87,   195] loss: 24131.199\n",
      "[87,   200] loss: 31790.391\n",
      "[87,   205] loss: 22455.905\n",
      "[87,   210] loss: 30285.466\n",
      "[87,   215] loss: 28398.305\n",
      "[87,   220] loss: 26904.805\n",
      "[87,   225] loss: 42622.866\n",
      "[87,   230] loss: 27841.814\n",
      "[88,     5] loss: 31262.686\n",
      "[88,    10] loss: 20915.461\n",
      "[88,    15] loss: 27703.428\n",
      "[88,    20] loss: 23862.291\n",
      "[88,    25] loss: 24782.161\n",
      "[88,    30] loss: 41750.764\n",
      "[88,    35] loss: 16744.116\n",
      "[88,    40] loss: 33812.069\n",
      "[88,    45] loss: 33546.971\n",
      "[88,    50] loss: 26875.712\n",
      "[88,    55] loss: 47515.441\n",
      "[88,    60] loss: 33706.572\n",
      "[88,    65] loss: 35305.171\n",
      "[88,    70] loss: 27464.497\n",
      "[88,    75] loss: 26499.549\n",
      "[88,    80] loss: 24808.035\n",
      "[88,    85] loss: 27955.749\n",
      "[88,    90] loss: 22526.390\n",
      "[88,    95] loss: 22244.493\n",
      "[88,   100] loss: 28242.352\n",
      "[88,   105] loss: 21617.113\n",
      "[88,   110] loss: 37008.449\n",
      "[88,   115] loss: 27460.688\n",
      "[88,   120] loss: 25455.310\n",
      "[88,   125] loss: 26518.970\n",
      "[88,   130] loss: 24583.990\n",
      "[88,   135] loss: 29728.467\n",
      "[88,   140] loss: 18224.221\n",
      "[88,   145] loss: 29513.534\n",
      "[88,   150] loss: 43906.043\n",
      "[88,   155] loss: 37147.398\n",
      "[88,   160] loss: 25037.174\n",
      "[88,   165] loss: 17740.703\n",
      "[88,   170] loss: 22884.229\n",
      "[88,   175] loss: 24097.760\n",
      "[88,   180] loss: 20639.485\n",
      "[88,   185] loss: 24697.920\n",
      "[88,   190] loss: 25673.014\n",
      "[88,   195] loss: 25101.351\n",
      "[88,   200] loss: 21323.850\n",
      "[88,   205] loss: 20968.625\n",
      "[88,   210] loss: 27741.995\n",
      "[88,   215] loss: 26770.186\n",
      "[88,   220] loss: 26890.456\n",
      "[88,   225] loss: 35597.825\n",
      "[88,   230] loss: 15604.167\n",
      "[89,     5] loss: 27917.450\n",
      "[89,    10] loss: 43305.682\n",
      "[89,    15] loss: 44745.066\n",
      "[89,    20] loss: 26089.087\n",
      "[89,    25] loss: 27807.581\n",
      "[89,    30] loss: 30002.579\n",
      "[89,    35] loss: 30505.914\n",
      "[89,    40] loss: 25804.250\n",
      "[89,    45] loss: 19171.763\n",
      "[89,    50] loss: 34683.031\n",
      "[89,    55] loss: 27046.404\n",
      "[89,    60] loss: 22382.965\n",
      "[89,    65] loss: 34943.345\n",
      "[89,    70] loss: 15924.957\n",
      "[89,    75] loss: 25857.495\n",
      "[89,    80] loss: 31531.077\n",
      "[89,    85] loss: 32961.637\n",
      "[89,    90] loss: 21869.303\n",
      "[89,    95] loss: 26696.682\n",
      "[89,   100] loss: 29733.338\n",
      "[89,   105] loss: 25860.646\n",
      "[89,   110] loss: 30050.948\n",
      "[89,   115] loss: 20055.372\n",
      "[89,   120] loss: 20890.019\n",
      "[89,   125] loss: 31935.851\n",
      "[89,   130] loss: 26455.717\n",
      "[89,   135] loss: 32375.235\n",
      "[89,   140] loss: 20273.676\n",
      "[89,   145] loss: 26760.632\n",
      "[89,   150] loss: 24847.627\n",
      "[89,   155] loss: 35137.725\n",
      "[89,   160] loss: 21898.284\n",
      "[89,   165] loss: 27822.916\n",
      "[89,   170] loss: 33852.230\n",
      "[89,   175] loss: 31192.414\n",
      "[89,   180] loss: 14520.013\n",
      "[89,   185] loss: 15428.242\n",
      "[89,   190] loss: 26087.958\n",
      "[89,   195] loss: 27548.628\n",
      "[89,   200] loss: 30718.475\n",
      "[89,   205] loss: 19097.139\n",
      "[89,   210] loss: 30225.696\n",
      "[89,   215] loss: 21824.261\n",
      "[89,   220] loss: 27237.992\n",
      "[89,   225] loss: 26304.116\n",
      "[89,   230] loss: 34963.151\n",
      "[90,     5] loss: 40042.427\n",
      "[90,    10] loss: 27048.834\n",
      "[90,    15] loss: 24935.172\n",
      "[90,    20] loss: 24930.332\n",
      "[90,    25] loss: 24065.470\n",
      "[90,    30] loss: 25055.084\n",
      "[90,    35] loss: 20326.540\n",
      "[90,    40] loss: 38036.409\n",
      "[90,    45] loss: 36511.670\n",
      "[90,    50] loss: 24707.411\n",
      "[90,    55] loss: 19779.160\n",
      "[90,    60] loss: 39050.286\n",
      "[90,    65] loss: 26998.841\n",
      "[90,    70] loss: 23676.109\n",
      "[90,    75] loss: 45306.117\n",
      "[90,    80] loss: 29565.889\n",
      "[90,    85] loss: 22326.063\n",
      "[90,    90] loss: 25974.248\n",
      "[90,    95] loss: 22496.864\n",
      "[90,   100] loss: 23889.071\n",
      "[90,   105] loss: 23585.104\n",
      "[90,   110] loss: 26076.938\n",
      "[90,   115] loss: 28373.474\n",
      "[90,   120] loss: 19447.652\n",
      "[90,   125] loss: 15442.559\n",
      "[90,   130] loss: 23853.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90,   135] loss: 24003.168\n",
      "[90,   140] loss: 32364.688\n",
      "[90,   145] loss: 23216.787\n",
      "[90,   150] loss: 21079.590\n",
      "[90,   155] loss: 22630.718\n",
      "[90,   160] loss: 26543.295\n",
      "[90,   165] loss: 34961.870\n",
      "[90,   170] loss: 26659.889\n",
      "[90,   175] loss: 23197.728\n",
      "[90,   180] loss: 27197.674\n",
      "[90,   185] loss: 36079.220\n",
      "[90,   190] loss: 28129.834\n",
      "[90,   195] loss: 23422.187\n",
      "[90,   200] loss: 24215.851\n",
      "[90,   205] loss: 23939.755\n",
      "[90,   210] loss: 32920.016\n",
      "[90,   215] loss: 36702.687\n",
      "[90,   220] loss: 21083.441\n",
      "[90,   225] loss: 43931.183\n",
      "[90,   230] loss: 15181.277\n",
      "[91,     5] loss: 33234.587\n",
      "[91,    10] loss: 19661.902\n",
      "[91,    15] loss: 30893.313\n",
      "[91,    20] loss: 28292.870\n",
      "[91,    25] loss: 31916.548\n",
      "[91,    30] loss: 34581.207\n",
      "[91,    35] loss: 31180.419\n",
      "[91,    40] loss: 39994.809\n",
      "[91,    45] loss: 27902.136\n",
      "[91,    50] loss: 25233.472\n",
      "[91,    55] loss: 29419.015\n",
      "[91,    60] loss: 23597.708\n",
      "[91,    65] loss: 27007.995\n",
      "[91,    70] loss: 27585.273\n",
      "[91,    75] loss: 17704.305\n",
      "[91,    80] loss: 28031.281\n",
      "[91,    85] loss: 40430.130\n",
      "[91,    90] loss: 33442.689\n",
      "[91,    95] loss: 18788.253\n",
      "[91,   100] loss: 22787.646\n",
      "[91,   105] loss: 27826.598\n",
      "[91,   110] loss: 16339.141\n",
      "[91,   115] loss: 30786.232\n",
      "[91,   120] loss: 19557.446\n",
      "[91,   125] loss: 20416.237\n",
      "[91,   130] loss: 35948.550\n",
      "[91,   135] loss: 22063.351\n",
      "[91,   140] loss: 32537.417\n",
      "[91,   145] loss: 23472.454\n",
      "[91,   150] loss: 23293.075\n",
      "[91,   155] loss: 27993.043\n",
      "[91,   160] loss: 32391.811\n",
      "[91,   165] loss: 21402.130\n",
      "[91,   170] loss: 24510.083\n",
      "[91,   175] loss: 20611.308\n",
      "[91,   180] loss: 22524.507\n",
      "[91,   185] loss: 32018.483\n",
      "[91,   190] loss: 20505.626\n",
      "[91,   195] loss: 37221.530\n",
      "[91,   200] loss: 30822.504\n",
      "[91,   205] loss: 19652.097\n",
      "[91,   210] loss: 29312.465\n",
      "[91,   215] loss: 48996.086\n",
      "[91,   220] loss: 22853.517\n",
      "[91,   225] loss: 17166.734\n",
      "[91,   230] loss: 26955.374\n",
      "[92,     5] loss: 26675.561\n",
      "[92,    10] loss: 31247.242\n",
      "[92,    15] loss: 23713.302\n",
      "[92,    20] loss: 25023.573\n",
      "[92,    25] loss: 28287.135\n",
      "[92,    30] loss: 24777.965\n",
      "[92,    35] loss: 23582.274\n",
      "[92,    40] loss: 25978.779\n",
      "[92,    45] loss: 26292.897\n",
      "[92,    50] loss: 13071.418\n",
      "[92,    55] loss: 32412.325\n",
      "[92,    60] loss: 23293.414\n",
      "[92,    65] loss: 23589.399\n",
      "[92,    70] loss: 32979.720\n",
      "[92,    75] loss: 39160.378\n",
      "[92,    80] loss: 25297.829\n",
      "[92,    85] loss: 20452.316\n",
      "[92,    90] loss: 19524.274\n",
      "[92,    95] loss: 43596.440\n",
      "[92,   100] loss: 30338.222\n",
      "[92,   105] loss: 27116.833\n",
      "[92,   110] loss: 22995.663\n",
      "[92,   115] loss: 35078.457\n",
      "[92,   120] loss: 50266.702\n",
      "[92,   125] loss: 22118.648\n",
      "[92,   130] loss: 20320.037\n",
      "[92,   135] loss: 22476.199\n",
      "[92,   140] loss: 31841.780\n",
      "[92,   145] loss: 28613.651\n",
      "[92,   150] loss: 25129.236\n",
      "[92,   155] loss: 29830.253\n",
      "[92,   160] loss: 35832.597\n",
      "[92,   165] loss: 29545.196\n",
      "[92,   170] loss: 21626.583\n",
      "[92,   175] loss: 21471.852\n",
      "[92,   180] loss: 33186.888\n",
      "[92,   185] loss: 29814.666\n",
      "[92,   190] loss: 27029.625\n",
      "[92,   195] loss: 26654.666\n",
      "[92,   200] loss: 30248.425\n",
      "[92,   205] loss: 19531.389\n",
      "[92,   210] loss: 22722.468\n",
      "[92,   215] loss: 29860.260\n",
      "[92,   220] loss: 27684.404\n",
      "[92,   225] loss: 23866.518\n",
      "[92,   230] loss: 25857.793\n",
      "[93,     5] loss: 26275.007\n",
      "[93,    10] loss: 25057.420\n",
      "[93,    15] loss: 27844.275\n",
      "[93,    20] loss: 25710.715\n",
      "[93,    25] loss: 21846.196\n",
      "[93,    30] loss: 24376.688\n",
      "[93,    35] loss: 19727.005\n",
      "[93,    40] loss: 34798.837\n",
      "[93,    45] loss: 19885.862\n",
      "[93,    50] loss: 23759.164\n",
      "[93,    55] loss: 24642.115\n",
      "[93,    60] loss: 38093.497\n",
      "[93,    65] loss: 18257.597\n",
      "[93,    70] loss: 20314.828\n",
      "[93,    75] loss: 28356.427\n",
      "[93,    80] loss: 28399.863\n",
      "[93,    85] loss: 25079.705\n",
      "[93,    90] loss: 15292.675\n",
      "[93,    95] loss: 41707.412\n",
      "[93,   100] loss: 23851.591\n",
      "[93,   105] loss: 32619.439\n",
      "[93,   110] loss: 31169.175\n",
      "[93,   115] loss: 24061.671\n",
      "[93,   120] loss: 27002.742\n",
      "[93,   125] loss: 30246.155\n",
      "[93,   130] loss: 24574.041\n",
      "[93,   135] loss: 25151.132\n",
      "[93,   140] loss: 14923.338\n",
      "[93,   145] loss: 31659.588\n",
      "[93,   150] loss: 31230.045\n",
      "[93,   155] loss: 19974.811\n",
      "[93,   160] loss: 38358.941\n",
      "[93,   165] loss: 33721.650\n",
      "[93,   170] loss: 29764.187\n",
      "[93,   175] loss: 15434.836\n",
      "[93,   180] loss: 21077.856\n",
      "[93,   185] loss: 20105.495\n",
      "[93,   190] loss: 25002.914\n",
      "[93,   195] loss: 24827.303\n",
      "[93,   200] loss: 24348.097\n",
      "[93,   205] loss: 26476.842\n",
      "[93,   210] loss: 44982.357\n",
      "[93,   215] loss: 24256.901\n",
      "[93,   220] loss: 49976.321\n",
      "[93,   225] loss: 26612.277\n",
      "[93,   230] loss: 32390.153\n",
      "[94,     5] loss: 32332.800\n",
      "[94,    10] loss: 30270.305\n",
      "[94,    15] loss: 26111.080\n",
      "[94,    20] loss: 38558.244\n",
      "[94,    25] loss: 26553.523\n",
      "[94,    30] loss: 21092.316\n",
      "[94,    35] loss: 29193.970\n",
      "[94,    40] loss: 20989.407\n",
      "[94,    45] loss: 30562.561\n",
      "[94,    50] loss: 25010.246\n",
      "[94,    55] loss: 20794.594\n",
      "[94,    60] loss: 20010.478\n",
      "[94,    65] loss: 17631.688\n",
      "[94,    70] loss: 34263.897\n",
      "[94,    75] loss: 21149.664\n",
      "[94,    80] loss: 20700.975\n",
      "[94,    85] loss: 69634.507\n",
      "[94,    90] loss: 27336.787\n",
      "[94,    95] loss: 32737.606\n",
      "[94,   100] loss: 28530.507\n",
      "[94,   105] loss: 21977.383\n",
      "[94,   110] loss: 23471.157\n",
      "[94,   115] loss: 25843.437\n",
      "[94,   120] loss: 23350.839\n",
      "[94,   125] loss: 21070.604\n",
      "[94,   130] loss: 32583.177\n",
      "[94,   135] loss: 34605.761\n",
      "[94,   140] loss: 23768.954\n",
      "[94,   145] loss: 18860.341\n",
      "[94,   150] loss: 35847.371\n",
      "[94,   155] loss: 22727.873\n",
      "[94,   160] loss: 27664.957\n",
      "[94,   165] loss: 22652.279\n",
      "[94,   170] loss: 26850.114\n",
      "[94,   175] loss: 37412.079\n",
      "[94,   180] loss: 38149.188\n",
      "[94,   185] loss: 29338.763\n",
      "[94,   190] loss: 20185.114\n",
      "[94,   195] loss: 26571.914\n",
      "[94,   200] loss: 25963.083\n",
      "[94,   205] loss: 24856.276\n",
      "[94,   210] loss: 17683.145\n",
      "[94,   215] loss: 24179.353\n",
      "[94,   220] loss: 22741.767\n",
      "[94,   225] loss: 28340.283\n",
      "[94,   230] loss: 21786.269\n",
      "[95,     5] loss: 21870.934\n",
      "[95,    10] loss: 14903.319\n",
      "[95,    15] loss: 29544.119\n",
      "[95,    20] loss: 36637.223\n",
      "[95,    25] loss: 45658.474\n",
      "[95,    30] loss: 26435.614\n",
      "[95,    35] loss: 21235.633\n",
      "[95,    40] loss: 30290.434\n",
      "[95,    45] loss: 27667.886\n",
      "[95,    50] loss: 35066.842\n",
      "[95,    55] loss: 16086.709\n",
      "[95,    60] loss: 49712.873\n",
      "[95,    65] loss: 26584.938\n",
      "[95,    70] loss: 22209.239\n",
      "[95,    75] loss: 22869.838\n",
      "[95,    80] loss: 22523.496\n",
      "[95,    85] loss: 20648.910\n",
      "[95,    90] loss: 21678.153\n",
      "[95,    95] loss: 39553.707\n",
      "[95,   100] loss: 32654.064\n",
      "[95,   105] loss: 26606.525\n",
      "[95,   110] loss: 39904.702\n",
      "[95,   115] loss: 25040.428\n",
      "[95,   120] loss: 35345.403\n",
      "[95,   125] loss: 21678.785\n",
      "[95,   130] loss: 21862.076\n",
      "[95,   135] loss: 31169.378\n",
      "[95,   140] loss: 28657.325\n",
      "[95,   145] loss: 26156.411\n",
      "[95,   150] loss: 29519.141\n",
      "[95,   155] loss: 28175.960\n",
      "[95,   160] loss: 37131.013\n",
      "[95,   165] loss: 25197.042\n",
      "[95,   170] loss: 21790.381\n",
      "[95,   175] loss: 18796.140\n",
      "[95,   180] loss: 25910.121\n",
      "[95,   185] loss: 34681.184\n",
      "[95,   190] loss: 19865.839\n",
      "[95,   195] loss: 24653.393\n",
      "[95,   200] loss: 25191.849\n",
      "[95,   205] loss: 12646.701\n",
      "[95,   210] loss: 26851.115\n",
      "[95,   215] loss: 27856.816\n",
      "[95,   220] loss: 26742.626\n",
      "[95,   225] loss: 23130.187\n",
      "[95,   230] loss: 31833.755\n",
      "[96,     5] loss: 24511.673\n",
      "[96,    10] loss: 24911.999\n",
      "[96,    15] loss: 22360.127\n",
      "[96,    20] loss: 34608.800\n",
      "[96,    25] loss: 23843.393\n",
      "[96,    30] loss: 22137.244\n",
      "[96,    35] loss: 24415.037\n",
      "[96,    40] loss: 30386.917\n",
      "[96,    45] loss: 43682.807\n",
      "[96,    50] loss: 24331.263\n",
      "[96,    55] loss: 28414.342\n",
      "[96,    60] loss: 28448.359\n",
      "[96,    65] loss: 19007.348\n",
      "[96,    70] loss: 28003.092\n",
      "[96,    75] loss: 37896.123\n",
      "[96,    80] loss: 24086.154\n",
      "[96,    85] loss: 33949.655\n",
      "[96,    90] loss: 20102.625\n",
      "[96,    95] loss: 27108.278\n",
      "[96,   100] loss: 35952.693\n",
      "[96,   105] loss: 23911.291\n",
      "[96,   110] loss: 25340.261\n",
      "[96,   115] loss: 28875.422\n",
      "[96,   120] loss: 20357.405\n",
      "[96,   125] loss: 19218.907\n",
      "[96,   130] loss: 35450.728\n",
      "[96,   135] loss: 27752.614\n",
      "[96,   140] loss: 20762.570\n",
      "[96,   145] loss: 23628.325\n",
      "[96,   150] loss: 30232.737\n",
      "[96,   155] loss: 28302.485\n",
      "[96,   160] loss: 15185.301\n",
      "[96,   165] loss: 32954.178\n",
      "[96,   170] loss: 25849.341\n",
      "[96,   175] loss: 23325.324\n",
      "[96,   180] loss: 27781.440\n",
      "[96,   185] loss: 43883.118\n",
      "[96,   190] loss: 25378.650\n",
      "[96,   195] loss: 22703.321\n",
      "[96,   200] loss: 23917.776\n",
      "[96,   205] loss: 17356.037\n",
      "[96,   210] loss: 37662.034\n",
      "[96,   215] loss: 24063.969\n",
      "[96,   220] loss: 29249.410\n",
      "[96,   225] loss: 30357.499\n",
      "[96,   230] loss: 28461.586\n",
      "[97,     5] loss: 21273.048\n",
      "[97,    10] loss: 25616.097\n",
      "[97,    15] loss: 33429.929\n",
      "[97,    20] loss: 36882.419\n",
      "[97,    25] loss: 23467.653\n",
      "[97,    30] loss: 35026.489\n",
      "[97,    35] loss: 21267.788\n",
      "[97,    40] loss: 27643.376\n",
      "[97,    45] loss: 36104.956\n",
      "[97,    50] loss: 20342.433\n",
      "[97,    55] loss: 22746.649\n",
      "[97,    60] loss: 26152.916\n",
      "[97,    65] loss: 25816.340\n",
      "[97,    70] loss: 28614.983\n",
      "[97,    75] loss: 27280.833\n",
      "[97,    80] loss: 27259.529\n",
      "[97,    85] loss: 42803.117\n",
      "[97,    90] loss: 18470.543\n",
      "[97,    95] loss: 19593.547\n",
      "[97,   100] loss: 27075.564\n",
      "[97,   105] loss: 35902.757\n",
      "[97,   110] loss: 35100.011\n",
      "[97,   115] loss: 22505.582\n",
      "[97,   120] loss: 25667.798\n",
      "[97,   125] loss: 25156.615\n",
      "[97,   130] loss: 30837.342\n",
      "[97,   135] loss: 24426.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97,   140] loss: 24850.475\n",
      "[97,   145] loss: 23323.534\n",
      "[97,   150] loss: 25977.432\n",
      "[97,   155] loss: 30026.702\n",
      "[97,   160] loss: 34851.691\n",
      "[97,   165] loss: 22232.602\n",
      "[97,   170] loss: 25415.006\n",
      "[97,   175] loss: 35231.270\n",
      "[97,   180] loss: 22513.224\n",
      "[97,   185] loss: 31380.883\n",
      "[97,   190] loss: 34959.916\n",
      "[97,   195] loss: 19007.591\n",
      "[97,   200] loss: 28066.726\n",
      "[97,   205] loss: 21336.062\n",
      "[97,   210] loss: 21930.459\n",
      "[97,   215] loss: 30671.329\n",
      "[97,   220] loss: 28291.034\n",
      "[97,   225] loss: 26116.556\n",
      "[97,   230] loss: 26047.250\n",
      "[98,     5] loss: 22041.216\n",
      "[98,    10] loss: 26434.921\n",
      "[98,    15] loss: 25956.573\n",
      "[98,    20] loss: 26439.055\n",
      "[98,    25] loss: 25542.878\n",
      "[98,    30] loss: 28601.169\n",
      "[98,    35] loss: 33599.269\n",
      "[98,    40] loss: 22531.457\n",
      "[98,    45] loss: 22529.226\n",
      "[98,    50] loss: 29683.036\n",
      "[98,    55] loss: 29955.752\n",
      "[98,    60] loss: 21709.197\n",
      "[98,    65] loss: 26271.553\n",
      "[98,    70] loss: 29641.142\n",
      "[98,    75] loss: 24863.446\n",
      "[98,    80] loss: 19832.043\n",
      "[98,    85] loss: 32369.901\n",
      "[98,    90] loss: 36015.423\n",
      "[98,    95] loss: 33268.161\n",
      "[98,   100] loss: 29301.196\n",
      "[98,   105] loss: 31900.647\n",
      "[98,   110] loss: 20150.336\n",
      "[98,   115] loss: 33972.998\n",
      "[98,   120] loss: 31817.658\n",
      "[98,   125] loss: 27705.977\n",
      "[98,   130] loss: 12194.816\n",
      "[98,   135] loss: 35644.944\n",
      "[98,   140] loss: 23413.768\n",
      "[98,   145] loss: 19764.979\n",
      "[98,   150] loss: 23523.815\n",
      "[98,   155] loss: 18954.843\n",
      "[98,   160] loss: 27031.788\n",
      "[98,   165] loss: 25462.577\n",
      "[98,   170] loss: 28931.577\n",
      "[98,   175] loss: 32733.449\n",
      "[98,   180] loss: 25193.659\n",
      "[98,   185] loss: 23015.221\n",
      "[98,   190] loss: 18893.207\n",
      "[98,   195] loss: 52961.555\n",
      "[98,   200] loss: 37932.906\n",
      "[98,   205] loss: 24220.125\n",
      "[98,   210] loss: 31786.186\n",
      "[98,   215] loss: 20295.929\n",
      "[98,   220] loss: 19001.456\n",
      "[98,   225] loss: 36806.818\n",
      "[98,   230] loss: 19841.379\n",
      "[99,     5] loss: 50229.407\n",
      "[99,    10] loss: 27969.355\n",
      "[99,    15] loss: 21273.040\n",
      "[99,    20] loss: 29792.789\n",
      "[99,    25] loss: 33658.334\n",
      "[99,    30] loss: 13280.865\n",
      "[99,    35] loss: 30976.355\n",
      "[99,    40] loss: 24792.077\n",
      "[99,    45] loss: 38751.477\n",
      "[99,    50] loss: 35586.019\n",
      "[99,    55] loss: 25326.452\n",
      "[99,    60] loss: 24874.515\n",
      "[99,    65] loss: 27677.891\n",
      "[99,    70] loss: 25731.490\n",
      "[99,    75] loss: 22807.256\n",
      "[99,    80] loss: 21053.123\n",
      "[99,    85] loss: 33511.638\n",
      "[99,    90] loss: 18410.492\n",
      "[99,    95] loss: 18939.063\n",
      "[99,   100] loss: 30001.446\n",
      "[99,   105] loss: 25527.650\n",
      "[99,   110] loss: 24625.608\n",
      "[99,   115] loss: 24798.053\n",
      "[99,   120] loss: 22128.402\n",
      "[99,   125] loss: 38780.747\n",
      "[99,   130] loss: 23776.874\n",
      "[99,   135] loss: 33576.931\n",
      "[99,   140] loss: 45128.905\n",
      "[99,   145] loss: 20553.189\n",
      "[99,   150] loss: 25970.564\n",
      "[99,   155] loss: 31734.941\n",
      "[99,   160] loss: 27769.616\n",
      "[99,   165] loss: 21461.589\n",
      "[99,   170] loss: 34158.175\n",
      "[99,   175] loss: 34139.005\n",
      "[99,   180] loss: 18683.319\n",
      "[99,   185] loss: 17848.479\n",
      "[99,   190] loss: 29573.333\n",
      "[99,   195] loss: 21875.858\n",
      "[99,   200] loss: 29881.307\n",
      "[99,   205] loss: 18887.301\n",
      "[99,   210] loss: 18808.935\n",
      "[99,   215] loss: 23351.701\n",
      "[99,   220] loss: 30624.948\n",
      "[99,   225] loss: 27875.104\n",
      "[99,   230] loss: 25181.917\n",
      "[100,     5] loss: 28872.099\n",
      "[100,    10] loss: 28329.722\n",
      "[100,    15] loss: 26999.041\n",
      "[100,    20] loss: 23943.625\n",
      "[100,    25] loss: 22549.717\n",
      "[100,    30] loss: 24712.852\n",
      "[100,    35] loss: 24435.135\n",
      "[100,    40] loss: 30117.380\n",
      "[100,    45] loss: 23613.848\n",
      "[100,    50] loss: 25058.339\n",
      "[100,    55] loss: 37330.979\n",
      "[100,    60] loss: 25287.223\n",
      "[100,    65] loss: 24262.460\n",
      "[100,    70] loss: 31531.391\n",
      "[100,    75] loss: 30423.666\n",
      "[100,    80] loss: 22357.248\n",
      "[100,    85] loss: 16144.296\n",
      "[100,    90] loss: 25417.121\n",
      "[100,    95] loss: 32273.400\n",
      "[100,   100] loss: 23689.145\n",
      "[100,   105] loss: 25093.283\n",
      "[100,   110] loss: 20095.533\n",
      "[100,   115] loss: 30099.307\n",
      "[100,   120] loss: 22291.539\n",
      "[100,   125] loss: 29163.549\n",
      "[100,   130] loss: 26777.482\n",
      "[100,   135] loss: 21139.946\n",
      "[100,   140] loss: 27512.702\n",
      "[100,   145] loss: 15601.371\n",
      "[100,   150] loss: 35675.863\n",
      "[100,   155] loss: 22666.668\n",
      "[100,   160] loss: 47610.968\n",
      "[100,   165] loss: 20091.859\n",
      "[100,   170] loss: 42377.498\n",
      "[100,   175] loss: 20614.456\n",
      "[100,   180] loss: 28443.812\n",
      "[100,   185] loss: 26517.427\n",
      "[100,   190] loss: 37143.202\n",
      "[100,   195] loss: 23210.334\n",
      "[100,   200] loss: 41165.328\n",
      "[100,   205] loss: 27902.215\n",
      "[100,   210] loss: 29799.572\n",
      "[100,   215] loss: 37321.142\n",
      "[100,   220] loss: 19308.215\n",
      "[100,   225] loss: 20965.874\n",
      "[100,   230] loss: 23637.641\n",
      "[101,     5] loss: 36183.439\n",
      "[101,    10] loss: 28810.118\n",
      "[101,    15] loss: 19976.875\n",
      "[101,    20] loss: 23183.077\n",
      "[101,    25] loss: 16536.880\n",
      "[101,    30] loss: 28360.968\n",
      "[101,    35] loss: 31120.558\n",
      "[101,    40] loss: 34044.753\n",
      "[101,    45] loss: 24293.369\n",
      "[101,    50] loss: 27567.253\n",
      "[101,    55] loss: 29568.671\n",
      "[101,    60] loss: 27874.193\n",
      "[101,    65] loss: 27734.774\n",
      "[101,    70] loss: 25482.884\n",
      "[101,    75] loss: 24075.290\n",
      "[101,    80] loss: 27996.772\n",
      "[101,    85] loss: 19928.270\n",
      "[101,    90] loss: 21928.107\n",
      "[101,    95] loss: 27819.462\n",
      "[101,   100] loss: 39925.421\n",
      "[101,   105] loss: 25933.400\n",
      "[101,   110] loss: 28569.124\n",
      "[101,   115] loss: 26202.641\n",
      "[101,   120] loss: 38528.655\n",
      "[101,   125] loss: 12734.099\n",
      "[101,   130] loss: 44608.533\n",
      "[101,   135] loss: 27628.512\n",
      "[101,   140] loss: 14647.544\n",
      "[101,   145] loss: 26237.496\n",
      "[101,   150] loss: 20513.677\n",
      "[101,   155] loss: 26189.256\n",
      "[101,   160] loss: 23408.424\n",
      "[101,   165] loss: 26300.412\n",
      "[101,   170] loss: 23428.258\n",
      "[101,   175] loss: 21298.212\n",
      "[101,   180] loss: 31352.696\n",
      "[101,   185] loss: 28101.355\n",
      "[101,   190] loss: 25806.998\n",
      "[101,   195] loss: 29554.613\n",
      "[101,   200] loss: 25956.904\n",
      "[101,   205] loss: 33566.734\n",
      "[101,   210] loss: 15153.681\n",
      "[101,   215] loss: 32765.847\n",
      "[101,   220] loss: 28971.502\n",
      "[101,   225] loss: 22299.742\n",
      "[101,   230] loss: 19686.282\n",
      "[102,     5] loss: 31044.617\n",
      "[102,    10] loss: 27889.122\n",
      "[102,    15] loss: 29447.349\n",
      "[102,    20] loss: 43743.654\n",
      "[102,    25] loss: 21584.813\n",
      "[102,    30] loss: 22343.818\n",
      "[102,    35] loss: 19501.098\n",
      "[102,    40] loss: 46645.618\n",
      "[102,    45] loss: 34432.975\n",
      "[102,    50] loss: 28138.113\n",
      "[102,    55] loss: 28231.950\n",
      "[102,    60] loss: 24418.923\n",
      "[102,    65] loss: 28438.823\n",
      "[102,    70] loss: 30945.950\n",
      "[102,    75] loss: 27457.493\n",
      "[102,    80] loss: 28423.855\n",
      "[102,    85] loss: 25712.793\n",
      "[102,    90] loss: 23123.797\n",
      "[102,    95] loss: 24824.046\n",
      "[102,   100] loss: 25465.812\n",
      "[102,   105] loss: 47850.599\n",
      "[102,   110] loss: 33524.988\n",
      "[102,   115] loss: 24456.346\n",
      "[102,   120] loss: 21724.452\n",
      "[102,   125] loss: 18184.188\n",
      "[102,   130] loss: 23652.342\n",
      "[102,   135] loss: 20654.065\n",
      "[102,   140] loss: 24295.117\n",
      "[102,   145] loss: 36355.746\n",
      "[102,   150] loss: 24801.090\n",
      "[102,   155] loss: 21865.646\n",
      "[102,   160] loss: 21652.959\n",
      "[102,   165] loss: 24328.668\n",
      "[102,   170] loss: 23119.991\n",
      "[102,   175] loss: 21415.504\n",
      "[102,   180] loss: 27835.817\n",
      "[102,   185] loss: 17607.735\n",
      "[102,   190] loss: 23328.247\n",
      "[102,   195] loss: 33666.744\n",
      "[102,   200] loss: 25320.690\n",
      "[102,   205] loss: 25107.504\n",
      "[102,   210] loss: 33270.687\n",
      "[102,   215] loss: 23844.582\n",
      "[102,   220] loss: 17093.318\n",
      "[102,   225] loss: 29583.410\n",
      "[102,   230] loss: 34893.185\n",
      "[103,     5] loss: 28569.708\n",
      "[103,    10] loss: 30768.583\n",
      "[103,    15] loss: 23519.379\n",
      "[103,    20] loss: 19786.280\n",
      "[103,    25] loss: 28042.111\n",
      "[103,    30] loss: 21185.634\n",
      "[103,    35] loss: 23882.125\n",
      "[103,    40] loss: 24687.806\n",
      "[103,    45] loss: 30315.141\n",
      "[103,    50] loss: 27751.523\n",
      "[103,    55] loss: 23085.991\n",
      "[103,    60] loss: 32971.457\n",
      "[103,    65] loss: 29541.864\n",
      "[103,    70] loss: 46419.749\n",
      "[103,    75] loss: 34178.886\n",
      "[103,    80] loss: 44326.598\n",
      "[103,    85] loss: 26182.742\n",
      "[103,    90] loss: 12790.228\n",
      "[103,    95] loss: 29739.732\n",
      "[103,   100] loss: 35748.154\n",
      "[103,   105] loss: 23686.982\n",
      "[103,   110] loss: 22477.004\n",
      "[103,   115] loss: 32524.561\n",
      "[103,   120] loss: 22090.763\n",
      "[103,   125] loss: 28361.642\n",
      "[103,   130] loss: 25407.753\n",
      "[103,   135] loss: 27450.737\n",
      "[103,   140] loss: 27465.127\n",
      "[103,   145] loss: 26255.932\n",
      "[103,   150] loss: 22366.998\n",
      "[103,   155] loss: 23443.485\n",
      "[103,   160] loss: 31910.063\n",
      "[103,   165] loss: 20492.036\n",
      "[103,   170] loss: 26322.478\n",
      "[103,   175] loss: 17076.328\n",
      "[103,   180] loss: 22951.583\n",
      "[103,   185] loss: 26779.670\n",
      "[103,   190] loss: 20790.673\n",
      "[103,   195] loss: 37525.786\n",
      "[103,   200] loss: 19949.704\n",
      "[103,   205] loss: 17109.841\n",
      "[103,   210] loss: 20202.696\n",
      "[103,   215] loss: 20677.400\n",
      "[103,   220] loss: 37610.368\n",
      "[103,   225] loss: 27721.806\n",
      "[103,   230] loss: 44187.007\n",
      "[104,     5] loss: 26230.584\n",
      "[104,    10] loss: 17682.234\n",
      "[104,    15] loss: 24240.115\n",
      "[104,    20] loss: 28920.674\n",
      "[104,    25] loss: 45450.048\n",
      "[104,    30] loss: 29518.504\n",
      "[104,    35] loss: 25470.881\n",
      "[104,    40] loss: 25809.055\n",
      "[104,    45] loss: 22667.797\n",
      "[104,    50] loss: 22525.276\n",
      "[104,    55] loss: 34182.282\n",
      "[104,    60] loss: 39128.877\n",
      "[104,    65] loss: 26942.991\n",
      "[104,    70] loss: 27831.888\n",
      "[104,    75] loss: 23842.556\n",
      "[104,    80] loss: 13183.203\n",
      "[104,    85] loss: 18918.960\n",
      "[104,    90] loss: 32111.201\n",
      "[104,    95] loss: 33336.624\n",
      "[104,   100] loss: 27105.065\n",
      "[104,   105] loss: 30616.589\n",
      "[104,   110] loss: 19882.384\n",
      "[104,   115] loss: 24570.043\n",
      "[104,   120] loss: 24106.371\n",
      "[104,   125] loss: 18674.150\n",
      "[104,   130] loss: 26088.976\n",
      "[104,   135] loss: 24819.638\n",
      "[104,   140] loss: 21473.950\n",
      "[104,   145] loss: 23710.078\n",
      "[104,   150] loss: 27098.457\n",
      "[104,   155] loss: 28735.540\n",
      "[104,   160] loss: 27762.693\n",
      "[104,   165] loss: 23261.004\n",
      "[104,   170] loss: 28109.036\n",
      "[104,   175] loss: 30097.428\n",
      "[104,   180] loss: 24651.641\n",
      "[104,   185] loss: 51773.357\n",
      "[104,   190] loss: 25544.247\n",
      "[104,   195] loss: 26579.229\n",
      "[104,   200] loss: 22150.130\n",
      "[104,   205] loss: 21441.286\n",
      "[104,   210] loss: 18009.058\n",
      "[104,   215] loss: 43560.757\n",
      "[104,   220] loss: 30009.231\n",
      "[104,   225] loss: 23628.378\n",
      "[104,   230] loss: 35987.604\n",
      "[105,     5] loss: 21253.260\n",
      "[105,    10] loss: 25231.501\n",
      "[105,    15] loss: 25771.814\n",
      "[105,    20] loss: 47625.495\n",
      "[105,    25] loss: 30618.770\n",
      "[105,    30] loss: 32183.164\n",
      "[105,    35] loss: 22767.035\n",
      "[105,    40] loss: 33050.252\n",
      "[105,    45] loss: 30350.560\n",
      "[105,    50] loss: 31609.154\n",
      "[105,    55] loss: 39289.380\n",
      "[105,    60] loss: 25484.414\n",
      "[105,    65] loss: 29809.181\n",
      "[105,    70] loss: 22658.562\n",
      "[105,    75] loss: 28380.174\n",
      "[105,    80] loss: 25925.577\n",
      "[105,    85] loss: 20416.382\n",
      "[105,    90] loss: 21301.725\n",
      "[105,    95] loss: 34818.012\n",
      "[105,   100] loss: 40116.743\n",
      "[105,   105] loss: 16036.493\n",
      "[105,   110] loss: 23324.927\n",
      "[105,   115] loss: 31442.815\n",
      "[105,   120] loss: 28810.968\n",
      "[105,   125] loss: 26315.661\n",
      "[105,   130] loss: 32670.885\n",
      "[105,   135] loss: 22592.347\n",
      "[105,   140] loss: 17753.519\n",
      "[105,   145] loss: 15150.007\n",
      "[105,   150] loss: 25768.962\n",
      "[105,   155] loss: 22170.158\n",
      "[105,   160] loss: 33799.729\n",
      "[105,   165] loss: 28141.294\n",
      "[105,   170] loss: 28018.222\n",
      "[105,   175] loss: 29442.008\n",
      "[105,   180] loss: 23128.017\n",
      "[105,   185] loss: 36883.169\n",
      "[105,   190] loss: 24685.917\n",
      "[105,   195] loss: 19582.894\n",
      "[105,   200] loss: 18492.821\n",
      "[105,   205] loss: 33606.132\n",
      "[105,   210] loss: 21051.237\n",
      "[105,   215] loss: 23053.778\n",
      "[105,   220] loss: 32703.837\n",
      "[105,   225] loss: 16501.214\n",
      "[105,   230] loss: 23441.218\n",
      "[106,     5] loss: 24727.673\n",
      "[106,    10] loss: 29642.422\n",
      "[106,    15] loss: 28661.331\n",
      "[106,    20] loss: 16867.301\n",
      "[106,    25] loss: 18119.329\n",
      "[106,    30] loss: 22229.068\n",
      "[106,    35] loss: 18797.450\n",
      "[106,    40] loss: 37148.636\n",
      "[106,    45] loss: 22367.001\n",
      "[106,    50] loss: 35265.676\n",
      "[106,    55] loss: 27459.288\n",
      "[106,    60] loss: 28742.419\n",
      "[106,    65] loss: 25438.958\n",
      "[106,    70] loss: 27263.474\n",
      "[106,    75] loss: 25675.568\n",
      "[106,    80] loss: 29185.362\n",
      "[106,    85] loss: 31240.505\n",
      "[106,    90] loss: 53832.450\n",
      "[106,    95] loss: 26710.637\n",
      "[106,   100] loss: 23062.941\n",
      "[106,   105] loss: 20313.598\n",
      "[106,   110] loss: 30707.755\n",
      "[106,   115] loss: 27088.962\n",
      "[106,   120] loss: 34889.931\n",
      "[106,   125] loss: 24334.888\n",
      "[106,   130] loss: 30972.060\n",
      "[106,   135] loss: 25285.457\n",
      "[106,   140] loss: 21733.552\n",
      "[106,   145] loss: 29747.488\n",
      "[106,   150] loss: 28466.403\n",
      "[106,   155] loss: 21184.449\n",
      "[106,   160] loss: 25843.326\n",
      "[106,   165] loss: 35640.300\n",
      "[106,   170] loss: 24455.590\n",
      "[106,   175] loss: 21724.572\n",
      "[106,   180] loss: 36167.516\n",
      "[106,   185] loss: 19845.339\n",
      "[106,   190] loss: 22072.029\n",
      "[106,   195] loss: 20181.374\n",
      "[106,   200] loss: 31258.295\n",
      "[106,   205] loss: 26354.701\n",
      "[106,   210] loss: 18869.921\n",
      "[106,   215] loss: 32270.927\n",
      "[106,   220] loss: 35534.898\n",
      "[106,   225] loss: 22612.984\n",
      "[106,   230] loss: 21482.524\n",
      "[107,     5] loss: 22319.329\n",
      "[107,    10] loss: 38604.249\n",
      "[107,    15] loss: 18257.705\n",
      "[107,    20] loss: 24317.886\n",
      "[107,    25] loss: 37252.338\n",
      "[107,    30] loss: 33206.022\n",
      "[107,    35] loss: 20801.065\n",
      "[107,    40] loss: 40806.407\n",
      "[107,    45] loss: 30411.372\n",
      "[107,    50] loss: 18850.420\n",
      "[107,    55] loss: 43056.735\n",
      "[107,    60] loss: 29763.107\n",
      "[107,    65] loss: 24510.862\n",
      "[107,    70] loss: 25475.326\n",
      "[107,    75] loss: 49588.873\n",
      "[107,    80] loss: 24084.475\n",
      "[107,    85] loss: 23427.859\n",
      "[107,    90] loss: 23152.895\n",
      "[107,    95] loss: 27535.450\n",
      "[107,   100] loss: 14406.132\n",
      "[107,   105] loss: 18638.010\n",
      "[107,   110] loss: 29189.707\n",
      "[107,   115] loss: 26815.659\n",
      "[107,   120] loss: 16558.112\n",
      "[107,   125] loss: 19639.835\n",
      "[107,   130] loss: 24474.629\n",
      "[107,   135] loss: 23338.054\n",
      "[107,   140] loss: 15908.962\n",
      "[107,   145] loss: 34485.095\n",
      "[107,   150] loss: 30034.399\n",
      "[107,   155] loss: 28991.327\n",
      "[107,   160] loss: 25716.010\n",
      "[107,   165] loss: 19373.132\n",
      "[107,   170] loss: 41787.770\n",
      "[107,   175] loss: 22134.234\n",
      "[107,   180] loss: 18827.447\n",
      "[107,   185] loss: 30113.015\n",
      "[107,   190] loss: 29259.740\n",
      "[107,   195] loss: 35405.125\n",
      "[107,   200] loss: 22822.750\n",
      "[107,   205] loss: 23712.228\n",
      "[107,   210] loss: 39803.141\n",
      "[107,   215] loss: 26204.969\n",
      "[107,   220] loss: 20757.518\n",
      "[107,   225] loss: 25493.727\n",
      "[107,   230] loss: 26163.817\n",
      "[108,     5] loss: 22276.577\n",
      "[108,    10] loss: 23759.218\n",
      "[108,    15] loss: 22174.214\n",
      "[108,    20] loss: 29437.193\n",
      "[108,    25] loss: 19217.673\n",
      "[108,    30] loss: 19386.363\n",
      "[108,    35] loss: 22167.026\n",
      "[108,    40] loss: 22941.496\n",
      "[108,    45] loss: 25170.286\n",
      "[108,    50] loss: 20479.857\n",
      "[108,    55] loss: 26114.901\n",
      "[108,    60] loss: 21484.671\n",
      "[108,    65] loss: 24116.639\n",
      "[108,    70] loss: 22437.682\n",
      "[108,    75] loss: 26714.101\n",
      "[108,    80] loss: 30114.041\n",
      "[108,    85] loss: 20269.598\n",
      "[108,    90] loss: 29691.823\n",
      "[108,    95] loss: 18428.816\n",
      "[108,   100] loss: 36205.173\n",
      "[108,   105] loss: 33360.991\n",
      "[108,   110] loss: 23310.929\n",
      "[108,   115] loss: 22271.350\n",
      "[108,   120] loss: 18754.446\n",
      "[108,   125] loss: 37296.160\n",
      "[108,   130] loss: 30573.814\n",
      "[108,   135] loss: 41529.175\n",
      "[108,   140] loss: 26512.848\n",
      "[108,   145] loss: 35700.020\n",
      "[108,   150] loss: 26778.932\n",
      "[108,   155] loss: 57726.447\n",
      "[108,   160] loss: 32643.421\n",
      "[108,   165] loss: 19266.992\n",
      "[108,   170] loss: 32279.627\n",
      "[108,   175] loss: 23562.659\n",
      "[108,   180] loss: 27556.410\n",
      "[108,   185] loss: 30196.096\n",
      "[108,   190] loss: 26838.073\n",
      "[108,   195] loss: 28436.585\n",
      "[108,   200] loss: 15734.152\n",
      "[108,   205] loss: 17271.105\n",
      "[108,   210] loss: 27013.429\n",
      "[108,   215] loss: 23014.539\n",
      "[108,   220] loss: 27375.510\n",
      "[108,   225] loss: 31043.678\n",
      "[108,   230] loss: 23637.644\n",
      "[109,     5] loss: 31624.676\n",
      "[109,    10] loss: 22295.202\n",
      "[109,    15] loss: 18001.199\n",
      "[109,    20] loss: 25434.031\n",
      "[109,    25] loss: 27895.226\n",
      "[109,    30] loss: 20138.854\n",
      "[109,    35] loss: 27399.835\n",
      "[109,    40] loss: 30549.836\n",
      "[109,    45] loss: 58360.446\n",
      "[109,    50] loss: 17929.226\n",
      "[109,    55] loss: 22723.898\n",
      "[109,    60] loss: 21264.111\n",
      "[109,    65] loss: 25101.381\n",
      "[109,    70] loss: 24743.741\n",
      "[109,    75] loss: 33358.048\n",
      "[109,    80] loss: 27597.195\n",
      "[109,    85] loss: 30275.838\n",
      "[109,    90] loss: 25774.433\n",
      "[109,    95] loss: 27270.145\n",
      "[109,   100] loss: 25314.632\n",
      "[109,   105] loss: 28195.056\n",
      "[109,   110] loss: 35447.414\n",
      "[109,   115] loss: 20679.301\n",
      "[109,   120] loss: 26930.046\n",
      "[109,   125] loss: 27846.241\n",
      "[109,   130] loss: 23491.104\n",
      "[109,   135] loss: 22398.381\n",
      "[109,   140] loss: 25101.691\n",
      "[109,   145] loss: 19738.263\n",
      "[109,   150] loss: 20889.254\n",
      "[109,   155] loss: 23262.395\n",
      "[109,   160] loss: 19146.729\n",
      "[109,   165] loss: 24561.014\n",
      "[109,   170] loss: 21060.927\n",
      "[109,   175] loss: 39454.752\n",
      "[109,   180] loss: 28406.631\n",
      "[109,   185] loss: 27085.616\n",
      "[109,   190] loss: 31395.761\n",
      "[109,   195] loss: 35987.374\n",
      "[109,   200] loss: 26470.103\n",
      "[109,   205] loss: 23234.588\n",
      "[109,   210] loss: 32740.154\n",
      "[109,   215] loss: 23704.399\n",
      "[109,   220] loss: 25831.820\n",
      "[109,   225] loss: 19354.624\n",
      "[109,   230] loss: 35933.456\n",
      "[110,     5] loss: 27766.555\n",
      "[110,    10] loss: 36739.993\n",
      "[110,    15] loss: 29893.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110,    20] loss: 31504.007\n",
      "[110,    25] loss: 27628.070\n",
      "[110,    30] loss: 32789.650\n",
      "[110,    35] loss: 26253.582\n",
      "[110,    40] loss: 30147.434\n",
      "[110,    45] loss: 23131.678\n",
      "[110,    50] loss: 24974.176\n",
      "[110,    55] loss: 33275.229\n",
      "[110,    60] loss: 34244.961\n",
      "[110,    65] loss: 33651.955\n",
      "[110,    70] loss: 19947.506\n",
      "[110,    75] loss: 23048.165\n",
      "[110,    80] loss: 21439.015\n",
      "[110,    85] loss: 23578.724\n",
      "[110,    90] loss: 28104.963\n",
      "[110,    95] loss: 24223.373\n",
      "[110,   100] loss: 35884.544\n",
      "[110,   105] loss: 21226.105\n",
      "[110,   110] loss: 23131.655\n",
      "[110,   115] loss: 29201.963\n",
      "[110,   120] loss: 27455.548\n",
      "[110,   125] loss: 60041.295\n",
      "[110,   130] loss: 22446.452\n",
      "[110,   135] loss: 17438.157\n",
      "[110,   140] loss: 23084.928\n",
      "[110,   145] loss: 22959.688\n",
      "[110,   150] loss: 22511.273\n",
      "[110,   155] loss: 29800.389\n",
      "[110,   160] loss: 22594.307\n",
      "[110,   165] loss: 24025.166\n",
      "[110,   170] loss: 25847.769\n",
      "[110,   175] loss: 19491.115\n",
      "[110,   180] loss: 35152.568\n",
      "[110,   185] loss: 21523.235\n",
      "[110,   190] loss: 26664.258\n",
      "[110,   195] loss: 30334.287\n",
      "[110,   200] loss: 27825.243\n",
      "[110,   205] loss: 18998.091\n",
      "[110,   210] loss: 24091.435\n",
      "[110,   215] loss: 32108.100\n",
      "[110,   220] loss: 24805.839\n",
      "[110,   225] loss: 26269.468\n",
      "[110,   230] loss: 15763.935\n",
      "[111,     5] loss: 20139.149\n",
      "[111,    10] loss: 29541.824\n",
      "[111,    15] loss: 24361.301\n",
      "[111,    20] loss: 35932.607\n",
      "[111,    25] loss: 22766.167\n",
      "[111,    30] loss: 25962.801\n",
      "[111,    35] loss: 25411.496\n",
      "[111,    40] loss: 19189.555\n",
      "[111,    45] loss: 18627.782\n",
      "[111,    50] loss: 29455.689\n",
      "[111,    55] loss: 29860.564\n",
      "[111,    60] loss: 29523.755\n",
      "[111,    65] loss: 27891.783\n",
      "[111,    70] loss: 63242.497\n",
      "[111,    75] loss: 34410.548\n",
      "[111,    80] loss: 30633.102\n",
      "[111,    85] loss: 24043.450\n",
      "[111,    90] loss: 22879.450\n",
      "[111,    95] loss: 35804.662\n",
      "[111,   100] loss: 23072.389\n",
      "[111,   105] loss: 22038.325\n",
      "[111,   110] loss: 19856.821\n",
      "[111,   115] loss: 28788.283\n",
      "[111,   120] loss: 22925.785\n",
      "[111,   125] loss: 20986.177\n",
      "[111,   130] loss: 30873.239\n",
      "[111,   135] loss: 29587.690\n",
      "[111,   140] loss: 25730.788\n",
      "[111,   145] loss: 34665.565\n",
      "[111,   150] loss: 19213.200\n",
      "[111,   155] loss: 25904.730\n",
      "[111,   160] loss: 31769.468\n",
      "[111,   165] loss: 29796.787\n",
      "[111,   170] loss: 39914.255\n",
      "[111,   175] loss: 22004.678\n",
      "[111,   180] loss: 19052.871\n",
      "[111,   185] loss: 24242.643\n",
      "[111,   190] loss: 26068.275\n",
      "[111,   195] loss: 25565.945\n",
      "[111,   200] loss: 20661.120\n",
      "[111,   205] loss: 19413.034\n",
      "[111,   210] loss: 19632.896\n",
      "[111,   215] loss: 30254.704\n",
      "[111,   220] loss: 31634.028\n",
      "[111,   225] loss: 22545.065\n",
      "[111,   230] loss: 23805.103\n",
      "[112,     5] loss: 22287.603\n",
      "[112,    10] loss: 32500.020\n",
      "[112,    15] loss: 28036.993\n",
      "[112,    20] loss: 40508.666\n",
      "[112,    25] loss: 25275.298\n",
      "[112,    30] loss: 28164.255\n",
      "[112,    35] loss: 27045.892\n",
      "[112,    40] loss: 36999.950\n",
      "[112,    45] loss: 22123.844\n",
      "[112,    50] loss: 21099.646\n",
      "[112,    55] loss: 22813.201\n",
      "[112,    60] loss: 30082.284\n",
      "[112,    65] loss: 28008.521\n",
      "[112,    70] loss: 20664.515\n",
      "[112,    75] loss: 25712.957\n",
      "[112,    80] loss: 29540.895\n",
      "[112,    85] loss: 22359.805\n",
      "[112,    90] loss: 28218.797\n",
      "[112,    95] loss: 26125.220\n",
      "[112,   100] loss: 27368.827\n",
      "[112,   105] loss: 22511.672\n",
      "[112,   110] loss: 19494.356\n",
      "[112,   115] loss: 26585.521\n",
      "[112,   120] loss: 25792.982\n",
      "[112,   125] loss: 37732.992\n",
      "[112,   130] loss: 14922.791\n",
      "[112,   135] loss: 27541.502\n",
      "[112,   140] loss: 39530.209\n",
      "[112,   145] loss: 33794.192\n",
      "[112,   150] loss: 13347.862\n",
      "[112,   155] loss: 52165.338\n",
      "[112,   160] loss: 35099.613\n",
      "[112,   165] loss: 24324.974\n",
      "[112,   170] loss: 22781.605\n",
      "[112,   175] loss: 27744.354\n",
      "[112,   180] loss: 26433.988\n",
      "[112,   185] loss: 38325.192\n",
      "[112,   190] loss: 23199.624\n",
      "[112,   195] loss: 24871.769\n",
      "[112,   200] loss: 26126.219\n",
      "[112,   205] loss: 19077.847\n",
      "[112,   210] loss: 27453.927\n",
      "[112,   215] loss: 17670.945\n",
      "[112,   220] loss: 24084.245\n",
      "[112,   225] loss: 23513.834\n",
      "[112,   230] loss: 23348.662\n",
      "[113,     5] loss: 26932.473\n",
      "[113,    10] loss: 25226.955\n",
      "[113,    15] loss: 27352.778\n",
      "[113,    20] loss: 26357.754\n",
      "[113,    25] loss: 23788.195\n",
      "[113,    30] loss: 28342.004\n",
      "[113,    35] loss: 24742.457\n",
      "[113,    40] loss: 21324.628\n",
      "[113,    45] loss: 37639.425\n",
      "[113,    50] loss: 21031.021\n",
      "[113,    55] loss: 17840.306\n",
      "[113,    60] loss: 35906.908\n",
      "[113,    65] loss: 23303.357\n",
      "[113,    70] loss: 21054.320\n",
      "[113,    75] loss: 28791.591\n",
      "[113,    80] loss: 39869.122\n",
      "[113,    85] loss: 42232.737\n",
      "[113,    90] loss: 17960.181\n",
      "[113,    95] loss: 26269.050\n",
      "[113,   100] loss: 27168.468\n",
      "[113,   105] loss: 29628.665\n",
      "[113,   110] loss: 27773.281\n",
      "[113,   115] loss: 27885.823\n",
      "[113,   120] loss: 21275.035\n",
      "[113,   125] loss: 18225.426\n",
      "[113,   130] loss: 17907.101\n",
      "[113,   135] loss: 27405.009\n",
      "[113,   140] loss: 20985.874\n",
      "[113,   145] loss: 17179.099\n",
      "[113,   150] loss: 35419.952\n",
      "[113,   155] loss: 51294.620\n",
      "[113,   160] loss: 28269.751\n",
      "[113,   165] loss: 17939.548\n",
      "[113,   170] loss: 26614.036\n",
      "[113,   175] loss: 24424.098\n",
      "[113,   180] loss: 30709.879\n",
      "[113,   185] loss: 21537.025\n",
      "[113,   190] loss: 34251.466\n",
      "[113,   195] loss: 24648.042\n",
      "[113,   200] loss: 29296.550\n",
      "[113,   205] loss: 26874.663\n",
      "[113,   210] loss: 38013.443\n",
      "[113,   215] loss: 19675.794\n",
      "[113,   220] loss: 17819.734\n",
      "[113,   225] loss: 18398.818\n",
      "[113,   230] loss: 31166.498\n",
      "[114,     5] loss: 27096.264\n",
      "[114,    10] loss: 21995.767\n",
      "[114,    15] loss: 32216.899\n",
      "[114,    20] loss: 26431.801\n",
      "[114,    25] loss: 36985.619\n",
      "[114,    30] loss: 28551.024\n",
      "[114,    35] loss: 26884.213\n",
      "[114,    40] loss: 24300.079\n",
      "[114,    45] loss: 15260.284\n",
      "[114,    50] loss: 16019.037\n",
      "[114,    55] loss: 27337.880\n",
      "[114,    60] loss: 26321.754\n",
      "[114,    65] loss: 24967.750\n",
      "[114,    70] loss: 32231.618\n",
      "[114,    75] loss: 28008.990\n",
      "[114,    80] loss: 26051.352\n",
      "[114,    85] loss: 25206.464\n",
      "[114,    90] loss: 43626.795\n",
      "[114,    95] loss: 37500.978\n",
      "[114,   100] loss: 23869.604\n",
      "[114,   105] loss: 37185.646\n",
      "[114,   110] loss: 23557.499\n",
      "[114,   115] loss: 18845.375\n",
      "[114,   120] loss: 26313.493\n",
      "[114,   125] loss: 28749.156\n",
      "[114,   130] loss: 23605.541\n",
      "[114,   135] loss: 32192.230\n",
      "[114,   140] loss: 17862.073\n",
      "[114,   145] loss: 21486.111\n",
      "[114,   150] loss: 25834.074\n",
      "[114,   155] loss: 38809.298\n",
      "[114,   160] loss: 31757.461\n",
      "[114,   165] loss: 32315.825\n",
      "[114,   170] loss: 32167.618\n",
      "[114,   175] loss: 28539.256\n",
      "[114,   180] loss: 20091.131\n",
      "[114,   185] loss: 30872.025\n",
      "[114,   190] loss: 15404.285\n",
      "[114,   195] loss: 27962.594\n",
      "[114,   200] loss: 22589.977\n",
      "[114,   205] loss: 24578.244\n",
      "[114,   210] loss: 35567.692\n",
      "[114,   215] loss: 23135.852\n",
      "[114,   220] loss: 22374.256\n",
      "[114,   225] loss: 22028.217\n",
      "[114,   230] loss: 29885.427\n",
      "[115,     5] loss: 23591.330\n",
      "[115,    10] loss: 26387.106\n",
      "[115,    15] loss: 22247.002\n",
      "[115,    20] loss: 27948.795\n",
      "[115,    25] loss: 24648.725\n",
      "[115,    30] loss: 31495.683\n",
      "[115,    35] loss: 23694.540\n",
      "[115,    40] loss: 32993.499\n",
      "[115,    45] loss: 28479.297\n",
      "[115,    50] loss: 23809.375\n",
      "[115,    55] loss: 21870.635\n",
      "[115,    60] loss: 26524.270\n",
      "[115,    65] loss: 34364.443\n",
      "[115,    70] loss: 20521.582\n",
      "[115,    75] loss: 40920.285\n",
      "[115,    80] loss: 30645.046\n",
      "[115,    85] loss: 18493.875\n",
      "[115,    90] loss: 19764.502\n",
      "[115,    95] loss: 29964.702\n",
      "[115,   100] loss: 28832.859\n",
      "[115,   105] loss: 22344.597\n",
      "[115,   110] loss: 25739.715\n",
      "[115,   115] loss: 33670.146\n",
      "[115,   120] loss: 30775.937\n",
      "[115,   125] loss: 34037.772\n",
      "[115,   130] loss: 33313.990\n",
      "[115,   135] loss: 19910.062\n",
      "[115,   140] loss: 20511.171\n",
      "[115,   145] loss: 22865.761\n",
      "[115,   150] loss: 33422.870\n",
      "[115,   155] loss: 26861.647\n",
      "[115,   160] loss: 28290.039\n",
      "[115,   165] loss: 29081.746\n",
      "[115,   170] loss: 20062.366\n",
      "[115,   175] loss: 29232.896\n",
      "[115,   180] loss: 21535.372\n",
      "[115,   185] loss: 19262.806\n",
      "[115,   190] loss: 49744.098\n",
      "[115,   195] loss: 32506.464\n",
      "[115,   200] loss: 29755.028\n",
      "[115,   205] loss: 22785.843\n",
      "[115,   210] loss: 26187.360\n",
      "[115,   215] loss: 26864.079\n",
      "[115,   220] loss: 18022.854\n",
      "[115,   225] loss: 20245.227\n",
      "[115,   230] loss: 25730.695\n",
      "[116,     5] loss: 24628.832\n",
      "[116,    10] loss: 24617.730\n",
      "[116,    15] loss: 20837.940\n",
      "[116,    20] loss: 32907.779\n",
      "[116,    25] loss: 20513.614\n",
      "[116,    30] loss: 25626.754\n",
      "[116,    35] loss: 29638.534\n",
      "[116,    40] loss: 30914.844\n",
      "[116,    45] loss: 37386.400\n",
      "[116,    50] loss: 36452.728\n",
      "[116,    55] loss: 25439.798\n",
      "[116,    60] loss: 34915.966\n",
      "[116,    65] loss: 21982.135\n",
      "[116,    70] loss: 19825.996\n",
      "[116,    75] loss: 36137.118\n",
      "[116,    80] loss: 30806.789\n",
      "[116,    85] loss: 14409.150\n",
      "[116,    90] loss: 25811.404\n",
      "[116,    95] loss: 21730.637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116,   100] loss: 28070.095\n",
      "[116,   105] loss: 23940.242\n",
      "[116,   110] loss: 24841.876\n",
      "[116,   115] loss: 17218.510\n",
      "[116,   120] loss: 32562.722\n",
      "[116,   125] loss: 40390.324\n",
      "[116,   130] loss: 43796.006\n",
      "[116,   135] loss: 31799.762\n",
      "[116,   140] loss: 19945.661\n",
      "[116,   145] loss: 36713.268\n",
      "[116,   150] loss: 23564.353\n",
      "[116,   155] loss: 22730.041\n",
      "[116,   160] loss: 29866.281\n",
      "[116,   165] loss: 25366.481\n",
      "[116,   170] loss: 18134.523\n",
      "[116,   175] loss: 26878.222\n",
      "[116,   180] loss: 23581.207\n",
      "[116,   185] loss: 26833.056\n",
      "[116,   190] loss: 27354.177\n",
      "[116,   195] loss: 27993.914\n",
      "[116,   200] loss: 23482.478\n",
      "[116,   205] loss: 39904.988\n",
      "[116,   210] loss: 16982.964\n",
      "[116,   215] loss: 27169.268\n",
      "[116,   220] loss: 22156.160\n",
      "[116,   225] loss: 26795.209\n",
      "[116,   230] loss: 17823.067\n",
      "[117,     5] loss: 25710.147\n",
      "[117,    10] loss: 19020.401\n",
      "[117,    15] loss: 17713.130\n",
      "[117,    20] loss: 27835.579\n",
      "[117,    25] loss: 43902.139\n",
      "[117,    30] loss: 22398.081\n",
      "[117,    35] loss: 17418.056\n",
      "[117,    40] loss: 28742.671\n",
      "[117,    45] loss: 14743.624\n",
      "[117,    50] loss: 16595.370\n",
      "[117,    55] loss: 37131.463\n",
      "[117,    60] loss: 20971.913\n",
      "[117,    65] loss: 30745.219\n",
      "[117,    70] loss: 31561.935\n",
      "[117,    75] loss: 29588.603\n",
      "[117,    80] loss: 23243.613\n",
      "[117,    85] loss: 22156.165\n",
      "[117,    90] loss: 21337.417\n",
      "[117,    95] loss: 35743.088\n",
      "[117,   100] loss: 33305.980\n",
      "[117,   105] loss: 20612.220\n",
      "[117,   110] loss: 29820.301\n",
      "[117,   115] loss: 26647.684\n",
      "[117,   120] loss: 27434.834\n",
      "[117,   125] loss: 27019.558\n",
      "[117,   130] loss: 22986.777\n",
      "[117,   135] loss: 28525.899\n",
      "[117,   140] loss: 23408.545\n",
      "[117,   145] loss: 33193.259\n",
      "[117,   150] loss: 30862.970\n",
      "[117,   155] loss: 33075.441\n",
      "[117,   160] loss: 22277.911\n",
      "[117,   165] loss: 19916.667\n",
      "[117,   170] loss: 30867.005\n",
      "[117,   175] loss: 22404.289\n",
      "[117,   180] loss: 30099.646\n",
      "[117,   185] loss: 31485.266\n",
      "[117,   190] loss: 23080.783\n",
      "[117,   195] loss: 35949.732\n",
      "[117,   200] loss: 22170.395\n",
      "[117,   205] loss: 23770.124\n",
      "[117,   210] loss: 23846.774\n",
      "[117,   215] loss: 53143.574\n",
      "[117,   220] loss: 27934.204\n",
      "[117,   225] loss: 23437.977\n",
      "[117,   230] loss: 16123.461\n",
      "[118,     5] loss: 30269.882\n",
      "[118,    10] loss: 23941.806\n",
      "[118,    15] loss: 24517.254\n",
      "[118,    20] loss: 39721.393\n",
      "[118,    25] loss: 30866.887\n",
      "[118,    30] loss: 24521.718\n",
      "[118,    35] loss: 24089.396\n",
      "[118,    40] loss: 26207.963\n",
      "[118,    45] loss: 29618.240\n",
      "[118,    50] loss: 32580.071\n",
      "[118,    55] loss: 35158.003\n",
      "[118,    60] loss: 23484.169\n",
      "[118,    65] loss: 40113.057\n",
      "[118,    70] loss: 22922.005\n",
      "[118,    75] loss: 14489.450\n",
      "[118,    80] loss: 25301.669\n",
      "[118,    85] loss: 21080.466\n",
      "[118,    90] loss: 35489.680\n",
      "[118,    95] loss: 16962.071\n",
      "[118,   100] loss: 31794.257\n",
      "[118,   105] loss: 31314.777\n",
      "[118,   110] loss: 20713.094\n",
      "[118,   115] loss: 19909.883\n",
      "[118,   120] loss: 25129.547\n",
      "[118,   125] loss: 30863.050\n",
      "[118,   130] loss: 19701.546\n",
      "[118,   135] loss: 30680.443\n",
      "[118,   140] loss: 21706.861\n",
      "[118,   145] loss: 25991.889\n",
      "[118,   150] loss: 19766.304\n",
      "[118,   155] loss: 26394.668\n",
      "[118,   160] loss: 22424.619\n",
      "[118,   165] loss: 25800.955\n",
      "[118,   170] loss: 30620.488\n",
      "[118,   175] loss: 41227.715\n",
      "[118,   180] loss: 24475.322\n",
      "[118,   185] loss: 18770.310\n",
      "[118,   190] loss: 51078.040\n",
      "[118,   195] loss: 26045.885\n",
      "[118,   200] loss: 20532.583\n",
      "[118,   205] loss: 30538.778\n",
      "[118,   210] loss: 23639.570\n",
      "[118,   215] loss: 26633.122\n",
      "[118,   220] loss: 20231.525\n",
      "[118,   225] loss: 34429.399\n",
      "[118,   230] loss: 22531.298\n",
      "[119,     5] loss: 22539.265\n",
      "[119,    10] loss: 32345.736\n",
      "[119,    15] loss: 25990.406\n",
      "[119,    20] loss: 20331.928\n",
      "[119,    25] loss: 31228.800\n",
      "[119,    30] loss: 20909.279\n",
      "[119,    35] loss: 24156.297\n",
      "[119,    40] loss: 31403.048\n",
      "[119,    45] loss: 21056.206\n",
      "[119,    50] loss: 25774.066\n",
      "[119,    55] loss: 23713.918\n",
      "[119,    60] loss: 25075.454\n",
      "[119,    65] loss: 26021.437\n",
      "[119,    70] loss: 26100.282\n",
      "[119,    75] loss: 19060.205\n",
      "[119,    80] loss: 22904.029\n",
      "[119,    85] loss: 23909.290\n",
      "[119,    90] loss: 21979.136\n",
      "[119,    95] loss: 23707.537\n",
      "[119,   100] loss: 24034.109\n",
      "[119,   105] loss: 21354.199\n",
      "[119,   110] loss: 23056.347\n",
      "[119,   115] loss: 30257.416\n",
      "[119,   120] loss: 34280.009\n",
      "[119,   125] loss: 26381.929\n",
      "[119,   130] loss: 28949.882\n",
      "[119,   135] loss: 43290.923\n",
      "[119,   140] loss: 17747.265\n",
      "[119,   145] loss: 33009.861\n",
      "[119,   150] loss: 32123.860\n",
      "[119,   155] loss: 18020.363\n",
      "[119,   160] loss: 25720.449\n",
      "[119,   165] loss: 25877.203\n",
      "[119,   170] loss: 16886.042\n",
      "[119,   175] loss: 32358.462\n",
      "[119,   180] loss: 31286.851\n",
      "[119,   185] loss: 42568.454\n",
      "[119,   190] loss: 18014.151\n",
      "[119,   195] loss: 36636.843\n",
      "[119,   200] loss: 30246.511\n",
      "[119,   205] loss: 26302.532\n",
      "[119,   210] loss: 21199.436\n",
      "[119,   215] loss: 25238.373\n",
      "[119,   220] loss: 33663.152\n",
      "[119,   225] loss: 15584.645\n",
      "[119,   230] loss: 33943.286\n",
      "[120,     5] loss: 24170.723\n",
      "[120,    10] loss: 32989.299\n",
      "[120,    15] loss: 26272.115\n",
      "[120,    20] loss: 25219.746\n",
      "[120,    25] loss: 24863.084\n",
      "[120,    30] loss: 28967.933\n",
      "[120,    35] loss: 32115.984\n",
      "[120,    40] loss: 29290.457\n",
      "[120,    45] loss: 30027.968\n",
      "[120,    50] loss: 20881.527\n",
      "[120,    55] loss: 31025.015\n",
      "[120,    60] loss: 33114.264\n",
      "[120,    65] loss: 21554.992\n",
      "[120,    70] loss: 25050.767\n",
      "[120,    75] loss: 30149.508\n",
      "[120,    80] loss: 16849.779\n",
      "[120,    85] loss: 22711.163\n",
      "[120,    90] loss: 18355.487\n",
      "[120,    95] loss: 30728.263\n",
      "[120,   100] loss: 37734.170\n",
      "[120,   105] loss: 23603.152\n",
      "[120,   110] loss: 34003.564\n",
      "[120,   115] loss: 21445.354\n",
      "[120,   120] loss: 49825.937\n",
      "[120,   125] loss: 33138.848\n",
      "[120,   130] loss: 27811.991\n",
      "[120,   135] loss: 26432.894\n",
      "[120,   140] loss: 28026.816\n",
      "[120,   145] loss: 25346.949\n",
      "[120,   150] loss: 27195.057\n",
      "[120,   155] loss: 22611.767\n",
      "[120,   160] loss: 23221.986\n",
      "[120,   165] loss: 24276.123\n",
      "[120,   170] loss: 25101.955\n",
      "[120,   175] loss: 27893.591\n",
      "[120,   180] loss: 22335.567\n",
      "[120,   185] loss: 33423.899\n",
      "[120,   190] loss: 24306.009\n",
      "[120,   195] loss: 40991.465\n",
      "[120,   200] loss: 18468.415\n",
      "[120,   205] loss: 15630.568\n",
      "[120,   210] loss: 34592.429\n",
      "[120,   215] loss: 18737.002\n",
      "[120,   220] loss: 29900.421\n",
      "[120,   225] loss: 19048.181\n",
      "[120,   230] loss: 23921.728\n",
      "[121,     5] loss: 30317.313\n",
      "[121,    10] loss: 36749.610\n",
      "[121,    15] loss: 33964.569\n",
      "[121,    20] loss: 22119.542\n",
      "[121,    25] loss: 29963.733\n",
      "[121,    30] loss: 55299.891\n",
      "[121,    35] loss: 24057.218\n",
      "[121,    40] loss: 19478.213\n",
      "[121,    45] loss: 19922.692\n",
      "[121,    50] loss: 33771.355\n",
      "[121,    55] loss: 44042.667\n",
      "[121,    60] loss: 22046.593\n",
      "[121,    65] loss: 29743.475\n",
      "[121,    70] loss: 20589.636\n",
      "[121,    75] loss: 32362.987\n",
      "[121,    80] loss: 24877.912\n",
      "[121,    85] loss: 17179.052\n",
      "[121,    90] loss: 35029.051\n",
      "[121,    95] loss: 26030.225\n",
      "[121,   100] loss: 25327.994\n",
      "[121,   105] loss: 17513.531\n",
      "[121,   110] loss: 18371.331\n",
      "[121,   115] loss: 17394.286\n",
      "[121,   120] loss: 19006.567\n",
      "[121,   125] loss: 25620.808\n",
      "[121,   130] loss: 25636.123\n",
      "[121,   135] loss: 17844.439\n",
      "[121,   140] loss: 19945.321\n",
      "[121,   145] loss: 53477.765\n",
      "[121,   150] loss: 31774.820\n",
      "[121,   155] loss: 18119.950\n",
      "[121,   160] loss: 37159.979\n",
      "[121,   165] loss: 20226.007\n",
      "[121,   170] loss: 32160.457\n",
      "[121,   175] loss: 23855.606\n",
      "[121,   180] loss: 19713.934\n",
      "[121,   185] loss: 36693.308\n",
      "[121,   190] loss: 22266.687\n",
      "[121,   195] loss: 22310.740\n",
      "[121,   200] loss: 28486.036\n",
      "[121,   205] loss: 21459.467\n",
      "[121,   210] loss: 22788.717\n",
      "[121,   215] loss: 24439.483\n",
      "[121,   220] loss: 27770.452\n",
      "[121,   225] loss: 24045.544\n",
      "[121,   230] loss: 26930.119\n",
      "[122,     5] loss: 28548.391\n",
      "[122,    10] loss: 35301.247\n",
      "[122,    15] loss: 31765.996\n",
      "[122,    20] loss: 17911.826\n",
      "[122,    25] loss: 17031.378\n",
      "[122,    30] loss: 31661.561\n",
      "[122,    35] loss: 18778.599\n",
      "[122,    40] loss: 21225.863\n",
      "[122,    45] loss: 20852.220\n",
      "[122,    50] loss: 47567.986\n",
      "[122,    55] loss: 23220.649\n",
      "[122,    60] loss: 18457.959\n",
      "[122,    65] loss: 33748.574\n",
      "[122,    70] loss: 45020.710\n",
      "[122,    75] loss: 22925.741\n",
      "[122,    80] loss: 22704.134\n",
      "[122,    85] loss: 29901.124\n",
      "[122,    90] loss: 24563.686\n",
      "[122,    95] loss: 31919.402\n",
      "[122,   100] loss: 27094.672\n",
      "[122,   105] loss: 20045.036\n",
      "[122,   110] loss: 31507.757\n",
      "[122,   115] loss: 16841.727\n",
      "[122,   120] loss: 22622.171\n",
      "[122,   125] loss: 26022.498\n",
      "[122,   130] loss: 32189.846\n",
      "[122,   135] loss: 32117.761\n",
      "[122,   140] loss: 26237.762\n",
      "[122,   145] loss: 18388.954\n",
      "[122,   150] loss: 19376.372\n",
      "[122,   155] loss: 27280.055\n",
      "[122,   160] loss: 38804.872\n",
      "[122,   165] loss: 25833.001\n",
      "[122,   170] loss: 19780.463\n",
      "[122,   175] loss: 19618.199\n",
      "[122,   180] loss: 34020.796\n",
      "[122,   185] loss: 23124.322\n",
      "[122,   190] loss: 22424.845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122,   195] loss: 52617.400\n",
      "[122,   200] loss: 33345.890\n",
      "[122,   205] loss: 21636.856\n",
      "[122,   210] loss: 31607.234\n",
      "[122,   215] loss: 21558.448\n",
      "[122,   220] loss: 24803.912\n",
      "[122,   225] loss: 19835.162\n",
      "[122,   230] loss: 21838.674\n",
      "[123,     5] loss: 22934.952\n",
      "[123,    10] loss: 25192.765\n",
      "[123,    15] loss: 31270.322\n",
      "[123,    20] loss: 21259.603\n",
      "[123,    25] loss: 35696.279\n",
      "[123,    30] loss: 19718.588\n",
      "[123,    35] loss: 27717.773\n",
      "[123,    40] loss: 23103.521\n",
      "[123,    45] loss: 20508.028\n",
      "[123,    50] loss: 19408.441\n",
      "[123,    55] loss: 21726.734\n",
      "[123,    60] loss: 21696.792\n",
      "[123,    65] loss: 35671.814\n",
      "[123,    70] loss: 53390.988\n",
      "[123,    75] loss: 29113.407\n",
      "[123,    80] loss: 22407.064\n",
      "[123,    85] loss: 23344.003\n",
      "[123,    90] loss: 40864.490\n",
      "[123,    95] loss: 15857.694\n",
      "[123,   100] loss: 25743.040\n",
      "[123,   105] loss: 21663.629\n",
      "[123,   110] loss: 27909.204\n",
      "[123,   115] loss: 23086.408\n",
      "[123,   120] loss: 24794.222\n",
      "[123,   125] loss: 23238.218\n",
      "[123,   130] loss: 21683.392\n",
      "[123,   135] loss: 17644.775\n",
      "[123,   140] loss: 15872.202\n",
      "[123,   145] loss: 20417.021\n",
      "[123,   150] loss: 26413.218\n",
      "[123,   155] loss: 34073.293\n",
      "[123,   160] loss: 51072.150\n",
      "[123,   165] loss: 25798.969\n",
      "[123,   170] loss: 31553.503\n",
      "[123,   175] loss: 29666.961\n",
      "[123,   180] loss: 27948.462\n",
      "[123,   185] loss: 24597.138\n",
      "[123,   190] loss: 18480.079\n",
      "[123,   195] loss: 17661.171\n",
      "[123,   200] loss: 36651.243\n",
      "[123,   205] loss: 29006.697\n",
      "[123,   210] loss: 20731.093\n",
      "[123,   215] loss: 16510.995\n",
      "[123,   220] loss: 36007.244\n",
      "[123,   225] loss: 39360.248\n",
      "[123,   230] loss: 40197.082\n",
      "[124,     5] loss: 22235.339\n",
      "[124,    10] loss: 24348.599\n",
      "[124,    15] loss: 26337.483\n",
      "[124,    20] loss: 27035.081\n",
      "[124,    25] loss: 39844.938\n",
      "[124,    30] loss: 22197.358\n",
      "[124,    35] loss: 24743.891\n",
      "[124,    40] loss: 24825.107\n",
      "[124,    45] loss: 15981.644\n",
      "[124,    50] loss: 24766.440\n",
      "[124,    55] loss: 32231.233\n",
      "[124,    60] loss: 55920.940\n",
      "[124,    65] loss: 30164.040\n",
      "[124,    70] loss: 34147.400\n",
      "[124,    75] loss: 21729.625\n",
      "[124,    80] loss: 19900.877\n",
      "[124,    85] loss: 43053.502\n",
      "[124,    90] loss: 29874.568\n",
      "[124,    95] loss: 29477.352\n",
      "[124,   100] loss: 14382.392\n",
      "[124,   105] loss: 24492.686\n",
      "[124,   110] loss: 28414.330\n",
      "[124,   115] loss: 18039.444\n",
      "[124,   120] loss: 22379.758\n",
      "[124,   125] loss: 24813.463\n",
      "[124,   130] loss: 21743.796\n",
      "[124,   135] loss: 28050.777\n",
      "[124,   140] loss: 37604.529\n",
      "[124,   145] loss: 23087.282\n",
      "[124,   150] loss: 23912.434\n",
      "[124,   155] loss: 24004.694\n",
      "[124,   160] loss: 29684.683\n",
      "[124,   165] loss: 16285.621\n",
      "[124,   170] loss: 21189.436\n",
      "[124,   175] loss: 27955.039\n",
      "[124,   180] loss: 27276.190\n",
      "[124,   185] loss: 34596.321\n",
      "[124,   190] loss: 33379.718\n",
      "[124,   195] loss: 26823.801\n",
      "[124,   200] loss: 21652.139\n",
      "[124,   205] loss: 30991.617\n",
      "[124,   210] loss: 24706.472\n",
      "[124,   215] loss: 21168.829\n",
      "[124,   220] loss: 25713.913\n",
      "[124,   225] loss: 26301.822\n",
      "[124,   230] loss: 26238.239\n",
      "[125,     5] loss: 34389.167\n",
      "[125,    10] loss: 24844.378\n",
      "[125,    15] loss: 29871.551\n",
      "[125,    20] loss: 24311.754\n",
      "[125,    25] loss: 24138.479\n",
      "[125,    30] loss: 31210.156\n",
      "[125,    35] loss: 23175.574\n",
      "[125,    40] loss: 28743.130\n",
      "[125,    45] loss: 22296.252\n",
      "[125,    50] loss: 17883.657\n",
      "[125,    55] loss: 17324.368\n",
      "[125,    60] loss: 26494.285\n",
      "[125,    65] loss: 16861.557\n",
      "[125,    70] loss: 24561.403\n",
      "[125,    75] loss: 41443.664\n",
      "[125,    80] loss: 44750.628\n",
      "[125,    85] loss: 26180.110\n",
      "[125,    90] loss: 26526.396\n",
      "[125,    95] loss: 21172.424\n",
      "[125,   100] loss: 21128.453\n",
      "[125,   105] loss: 35243.710\n",
      "[125,   110] loss: 26516.106\n",
      "[125,   115] loss: 40170.331\n",
      "[125,   120] loss: 30170.070\n",
      "[125,   125] loss: 24685.622\n",
      "[125,   130] loss: 20013.958\n",
      "[125,   135] loss: 16200.345\n",
      "[125,   140] loss: 33322.292\n",
      "[125,   145] loss: 23774.455\n",
      "[125,   150] loss: 30027.740\n",
      "[125,   155] loss: 46305.942\n",
      "[125,   160] loss: 24643.019\n",
      "[125,   165] loss: 21978.047\n",
      "[125,   170] loss: 28559.621\n",
      "[125,   175] loss: 26685.070\n",
      "[125,   180] loss: 31158.973\n",
      "[125,   185] loss: 23042.123\n",
      "[125,   190] loss: 24021.119\n",
      "[125,   195] loss: 31320.770\n",
      "[125,   200] loss: 21478.405\n",
      "[125,   205] loss: 22610.771\n",
      "[125,   210] loss: 24770.373\n",
      "[125,   215] loss: 24077.269\n",
      "[125,   220] loss: 24506.453\n",
      "[125,   225] loss: 20116.904\n",
      "[125,   230] loss: 23231.877\n",
      "[126,     5] loss: 24179.807\n",
      "[126,    10] loss: 21972.882\n",
      "[126,    15] loss: 28449.343\n",
      "[126,    20] loss: 28070.998\n",
      "[126,    25] loss: 23659.520\n",
      "[126,    30] loss: 23516.225\n",
      "[126,    35] loss: 24893.898\n",
      "[126,    40] loss: 23037.698\n",
      "[126,    45] loss: 30434.576\n",
      "[126,    50] loss: 26494.066\n",
      "[126,    55] loss: 50418.532\n",
      "[126,    60] loss: 21981.164\n",
      "[126,    65] loss: 33922.076\n",
      "[126,    70] loss: 18857.704\n",
      "[126,    75] loss: 36674.384\n",
      "[126,    80] loss: 31160.090\n",
      "[126,    85] loss: 21936.314\n",
      "[126,    90] loss: 27060.636\n",
      "[126,    95] loss: 27339.527\n",
      "[126,   100] loss: 20972.488\n",
      "[126,   105] loss: 25689.879\n",
      "[126,   110] loss: 31429.780\n",
      "[126,   115] loss: 20597.333\n",
      "[126,   120] loss: 27775.425\n",
      "[126,   125] loss: 22369.419\n",
      "[126,   130] loss: 27004.508\n",
      "[126,   135] loss: 28384.558\n",
      "[126,   140] loss: 22128.266\n",
      "[126,   145] loss: 29079.486\n",
      "[126,   150] loss: 31234.235\n",
      "[126,   155] loss: 20871.111\n",
      "[126,   160] loss: 23330.467\n",
      "[126,   165] loss: 24346.246\n",
      "[126,   170] loss: 24923.832\n",
      "[126,   175] loss: 33384.600\n",
      "[126,   180] loss: 27743.978\n",
      "[126,   185] loss: 31925.422\n",
      "[126,   190] loss: 35873.483\n",
      "[126,   195] loss: 33981.136\n",
      "[126,   200] loss: 32295.933\n",
      "[126,   205] loss: 25137.158\n",
      "[126,   210] loss: 23782.560\n",
      "[126,   215] loss: 25965.273\n",
      "[126,   220] loss: 17687.178\n",
      "[126,   225] loss: 19317.810\n",
      "[126,   230] loss: 24850.511\n",
      "[127,     5] loss: 25793.123\n",
      "[127,    10] loss: 36420.391\n",
      "[127,    15] loss: 23025.210\n",
      "[127,    20] loss: 26812.605\n",
      "[127,    25] loss: 62576.702\n",
      "[127,    30] loss: 36273.004\n",
      "[127,    35] loss: 31205.079\n",
      "[127,    40] loss: 24212.564\n",
      "[127,    45] loss: 19425.940\n",
      "[127,    50] loss: 20128.994\n",
      "[127,    55] loss: 20840.571\n",
      "[127,    60] loss: 21828.612\n",
      "[127,    65] loss: 21814.601\n",
      "[127,    70] loss: 24848.898\n",
      "[127,    75] loss: 17957.277\n",
      "[127,    80] loss: 16798.468\n",
      "[127,    85] loss: 25764.127\n",
      "[127,    90] loss: 21115.906\n",
      "[127,    95] loss: 17134.468\n",
      "[127,   100] loss: 24793.603\n",
      "[127,   105] loss: 25934.370\n",
      "[127,   110] loss: 25147.405\n",
      "[127,   115] loss: 26677.880\n",
      "[127,   120] loss: 20971.774\n",
      "[127,   125] loss: 33283.327\n",
      "[127,   130] loss: 28961.683\n",
      "[127,   135] loss: 28094.926\n",
      "[127,   140] loss: 25387.844\n",
      "[127,   145] loss: 29068.134\n",
      "[127,   150] loss: 22283.167\n",
      "[127,   155] loss: 30648.551\n",
      "[127,   160] loss: 21045.605\n",
      "[127,   165] loss: 41161.039\n",
      "[127,   170] loss: 25886.606\n",
      "[127,   175] loss: 20111.242\n",
      "[127,   180] loss: 30934.912\n",
      "[127,   185] loss: 42026.566\n",
      "[127,   190] loss: 28606.791\n",
      "[127,   195] loss: 39193.773\n",
      "[127,   200] loss: 18094.352\n",
      "[127,   205] loss: 23632.468\n",
      "[127,   210] loss: 21772.458\n",
      "[127,   215] loss: 31823.955\n",
      "[127,   220] loss: 26407.562\n",
      "[127,   225] loss: 21921.946\n",
      "[127,   230] loss: 24874.886\n",
      "[128,     5] loss: 32715.281\n",
      "[128,    10] loss: 34880.784\n",
      "[128,    15] loss: 21175.563\n",
      "[128,    20] loss: 30309.061\n",
      "[128,    25] loss: 28444.418\n",
      "[128,    30] loss: 28845.581\n",
      "[128,    35] loss: 23738.219\n",
      "[128,    40] loss: 21179.894\n",
      "[128,    45] loss: 26948.256\n",
      "[128,    50] loss: 30282.680\n",
      "[128,    55] loss: 19670.629\n",
      "[128,    60] loss: 33207.175\n",
      "[128,    65] loss: 29286.248\n",
      "[128,    70] loss: 21540.742\n",
      "[128,    75] loss: 24900.624\n",
      "[128,    80] loss: 22887.798\n",
      "[128,    85] loss: 52283.371\n",
      "[128,    90] loss: 29766.349\n",
      "[128,    95] loss: 23790.419\n",
      "[128,   100] loss: 25516.853\n",
      "[128,   105] loss: 21467.812\n",
      "[128,   110] loss: 20580.330\n",
      "[128,   115] loss: 22562.563\n",
      "[128,   120] loss: 32462.100\n",
      "[128,   125] loss: 43764.558\n",
      "[128,   130] loss: 24750.818\n",
      "[128,   135] loss: 21602.833\n",
      "[128,   140] loss: 21871.831\n",
      "[128,   145] loss: 23281.903\n",
      "[128,   150] loss: 30550.875\n",
      "[128,   155] loss: 22065.094\n",
      "[128,   160] loss: 22449.210\n",
      "[128,   165] loss: 21485.389\n",
      "[128,   170] loss: 26195.042\n",
      "[128,   175] loss: 23420.255\n",
      "[128,   180] loss: 31115.754\n",
      "[128,   185] loss: 23711.190\n",
      "[128,   190] loss: 20907.943\n",
      "[128,   195] loss: 25449.314\n",
      "[128,   200] loss: 19007.229\n",
      "[128,   205] loss: 27895.270\n",
      "[128,   210] loss: 32741.014\n",
      "[128,   215] loss: 26590.753\n",
      "[128,   220] loss: 29430.318\n",
      "[128,   225] loss: 21210.972\n",
      "[128,   230] loss: 32042.697\n",
      "[129,     5] loss: 20399.162\n",
      "[129,    10] loss: 21379.139\n",
      "[129,    15] loss: 23934.678\n",
      "[129,    20] loss: 33422.705\n",
      "[129,    25] loss: 30198.661\n",
      "[129,    30] loss: 24510.719\n",
      "[129,    35] loss: 29564.073\n",
      "[129,    40] loss: 32816.231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129,    45] loss: 29165.617\n",
      "[129,    50] loss: 28535.401\n",
      "[129,    55] loss: 26427.685\n",
      "[129,    60] loss: 20834.602\n",
      "[129,    65] loss: 33489.893\n",
      "[129,    70] loss: 28192.817\n",
      "[129,    75] loss: 26233.642\n",
      "[129,    80] loss: 30789.089\n",
      "[129,    85] loss: 24008.189\n",
      "[129,    90] loss: 28925.896\n",
      "[129,    95] loss: 23331.707\n",
      "[129,   100] loss: 25019.084\n",
      "[129,   105] loss: 20514.065\n",
      "[129,   110] loss: 30189.875\n",
      "[129,   115] loss: 18939.347\n",
      "[129,   120] loss: 40195.210\n",
      "[129,   125] loss: 17731.775\n",
      "[129,   130] loss: 27400.274\n",
      "[129,   135] loss: 28643.536\n",
      "[129,   140] loss: 23649.226\n",
      "[129,   145] loss: 21227.872\n",
      "[129,   150] loss: 26004.012\n",
      "[129,   155] loss: 23926.571\n",
      "[129,   160] loss: 31277.136\n",
      "[129,   165] loss: 24704.711\n",
      "[129,   170] loss: 29236.641\n",
      "[129,   175] loss: 18156.954\n",
      "[129,   180] loss: 28800.156\n",
      "[129,   185] loss: 23412.095\n",
      "[129,   190] loss: 23182.975\n",
      "[129,   195] loss: 21675.354\n",
      "[129,   200] loss: 30044.968\n",
      "[129,   205] loss: 25774.630\n",
      "[129,   210] loss: 26113.147\n",
      "[129,   215] loss: 22746.154\n",
      "[129,   220] loss: 26301.482\n",
      "[129,   225] loss: 45958.399\n",
      "[129,   230] loss: 32022.028\n",
      "[130,     5] loss: 18950.978\n",
      "[130,    10] loss: 40751.985\n",
      "[130,    15] loss: 21100.934\n",
      "[130,    20] loss: 21554.697\n",
      "[130,    25] loss: 30889.702\n",
      "[130,    30] loss: 22583.456\n",
      "[130,    35] loss: 44611.411\n",
      "[130,    40] loss: 29710.387\n",
      "[130,    45] loss: 22881.057\n",
      "[130,    50] loss: 24842.161\n",
      "[130,    55] loss: 19909.816\n",
      "[130,    60] loss: 25464.177\n",
      "[130,    65] loss: 27637.789\n",
      "[130,    70] loss: 31084.321\n",
      "[130,    75] loss: 25125.410\n",
      "[130,    80] loss: 19968.951\n",
      "[130,    85] loss: 21948.464\n",
      "[130,    90] loss: 28737.930\n",
      "[130,    95] loss: 26075.115\n",
      "[130,   100] loss: 31282.435\n",
      "[130,   105] loss: 22440.182\n",
      "[130,   110] loss: 22751.569\n",
      "[130,   115] loss: 29310.499\n",
      "[130,   120] loss: 26601.076\n",
      "[130,   125] loss: 35279.294\n",
      "[130,   130] loss: 30564.245\n",
      "[130,   135] loss: 23034.343\n",
      "[130,   140] loss: 19057.856\n",
      "[130,   145] loss: 19883.027\n",
      "[130,   150] loss: 21598.230\n",
      "[130,   155] loss: 23992.669\n",
      "[130,   160] loss: 23966.557\n",
      "[130,   165] loss: 18033.589\n",
      "[130,   170] loss: 21664.896\n",
      "[130,   175] loss: 21193.380\n",
      "[130,   180] loss: 34101.499\n",
      "[130,   185] loss: 38608.945\n",
      "[130,   190] loss: 33587.819\n",
      "[130,   195] loss: 37887.730\n",
      "[130,   200] loss: 26659.255\n",
      "[130,   205] loss: 32656.244\n",
      "[130,   210] loss: 29229.113\n",
      "[130,   215] loss: 32743.368\n",
      "[130,   220] loss: 17375.234\n",
      "[130,   225] loss: 26936.840\n",
      "[130,   230] loss: 22593.629\n",
      "[131,     5] loss: 25378.558\n",
      "[131,    10] loss: 30837.316\n",
      "[131,    15] loss: 21312.189\n",
      "[131,    20] loss: 23719.928\n",
      "[131,    25] loss: 22268.972\n",
      "[131,    30] loss: 28750.335\n",
      "[131,    35] loss: 26546.578\n",
      "[131,    40] loss: 22153.497\n",
      "[131,    45] loss: 18445.158\n",
      "[131,    50] loss: 27488.096\n",
      "[131,    55] loss: 54758.464\n",
      "[131,    60] loss: 38513.489\n",
      "[131,    65] loss: 49859.463\n",
      "[131,    70] loss: 31500.127\n",
      "[131,    75] loss: 24135.458\n",
      "[131,    80] loss: 30831.900\n",
      "[131,    85] loss: 20598.921\n",
      "[131,    90] loss: 22857.588\n",
      "[131,    95] loss: 25921.912\n",
      "[131,   100] loss: 25339.531\n",
      "[131,   105] loss: 22502.530\n",
      "[131,   110] loss: 23831.838\n",
      "[131,   115] loss: 21084.374\n",
      "[131,   120] loss: 22526.298\n",
      "[131,   125] loss: 31351.045\n",
      "[131,   130] loss: 31050.871\n",
      "[131,   135] loss: 20749.336\n",
      "[131,   140] loss: 18345.298\n",
      "[131,   145] loss: 27317.107\n",
      "[131,   150] loss: 19077.247\n",
      "[131,   155] loss: 26842.530\n",
      "[131,   160] loss: 28078.734\n",
      "[131,   165] loss: 29142.661\n",
      "[131,   170] loss: 31171.462\n",
      "[131,   175] loss: 20589.409\n",
      "[131,   180] loss: 25660.112\n",
      "[131,   185] loss: 30647.629\n",
      "[131,   190] loss: 36203.267\n",
      "[131,   195] loss: 31209.335\n",
      "[131,   200] loss: 23420.748\n",
      "[131,   205] loss: 32656.065\n",
      "[131,   210] loss: 18258.009\n",
      "[131,   215] loss: 19814.461\n",
      "[131,   220] loss: 19951.184\n",
      "[131,   225] loss: 27638.362\n",
      "[131,   230] loss: 22398.616\n",
      "[132,     5] loss: 24848.700\n",
      "[132,    10] loss: 22138.241\n",
      "[132,    15] loss: 36198.056\n",
      "[132,    20] loss: 20061.130\n",
      "[132,    25] loss: 23562.595\n",
      "[132,    30] loss: 25387.822\n",
      "[132,    35] loss: 26245.805\n",
      "[132,    40] loss: 31868.748\n",
      "[132,    45] loss: 30175.179\n",
      "[132,    50] loss: 22525.089\n",
      "[132,    55] loss: 30499.778\n",
      "[132,    60] loss: 26783.538\n",
      "[132,    65] loss: 23696.811\n",
      "[132,    70] loss: 20553.576\n",
      "[132,    75] loss: 48100.350\n",
      "[132,    80] loss: 29032.188\n",
      "[132,    85] loss: 20340.923\n",
      "[132,    90] loss: 19508.724\n",
      "[132,    95] loss: 29635.464\n",
      "[132,   100] loss: 25441.802\n",
      "[132,   105] loss: 31519.670\n",
      "[132,   110] loss: 28901.179\n",
      "[132,   115] loss: 24099.603\n",
      "[132,   120] loss: 29542.545\n",
      "[132,   125] loss: 40938.288\n",
      "[132,   130] loss: 22570.640\n",
      "[132,   135] loss: 23056.056\n",
      "[132,   140] loss: 23858.079\n",
      "[132,   145] loss: 35555.717\n",
      "[132,   150] loss: 25866.500\n",
      "[132,   155] loss: 29365.751\n",
      "[132,   160] loss: 25061.097\n",
      "[132,   165] loss: 16337.137\n",
      "[132,   170] loss: 34164.847\n",
      "[132,   175] loss: 25375.573\n",
      "[132,   180] loss: 30271.642\n",
      "[132,   185] loss: 39738.698\n",
      "[132,   190] loss: 28394.452\n",
      "[132,   195] loss: 32002.909\n",
      "[132,   200] loss: 17537.057\n",
      "[132,   205] loss: 20333.231\n",
      "[132,   210] loss: 23437.510\n",
      "[132,   215] loss: 25527.265\n",
      "[132,   220] loss: 23545.289\n",
      "[132,   225] loss: 19346.119\n",
      "[132,   230] loss: 20081.241\n",
      "[133,     5] loss: 31354.055\n",
      "[133,    10] loss: 34427.937\n",
      "[133,    15] loss: 18912.897\n",
      "[133,    20] loss: 26675.676\n",
      "[133,    25] loss: 17592.707\n",
      "[133,    30] loss: 32628.174\n",
      "[133,    35] loss: 27984.175\n",
      "[133,    40] loss: 35342.299\n",
      "[133,    45] loss: 26624.377\n",
      "[133,    50] loss: 47805.969\n",
      "[133,    55] loss: 26365.219\n",
      "[133,    60] loss: 21297.000\n",
      "[133,    65] loss: 20206.146\n",
      "[133,    70] loss: 14411.149\n",
      "[133,    75] loss: 22048.758\n",
      "[133,    80] loss: 28976.349\n",
      "[133,    85] loss: 21638.199\n",
      "[133,    90] loss: 23310.209\n",
      "[133,    95] loss: 27783.504\n",
      "[133,   100] loss: 19808.634\n",
      "[133,   105] loss: 28775.236\n",
      "[133,   110] loss: 29085.200\n",
      "[133,   115] loss: 21129.278\n",
      "[133,   120] loss: 18920.610\n",
      "[133,   125] loss: 26712.004\n",
      "[133,   130] loss: 17914.926\n",
      "[133,   135] loss: 27746.036\n",
      "[133,   140] loss: 26439.846\n",
      "[133,   145] loss: 25874.814\n",
      "[133,   150] loss: 40505.675\n",
      "[133,   155] loss: 27242.932\n",
      "[133,   160] loss: 17554.835\n",
      "[133,   165] loss: 36856.677\n",
      "[133,   170] loss: 25995.273\n",
      "[133,   175] loss: 26693.075\n",
      "[133,   180] loss: 29102.606\n",
      "[133,   185] loss: 28107.542\n",
      "[133,   190] loss: 28825.893\n",
      "[133,   195] loss: 30119.732\n",
      "[133,   200] loss: 24100.883\n",
      "[133,   205] loss: 35197.097\n",
      "[133,   210] loss: 22592.270\n",
      "[133,   215] loss: 28237.753\n",
      "[133,   220] loss: 23702.628\n",
      "[133,   225] loss: 31794.275\n",
      "[133,   230] loss: 23368.812\n",
      "[134,     5] loss: 27187.971\n",
      "[134,    10] loss: 23383.714\n",
      "[134,    15] loss: 21611.712\n",
      "[134,    20] loss: 27881.118\n",
      "[134,    25] loss: 24702.202\n",
      "[134,    30] loss: 53096.329\n",
      "[134,    35] loss: 19649.316\n",
      "[134,    40] loss: 30031.186\n",
      "[134,    45] loss: 28878.561\n",
      "[134,    50] loss: 29098.265\n",
      "[134,    55] loss: 26594.328\n",
      "[134,    60] loss: 26396.393\n",
      "[134,    65] loss: 26030.215\n",
      "[134,    70] loss: 19627.892\n",
      "[134,    75] loss: 18952.187\n",
      "[134,    80] loss: 29201.811\n",
      "[134,    85] loss: 29170.212\n",
      "[134,    90] loss: 17288.989\n",
      "[134,    95] loss: 17944.470\n",
      "[134,   100] loss: 30295.271\n",
      "[134,   105] loss: 21388.512\n",
      "[134,   110] loss: 24520.345\n",
      "[134,   115] loss: 28785.912\n",
      "[134,   120] loss: 23817.162\n",
      "[134,   125] loss: 18388.562\n",
      "[134,   130] loss: 28025.014\n",
      "[134,   135] loss: 29650.813\n",
      "[134,   140] loss: 22078.978\n",
      "[134,   145] loss: 20333.726\n",
      "[134,   150] loss: 31148.521\n",
      "[134,   155] loss: 22331.512\n",
      "[134,   160] loss: 22606.463\n",
      "[134,   165] loss: 24573.252\n",
      "[134,   170] loss: 24476.245\n",
      "[134,   175] loss: 26207.471\n",
      "[134,   180] loss: 27897.845\n",
      "[134,   185] loss: 35142.263\n",
      "[134,   190] loss: 26872.261\n",
      "[134,   195] loss: 27402.387\n",
      "[134,   200] loss: 26070.091\n",
      "[134,   205] loss: 39379.477\n",
      "[134,   210] loss: 22039.369\n",
      "[134,   215] loss: 33336.874\n",
      "[134,   220] loss: 24302.843\n",
      "[134,   225] loss: 29831.633\n",
      "[134,   230] loss: 40483.896\n",
      "[135,     5] loss: 23848.031\n",
      "[135,    10] loss: 28794.373\n",
      "[135,    15] loss: 32769.272\n",
      "[135,    20] loss: 21383.543\n",
      "[135,    25] loss: 35620.409\n",
      "[135,    30] loss: 20667.624\n",
      "[135,    35] loss: 22639.448\n",
      "[135,    40] loss: 22889.283\n",
      "[135,    45] loss: 20970.977\n",
      "[135,    50] loss: 32327.875\n",
      "[135,    55] loss: 26002.056\n",
      "[135,    60] loss: 44024.480\n",
      "[135,    65] loss: 20629.813\n",
      "[135,    70] loss: 21613.211\n",
      "[135,    75] loss: 21914.663\n",
      "[135,    80] loss: 28223.574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135,    85] loss: 17025.846\n",
      "[135,    90] loss: 30306.301\n",
      "[135,    95] loss: 22557.199\n",
      "[135,   100] loss: 23404.772\n",
      "[135,   105] loss: 20911.007\n",
      "[135,   110] loss: 23763.438\n",
      "[135,   115] loss: 19557.396\n",
      "[135,   120] loss: 20157.296\n",
      "[135,   125] loss: 21171.411\n",
      "[135,   130] loss: 31266.918\n",
      "[135,   135] loss: 32604.021\n",
      "[135,   140] loss: 34305.923\n",
      "[135,   145] loss: 20342.697\n",
      "[135,   150] loss: 26337.494\n",
      "[135,   155] loss: 27909.210\n",
      "[135,   160] loss: 30862.838\n",
      "[135,   165] loss: 18743.733\n",
      "[135,   170] loss: 19321.219\n",
      "[135,   175] loss: 23518.332\n",
      "[135,   180] loss: 22071.789\n",
      "[135,   185] loss: 35228.130\n",
      "[135,   190] loss: 24530.451\n",
      "[135,   195] loss: 32604.894\n",
      "[135,   200] loss: 22012.923\n",
      "[135,   205] loss: 42277.805\n",
      "[135,   210] loss: 27792.697\n",
      "[135,   215] loss: 46696.952\n",
      "[135,   220] loss: 38462.023\n",
      "[135,   225] loss: 23138.921\n",
      "[135,   230] loss: 25629.763\n",
      "[136,     5] loss: 29679.998\n",
      "[136,    10] loss: 18533.304\n",
      "[136,    15] loss: 35248.017\n",
      "[136,    20] loss: 18223.840\n",
      "[136,    25] loss: 33444.534\n",
      "[136,    30] loss: 19493.136\n",
      "[136,    35] loss: 26407.581\n",
      "[136,    40] loss: 31087.342\n",
      "[136,    45] loss: 19106.713\n",
      "[136,    50] loss: 41839.114\n",
      "[136,    55] loss: 23668.125\n",
      "[136,    60] loss: 20094.231\n",
      "[136,    65] loss: 30919.469\n",
      "[136,    70] loss: 20954.557\n",
      "[136,    75] loss: 22163.352\n",
      "[136,    80] loss: 22517.578\n",
      "[136,    85] loss: 28688.179\n",
      "[136,    90] loss: 26504.404\n",
      "[136,    95] loss: 28500.811\n",
      "[136,   100] loss: 19900.966\n",
      "[136,   105] loss: 19403.813\n",
      "[136,   110] loss: 34848.832\n",
      "[136,   115] loss: 21957.932\n",
      "[136,   120] loss: 24965.690\n",
      "[136,   125] loss: 28663.526\n",
      "[136,   130] loss: 37923.799\n",
      "[136,   135] loss: 28036.605\n",
      "[136,   140] loss: 20832.479\n",
      "[136,   145] loss: 18767.240\n",
      "[136,   150] loss: 19582.977\n",
      "[136,   155] loss: 28506.013\n",
      "[136,   160] loss: 33156.205\n",
      "[136,   165] loss: 24511.735\n",
      "[136,   170] loss: 47137.552\n",
      "[136,   175] loss: 27274.561\n",
      "[136,   180] loss: 28606.452\n",
      "[136,   185] loss: 24539.801\n",
      "[136,   190] loss: 33255.517\n",
      "[136,   195] loss: 34376.932\n",
      "[136,   200] loss: 20645.391\n",
      "[136,   205] loss: 17092.321\n",
      "[136,   210] loss: 24765.837\n",
      "[136,   215] loss: 21006.397\n",
      "[136,   220] loss: 24882.958\n",
      "[136,   225] loss: 35473.659\n",
      "[136,   230] loss: 29736.971\n",
      "[137,     5] loss: 26650.234\n",
      "[137,    10] loss: 12324.078\n",
      "[137,    15] loss: 39152.456\n",
      "[137,    20] loss: 30620.539\n",
      "[137,    25] loss: 26432.856\n",
      "[137,    30] loss: 37689.975\n",
      "[137,    35] loss: 30372.210\n",
      "[137,    40] loss: 21033.603\n",
      "[137,    45] loss: 30722.065\n",
      "[137,    50] loss: 21777.125\n",
      "[137,    55] loss: 29232.365\n",
      "[137,    60] loss: 29054.090\n",
      "[137,    65] loss: 33696.864\n",
      "[137,    70] loss: 20365.574\n",
      "[137,    75] loss: 24860.927\n",
      "[137,    80] loss: 18066.478\n",
      "[137,    85] loss: 29461.784\n",
      "[137,    90] loss: 19200.139\n",
      "[137,    95] loss: 28145.435\n",
      "[137,   100] loss: 19856.326\n",
      "[137,   105] loss: 24241.301\n",
      "[137,   110] loss: 23275.398\n",
      "[137,   115] loss: 30973.260\n",
      "[137,   120] loss: 28835.771\n",
      "[137,   125] loss: 47883.482\n",
      "[137,   130] loss: 17423.392\n",
      "[137,   135] loss: 27540.879\n",
      "[137,   140] loss: 26357.948\n",
      "[137,   145] loss: 15932.439\n",
      "[137,   150] loss: 42350.669\n",
      "[137,   155] loss: 24406.731\n",
      "[137,   160] loss: 27622.943\n",
      "[137,   165] loss: 24248.135\n",
      "[137,   170] loss: 23302.834\n",
      "[137,   175] loss: 26461.739\n",
      "[137,   180] loss: 23871.379\n",
      "[137,   185] loss: 36791.438\n",
      "[137,   190] loss: 22485.554\n",
      "[137,   195] loss: 21646.063\n",
      "[137,   200] loss: 21371.412\n",
      "[137,   205] loss: 21466.751\n",
      "[137,   210] loss: 30966.087\n",
      "[137,   215] loss: 22564.242\n",
      "[137,   220] loss: 29302.774\n",
      "[137,   225] loss: 21458.293\n",
      "[137,   230] loss: 36403.045\n",
      "[138,     5] loss: 21970.564\n",
      "[138,    10] loss: 21695.515\n",
      "[138,    15] loss: 19672.176\n",
      "[138,    20] loss: 30257.513\n",
      "[138,    25] loss: 25752.202\n",
      "[138,    30] loss: 24977.680\n",
      "[138,    35] loss: 28594.743\n",
      "[138,    40] loss: 30187.569\n",
      "[138,    45] loss: 21379.625\n",
      "[138,    50] loss: 22705.529\n",
      "[138,    55] loss: 28920.858\n",
      "[138,    60] loss: 28079.796\n",
      "[138,    65] loss: 19527.487\n",
      "[138,    70] loss: 27770.092\n",
      "[138,    75] loss: 37162.807\n",
      "[138,    80] loss: 18858.696\n",
      "[138,    85] loss: 32489.277\n",
      "[138,    90] loss: 22680.359\n",
      "[138,    95] loss: 52930.393\n",
      "[138,   100] loss: 27907.025\n",
      "[138,   105] loss: 20852.812\n",
      "[138,   110] loss: 20312.854\n",
      "[138,   115] loss: 22614.320\n",
      "[138,   120] loss: 24781.449\n",
      "[138,   125] loss: 31782.365\n",
      "[138,   130] loss: 24157.885\n",
      "[138,   135] loss: 28485.287\n",
      "[138,   140] loss: 26658.361\n",
      "[138,   145] loss: 35055.574\n",
      "[138,   150] loss: 25527.716\n",
      "[138,   155] loss: 37963.242\n",
      "[138,   160] loss: 24921.656\n",
      "[138,   165] loss: 19127.894\n",
      "[138,   170] loss: 36022.267\n",
      "[138,   175] loss: 16508.096\n",
      "[138,   180] loss: 31231.255\n",
      "[138,   185] loss: 18097.433\n",
      "[138,   190] loss: 30962.128\n",
      "[138,   195] loss: 24733.212\n",
      "[138,   200] loss: 20324.359\n",
      "[138,   205] loss: 34257.890\n",
      "[138,   210] loss: 25717.945\n",
      "[138,   215] loss: 27518.425\n",
      "[138,   220] loss: 25366.168\n",
      "[138,   225] loss: 26861.676\n",
      "[138,   230] loss: 25337.861\n",
      "[139,     5] loss: 28650.192\n",
      "[139,    10] loss: 25201.392\n",
      "[139,    15] loss: 19779.285\n",
      "[139,    20] loss: 26706.539\n",
      "[139,    25] loss: 18475.058\n",
      "[139,    30] loss: 20000.007\n",
      "[139,    35] loss: 30841.229\n",
      "[139,    40] loss: 31400.178\n",
      "[139,    45] loss: 29670.953\n",
      "[139,    50] loss: 24080.290\n",
      "[139,    55] loss: 31301.263\n",
      "[139,    60] loss: 23802.201\n",
      "[139,    65] loss: 22936.178\n",
      "[139,    70] loss: 31258.770\n",
      "[139,    75] loss: 29743.111\n",
      "[139,    80] loss: 37903.588\n",
      "[139,    85] loss: 32512.385\n",
      "[139,    90] loss: 25269.839\n",
      "[139,    95] loss: 25283.643\n",
      "[139,   100] loss: 28671.248\n",
      "[139,   105] loss: 25996.202\n",
      "[139,   110] loss: 23839.734\n",
      "[139,   115] loss: 22220.605\n",
      "[139,   120] loss: 25050.236\n",
      "[139,   125] loss: 29570.544\n",
      "[139,   130] loss: 32336.127\n",
      "[139,   135] loss: 49878.811\n",
      "[139,   140] loss: 24857.620\n",
      "[139,   145] loss: 15818.983\n",
      "[139,   150] loss: 27791.017\n",
      "[139,   155] loss: 23601.972\n",
      "[139,   160] loss: 21443.118\n",
      "[139,   165] loss: 18405.380\n",
      "[139,   170] loss: 27506.191\n",
      "[139,   175] loss: 33575.418\n",
      "[139,   180] loss: 37092.850\n",
      "[139,   185] loss: 31468.449\n",
      "[139,   190] loss: 17877.166\n",
      "[139,   195] loss: 38447.160\n",
      "[139,   200] loss: 21207.520\n",
      "[139,   205] loss: 20626.976\n",
      "[139,   210] loss: 25877.317\n",
      "[139,   215] loss: 28792.302\n",
      "[139,   220] loss: 21812.177\n",
      "[139,   225] loss: 22080.979\n",
      "[139,   230] loss: 23125.254\n",
      "[140,     5] loss: 20883.173\n",
      "[140,    10] loss: 25025.042\n",
      "[140,    15] loss: 29485.238\n",
      "[140,    20] loss: 27801.426\n",
      "[140,    25] loss: 48019.976\n",
      "[140,    30] loss: 39223.777\n",
      "[140,    35] loss: 28964.038\n",
      "[140,    40] loss: 21212.447\n",
      "[140,    45] loss: 23535.262\n",
      "[140,    50] loss: 25389.435\n",
      "[140,    55] loss: 24766.640\n",
      "[140,    60] loss: 21710.121\n",
      "[140,    65] loss: 28617.031\n",
      "[140,    70] loss: 28221.612\n",
      "[140,    75] loss: 26869.610\n",
      "[140,    80] loss: 35545.493\n",
      "[140,    85] loss: 25263.301\n",
      "[140,    90] loss: 24935.650\n",
      "[140,    95] loss: 24416.243\n",
      "[140,   100] loss: 18947.323\n",
      "[140,   105] loss: 20976.656\n",
      "[140,   110] loss: 29360.004\n",
      "[140,   115] loss: 23732.706\n",
      "[140,   120] loss: 27253.555\n",
      "[140,   125] loss: 26070.721\n",
      "[140,   130] loss: 20937.493\n",
      "[140,   135] loss: 24077.268\n",
      "[140,   140] loss: 23681.696\n",
      "[140,   145] loss: 26764.177\n",
      "[140,   150] loss: 37113.929\n",
      "[140,   155] loss: 18920.310\n",
      "[140,   160] loss: 23705.052\n",
      "[140,   165] loss: 16121.111\n",
      "[140,   170] loss: 26093.909\n",
      "[140,   175] loss: 22076.994\n",
      "[140,   180] loss: 20384.777\n",
      "[140,   185] loss: 45931.215\n",
      "[140,   190] loss: 28067.875\n",
      "[140,   195] loss: 26261.604\n",
      "[140,   200] loss: 31056.841\n",
      "[140,   205] loss: 22877.143\n",
      "[140,   210] loss: 21947.015\n",
      "[140,   215] loss: 26158.492\n",
      "[140,   220] loss: 23900.960\n",
      "[140,   225] loss: 25408.627\n",
      "[140,   230] loss: 36100.714\n",
      "[141,     5] loss: 21036.456\n",
      "[141,    10] loss: 22343.073\n",
      "[141,    15] loss: 28762.925\n",
      "[141,    20] loss: 32111.053\n",
      "[141,    25] loss: 30945.448\n",
      "[141,    30] loss: 27338.749\n",
      "[141,    35] loss: 19543.664\n",
      "[141,    40] loss: 26852.254\n",
      "[141,    45] loss: 26870.996\n",
      "[141,    50] loss: 28582.515\n",
      "[141,    55] loss: 36938.263\n",
      "[141,    60] loss: 18556.155\n",
      "[141,    65] loss: 30021.136\n",
      "[141,    70] loss: 24014.039\n",
      "[141,    75] loss: 35815.184\n",
      "[141,    80] loss: 31530.917\n",
      "[141,    85] loss: 28006.165\n",
      "[141,    90] loss: 28183.998\n",
      "[141,    95] loss: 20728.081\n",
      "[141,   100] loss: 20687.804\n",
      "[141,   105] loss: 24309.203\n",
      "[141,   110] loss: 31866.753\n",
      "[141,   115] loss: 30840.332\n",
      "[141,   120] loss: 22497.227\n",
      "[141,   125] loss: 18129.023\n",
      "[141,   130] loss: 24011.564\n",
      "[141,   135] loss: 24578.201\n",
      "[141,   140] loss: 26298.523\n",
      "[141,   145] loss: 46853.806\n",
      "[141,   150] loss: 28911.628\n",
      "[141,   155] loss: 28793.144\n",
      "[141,   160] loss: 16968.663\n",
      "[141,   165] loss: 30127.681\n",
      "[141,   170] loss: 24131.279\n",
      "[141,   175] loss: 33533.643\n",
      "[141,   180] loss: 13631.886\n",
      "[141,   185] loss: 29176.645\n",
      "[141,   190] loss: 32755.726\n",
      "[141,   195] loss: 22038.711\n",
      "[141,   200] loss: 25698.762\n",
      "[141,   205] loss: 23337.992\n",
      "[141,   210] loss: 23407.032\n",
      "[141,   215] loss: 33295.180\n",
      "[141,   220] loss: 23714.118\n",
      "[141,   225] loss: 28866.408\n",
      "[141,   230] loss: 14780.773\n",
      "[142,     5] loss: 23909.236\n",
      "[142,    10] loss: 23068.673\n",
      "[142,    15] loss: 22309.246\n",
      "[142,    20] loss: 23546.665\n",
      "[142,    25] loss: 25858.967\n",
      "[142,    30] loss: 34612.025\n",
      "[142,    35] loss: 35060.532\n",
      "[142,    40] loss: 27102.486\n",
      "[142,    45] loss: 23956.709\n",
      "[142,    50] loss: 20100.727\n",
      "[142,    55] loss: 28107.287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142,    60] loss: 32755.976\n",
      "[142,    65] loss: 26993.662\n",
      "[142,    70] loss: 31311.970\n",
      "[142,    75] loss: 25506.635\n",
      "[142,    80] loss: 19026.364\n",
      "[142,    85] loss: 25621.587\n",
      "[142,    90] loss: 27976.673\n",
      "[142,    95] loss: 17149.492\n",
      "[142,   100] loss: 43387.725\n",
      "[142,   105] loss: 26061.894\n",
      "[142,   110] loss: 16375.681\n",
      "[142,   115] loss: 26731.953\n",
      "[142,   120] loss: 25499.617\n",
      "[142,   125] loss: 31311.017\n",
      "[142,   130] loss: 20871.033\n",
      "[142,   135] loss: 26470.378\n",
      "[142,   140] loss: 22657.798\n",
      "[142,   145] loss: 24760.310\n",
      "[142,   150] loss: 18412.181\n",
      "[142,   155] loss: 37016.643\n",
      "[142,   160] loss: 27363.295\n",
      "[142,   165] loss: 22634.939\n",
      "[142,   170] loss: 20899.897\n",
      "[142,   175] loss: 19993.673\n",
      "[142,   180] loss: 36110.323\n",
      "[142,   185] loss: 35497.268\n",
      "[142,   190] loss: 30469.537\n",
      "[142,   195] loss: 18785.631\n",
      "[142,   200] loss: 22331.113\n",
      "[142,   205] loss: 24320.524\n",
      "[142,   210] loss: 30265.692\n",
      "[142,   215] loss: 25101.072\n",
      "[142,   220] loss: 25577.065\n",
      "[142,   225] loss: 26799.254\n",
      "[142,   230] loss: 30891.996\n",
      "[143,     5] loss: 30507.871\n",
      "[143,    10] loss: 25157.587\n",
      "[143,    15] loss: 22179.295\n",
      "[143,    20] loss: 26713.181\n",
      "[143,    25] loss: 48124.348\n",
      "[143,    30] loss: 22959.340\n",
      "[143,    35] loss: 20925.233\n",
      "[143,    40] loss: 32043.575\n",
      "[143,    45] loss: 26871.223\n",
      "[143,    50] loss: 30385.321\n",
      "[143,    55] loss: 16004.776\n",
      "[143,    60] loss: 22353.142\n",
      "[143,    65] loss: 30543.066\n",
      "[143,    70] loss: 15627.878\n",
      "[143,    75] loss: 27708.575\n",
      "[143,    80] loss: 23370.392\n",
      "[143,    85] loss: 25486.449\n",
      "[143,    90] loss: 26183.696\n",
      "[143,    95] loss: 33951.080\n",
      "[143,   100] loss: 19967.964\n",
      "[143,   105] loss: 28105.857\n",
      "[143,   110] loss: 28138.071\n",
      "[143,   115] loss: 36187.610\n",
      "[143,   120] loss: 24180.853\n",
      "[143,   125] loss: 28100.582\n",
      "[143,   130] loss: 27266.078\n",
      "[143,   135] loss: 20767.811\n",
      "[143,   140] loss: 25191.075\n",
      "[143,   145] loss: 33399.529\n",
      "[143,   150] loss: 39090.410\n",
      "[143,   155] loss: 20252.908\n",
      "[143,   160] loss: 27853.725\n",
      "[143,   165] loss: 25184.248\n",
      "[143,   170] loss: 22028.334\n",
      "[143,   175] loss: 25546.492\n",
      "[143,   180] loss: 27253.398\n",
      "[143,   185] loss: 20854.047\n",
      "[143,   190] loss: 28020.506\n",
      "[143,   195] loss: 26344.440\n",
      "[143,   200] loss: 23882.675\n",
      "[143,   205] loss: 21194.567\n",
      "[143,   210] loss: 21782.218\n",
      "[143,   215] loss: 20245.666\n",
      "[143,   220] loss: 21437.661\n",
      "[143,   225] loss: 49079.391\n",
      "[143,   230] loss: 25227.588\n",
      "[144,     5] loss: 20309.794\n",
      "[144,    10] loss: 21462.741\n",
      "[144,    15] loss: 44761.495\n",
      "[144,    20] loss: 31587.248\n",
      "[144,    25] loss: 23197.332\n",
      "[144,    30] loss: 34058.608\n",
      "[144,    35] loss: 26476.361\n",
      "[144,    40] loss: 27333.462\n",
      "[144,    45] loss: 35941.418\n",
      "[144,    50] loss: 22281.771\n",
      "[144,    55] loss: 37313.330\n",
      "[144,    60] loss: 26016.684\n",
      "[144,    65] loss: 20000.832\n",
      "[144,    70] loss: 30908.779\n",
      "[144,    75] loss: 21950.928\n",
      "[144,    80] loss: 21055.535\n",
      "[144,    85] loss: 19889.888\n",
      "[144,    90] loss: 19037.911\n",
      "[144,    95] loss: 21716.585\n",
      "[144,   100] loss: 16544.882\n",
      "[144,   105] loss: 21316.437\n",
      "[144,   110] loss: 22882.755\n",
      "[144,   115] loss: 22637.366\n",
      "[144,   120] loss: 24592.061\n",
      "[144,   125] loss: 24382.258\n",
      "[144,   130] loss: 22081.586\n",
      "[144,   135] loss: 32851.233\n",
      "[144,   140] loss: 32482.679\n",
      "[144,   145] loss: 24104.418\n",
      "[144,   150] loss: 20393.437\n",
      "[144,   155] loss: 17749.550\n",
      "[144,   160] loss: 38009.336\n",
      "[144,   165] loss: 24934.136\n",
      "[144,   170] loss: 23549.867\n",
      "[144,   175] loss: 28841.158\n",
      "[144,   180] loss: 19905.616\n",
      "[144,   185] loss: 39632.239\n",
      "[144,   190] loss: 26359.632\n",
      "[144,   195] loss: 16860.482\n",
      "[144,   200] loss: 25058.350\n",
      "[144,   205] loss: 34176.162\n",
      "[144,   210] loss: 51006.988\n",
      "[144,   215] loss: 38660.228\n",
      "[144,   220] loss: 30547.113\n",
      "[144,   225] loss: 17299.221\n",
      "[144,   230] loss: 21767.446\n",
      "[145,     5] loss: 21103.048\n",
      "[145,    10] loss: 33545.539\n",
      "[145,    15] loss: 23726.209\n",
      "[145,    20] loss: 28623.945\n",
      "[145,    25] loss: 24832.781\n",
      "[145,    30] loss: 19688.852\n",
      "[145,    35] loss: 18494.542\n",
      "[145,    40] loss: 29152.366\n",
      "[145,    45] loss: 17747.426\n",
      "[145,    50] loss: 34860.272\n",
      "[145,    55] loss: 21206.251\n",
      "[145,    60] loss: 25862.125\n",
      "[145,    65] loss: 37510.321\n",
      "[145,    70] loss: 26534.809\n",
      "[145,    75] loss: 20845.869\n",
      "[145,    80] loss: 27396.764\n",
      "[145,    85] loss: 36419.635\n",
      "[145,    90] loss: 19797.832\n",
      "[145,    95] loss: 24408.262\n",
      "[145,   100] loss: 20616.849\n",
      "[145,   105] loss: 34100.371\n",
      "[145,   110] loss: 20362.539\n",
      "[145,   115] loss: 19983.284\n",
      "[145,   120] loss: 28668.645\n",
      "[145,   125] loss: 14909.430\n",
      "[145,   130] loss: 35030.209\n",
      "[145,   135] loss: 17610.764\n",
      "[145,   140] loss: 23730.773\n",
      "[145,   145] loss: 29898.350\n",
      "[145,   150] loss: 25443.936\n",
      "[145,   155] loss: 25368.596\n",
      "[145,   160] loss: 26180.354\n",
      "[145,   165] loss: 30681.250\n",
      "[145,   170] loss: 24414.032\n",
      "[145,   175] loss: 19088.359\n",
      "[145,   180] loss: 23092.878\n",
      "[145,   185] loss: 27595.197\n",
      "[145,   190] loss: 36416.865\n",
      "[145,   195] loss: 35611.757\n",
      "[145,   200] loss: 18591.846\n",
      "[145,   205] loss: 33111.135\n",
      "[145,   210] loss: 19694.567\n",
      "[145,   215] loss: 29254.456\n",
      "[145,   220] loss: 29744.905\n",
      "[145,   225] loss: 55745.205\n",
      "[145,   230] loss: 29822.002\n",
      "[146,     5] loss: 21600.196\n",
      "[146,    10] loss: 24973.899\n",
      "[146,    15] loss: 27192.748\n",
      "[146,    20] loss: 21556.709\n",
      "[146,    25] loss: 37188.695\n",
      "[146,    30] loss: 28087.785\n",
      "[146,    35] loss: 24105.771\n",
      "[146,    40] loss: 33077.914\n",
      "[146,    45] loss: 16384.927\n",
      "[146,    50] loss: 35298.235\n",
      "[146,    55] loss: 23522.032\n",
      "[146,    60] loss: 18395.798\n",
      "[146,    65] loss: 28457.642\n",
      "[146,    70] loss: 24505.015\n",
      "[146,    75] loss: 27816.349\n",
      "[146,    80] loss: 23586.037\n",
      "[146,    85] loss: 31740.327\n",
      "[146,    90] loss: 21720.426\n",
      "[146,    95] loss: 27484.041\n",
      "[146,   100] loss: 17786.727\n",
      "[146,   105] loss: 29457.154\n",
      "[146,   110] loss: 17592.783\n",
      "[146,   115] loss: 24696.501\n",
      "[146,   120] loss: 19767.074\n",
      "[146,   125] loss: 22036.620\n",
      "[146,   130] loss: 39115.772\n",
      "[146,   135] loss: 53960.214\n",
      "[146,   140] loss: 31015.336\n",
      "[146,   145] loss: 30423.316\n",
      "[146,   150] loss: 25412.900\n",
      "[146,   155] loss: 26801.040\n",
      "[146,   160] loss: 16389.806\n",
      "[146,   165] loss: 20209.372\n",
      "[146,   170] loss: 24343.788\n",
      "[146,   175] loss: 33790.011\n",
      "[146,   180] loss: 26458.483\n",
      "[146,   185] loss: 33063.048\n",
      "[146,   190] loss: 33229.036\n",
      "[146,   195] loss: 20970.870\n",
      "[146,   200] loss: 31598.482\n",
      "[146,   205] loss: 25188.056\n",
      "[146,   210] loss: 27793.551\n",
      "[146,   215] loss: 23808.258\n",
      "[146,   220] loss: 23712.576\n",
      "[146,   225] loss: 28506.127\n",
      "[146,   230] loss: 23887.382\n",
      "[147,     5] loss: 19871.298\n",
      "[147,    10] loss: 32396.638\n",
      "[147,    15] loss: 39548.169\n",
      "[147,    20] loss: 31288.659\n",
      "[147,    25] loss: 21636.981\n",
      "[147,    30] loss: 30707.877\n",
      "[147,    35] loss: 24158.020\n",
      "[147,    40] loss: 19369.164\n",
      "[147,    45] loss: 22049.323\n",
      "[147,    50] loss: 19625.080\n",
      "[147,    55] loss: 30301.982\n",
      "[147,    60] loss: 49916.354\n",
      "[147,    65] loss: 34776.099\n",
      "[147,    70] loss: 30036.184\n",
      "[147,    75] loss: 28802.493\n",
      "[147,    80] loss: 23322.232\n",
      "[147,    85] loss: 22330.523\n",
      "[147,    90] loss: 24466.589\n",
      "[147,    95] loss: 22956.442\n",
      "[147,   100] loss: 26976.155\n",
      "[147,   105] loss: 18199.043\n",
      "[147,   110] loss: 24253.559\n",
      "[147,   115] loss: 16636.678\n",
      "[147,   120] loss: 36171.979\n",
      "[147,   125] loss: 25743.201\n",
      "[147,   130] loss: 36661.749\n",
      "[147,   135] loss: 23605.587\n",
      "[147,   140] loss: 28417.371\n",
      "[147,   145] loss: 15637.177\n",
      "[147,   150] loss: 16993.726\n",
      "[147,   155] loss: 25419.277\n",
      "[147,   160] loss: 23944.620\n",
      "[147,   165] loss: 25724.453\n",
      "[147,   170] loss: 26463.209\n",
      "[147,   175] loss: 17842.008\n",
      "[147,   180] loss: 24105.482\n",
      "[147,   185] loss: 27811.521\n",
      "[147,   190] loss: 28152.225\n",
      "[147,   195] loss: 28118.875\n",
      "[147,   200] loss: 23735.408\n",
      "[147,   205] loss: 25767.500\n",
      "[147,   210] loss: 37510.986\n",
      "[147,   215] loss: 26036.520\n",
      "[147,   220] loss: 24217.517\n",
      "[147,   225] loss: 20798.292\n",
      "[147,   230] loss: 33512.654\n",
      "[148,     5] loss: 20360.963\n",
      "[148,    10] loss: 22765.718\n",
      "[148,    15] loss: 49310.910\n",
      "[148,    20] loss: 31719.664\n",
      "[148,    25] loss: 33206.080\n",
      "[148,    30] loss: 25266.554\n",
      "[148,    35] loss: 28091.461\n",
      "[148,    40] loss: 18918.394\n",
      "[148,    45] loss: 24997.035\n",
      "[148,    50] loss: 30546.394\n",
      "[148,    55] loss: 27178.999\n",
      "[148,    60] loss: 16911.242\n",
      "[148,    65] loss: 32735.460\n",
      "[148,    70] loss: 15028.824\n",
      "[148,    75] loss: 30883.538\n",
      "[148,    80] loss: 19007.042\n",
      "[148,    85] loss: 21108.999\n",
      "[148,    90] loss: 27948.089\n",
      "[148,    95] loss: 17551.903\n",
      "[148,   100] loss: 27620.886\n",
      "[148,   105] loss: 26871.498\n",
      "[148,   110] loss: 25045.330\n",
      "[148,   115] loss: 30888.529\n",
      "[148,   120] loss: 24573.781\n",
      "[148,   125] loss: 29221.234\n",
      "[148,   130] loss: 26527.623\n",
      "[148,   135] loss: 20307.409\n",
      "[148,   140] loss: 25298.854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148,   145] loss: 27172.577\n",
      "[148,   150] loss: 36089.360\n",
      "[148,   155] loss: 38607.371\n",
      "[148,   160] loss: 39285.707\n",
      "[148,   165] loss: 27071.539\n",
      "[148,   170] loss: 20763.825\n",
      "[148,   175] loss: 21894.668\n",
      "[148,   180] loss: 42272.001\n",
      "[148,   185] loss: 17599.091\n",
      "[148,   190] loss: 25552.589\n",
      "[148,   195] loss: 23502.526\n",
      "[148,   200] loss: 29428.139\n",
      "[148,   205] loss: 24838.994\n",
      "[148,   210] loss: 25473.601\n",
      "[148,   215] loss: 27296.422\n",
      "[148,   220] loss: 24267.436\n",
      "[148,   225] loss: 20433.950\n",
      "[148,   230] loss: 30551.938\n",
      "[149,     5] loss: 20745.043\n",
      "[149,    10] loss: 24232.625\n",
      "[149,    15] loss: 21475.569\n",
      "[149,    20] loss: 21304.210\n",
      "[149,    25] loss: 20152.973\n",
      "[149,    30] loss: 37521.766\n",
      "[149,    35] loss: 28571.798\n",
      "[149,    40] loss: 18394.938\n",
      "[149,    45] loss: 27086.503\n",
      "[149,    50] loss: 20993.532\n",
      "[149,    55] loss: 26056.792\n",
      "[149,    60] loss: 25294.310\n",
      "[149,    65] loss: 21926.520\n",
      "[149,    70] loss: 16914.088\n",
      "[149,    75] loss: 26740.151\n",
      "[149,    80] loss: 29320.330\n",
      "[149,    85] loss: 21474.999\n",
      "[149,    90] loss: 30365.270\n",
      "[149,    95] loss: 26972.749\n",
      "[149,   100] loss: 26977.955\n",
      "[149,   105] loss: 28699.523\n",
      "[149,   110] loss: 30801.419\n",
      "[149,   115] loss: 20763.227\n",
      "[149,   120] loss: 23043.505\n",
      "[149,   125] loss: 24406.943\n",
      "[149,   130] loss: 24668.870\n",
      "[149,   135] loss: 18810.452\n",
      "[149,   140] loss: 33175.163\n",
      "[149,   145] loss: 19571.200\n",
      "[149,   150] loss: 31241.190\n",
      "[149,   155] loss: 26759.812\n",
      "[149,   160] loss: 46187.280\n",
      "[149,   165] loss: 34692.244\n",
      "[149,   170] loss: 27150.651\n",
      "[149,   175] loss: 32430.788\n",
      "[149,   180] loss: 28720.743\n",
      "[149,   185] loss: 22062.902\n",
      "[149,   190] loss: 25008.321\n",
      "[149,   195] loss: 36006.433\n",
      "[149,   200] loss: 25548.078\n",
      "[149,   205] loss: 21645.111\n",
      "[149,   210] loss: 31283.131\n",
      "[149,   215] loss: 21266.014\n",
      "[149,   220] loss: 18265.411\n",
      "[149,   225] loss: 24592.049\n",
      "[149,   230] loss: 49506.862\n",
      "[150,     5] loss: 24441.048\n",
      "[150,    10] loss: 23872.632\n",
      "[150,    15] loss: 19833.337\n",
      "[150,    20] loss: 20395.898\n",
      "[150,    25] loss: 32219.113\n",
      "[150,    30] loss: 25646.683\n",
      "[150,    35] loss: 16222.558\n",
      "[150,    40] loss: 23232.210\n",
      "[150,    45] loss: 17021.768\n",
      "[150,    50] loss: 30320.125\n",
      "[150,    55] loss: 33557.224\n",
      "[150,    60] loss: 22621.851\n",
      "[150,    65] loss: 29194.030\n",
      "[150,    70] loss: 31484.353\n",
      "[150,    75] loss: 23391.759\n",
      "[150,    80] loss: 27467.136\n",
      "[150,    85] loss: 29700.614\n",
      "[150,    90] loss: 48047.486\n",
      "[150,    95] loss: 23717.036\n",
      "[150,   100] loss: 24204.856\n",
      "[150,   105] loss: 28258.443\n",
      "[150,   110] loss: 23277.319\n",
      "[150,   115] loss: 27310.495\n",
      "[150,   120] loss: 21023.398\n",
      "[150,   125] loss: 30801.252\n",
      "[150,   130] loss: 18980.726\n",
      "[150,   135] loss: 28356.478\n",
      "[150,   140] loss: 20112.148\n",
      "[150,   145] loss: 34218.824\n",
      "[150,   150] loss: 20746.099\n",
      "[150,   155] loss: 25794.705\n",
      "[150,   160] loss: 22124.824\n",
      "[150,   165] loss: 21446.588\n",
      "[150,   170] loss: 29957.834\n",
      "[150,   175] loss: 20890.846\n",
      "[150,   180] loss: 32606.661\n",
      "[150,   185] loss: 19003.675\n",
      "[150,   190] loss: 32306.205\n",
      "[150,   195] loss: 34564.725\n",
      "[150,   200] loss: 16857.532\n",
      "[150,   205] loss: 36845.781\n",
      "[150,   210] loss: 50813.995\n",
      "[150,   215] loss: 17224.871\n",
      "[150,   220] loss: 25633.042\n",
      "[150,   225] loss: 27449.412\n",
      "[150,   230] loss: 36759.105\n",
      "[151,     5] loss: 14243.664\n",
      "[151,    10] loss: 24279.519\n",
      "[151,    15] loss: 33010.818\n",
      "[151,    20] loss: 23053.661\n",
      "[151,    25] loss: 30807.642\n",
      "[151,    30] loss: 26804.355\n",
      "[151,    35] loss: 20553.406\n",
      "[151,    40] loss: 32868.660\n",
      "[151,    45] loss: 27065.540\n",
      "[151,    50] loss: 29801.912\n",
      "[151,    55] loss: 21813.317\n",
      "[151,    60] loss: 18296.541\n",
      "[151,    65] loss: 16184.940\n",
      "[151,    70] loss: 24143.926\n",
      "[151,    75] loss: 23193.388\n",
      "[151,    80] loss: 18515.729\n",
      "[151,    85] loss: 28373.714\n",
      "[151,    90] loss: 22380.435\n",
      "[151,    95] loss: 27338.992\n",
      "[151,   100] loss: 40535.601\n",
      "[151,   105] loss: 18450.828\n",
      "[151,   110] loss: 33990.548\n",
      "[151,   115] loss: 28916.389\n",
      "[151,   120] loss: 17486.219\n",
      "[151,   125] loss: 38626.059\n",
      "[151,   130] loss: 26619.499\n",
      "[151,   135] loss: 16495.013\n",
      "[151,   140] loss: 30811.304\n",
      "[151,   145] loss: 21942.018\n",
      "[151,   150] loss: 28983.485\n",
      "[151,   155] loss: 28006.008\n",
      "[151,   160] loss: 38029.297\n",
      "[151,   165] loss: 28771.212\n",
      "[151,   170] loss: 31583.125\n",
      "[151,   175] loss: 32000.309\n",
      "[151,   180] loss: 18397.809\n",
      "[151,   185] loss: 17825.718\n",
      "[151,   190] loss: 18209.095\n",
      "[151,   195] loss: 29099.121\n",
      "[151,   200] loss: 55462.202\n",
      "[151,   205] loss: 25215.503\n",
      "[151,   210] loss: 33359.013\n",
      "[151,   215] loss: 26494.053\n",
      "[151,   220] loss: 24319.271\n",
      "[151,   225] loss: 28969.584\n",
      "[151,   230] loss: 21432.625\n",
      "[152,     5] loss: 36755.859\n",
      "[152,    10] loss: 16626.127\n",
      "[152,    15] loss: 20444.954\n",
      "[152,    20] loss: 35885.938\n",
      "[152,    25] loss: 21130.463\n",
      "[152,    30] loss: 21316.750\n",
      "[152,    35] loss: 28471.690\n",
      "[152,    40] loss: 24755.273\n",
      "[152,    45] loss: 26821.617\n",
      "[152,    50] loss: 23208.297\n",
      "[152,    55] loss: 22620.332\n",
      "[152,    60] loss: 30998.361\n",
      "[152,    65] loss: 21591.503\n",
      "[152,    70] loss: 32592.665\n",
      "[152,    75] loss: 20130.955\n",
      "[152,    80] loss: 22735.569\n",
      "[152,    85] loss: 20825.161\n",
      "[152,    90] loss: 23376.765\n",
      "[152,    95] loss: 22007.123\n",
      "[152,   100] loss: 27574.092\n",
      "[152,   105] loss: 37631.437\n",
      "[152,   110] loss: 26504.172\n",
      "[152,   115] loss: 32120.606\n",
      "[152,   120] loss: 63026.722\n",
      "[152,   125] loss: 29091.107\n",
      "[152,   130] loss: 17829.429\n",
      "[152,   135] loss: 22376.645\n",
      "[152,   140] loss: 14725.211\n",
      "[152,   145] loss: 32019.020\n",
      "[152,   150] loss: 26775.657\n",
      "[152,   155] loss: 21723.411\n",
      "[152,   160] loss: 17553.632\n",
      "[152,   165] loss: 20433.800\n",
      "[152,   170] loss: 30643.000\n",
      "[152,   175] loss: 28590.306\n",
      "[152,   180] loss: 18800.120\n",
      "[152,   185] loss: 21044.850\n",
      "[152,   190] loss: 34025.126\n",
      "[152,   195] loss: 41874.906\n",
      "[152,   200] loss: 16459.382\n",
      "[152,   205] loss: 30362.836\n",
      "[152,   210] loss: 25838.908\n",
      "[152,   215] loss: 28175.237\n",
      "[152,   220] loss: 25397.368\n",
      "[152,   225] loss: 31393.369\n",
      "[152,   230] loss: 22081.124\n",
      "[153,     5] loss: 27919.424\n",
      "[153,    10] loss: 28979.031\n",
      "[153,    15] loss: 22825.003\n",
      "[153,    20] loss: 35440.465\n",
      "[153,    25] loss: 19285.166\n",
      "[153,    30] loss: 20368.323\n",
      "[153,    35] loss: 23062.852\n",
      "[153,    40] loss: 19901.150\n",
      "[153,    45] loss: 23539.938\n",
      "[153,    50] loss: 28866.316\n",
      "[153,    55] loss: 24044.528\n",
      "[153,    60] loss: 20470.944\n",
      "[153,    65] loss: 25554.684\n",
      "[153,    70] loss: 27168.376\n",
      "[153,    75] loss: 26592.506\n",
      "[153,    80] loss: 22930.103\n",
      "[153,    85] loss: 19513.440\n",
      "[153,    90] loss: 35651.122\n",
      "[153,    95] loss: 31654.115\n",
      "[153,   100] loss: 19141.365\n",
      "[153,   105] loss: 16165.593\n",
      "[153,   110] loss: 25184.634\n",
      "[153,   115] loss: 34360.364\n",
      "[153,   120] loss: 30340.676\n",
      "[153,   125] loss: 27849.022\n",
      "[153,   130] loss: 24584.314\n",
      "[153,   135] loss: 33377.492\n",
      "[153,   140] loss: 31283.142\n",
      "[153,   145] loss: 24399.769\n",
      "[153,   150] loss: 22126.595\n",
      "[153,   155] loss: 19949.317\n",
      "[153,   160] loss: 35891.504\n",
      "[153,   165] loss: 24191.855\n",
      "[153,   170] loss: 20903.206\n",
      "[153,   175] loss: 24420.000\n",
      "[153,   180] loss: 22832.479\n",
      "[153,   185] loss: 18628.756\n",
      "[153,   190] loss: 54230.369\n",
      "[153,   195] loss: 28738.979\n",
      "[153,   200] loss: 23879.517\n",
      "[153,   205] loss: 32490.285\n",
      "[153,   210] loss: 24984.266\n",
      "[153,   215] loss: 26371.891\n",
      "[153,   220] loss: 27541.692\n",
      "[153,   225] loss: 17600.638\n",
      "[153,   230] loss: 20506.080\n",
      "[154,     5] loss: 22854.094\n",
      "[154,    10] loss: 28401.552\n",
      "[154,    15] loss: 27705.341\n",
      "[154,    20] loss: 24139.200\n",
      "[154,    25] loss: 23463.060\n",
      "[154,    30] loss: 26267.479\n",
      "[154,    35] loss: 47047.900\n",
      "[154,    40] loss: 20540.771\n",
      "[154,    45] loss: 18860.450\n",
      "[154,    50] loss: 25756.131\n",
      "[154,    55] loss: 27488.140\n",
      "[154,    60] loss: 23484.047\n",
      "[154,    65] loss: 29292.966\n",
      "[154,    70] loss: 21283.394\n",
      "[154,    75] loss: 30964.779\n",
      "[154,    80] loss: 23240.199\n",
      "[154,    85] loss: 20908.385\n",
      "[154,    90] loss: 19729.758\n",
      "[154,    95] loss: 30100.492\n",
      "[154,   100] loss: 20423.353\n",
      "[154,   105] loss: 27262.268\n",
      "[154,   110] loss: 27149.575\n",
      "[154,   115] loss: 24858.626\n",
      "[154,   120] loss: 19088.007\n",
      "[154,   125] loss: 21103.661\n",
      "[154,   130] loss: 30544.398\n",
      "[154,   135] loss: 31868.023\n",
      "[154,   140] loss: 28179.104\n",
      "[154,   145] loss: 24820.352\n",
      "[154,   150] loss: 40757.375\n",
      "[154,   155] loss: 28683.087\n",
      "[154,   160] loss: 31771.593\n",
      "[154,   165] loss: 31732.470\n",
      "[154,   170] loss: 38455.115\n",
      "[154,   175] loss: 25740.833\n",
      "[154,   180] loss: 19126.278\n",
      "[154,   185] loss: 18920.564\n",
      "[154,   190] loss: 28196.278\n",
      "[154,   195] loss: 23661.971\n",
      "[154,   200] loss: 37904.800\n",
      "[154,   205] loss: 23067.834\n",
      "[154,   210] loss: 22701.400\n",
      "[154,   215] loss: 26273.124\n",
      "[154,   220] loss: 23481.427\n",
      "[154,   225] loss: 36119.773\n",
      "[154,   230] loss: 22410.810\n",
      "[155,     5] loss: 29602.742\n",
      "[155,    10] loss: 18063.460\n",
      "[155,    15] loss: 29616.459\n",
      "[155,    20] loss: 24141.625\n",
      "[155,    25] loss: 31050.285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155,    30] loss: 27461.554\n",
      "[155,    35] loss: 19165.348\n",
      "[155,    40] loss: 34578.602\n",
      "[155,    45] loss: 19005.951\n",
      "[155,    50] loss: 23505.806\n",
      "[155,    55] loss: 21364.284\n",
      "[155,    60] loss: 22366.977\n",
      "[155,    65] loss: 23150.172\n",
      "[155,    70] loss: 21112.788\n",
      "[155,    75] loss: 43781.984\n",
      "[155,    80] loss: 22519.738\n",
      "[155,    85] loss: 27052.039\n",
      "[155,    90] loss: 31477.542\n",
      "[155,    95] loss: 13698.407\n",
      "[155,   100] loss: 25211.596\n",
      "[155,   105] loss: 29472.677\n",
      "[155,   110] loss: 21703.079\n",
      "[155,   115] loss: 26107.968\n",
      "[155,   120] loss: 19211.648\n",
      "[155,   125] loss: 20399.968\n",
      "[155,   130] loss: 20840.021\n",
      "[155,   135] loss: 32965.992\n",
      "[155,   140] loss: 32572.652\n",
      "[155,   145] loss: 24355.801\n",
      "[155,   150] loss: 28870.843\n",
      "[155,   155] loss: 21663.135\n",
      "[155,   160] loss: 29898.236\n",
      "[155,   165] loss: 24704.927\n",
      "[155,   170] loss: 18003.522\n",
      "[155,   175] loss: 20481.110\n",
      "[155,   180] loss: 26880.378\n",
      "[155,   185] loss: 24282.580\n",
      "[155,   190] loss: 25528.102\n",
      "[155,   195] loss: 24396.738\n",
      "[155,   200] loss: 24179.121\n",
      "[155,   205] loss: 31012.757\n",
      "[155,   210] loss: 15191.050\n",
      "[155,   215] loss: 37576.335\n",
      "[155,   220] loss: 50757.670\n",
      "[155,   225] loss: 51503.782\n",
      "[155,   230] loss: 20366.271\n",
      "[156,     5] loss: 27585.499\n",
      "[156,    10] loss: 24043.209\n",
      "[156,    15] loss: 48078.466\n",
      "[156,    20] loss: 31405.316\n",
      "[156,    25] loss: 18025.639\n",
      "[156,    30] loss: 25164.381\n",
      "[156,    35] loss: 23175.701\n",
      "[156,    40] loss: 34204.269\n",
      "[156,    45] loss: 19952.077\n",
      "[156,    50] loss: 28517.067\n",
      "[156,    55] loss: 53175.243\n",
      "[156,    60] loss: 30551.982\n",
      "[156,    65] loss: 27185.397\n",
      "[156,    70] loss: 25157.295\n",
      "[156,    75] loss: 25480.662\n",
      "[156,    80] loss: 27569.841\n",
      "[156,    85] loss: 41465.470\n",
      "[156,    90] loss: 21890.928\n",
      "[156,    95] loss: 24408.496\n",
      "[156,   100] loss: 32300.322\n",
      "[156,   105] loss: 23204.550\n",
      "[156,   110] loss: 27746.977\n",
      "[156,   115] loss: 20783.563\n",
      "[156,   120] loss: 33082.801\n",
      "[156,   125] loss: 23412.157\n",
      "[156,   130] loss: 30173.324\n",
      "[156,   135] loss: 16010.237\n",
      "[156,   140] loss: 16638.251\n",
      "[156,   145] loss: 28958.129\n",
      "[156,   150] loss: 23073.569\n",
      "[156,   155] loss: 17312.475\n",
      "[156,   160] loss: 20394.116\n",
      "[156,   165] loss: 23928.743\n",
      "[156,   170] loss: 28499.072\n",
      "[156,   175] loss: 40805.656\n",
      "[156,   180] loss: 27279.842\n",
      "[156,   185] loss: 26652.734\n",
      "[156,   190] loss: 24093.729\n",
      "[156,   195] loss: 23758.839\n",
      "[156,   200] loss: 20764.120\n",
      "[156,   205] loss: 22720.232\n",
      "[156,   210] loss: 19836.346\n",
      "[156,   215] loss: 25068.950\n",
      "[156,   220] loss: 17383.354\n",
      "[156,   225] loss: 23779.342\n",
      "[156,   230] loss: 24102.140\n",
      "[157,     5] loss: 20363.777\n",
      "[157,    10] loss: 31708.043\n",
      "[157,    15] loss: 25771.791\n",
      "[157,    20] loss: 33305.951\n",
      "[157,    25] loss: 34384.526\n",
      "[157,    30] loss: 23279.200\n",
      "[157,    35] loss: 23540.850\n",
      "[157,    40] loss: 26194.391\n",
      "[157,    45] loss: 23172.673\n",
      "[157,    50] loss: 19058.149\n",
      "[157,    55] loss: 26902.324\n",
      "[157,    60] loss: 28274.518\n",
      "[157,    65] loss: 30665.000\n",
      "[157,    70] loss: 47117.632\n",
      "[157,    75] loss: 20990.596\n",
      "[157,    80] loss: 30267.497\n",
      "[157,    85] loss: 23617.657\n",
      "[157,    90] loss: 25870.110\n",
      "[157,    95] loss: 25403.442\n",
      "[157,   100] loss: 21706.407\n",
      "[157,   105] loss: 21562.692\n",
      "[157,   110] loss: 54790.161\n",
      "[157,   115] loss: 23686.210\n",
      "[157,   120] loss: 17072.886\n",
      "[157,   125] loss: 16742.639\n",
      "[157,   130] loss: 26062.275\n",
      "[157,   135] loss: 29353.656\n",
      "[157,   140] loss: 19066.635\n",
      "[157,   145] loss: 20208.001\n",
      "[157,   150] loss: 25546.494\n",
      "[157,   155] loss: 31120.162\n",
      "[157,   160] loss: 19535.159\n",
      "[157,   165] loss: 23614.724\n",
      "[157,   170] loss: 30633.409\n",
      "[157,   175] loss: 35284.450\n",
      "[157,   180] loss: 26308.162\n",
      "[157,   185] loss: 29671.968\n",
      "[157,   190] loss: 21718.691\n",
      "[157,   195] loss: 25739.226\n",
      "[157,   200] loss: 18244.730\n",
      "[157,   205] loss: 21970.757\n",
      "[157,   210] loss: 40305.208\n",
      "[157,   215] loss: 32046.170\n",
      "[157,   220] loss: 22109.513\n",
      "[157,   225] loss: 20179.628\n",
      "[157,   230] loss: 23961.888\n",
      "[158,     5] loss: 20846.155\n",
      "[158,    10] loss: 22729.816\n",
      "[158,    15] loss: 25778.325\n",
      "[158,    20] loss: 33752.864\n",
      "[158,    25] loss: 25054.864\n",
      "[158,    30] loss: 21095.188\n",
      "[158,    35] loss: 39133.886\n",
      "[158,    40] loss: 34868.285\n",
      "[158,    45] loss: 21608.469\n",
      "[158,    50] loss: 20661.506\n",
      "[158,    55] loss: 30031.879\n",
      "[158,    60] loss: 27182.564\n",
      "[158,    65] loss: 45590.387\n",
      "[158,    70] loss: 26924.950\n",
      "[158,    75] loss: 21431.023\n",
      "[158,    80] loss: 36161.241\n",
      "[158,    85] loss: 19855.873\n",
      "[158,    90] loss: 28321.361\n",
      "[158,    95] loss: 36863.505\n",
      "[158,   100] loss: 15829.179\n",
      "[158,   105] loss: 18639.891\n",
      "[158,   110] loss: 38980.341\n",
      "[158,   115] loss: 30526.333\n",
      "[158,   120] loss: 20525.791\n",
      "[158,   125] loss: 25717.118\n",
      "[158,   130] loss: 14711.874\n",
      "[158,   135] loss: 26897.850\n",
      "[158,   140] loss: 24192.314\n",
      "[158,   145] loss: 24568.442\n",
      "[158,   150] loss: 26373.939\n",
      "[158,   155] loss: 26420.467\n",
      "[158,   160] loss: 35731.675\n",
      "[158,   165] loss: 30051.569\n",
      "[158,   170] loss: 25119.779\n",
      "[158,   175] loss: 28878.046\n",
      "[158,   180] loss: 32544.944\n",
      "[158,   185] loss: 32669.046\n",
      "[158,   190] loss: 24441.996\n",
      "[158,   195] loss: 30287.993\n",
      "[158,   200] loss: 21925.591\n",
      "[158,   205] loss: 19258.806\n",
      "[158,   210] loss: 27302.532\n",
      "[158,   215] loss: 19735.145\n",
      "[158,   220] loss: 28463.057\n",
      "[158,   225] loss: 16071.416\n",
      "[158,   230] loss: 23468.690\n",
      "[159,     5] loss: 26927.884\n",
      "[159,    10] loss: 42955.091\n",
      "[159,    15] loss: 20247.060\n",
      "[159,    20] loss: 25016.159\n",
      "[159,    25] loss: 34569.697\n",
      "[159,    30] loss: 35794.281\n",
      "[159,    35] loss: 24002.518\n",
      "[159,    40] loss: 22264.882\n",
      "[159,    45] loss: 22089.388\n",
      "[159,    50] loss: 26948.463\n",
      "[159,    55] loss: 20229.604\n",
      "[159,    60] loss: 22448.799\n",
      "[159,    65] loss: 25937.368\n",
      "[159,    70] loss: 29361.804\n",
      "[159,    75] loss: 23457.697\n",
      "[159,    80] loss: 33638.804\n",
      "[159,    85] loss: 20346.756\n",
      "[159,    90] loss: 18492.016\n",
      "[159,    95] loss: 17632.587\n",
      "[159,   100] loss: 28295.246\n",
      "[159,   105] loss: 29513.840\n",
      "[159,   110] loss: 24134.071\n",
      "[159,   115] loss: 25711.082\n",
      "[159,   120] loss: 22630.472\n",
      "[159,   125] loss: 25813.428\n",
      "[159,   130] loss: 20587.754\n",
      "[159,   135] loss: 19902.329\n",
      "[159,   140] loss: 28729.339\n",
      "[159,   145] loss: 39601.548\n",
      "[159,   150] loss: 30090.021\n",
      "[159,   155] loss: 20801.145\n",
      "[159,   160] loss: 37735.120\n",
      "[159,   165] loss: 31091.374\n",
      "[159,   170] loss: 28755.388\n",
      "[159,   175] loss: 28306.236\n",
      "[159,   180] loss: 32242.429\n",
      "[159,   185] loss: 25870.174\n",
      "[159,   190] loss: 27537.125\n",
      "[159,   195] loss: 31825.555\n",
      "[159,   200] loss: 27000.370\n",
      "[159,   205] loss: 19824.613\n",
      "[159,   210] loss: 17726.947\n",
      "[159,   215] loss: 43751.752\n",
      "[159,   220] loss: 18125.294\n",
      "[159,   225] loss: 22291.180\n",
      "[159,   230] loss: 23580.323\n",
      "[160,     5] loss: 26372.428\n",
      "[160,    10] loss: 31816.576\n",
      "[160,    15] loss: 17851.156\n",
      "[160,    20] loss: 22670.109\n",
      "[160,    25] loss: 32756.216\n",
      "[160,    30] loss: 23886.794\n",
      "[160,    35] loss: 20125.065\n",
      "[160,    40] loss: 21144.106\n",
      "[160,    45] loss: 22774.755\n",
      "[160,    50] loss: 18716.945\n",
      "[160,    55] loss: 28290.180\n",
      "[160,    60] loss: 16563.162\n",
      "[160,    65] loss: 27297.949\n",
      "[160,    70] loss: 32604.053\n",
      "[160,    75] loss: 54561.370\n",
      "[160,    80] loss: 31415.287\n",
      "[160,    85] loss: 29117.114\n",
      "[160,    90] loss: 29696.656\n",
      "[160,    95] loss: 28455.268\n",
      "[160,   100] loss: 38709.230\n",
      "[160,   105] loss: 24984.226\n",
      "[160,   110] loss: 28200.907\n",
      "[160,   115] loss: 28990.273\n",
      "[160,   120] loss: 30755.942\n",
      "[160,   125] loss: 17866.927\n",
      "[160,   130] loss: 24249.157\n",
      "[160,   135] loss: 27797.008\n",
      "[160,   140] loss: 25916.457\n",
      "[160,   145] loss: 17197.287\n",
      "[160,   150] loss: 40181.145\n",
      "[160,   155] loss: 20053.164\n",
      "[160,   160] loss: 33297.619\n",
      "[160,   165] loss: 24717.257\n",
      "[160,   170] loss: 22806.240\n",
      "[160,   175] loss: 32646.185\n",
      "[160,   180] loss: 23267.909\n",
      "[160,   185] loss: 29166.254\n",
      "[160,   190] loss: 27166.855\n",
      "[160,   195] loss: 18147.194\n",
      "[160,   200] loss: 29866.371\n",
      "[160,   205] loss: 25162.202\n",
      "[160,   210] loss: 24888.966\n",
      "[160,   215] loss: 23506.043\n",
      "[160,   220] loss: 32928.226\n",
      "[160,   225] loss: 14181.163\n",
      "[160,   230] loss: 19402.352\n",
      "[161,     5] loss: 26722.549\n",
      "[161,    10] loss: 27341.464\n",
      "[161,    15] loss: 17661.235\n",
      "[161,    20] loss: 26469.463\n",
      "[161,    25] loss: 28001.376\n",
      "[161,    30] loss: 27317.598\n",
      "[161,    35] loss: 35366.775\n",
      "[161,    40] loss: 29706.990\n",
      "[161,    45] loss: 34144.512\n",
      "[161,    50] loss: 29071.056\n",
      "[161,    55] loss: 20470.745\n",
      "[161,    60] loss: 30254.073\n",
      "[161,    65] loss: 30068.694\n",
      "[161,    70] loss: 27428.580\n",
      "[161,    75] loss: 23257.495\n",
      "[161,    80] loss: 31324.739\n",
      "[161,    85] loss: 50465.788\n",
      "[161,    90] loss: 23599.948\n",
      "[161,    95] loss: 29779.582\n",
      "[161,   100] loss: 28102.514\n",
      "[161,   105] loss: 19489.517\n",
      "[161,   110] loss: 26638.285\n",
      "[161,   115] loss: 24743.421\n",
      "[161,   120] loss: 14823.812\n",
      "[161,   125] loss: 27874.293\n",
      "[161,   130] loss: 14815.481\n",
      "[161,   135] loss: 21584.338\n",
      "[161,   140] loss: 29524.141\n",
      "[161,   145] loss: 21977.931\n",
      "[161,   150] loss: 26873.921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161,   155] loss: 28072.710\n",
      "[161,   160] loss: 20490.531\n",
      "[161,   165] loss: 22838.842\n",
      "[161,   170] loss: 19909.194\n",
      "[161,   175] loss: 27728.823\n",
      "[161,   180] loss: 22382.839\n",
      "[161,   185] loss: 28370.848\n",
      "[161,   190] loss: 32269.295\n",
      "[161,   195] loss: 23187.886\n",
      "[161,   200] loss: 26541.841\n",
      "[161,   205] loss: 22936.594\n",
      "[161,   210] loss: 14602.805\n",
      "[161,   215] loss: 28378.205\n",
      "[161,   220] loss: 32165.659\n",
      "[161,   225] loss: 25098.935\n",
      "[161,   230] loss: 30578.368\n",
      "[162,     5] loss: 30358.486\n",
      "[162,    10] loss: 16945.912\n",
      "[162,    15] loss: 16501.611\n",
      "[162,    20] loss: 23184.827\n",
      "[162,    25] loss: 28844.822\n",
      "[162,    30] loss: 16405.557\n",
      "[162,    35] loss: 23116.027\n",
      "[162,    40] loss: 32898.971\n",
      "[162,    45] loss: 34019.370\n",
      "[162,    50] loss: 47300.274\n",
      "[162,    55] loss: 30796.821\n",
      "[162,    60] loss: 22490.546\n",
      "[162,    65] loss: 24216.974\n",
      "[162,    70] loss: 25777.054\n",
      "[162,    75] loss: 36497.824\n",
      "[162,    80] loss: 19748.743\n",
      "[162,    85] loss: 40950.951\n",
      "[162,    90] loss: 18443.311\n",
      "[162,    95] loss: 27491.930\n",
      "[162,   100] loss: 27675.718\n",
      "[162,   105] loss: 17632.075\n",
      "[162,   110] loss: 24225.438\n",
      "[162,   115] loss: 20034.685\n",
      "[162,   120] loss: 20419.567\n",
      "[162,   125] loss: 27700.847\n",
      "[162,   130] loss: 25539.163\n",
      "[162,   135] loss: 52397.305\n",
      "[162,   140] loss: 35944.296\n",
      "[162,   145] loss: 19210.409\n",
      "[162,   150] loss: 23892.531\n",
      "[162,   155] loss: 18596.949\n",
      "[162,   160] loss: 30365.747\n",
      "[162,   165] loss: 19855.422\n",
      "[162,   170] loss: 24272.765\n",
      "[162,   175] loss: 22256.572\n",
      "[162,   180] loss: 23786.283\n",
      "[162,   185] loss: 28282.200\n",
      "[162,   190] loss: 19099.149\n",
      "[162,   195] loss: 22404.773\n",
      "[162,   200] loss: 40380.092\n",
      "[162,   205] loss: 20805.805\n",
      "[162,   210] loss: 27551.189\n",
      "[162,   215] loss: 35708.468\n",
      "[162,   220] loss: 27440.611\n",
      "[162,   225] loss: 24746.136\n",
      "[162,   230] loss: 24671.590\n",
      "[163,     5] loss: 22836.663\n",
      "[163,    10] loss: 30823.003\n",
      "[163,    15] loss: 33370.488\n",
      "[163,    20] loss: 19190.571\n",
      "[163,    25] loss: 23433.697\n",
      "[163,    30] loss: 27950.854\n",
      "[163,    35] loss: 28630.087\n",
      "[163,    40] loss: 23950.277\n",
      "[163,    45] loss: 28085.119\n",
      "[163,    50] loss: 28544.063\n",
      "[163,    55] loss: 19278.844\n",
      "[163,    60] loss: 45311.870\n",
      "[163,    65] loss: 24258.726\n",
      "[163,    70] loss: 23386.148\n",
      "[163,    75] loss: 22223.993\n",
      "[163,    80] loss: 29682.146\n",
      "[163,    85] loss: 24307.925\n",
      "[163,    90] loss: 32743.912\n",
      "[163,    95] loss: 29125.325\n",
      "[163,   100] loss: 17493.632\n",
      "[163,   105] loss: 24334.597\n",
      "[163,   110] loss: 29329.967\n",
      "[163,   115] loss: 32390.469\n",
      "[163,   120] loss: 26027.979\n",
      "[163,   125] loss: 21642.927\n",
      "[163,   130] loss: 32543.338\n",
      "[163,   135] loss: 32649.058\n",
      "[163,   140] loss: 33208.603\n",
      "[163,   145] loss: 19611.626\n",
      "[163,   150] loss: 21273.716\n",
      "[163,   155] loss: 22071.077\n",
      "[163,   160] loss: 28238.987\n",
      "[163,   165] loss: 22740.925\n",
      "[163,   170] loss: 27439.604\n",
      "[163,   175] loss: 25514.649\n",
      "[163,   180] loss: 34414.555\n",
      "[163,   185] loss: 20541.012\n",
      "[163,   190] loss: 27245.224\n",
      "[163,   195] loss: 22446.294\n",
      "[163,   200] loss: 27999.876\n",
      "[163,   205] loss: 28069.404\n",
      "[163,   210] loss: 27070.239\n",
      "[163,   215] loss: 16357.563\n",
      "[163,   220] loss: 26447.822\n",
      "[163,   225] loss: 25767.070\n",
      "[163,   230] loss: 30767.216\n",
      "[164,     5] loss: 23828.688\n",
      "[164,    10] loss: 32398.412\n",
      "[164,    15] loss: 21614.207\n",
      "[164,    20] loss: 55259.032\n",
      "[164,    25] loss: 18921.036\n",
      "[164,    30] loss: 13658.446\n",
      "[164,    35] loss: 29412.500\n",
      "[164,    40] loss: 37735.948\n",
      "[164,    45] loss: 28180.611\n",
      "[164,    50] loss: 20386.334\n",
      "[164,    55] loss: 25351.554\n",
      "[164,    60] loss: 25260.021\n",
      "[164,    65] loss: 29972.108\n",
      "[164,    70] loss: 22732.432\n",
      "[164,    75] loss: 23004.780\n",
      "[164,    80] loss: 25330.414\n",
      "[164,    85] loss: 17317.614\n",
      "[164,    90] loss: 29320.309\n",
      "[164,    95] loss: 29706.639\n",
      "[164,   100] loss: 22631.835\n",
      "[164,   105] loss: 25233.587\n",
      "[164,   110] loss: 28579.493\n",
      "[164,   115] loss: 21195.011\n",
      "[164,   120] loss: 14741.897\n",
      "[164,   125] loss: 32636.657\n",
      "[164,   130] loss: 26718.840\n",
      "[164,   135] loss: 16861.976\n",
      "[164,   140] loss: 24388.690\n",
      "[164,   145] loss: 17560.195\n",
      "[164,   150] loss: 33894.682\n",
      "[164,   155] loss: 22889.352\n",
      "[164,   160] loss: 21772.028\n",
      "[164,   165] loss: 21806.514\n",
      "[164,   170] loss: 26773.519\n",
      "[164,   175] loss: 37692.556\n",
      "[164,   180] loss: 32877.914\n",
      "[164,   185] loss: 25646.858\n",
      "[164,   190] loss: 41710.777\n",
      "[164,   195] loss: 22588.104\n",
      "[164,   200] loss: 24762.423\n",
      "[164,   205] loss: 19832.558\n",
      "[164,   210] loss: 41666.930\n",
      "[164,   215] loss: 25203.858\n",
      "[164,   220] loss: 23015.278\n",
      "[164,   225] loss: 29132.931\n",
      "[164,   230] loss: 25288.632\n",
      "[165,     5] loss: 14722.373\n",
      "[165,    10] loss: 28889.493\n",
      "[165,    15] loss: 21590.055\n",
      "[165,    20] loss: 35502.445\n",
      "[165,    25] loss: 22019.693\n",
      "[165,    30] loss: 25391.345\n",
      "[165,    35] loss: 23782.847\n",
      "[165,    40] loss: 37641.021\n",
      "[165,    45] loss: 40579.720\n",
      "[165,    50] loss: 26091.320\n",
      "[165,    55] loss: 27006.901\n",
      "[165,    60] loss: 49086.757\n",
      "[165,    65] loss: 13341.535\n",
      "[165,    70] loss: 18970.947\n",
      "[165,    75] loss: 31070.955\n",
      "[165,    80] loss: 22362.146\n",
      "[165,    85] loss: 24096.342\n",
      "[165,    90] loss: 27810.349\n",
      "[165,    95] loss: 19839.433\n",
      "[165,   100] loss: 22705.005\n",
      "[165,   105] loss: 23326.676\n",
      "[165,   110] loss: 39885.425\n",
      "[165,   115] loss: 29096.245\n",
      "[165,   120] loss: 32054.266\n",
      "[165,   125] loss: 25402.665\n",
      "[165,   130] loss: 18546.668\n",
      "[165,   135] loss: 22372.606\n",
      "[165,   140] loss: 21078.897\n",
      "[165,   145] loss: 20014.557\n",
      "[165,   150] loss: 20912.864\n",
      "[165,   155] loss: 27248.122\n",
      "[165,   160] loss: 28452.631\n",
      "[165,   165] loss: 29745.522\n",
      "[165,   170] loss: 23490.450\n",
      "[165,   175] loss: 21565.017\n",
      "[165,   180] loss: 22397.111\n",
      "[165,   185] loss: 23520.112\n",
      "[165,   190] loss: 35079.989\n",
      "[165,   195] loss: 34910.403\n",
      "[165,   200] loss: 24768.726\n",
      "[165,   205] loss: 22526.782\n",
      "[165,   210] loss: 25185.254\n",
      "[165,   215] loss: 29941.403\n",
      "[165,   220] loss: 30615.109\n",
      "[165,   225] loss: 31791.369\n",
      "[165,   230] loss: 17844.274\n",
      "[166,     5] loss: 24282.847\n",
      "[166,    10] loss: 16223.948\n",
      "[166,    15] loss: 31104.744\n",
      "[166,    20] loss: 25908.002\n",
      "[166,    25] loss: 26416.635\n",
      "[166,    30] loss: 25692.896\n",
      "[166,    35] loss: 18803.943\n",
      "[166,    40] loss: 19541.273\n",
      "[166,    45] loss: 15918.267\n",
      "[166,    50] loss: 23203.507\n",
      "[166,    55] loss: 33333.425\n",
      "[166,    60] loss: 28370.545\n",
      "[166,    65] loss: 36485.497\n",
      "[166,    70] loss: 39799.334\n",
      "[166,    75] loss: 34884.604\n",
      "[166,    80] loss: 25906.473\n",
      "[166,    85] loss: 31305.246\n",
      "[166,    90] loss: 21639.389\n",
      "[166,    95] loss: 29853.460\n",
      "[166,   100] loss: 27178.315\n",
      "[166,   105] loss: 48998.327\n",
      "[166,   110] loss: 19527.427\n",
      "[166,   115] loss: 22623.610\n",
      "[166,   120] loss: 23761.374\n",
      "[166,   125] loss: 19436.152\n",
      "[166,   130] loss: 18831.084\n",
      "[166,   135] loss: 20394.640\n",
      "[166,   140] loss: 24236.906\n",
      "[166,   145] loss: 24330.507\n",
      "[166,   150] loss: 23450.604\n",
      "[166,   155] loss: 29898.421\n",
      "[166,   160] loss: 27139.053\n",
      "[166,   165] loss: 22069.032\n",
      "[166,   170] loss: 22236.253\n",
      "[166,   175] loss: 27490.907\n",
      "[166,   180] loss: 25650.183\n",
      "[166,   185] loss: 19349.453\n",
      "[166,   190] loss: 53583.410\n",
      "[166,   195] loss: 26173.252\n",
      "[166,   200] loss: 26579.005\n",
      "[166,   205] loss: 20233.407\n",
      "[166,   210] loss: 21722.218\n",
      "[166,   215] loss: 31033.692\n",
      "[166,   220] loss: 33246.589\n",
      "[166,   225] loss: 22474.929\n",
      "[166,   230] loss: 31574.497\n",
      "[167,     5] loss: 24884.084\n",
      "[167,    10] loss: 24882.643\n",
      "[167,    15] loss: 25078.180\n",
      "[167,    20] loss: 21324.999\n",
      "[167,    25] loss: 21836.705\n",
      "[167,    30] loss: 19418.265\n",
      "[167,    35] loss: 33679.088\n",
      "[167,    40] loss: 28489.216\n",
      "[167,    45] loss: 20486.414\n",
      "[167,    50] loss: 20988.080\n",
      "[167,    55] loss: 29681.326\n",
      "[167,    60] loss: 17511.284\n",
      "[167,    65] loss: 46835.183\n",
      "[167,    70] loss: 22846.254\n",
      "[167,    75] loss: 27707.816\n",
      "[167,    80] loss: 25322.739\n",
      "[167,    85] loss: 43379.977\n",
      "[167,    90] loss: 16756.230\n",
      "[167,    95] loss: 12605.699\n",
      "[167,   100] loss: 25517.019\n",
      "[167,   105] loss: 29263.496\n",
      "[167,   110] loss: 21178.871\n",
      "[167,   115] loss: 25669.391\n",
      "[167,   120] loss: 23676.397\n",
      "[167,   125] loss: 22110.673\n",
      "[167,   130] loss: 18888.160\n",
      "[167,   135] loss: 32298.447\n",
      "[167,   140] loss: 54374.950\n",
      "[167,   145] loss: 24876.358\n",
      "[167,   150] loss: 20220.515\n",
      "[167,   155] loss: 17125.480\n",
      "[167,   160] loss: 38336.382\n",
      "[167,   165] loss: 23809.786\n",
      "[167,   170] loss: 20674.776\n",
      "[167,   175] loss: 21591.753\n",
      "[167,   180] loss: 31137.144\n",
      "[167,   185] loss: 18819.125\n",
      "[167,   190] loss: 35236.925\n",
      "[167,   195] loss: 23464.948\n",
      "[167,   200] loss: 26911.885\n",
      "[167,   205] loss: 40891.149\n",
      "[167,   210] loss: 31716.705\n",
      "[167,   215] loss: 30764.255\n",
      "[167,   220] loss: 29260.036\n",
      "[167,   225] loss: 29488.316\n",
      "[167,   230] loss: 20818.734\n",
      "[168,     5] loss: 40955.822\n",
      "[168,    10] loss: 18082.680\n",
      "[168,    15] loss: 29248.981\n",
      "[168,    20] loss: 33825.168\n",
      "[168,    25] loss: 31195.313\n",
      "[168,    30] loss: 25962.996\n",
      "[168,    35] loss: 16309.594\n",
      "[168,    40] loss: 31310.935\n",
      "[168,    45] loss: 24335.437\n",
      "[168,    50] loss: 23070.501\n",
      "[168,    55] loss: 25801.429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168,    60] loss: 21617.070\n",
      "[168,    65] loss: 18575.447\n",
      "[168,    70] loss: 40779.311\n",
      "[168,    75] loss: 26106.105\n",
      "[168,    80] loss: 20778.320\n",
      "[168,    85] loss: 28959.171\n",
      "[168,    90] loss: 17809.805\n",
      "[168,    95] loss: 42342.316\n",
      "[168,   100] loss: 20407.134\n",
      "[168,   105] loss: 44070.456\n",
      "[168,   110] loss: 15832.749\n",
      "[168,   115] loss: 30326.830\n",
      "[168,   120] loss: 24848.127\n",
      "[168,   125] loss: 28565.218\n",
      "[168,   130] loss: 18631.297\n",
      "[168,   135] loss: 23697.351\n",
      "[168,   140] loss: 29053.996\n",
      "[168,   145] loss: 18777.577\n",
      "[168,   150] loss: 26539.429\n",
      "[168,   155] loss: 24737.731\n",
      "[168,   160] loss: 23462.327\n",
      "[168,   165] loss: 29254.054\n",
      "[168,   170] loss: 20815.952\n",
      "[168,   175] loss: 26794.539\n",
      "[168,   180] loss: 21976.235\n",
      "[168,   185] loss: 25493.231\n",
      "[168,   190] loss: 31846.077\n",
      "[168,   195] loss: 22030.701\n",
      "[168,   200] loss: 22496.959\n",
      "[168,   205] loss: 30439.041\n",
      "[168,   210] loss: 28941.106\n",
      "[168,   215] loss: 29937.299\n",
      "[168,   220] loss: 42480.715\n",
      "[168,   225] loss: 22875.411\n",
      "[168,   230] loss: 20667.822\n",
      "[169,     5] loss: 17294.086\n",
      "[169,    10] loss: 26982.655\n",
      "[169,    15] loss: 20172.638\n",
      "[169,    20] loss: 29332.146\n",
      "[169,    25] loss: 25809.859\n",
      "[169,    30] loss: 18617.733\n",
      "[169,    35] loss: 18732.650\n",
      "[169,    40] loss: 39284.807\n",
      "[169,    45] loss: 17084.101\n",
      "[169,    50] loss: 20669.605\n",
      "[169,    55] loss: 29192.584\n",
      "[169,    60] loss: 25315.766\n",
      "[169,    65] loss: 25660.078\n",
      "[169,    70] loss: 17754.542\n",
      "[169,    75] loss: 25892.091\n",
      "[169,    80] loss: 58237.363\n",
      "[169,    85] loss: 26459.698\n",
      "[169,    90] loss: 33378.927\n",
      "[169,    95] loss: 18915.663\n",
      "[169,   100] loss: 17642.760\n",
      "[169,   105] loss: 20352.357\n",
      "[169,   110] loss: 21825.883\n",
      "[169,   115] loss: 37807.639\n",
      "[169,   120] loss: 21581.649\n",
      "[169,   125] loss: 19142.556\n",
      "[169,   130] loss: 32745.958\n",
      "[169,   135] loss: 35499.194\n",
      "[169,   140] loss: 25234.946\n",
      "[169,   145] loss: 28294.534\n",
      "[169,   150] loss: 26211.797\n",
      "[169,   155] loss: 33359.126\n",
      "[169,   160] loss: 15456.445\n",
      "[169,   165] loss: 33209.806\n",
      "[169,   170] loss: 34404.396\n",
      "[169,   175] loss: 26004.965\n",
      "[169,   180] loss: 28746.279\n",
      "[169,   185] loss: 33491.747\n",
      "[169,   190] loss: 29542.948\n",
      "[169,   195] loss: 24726.531\n",
      "[169,   200] loss: 26760.940\n",
      "[169,   205] loss: 24296.194\n",
      "[169,   210] loss: 24329.320\n",
      "[169,   215] loss: 27604.963\n",
      "[169,   220] loss: 18232.224\n",
      "[169,   225] loss: 28446.778\n",
      "[169,   230] loss: 30182.690\n",
      "[170,     5] loss: 31970.561\n",
      "[170,    10] loss: 28094.739\n",
      "[170,    15] loss: 28666.426\n",
      "[170,    20] loss: 18345.515\n",
      "[170,    25] loss: 28770.447\n",
      "[170,    30] loss: 25571.871\n",
      "[170,    35] loss: 20666.364\n",
      "[170,    40] loss: 26590.472\n",
      "[170,    45] loss: 30525.581\n",
      "[170,    50] loss: 22320.682\n",
      "[170,    55] loss: 20732.208\n",
      "[170,    60] loss: 20575.840\n",
      "[170,    65] loss: 24429.901\n",
      "[170,    70] loss: 52781.444\n",
      "[170,    75] loss: 19707.027\n",
      "[170,    80] loss: 25520.536\n",
      "[170,    85] loss: 37459.755\n",
      "[170,    90] loss: 27481.715\n",
      "[170,    95] loss: 20554.289\n",
      "[170,   100] loss: 25313.812\n",
      "[170,   105] loss: 19734.580\n",
      "[170,   110] loss: 30540.210\n",
      "[170,   115] loss: 27104.870\n",
      "[170,   120] loss: 23322.066\n",
      "[170,   125] loss: 35993.555\n",
      "[170,   130] loss: 20881.035\n",
      "[170,   135] loss: 21805.541\n",
      "[170,   140] loss: 27258.291\n",
      "[170,   145] loss: 30140.132\n",
      "[170,   150] loss: 30097.057\n",
      "[170,   155] loss: 20711.223\n",
      "[170,   160] loss: 26207.055\n",
      "[170,   165] loss: 27550.937\n",
      "[170,   170] loss: 33738.184\n",
      "[170,   175] loss: 20245.488\n",
      "[170,   180] loss: 20714.974\n",
      "[170,   185] loss: 47087.525\n",
      "[170,   190] loss: 23063.479\n",
      "[170,   195] loss: 23095.435\n",
      "[170,   200] loss: 24311.524\n",
      "[170,   205] loss: 27645.634\n",
      "[170,   210] loss: 24622.064\n",
      "[170,   215] loss: 23057.353\n",
      "[170,   220] loss: 22237.321\n",
      "[170,   225] loss: 30909.480\n",
      "[170,   230] loss: 23850.754\n",
      "[171,     5] loss: 19410.753\n",
      "[171,    10] loss: 20942.215\n",
      "[171,    15] loss: 18672.568\n",
      "[171,    20] loss: 23850.440\n",
      "[171,    25] loss: 36050.102\n",
      "[171,    30] loss: 23245.742\n",
      "[171,    35] loss: 24977.394\n",
      "[171,    40] loss: 22864.124\n",
      "[171,    45] loss: 36730.923\n",
      "[171,    50] loss: 26087.476\n",
      "[171,    55] loss: 31988.507\n",
      "[171,    60] loss: 20017.539\n",
      "[171,    65] loss: 20699.954\n",
      "[171,    70] loss: 29706.699\n",
      "[171,    75] loss: 66324.826\n",
      "[171,    80] loss: 27807.832\n",
      "[171,    85] loss: 33334.062\n",
      "[171,    90] loss: 30745.949\n",
      "[171,    95] loss: 17822.360\n",
      "[171,   100] loss: 26565.900\n",
      "[171,   105] loss: 18537.827\n",
      "[171,   110] loss: 19586.163\n",
      "[171,   115] loss: 24360.963\n",
      "[171,   120] loss: 37722.534\n",
      "[171,   125] loss: 28567.117\n",
      "[171,   130] loss: 20638.331\n",
      "[171,   135] loss: 24120.010\n",
      "[171,   140] loss: 28068.359\n",
      "[171,   145] loss: 30384.360\n",
      "[171,   150] loss: 31404.117\n",
      "[171,   155] loss: 23674.280\n",
      "[171,   160] loss: 35415.595\n",
      "[171,   165] loss: 28418.007\n",
      "[171,   170] loss: 19436.679\n",
      "[171,   175] loss: 22951.417\n",
      "[171,   180] loss: 25022.957\n",
      "[171,   185] loss: 16305.631\n",
      "[171,   190] loss: 15781.629\n",
      "[171,   195] loss: 22806.629\n",
      "[171,   200] loss: 20556.490\n",
      "[171,   205] loss: 18719.651\n",
      "[171,   210] loss: 23125.535\n",
      "[171,   215] loss: 42760.991\n",
      "[171,   220] loss: 32030.492\n",
      "[171,   225] loss: 32115.565\n",
      "[171,   230] loss: 20329.893\n",
      "[172,     5] loss: 27589.706\n",
      "[172,    10] loss: 20447.261\n",
      "[172,    15] loss: 26794.127\n",
      "[172,    20] loss: 27122.093\n",
      "[172,    25] loss: 41165.386\n",
      "[172,    30] loss: 18507.775\n",
      "[172,    35] loss: 28107.454\n",
      "[172,    40] loss: 47697.326\n",
      "[172,    45] loss: 25957.398\n",
      "[172,    50] loss: 26813.430\n",
      "[172,    55] loss: 24225.274\n",
      "[172,    60] loss: 24227.736\n",
      "[172,    65] loss: 32002.154\n",
      "[172,    70] loss: 23742.262\n",
      "[172,    75] loss: 27455.159\n",
      "[172,    80] loss: 22893.259\n",
      "[172,    85] loss: 28093.938\n",
      "[172,    90] loss: 25944.451\n",
      "[172,    95] loss: 20419.046\n",
      "[172,   100] loss: 31453.022\n",
      "[172,   105] loss: 24813.508\n",
      "[172,   110] loss: 18738.939\n",
      "[172,   115] loss: 30201.217\n",
      "[172,   120] loss: 21129.707\n",
      "[172,   125] loss: 16320.516\n",
      "[172,   130] loss: 28889.106\n",
      "[172,   135] loss: 23938.979\n",
      "[172,   140] loss: 25436.788\n",
      "[172,   145] loss: 28066.652\n",
      "[172,   150] loss: 34541.466\n",
      "[172,   155] loss: 24293.763\n",
      "[172,   160] loss: 21229.569\n",
      "[172,   165] loss: 47543.168\n",
      "[172,   170] loss: 25858.621\n",
      "[172,   175] loss: 33488.341\n",
      "[172,   180] loss: 26148.499\n",
      "[172,   185] loss: 29153.952\n",
      "[172,   190] loss: 18327.762\n",
      "[172,   195] loss: 26235.007\n",
      "[172,   200] loss: 23086.691\n",
      "[172,   205] loss: 20794.323\n",
      "[172,   210] loss: 24064.421\n",
      "[172,   215] loss: 28268.188\n",
      "[172,   220] loss: 30446.838\n",
      "[172,   225] loss: 20689.465\n",
      "[172,   230] loss: 18272.587\n",
      "[173,     5] loss: 26443.619\n",
      "[173,    10] loss: 32604.612\n",
      "[173,    15] loss: 19378.635\n",
      "[173,    20] loss: 26270.968\n",
      "[173,    25] loss: 23162.936\n",
      "[173,    30] loss: 16415.734\n",
      "[173,    35] loss: 27550.282\n",
      "[173,    40] loss: 24687.450\n",
      "[173,    45] loss: 35795.377\n",
      "[173,    50] loss: 32134.744\n",
      "[173,    55] loss: 18748.286\n",
      "[173,    60] loss: 13734.972\n",
      "[173,    65] loss: 18835.254\n",
      "[173,    70] loss: 34811.918\n",
      "[173,    75] loss: 35061.802\n",
      "[173,    80] loss: 21113.139\n",
      "[173,    85] loss: 53096.969\n",
      "[173,    90] loss: 23150.783\n",
      "[173,    95] loss: 20688.560\n",
      "[173,   100] loss: 36819.220\n",
      "[173,   105] loss: 24315.117\n",
      "[173,   110] loss: 24222.412\n",
      "[173,   115] loss: 23782.641\n",
      "[173,   120] loss: 25626.484\n",
      "[173,   125] loss: 24530.894\n",
      "[173,   130] loss: 19575.632\n",
      "[173,   135] loss: 22341.146\n",
      "[173,   140] loss: 22635.013\n",
      "[173,   145] loss: 17826.771\n",
      "[173,   150] loss: 22189.138\n",
      "[173,   155] loss: 23225.484\n",
      "[173,   160] loss: 27047.212\n",
      "[173,   165] loss: 21717.919\n",
      "[173,   170] loss: 22409.921\n",
      "[173,   175] loss: 49716.255\n",
      "[173,   180] loss: 22624.797\n",
      "[173,   185] loss: 45569.460\n",
      "[173,   190] loss: 25967.063\n",
      "[173,   195] loss: 24497.430\n",
      "[173,   200] loss: 30207.982\n",
      "[173,   205] loss: 16883.421\n",
      "[173,   210] loss: 24575.779\n",
      "[173,   215] loss: 39509.571\n",
      "[173,   220] loss: 27466.028\n",
      "[173,   225] loss: 18157.152\n",
      "[173,   230] loss: 20590.384\n",
      "[174,     5] loss: 32824.004\n",
      "[174,    10] loss: 18722.791\n",
      "[174,    15] loss: 19519.815\n",
      "[174,    20] loss: 34783.191\n",
      "[174,    25] loss: 61422.754\n",
      "[174,    30] loss: 28029.372\n",
      "[174,    35] loss: 35799.943\n",
      "[174,    40] loss: 26851.896\n",
      "[174,    45] loss: 26013.155\n",
      "[174,    50] loss: 20009.451\n",
      "[174,    55] loss: 21478.051\n",
      "[174,    60] loss: 25616.128\n",
      "[174,    65] loss: 30981.504\n",
      "[174,    70] loss: 47509.671\n",
      "[174,    75] loss: 23711.217\n",
      "[174,    80] loss: 23303.395\n",
      "[174,    85] loss: 22945.498\n",
      "[174,    90] loss: 23950.833\n",
      "[174,    95] loss: 26742.714\n",
      "[174,   100] loss: 17174.359\n",
      "[174,   105] loss: 21666.664\n",
      "[174,   110] loss: 21359.978\n",
      "[174,   115] loss: 26999.045\n",
      "[174,   120] loss: 29682.004\n",
      "[174,   125] loss: 17485.728\n",
      "[174,   130] loss: 39859.045\n",
      "[174,   135] loss: 17551.204\n",
      "[174,   140] loss: 36746.398\n",
      "[174,   145] loss: 25442.856\n",
      "[174,   150] loss: 23814.993\n",
      "[174,   155] loss: 23266.625\n",
      "[174,   160] loss: 17823.651\n",
      "[174,   165] loss: 27142.896\n",
      "[174,   170] loss: 21961.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174,   175] loss: 33606.414\n",
      "[174,   180] loss: 28930.531\n",
      "[174,   185] loss: 31530.329\n",
      "[174,   190] loss: 18271.736\n",
      "[174,   195] loss: 27820.850\n",
      "[174,   200] loss: 25261.186\n",
      "[174,   205] loss: 19293.203\n",
      "[174,   210] loss: 26440.426\n",
      "[174,   215] loss: 15277.468\n",
      "[174,   220] loss: 27972.138\n",
      "[174,   225] loss: 19259.438\n",
      "[174,   230] loss: 24661.467\n",
      "[175,     5] loss: 22260.783\n",
      "[175,    10] loss: 26464.718\n",
      "[175,    15] loss: 22654.591\n",
      "[175,    20] loss: 30700.873\n",
      "[175,    25] loss: 26162.820\n",
      "[175,    30] loss: 22210.099\n",
      "[175,    35] loss: 18386.944\n",
      "[175,    40] loss: 18990.131\n",
      "[175,    45] loss: 46715.455\n",
      "[175,    50] loss: 24016.761\n",
      "[175,    55] loss: 20938.248\n",
      "[175,    60] loss: 43596.358\n",
      "[175,    65] loss: 27249.533\n",
      "[175,    70] loss: 23551.912\n",
      "[175,    75] loss: 23039.852\n",
      "[175,    80] loss: 30646.129\n",
      "[175,    85] loss: 22565.635\n",
      "[175,    90] loss: 20257.781\n",
      "[175,    95] loss: 26891.401\n",
      "[175,   100] loss: 25784.521\n",
      "[175,   105] loss: 30081.701\n",
      "[175,   110] loss: 34523.863\n",
      "[175,   115] loss: 21614.676\n",
      "[175,   120] loss: 21784.475\n",
      "[175,   125] loss: 31517.255\n",
      "[175,   130] loss: 22796.741\n",
      "[175,   135] loss: 31476.175\n",
      "[175,   140] loss: 26229.920\n",
      "[175,   145] loss: 31597.524\n",
      "[175,   150] loss: 22477.963\n",
      "[175,   155] loss: 20785.291\n",
      "[175,   160] loss: 26816.552\n",
      "[175,   165] loss: 20700.763\n",
      "[175,   170] loss: 23945.361\n",
      "[175,   175] loss: 28000.100\n",
      "[175,   180] loss: 17356.636\n",
      "[175,   185] loss: 30960.045\n",
      "[175,   190] loss: 29042.877\n",
      "[175,   195] loss: 30315.636\n",
      "[175,   200] loss: 32487.843\n",
      "[175,   205] loss: 29064.251\n",
      "[175,   210] loss: 20648.079\n",
      "[175,   215] loss: 21377.894\n",
      "[175,   220] loss: 31043.121\n",
      "[175,   225] loss: 29935.932\n",
      "[175,   230] loss: 23847.164\n",
      "[176,     5] loss: 42127.843\n",
      "[176,    10] loss: 19914.098\n",
      "[176,    15] loss: 24738.700\n",
      "[176,    20] loss: 22294.069\n",
      "[176,    25] loss: 20034.884\n",
      "[176,    30] loss: 23681.937\n",
      "[176,    35] loss: 39383.977\n",
      "[176,    40] loss: 46741.987\n",
      "[176,    45] loss: 25789.196\n",
      "[176,    50] loss: 32779.965\n",
      "[176,    55] loss: 21028.568\n",
      "[176,    60] loss: 31910.224\n",
      "[176,    65] loss: 27560.585\n",
      "[176,    70] loss: 19826.841\n",
      "[176,    75] loss: 21632.984\n",
      "[176,    80] loss: 19372.032\n",
      "[176,    85] loss: 21047.429\n",
      "[176,    90] loss: 14954.730\n",
      "[176,    95] loss: 22902.053\n",
      "[176,   100] loss: 26271.942\n",
      "[176,   105] loss: 17422.045\n",
      "[176,   110] loss: 38948.719\n",
      "[176,   115] loss: 29804.246\n",
      "[176,   120] loss: 24184.677\n",
      "[176,   125] loss: 21855.658\n",
      "[176,   130] loss: 32043.098\n",
      "[176,   135] loss: 20867.275\n",
      "[176,   140] loss: 28808.248\n",
      "[176,   145] loss: 25954.785\n",
      "[176,   150] loss: 27998.750\n",
      "[176,   155] loss: 22921.658\n",
      "[176,   160] loss: 31653.065\n",
      "[176,   165] loss: 30791.346\n",
      "[176,   170] loss: 25618.862\n",
      "[176,   175] loss: 20902.134\n",
      "[176,   180] loss: 22315.831\n",
      "[176,   185] loss: 23875.458\n",
      "[176,   190] loss: 40590.197\n",
      "[176,   195] loss: 25158.708\n",
      "[176,   200] loss: 27148.449\n",
      "[176,   205] loss: 20892.196\n",
      "[176,   210] loss: 22202.507\n",
      "[176,   215] loss: 33221.113\n",
      "[176,   220] loss: 26890.457\n",
      "[176,   225] loss: 26374.168\n",
      "[176,   230] loss: 22801.037\n",
      "[177,     5] loss: 32171.121\n",
      "[177,    10] loss: 21162.034\n",
      "[177,    15] loss: 24481.937\n",
      "[177,    20] loss: 34846.612\n",
      "[177,    25] loss: 18704.942\n",
      "[177,    30] loss: 29802.111\n",
      "[177,    35] loss: 21781.779\n",
      "[177,    40] loss: 24287.112\n",
      "[177,    45] loss: 20189.552\n",
      "[177,    50] loss: 18211.744\n",
      "[177,    55] loss: 22361.486\n",
      "[177,    60] loss: 32550.259\n",
      "[177,    65] loss: 23574.309\n",
      "[177,    70] loss: 24651.557\n",
      "[177,    75] loss: 28744.789\n",
      "[177,    80] loss: 28199.548\n",
      "[177,    85] loss: 24693.086\n",
      "[177,    90] loss: 30722.597\n",
      "[177,    95] loss: 15457.962\n",
      "[177,   100] loss: 25447.832\n",
      "[177,   105] loss: 26614.466\n",
      "[177,   110] loss: 30462.479\n",
      "[177,   115] loss: 23519.431\n",
      "[177,   120] loss: 18867.637\n",
      "[177,   125] loss: 15947.199\n",
      "[177,   130] loss: 20153.593\n",
      "[177,   135] loss: 26123.341\n",
      "[177,   140] loss: 24132.734\n",
      "[177,   145] loss: 23565.060\n",
      "[177,   150] loss: 26733.421\n",
      "[177,   155] loss: 33623.327\n",
      "[177,   160] loss: 36262.789\n",
      "[177,   165] loss: 24356.756\n",
      "[177,   170] loss: 18074.384\n",
      "[177,   175] loss: 37251.762\n",
      "[177,   180] loss: 23510.631\n",
      "[177,   185] loss: 18957.015\n",
      "[177,   190] loss: 25907.320\n",
      "[177,   195] loss: 26941.718\n",
      "[177,   200] loss: 26725.146\n",
      "[177,   205] loss: 49305.657\n",
      "[177,   210] loss: 24094.524\n",
      "[177,   215] loss: 31603.607\n",
      "[177,   220] loss: 27354.014\n",
      "[177,   225] loss: 46490.649\n",
      "[177,   230] loss: 23811.976\n",
      "[178,     5] loss: 19339.181\n",
      "[178,    10] loss: 38883.257\n",
      "[178,    15] loss: 29117.430\n",
      "[178,    20] loss: 25506.514\n",
      "[178,    25] loss: 19177.037\n",
      "[178,    30] loss: 25889.925\n",
      "[178,    35] loss: 22175.021\n",
      "[178,    40] loss: 18057.376\n",
      "[178,    45] loss: 25788.523\n",
      "[178,    50] loss: 18816.772\n",
      "[178,    55] loss: 36463.425\n",
      "[178,    60] loss: 26430.996\n",
      "[178,    65] loss: 22124.330\n",
      "[178,    70] loss: 28676.514\n",
      "[178,    75] loss: 25696.229\n",
      "[178,    80] loss: 20603.361\n",
      "[178,    85] loss: 27301.830\n",
      "[178,    90] loss: 22252.241\n",
      "[178,    95] loss: 20772.233\n",
      "[178,   100] loss: 31755.672\n",
      "[178,   105] loss: 20865.620\n",
      "[178,   110] loss: 26253.899\n",
      "[178,   115] loss: 27849.304\n",
      "[178,   120] loss: 22378.330\n",
      "[178,   125] loss: 27876.809\n",
      "[178,   130] loss: 24382.583\n",
      "[178,   135] loss: 21974.071\n",
      "[178,   140] loss: 18405.185\n",
      "[178,   145] loss: 35206.678\n",
      "[178,   150] loss: 28219.234\n",
      "[178,   155] loss: 26504.459\n",
      "[178,   160] loss: 17780.119\n",
      "[178,   165] loss: 26526.562\n",
      "[178,   170] loss: 49314.605\n",
      "[178,   175] loss: 23626.573\n",
      "[178,   180] loss: 24291.337\n",
      "[178,   185] loss: 28726.101\n",
      "[178,   190] loss: 26941.240\n",
      "[178,   195] loss: 28081.579\n",
      "[178,   200] loss: 29011.713\n",
      "[178,   205] loss: 23221.258\n",
      "[178,   210] loss: 34098.005\n",
      "[178,   215] loss: 27599.259\n",
      "[178,   220] loss: 34511.338\n",
      "[178,   225] loss: 27260.561\n",
      "[178,   230] loss: 23654.922\n",
      "[179,     5] loss: 26312.279\n",
      "[179,    10] loss: 23798.632\n",
      "[179,    15] loss: 45935.796\n",
      "[179,    20] loss: 30680.681\n",
      "[179,    25] loss: 31837.672\n",
      "[179,    30] loss: 31737.975\n",
      "[179,    35] loss: 29523.211\n",
      "[179,    40] loss: 19976.884\n",
      "[179,    45] loss: 20452.657\n",
      "[179,    50] loss: 22839.563\n",
      "[179,    55] loss: 24566.847\n",
      "[179,    60] loss: 23862.719\n",
      "[179,    65] loss: 32032.810\n",
      "[179,    70] loss: 24991.013\n",
      "[179,    75] loss: 17455.121\n",
      "[179,    80] loss: 25883.324\n",
      "[179,    85] loss: 21693.070\n",
      "[179,    90] loss: 34630.450\n",
      "[179,    95] loss: 27798.747\n",
      "[179,   100] loss: 22297.359\n",
      "[179,   105] loss: 24608.381\n",
      "[179,   110] loss: 27439.816\n",
      "[179,   115] loss: 28687.610\n",
      "[179,   120] loss: 18851.850\n",
      "[179,   125] loss: 27377.356\n",
      "[179,   130] loss: 32604.199\n",
      "[179,   135] loss: 18948.442\n",
      "[179,   140] loss: 30240.964\n",
      "[179,   145] loss: 34597.725\n",
      "[179,   150] loss: 40158.058\n",
      "[179,   155] loss: 19313.449\n",
      "[179,   160] loss: 29581.076\n",
      "[179,   165] loss: 17492.136\n",
      "[179,   170] loss: 29612.843\n",
      "[179,   175] loss: 25725.046\n",
      "[179,   180] loss: 23754.860\n",
      "[179,   185] loss: 19514.517\n",
      "[179,   190] loss: 29799.701\n",
      "[179,   195] loss: 18384.848\n",
      "[179,   200] loss: 21556.561\n",
      "[179,   205] loss: 29636.428\n",
      "[179,   210] loss: 27400.418\n",
      "[179,   215] loss: 24964.812\n",
      "[179,   220] loss: 20128.150\n",
      "[179,   225] loss: 30205.869\n",
      "[179,   230] loss: 23372.474\n",
      "[180,     5] loss: 24873.572\n",
      "[180,    10] loss: 24464.801\n",
      "[180,    15] loss: 24682.175\n",
      "[180,    20] loss: 19795.178\n",
      "[180,    25] loss: 22845.566\n",
      "[180,    30] loss: 28190.975\n",
      "[180,    35] loss: 27492.797\n",
      "[180,    40] loss: 35237.409\n",
      "[180,    45] loss: 29254.145\n",
      "[180,    50] loss: 30681.694\n",
      "[180,    55] loss: 18675.848\n",
      "[180,    60] loss: 18489.086\n",
      "[180,    65] loss: 28435.443\n",
      "[180,    70] loss: 23488.382\n",
      "[180,    75] loss: 22283.678\n",
      "[180,    80] loss: 22477.764\n",
      "[180,    85] loss: 40654.551\n",
      "[180,    90] loss: 26516.945\n",
      "[180,    95] loss: 27894.933\n",
      "[180,   100] loss: 29879.881\n",
      "[180,   105] loss: 26952.577\n",
      "[180,   110] loss: 23997.652\n",
      "[180,   115] loss: 26649.902\n",
      "[180,   120] loss: 20269.026\n",
      "[180,   125] loss: 61809.052\n",
      "[180,   130] loss: 20403.254\n",
      "[180,   135] loss: 15643.645\n",
      "[180,   140] loss: 32475.502\n",
      "[180,   145] loss: 25120.966\n",
      "[180,   150] loss: 27949.990\n",
      "[180,   155] loss: 34264.057\n",
      "[180,   160] loss: 20794.709\n",
      "[180,   165] loss: 25601.749\n",
      "[180,   170] loss: 29041.478\n",
      "[180,   175] loss: 16272.847\n",
      "[180,   180] loss: 28583.943\n",
      "[180,   185] loss: 24120.939\n",
      "[180,   190] loss: 28014.852\n",
      "[180,   195] loss: 17244.399\n",
      "[180,   200] loss: 29932.334\n",
      "[180,   205] loss: 19386.913\n",
      "[180,   210] loss: 31265.305\n",
      "[180,   215] loss: 23304.817\n",
      "[180,   220] loss: 24490.146\n",
      "[180,   225] loss: 29642.219\n",
      "[180,   230] loss: 21486.270\n",
      "[181,     5] loss: 21490.132\n",
      "[181,    10] loss: 43836.228\n",
      "[181,    15] loss: 29690.562\n",
      "[181,    20] loss: 21533.416\n",
      "[181,    25] loss: 19536.317\n",
      "[181,    30] loss: 22561.556\n",
      "[181,    35] loss: 34230.490\n",
      "[181,    40] loss: 27175.213\n",
      "[181,    45] loss: 20469.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181,    50] loss: 50623.957\n",
      "[181,    55] loss: 23636.179\n",
      "[181,    60] loss: 23148.952\n",
      "[181,    65] loss: 16882.039\n",
      "[181,    70] loss: 32010.055\n",
      "[181,    75] loss: 21772.890\n",
      "[181,    80] loss: 22165.967\n",
      "[181,    85] loss: 26872.734\n",
      "[181,    90] loss: 28996.469\n",
      "[181,    95] loss: 21190.673\n",
      "[181,   100] loss: 24227.347\n",
      "[181,   105] loss: 28159.928\n",
      "[181,   110] loss: 26605.881\n",
      "[181,   115] loss: 29770.207\n",
      "[181,   120] loss: 22838.187\n",
      "[181,   125] loss: 22585.835\n",
      "[181,   130] loss: 37207.009\n",
      "[181,   135] loss: 27628.249\n",
      "[181,   140] loss: 17399.832\n",
      "[181,   145] loss: 34096.902\n",
      "[181,   150] loss: 24937.218\n",
      "[181,   155] loss: 26953.475\n",
      "[181,   160] loss: 27307.968\n",
      "[181,   165] loss: 25544.693\n",
      "[181,   170] loss: 23685.570\n",
      "[181,   175] loss: 35112.088\n",
      "[181,   180] loss: 23149.749\n",
      "[181,   185] loss: 21645.429\n",
      "[181,   190] loss: 18670.651\n",
      "[181,   195] loss: 29039.043\n",
      "[181,   200] loss: 21779.530\n",
      "[181,   205] loss: 34705.154\n",
      "[181,   210] loss: 35713.826\n",
      "[181,   215] loss: 24040.833\n",
      "[181,   220] loss: 13945.928\n",
      "[181,   225] loss: 31661.174\n",
      "[181,   230] loss: 19055.229\n",
      "[182,     5] loss: 25878.042\n",
      "[182,    10] loss: 23730.144\n",
      "[182,    15] loss: 28223.791\n",
      "[182,    20] loss: 28777.142\n",
      "[182,    25] loss: 21590.912\n",
      "[182,    30] loss: 26125.190\n",
      "[182,    35] loss: 28300.284\n",
      "[182,    40] loss: 32242.880\n",
      "[182,    45] loss: 26917.899\n",
      "[182,    50] loss: 33048.364\n",
      "[182,    55] loss: 28736.618\n",
      "[182,    60] loss: 20815.182\n",
      "[182,    65] loss: 24044.667\n",
      "[182,    70] loss: 29301.870\n",
      "[182,    75] loss: 25767.517\n",
      "[182,    80] loss: 15952.253\n",
      "[182,    85] loss: 28480.726\n",
      "[182,    90] loss: 25416.769\n",
      "[182,    95] loss: 22409.443\n",
      "[182,   100] loss: 21640.531\n",
      "[182,   105] loss: 19027.479\n",
      "[182,   110] loss: 25789.615\n",
      "[182,   115] loss: 31240.158\n",
      "[182,   120] loss: 24307.733\n",
      "[182,   125] loss: 20197.311\n",
      "[182,   130] loss: 23561.941\n",
      "[182,   135] loss: 19535.886\n",
      "[182,   140] loss: 34860.931\n",
      "[182,   145] loss: 19003.387\n",
      "[182,   150] loss: 26873.433\n",
      "[182,   155] loss: 25139.204\n",
      "[182,   160] loss: 22630.712\n",
      "[182,   165] loss: 24189.365\n",
      "[182,   170] loss: 30622.169\n",
      "[182,   175] loss: 27960.230\n",
      "[182,   180] loss: 25392.718\n",
      "[182,   185] loss: 25003.753\n",
      "[182,   190] loss: 44833.706\n",
      "[182,   195] loss: 16648.507\n",
      "[182,   200] loss: 26960.555\n",
      "[182,   205] loss: 38620.359\n",
      "[182,   210] loss: 35215.595\n",
      "[182,   215] loss: 47299.188\n",
      "[182,   220] loss: 15303.491\n",
      "[182,   225] loss: 28362.964\n",
      "[182,   230] loss: 19787.875\n",
      "[183,     5] loss: 15963.481\n",
      "[183,    10] loss: 20220.777\n",
      "[183,    15] loss: 24100.299\n",
      "[183,    20] loss: 19469.044\n",
      "[183,    25] loss: 18777.611\n",
      "[183,    30] loss: 32061.619\n",
      "[183,    35] loss: 33303.315\n",
      "[183,    40] loss: 25976.608\n",
      "[183,    45] loss: 36734.227\n",
      "[183,    50] loss: 28613.477\n",
      "[183,    55] loss: 21172.086\n",
      "[183,    60] loss: 28401.632\n",
      "[183,    65] loss: 31156.634\n",
      "[183,    70] loss: 17032.954\n",
      "[183,    75] loss: 34193.789\n",
      "[183,    80] loss: 22443.219\n",
      "[183,    85] loss: 18312.341\n",
      "[183,    90] loss: 19005.216\n",
      "[183,    95] loss: 22356.729\n",
      "[183,   100] loss: 17415.744\n",
      "[183,   105] loss: 18073.967\n",
      "[183,   110] loss: 25967.391\n",
      "[183,   115] loss: 46660.426\n",
      "[183,   120] loss: 26476.385\n",
      "[183,   125] loss: 36266.362\n",
      "[183,   130] loss: 25923.580\n",
      "[183,   135] loss: 35446.290\n",
      "[183,   140] loss: 28768.342\n",
      "[183,   145] loss: 23424.004\n",
      "[183,   150] loss: 22291.713\n",
      "[183,   155] loss: 31908.634\n",
      "[183,   160] loss: 12253.550\n",
      "[183,   165] loss: 32183.425\n",
      "[183,   170] loss: 37385.139\n",
      "[183,   175] loss: 20920.765\n",
      "[183,   180] loss: 36864.018\n",
      "[183,   185] loss: 22535.623\n",
      "[183,   190] loss: 33464.484\n",
      "[183,   195] loss: 22301.429\n",
      "[183,   200] loss: 34153.827\n",
      "[183,   205] loss: 26846.182\n",
      "[183,   210] loss: 28572.164\n",
      "[183,   215] loss: 26271.665\n",
      "[183,   220] loss: 23790.459\n",
      "[183,   225] loss: 26260.759\n",
      "[183,   230] loss: 29055.940\n",
      "[184,     5] loss: 28389.424\n",
      "[184,    10] loss: 26430.567\n",
      "[184,    15] loss: 20766.620\n",
      "[184,    20] loss: 28244.570\n",
      "[184,    25] loss: 20820.279\n",
      "[184,    30] loss: 19563.057\n",
      "[184,    35] loss: 20529.026\n",
      "[184,    40] loss: 41699.978\n",
      "[184,    45] loss: 18705.254\n",
      "[184,    50] loss: 31434.093\n",
      "[184,    55] loss: 33670.944\n",
      "[184,    60] loss: 26695.145\n",
      "[184,    65] loss: 26355.645\n",
      "[184,    70] loss: 21934.096\n",
      "[184,    75] loss: 30686.999\n",
      "[184,    80] loss: 22237.331\n",
      "[184,    85] loss: 28579.393\n",
      "[184,    90] loss: 25409.275\n",
      "[184,    95] loss: 26169.664\n",
      "[184,   100] loss: 25349.093\n",
      "[184,   105] loss: 41371.264\n",
      "[184,   110] loss: 21221.859\n",
      "[184,   115] loss: 24980.422\n",
      "[184,   120] loss: 26247.152\n",
      "[184,   125] loss: 24340.950\n",
      "[184,   130] loss: 21140.262\n",
      "[184,   135] loss: 41469.244\n",
      "[184,   140] loss: 23766.077\n",
      "[184,   145] loss: 17850.990\n",
      "[184,   150] loss: 19396.734\n",
      "[184,   155] loss: 27427.761\n",
      "[184,   160] loss: 44567.649\n",
      "[184,   165] loss: 29223.755\n",
      "[184,   170] loss: 19654.881\n",
      "[184,   175] loss: 55404.570\n",
      "[184,   180] loss: 13152.643\n",
      "[184,   185] loss: 26916.322\n",
      "[184,   190] loss: 29983.609\n",
      "[184,   195] loss: 22019.027\n",
      "[184,   200] loss: 20207.204\n",
      "[184,   205] loss: 24932.790\n",
      "[184,   210] loss: 21502.883\n",
      "[184,   215] loss: 25986.799\n",
      "[184,   220] loss: 25065.358\n",
      "[184,   225] loss: 22163.705\n",
      "[184,   230] loss: 22793.892\n",
      "[185,     5] loss: 21052.930\n",
      "[185,    10] loss: 37169.836\n",
      "[185,    15] loss: 30627.192\n",
      "[185,    20] loss: 22554.932\n",
      "[185,    25] loss: 17795.604\n",
      "[185,    30] loss: 17708.696\n",
      "[185,    35] loss: 23053.779\n",
      "[185,    40] loss: 29078.431\n",
      "[185,    45] loss: 29276.393\n",
      "[185,    50] loss: 17485.595\n",
      "[185,    55] loss: 28519.704\n",
      "[185,    60] loss: 29106.257\n",
      "[185,    65] loss: 29637.264\n",
      "[185,    70] loss: 27682.560\n",
      "[185,    75] loss: 28399.259\n",
      "[185,    80] loss: 33717.723\n",
      "[185,    85] loss: 62152.987\n",
      "[185,    90] loss: 29321.343\n",
      "[185,    95] loss: 37342.540\n",
      "[185,   100] loss: 29941.723\n",
      "[185,   105] loss: 21639.278\n",
      "[185,   110] loss: 32962.021\n",
      "[185,   115] loss: 24168.371\n",
      "[185,   120] loss: 29838.215\n",
      "[185,   125] loss: 15601.538\n",
      "[185,   130] loss: 20897.413\n",
      "[185,   135] loss: 21831.305\n",
      "[185,   140] loss: 18477.380\n",
      "[185,   145] loss: 21451.903\n",
      "[185,   150] loss: 22284.305\n",
      "[185,   155] loss: 22796.451\n",
      "[185,   160] loss: 23714.125\n",
      "[185,   165] loss: 48751.113\n",
      "[185,   170] loss: 18640.987\n",
      "[185,   175] loss: 19550.894\n",
      "[185,   180] loss: 22287.418\n",
      "[185,   185] loss: 22139.564\n",
      "[185,   190] loss: 16775.476\n",
      "[185,   195] loss: 24166.916\n",
      "[185,   200] loss: 28522.604\n",
      "[185,   205] loss: 21124.539\n",
      "[185,   210] loss: 26028.627\n",
      "[185,   215] loss: 29745.942\n",
      "[185,   220] loss: 27073.453\n",
      "[185,   225] loss: 27577.510\n",
      "[185,   230] loss: 25648.962\n",
      "[186,     5] loss: 32348.315\n",
      "[186,    10] loss: 18493.553\n",
      "[186,    15] loss: 22446.989\n",
      "[186,    20] loss: 20364.567\n",
      "[186,    25] loss: 32974.812\n",
      "[186,    30] loss: 22332.136\n",
      "[186,    35] loss: 32079.224\n",
      "[186,    40] loss: 26276.151\n",
      "[186,    45] loss: 23796.557\n",
      "[186,    50] loss: 21313.849\n",
      "[186,    55] loss: 38035.377\n",
      "[186,    60] loss: 27522.504\n",
      "[186,    65] loss: 21783.610\n",
      "[186,    70] loss: 32031.989\n",
      "[186,    75] loss: 22948.832\n",
      "[186,    80] loss: 23981.101\n",
      "[186,    85] loss: 28835.203\n",
      "[186,    90] loss: 27691.043\n",
      "[186,    95] loss: 24670.663\n",
      "[186,   100] loss: 24326.588\n",
      "[186,   105] loss: 30151.572\n",
      "[186,   110] loss: 24343.988\n",
      "[186,   115] loss: 36193.458\n",
      "[186,   120] loss: 26690.393\n",
      "[186,   125] loss: 17403.442\n",
      "[186,   130] loss: 20397.248\n",
      "[186,   135] loss: 24733.322\n",
      "[186,   140] loss: 26779.066\n",
      "[186,   145] loss: 25102.763\n",
      "[186,   150] loss: 34696.062\n",
      "[186,   155] loss: 38173.149\n",
      "[186,   160] loss: 22841.816\n",
      "[186,   165] loss: 21961.482\n",
      "[186,   170] loss: 12543.754\n",
      "[186,   175] loss: 17451.123\n",
      "[186,   180] loss: 26511.858\n",
      "[186,   185] loss: 20252.086\n",
      "[186,   190] loss: 47815.778\n",
      "[186,   195] loss: 29383.466\n",
      "[186,   200] loss: 24868.814\n",
      "[186,   205] loss: 23611.748\n",
      "[186,   210] loss: 29165.555\n",
      "[186,   215] loss: 25639.362\n",
      "[186,   220] loss: 26103.580\n",
      "[186,   225] loss: 39502.587\n",
      "[186,   230] loss: 20694.270\n",
      "[187,     5] loss: 51956.375\n",
      "[187,    10] loss: 26196.448\n",
      "[187,    15] loss: 31770.525\n",
      "[187,    20] loss: 33751.260\n",
      "[187,    25] loss: 28426.960\n",
      "[187,    30] loss: 19102.849\n",
      "[187,    35] loss: 18835.168\n",
      "[187,    40] loss: 22356.749\n",
      "[187,    45] loss: 22518.091\n",
      "[187,    50] loss: 26838.734\n",
      "[187,    55] loss: 22638.395\n",
      "[187,    60] loss: 24667.784\n",
      "[187,    65] loss: 17034.810\n",
      "[187,    70] loss: 28524.310\n",
      "[187,    75] loss: 24301.310\n",
      "[187,    80] loss: 27153.428\n",
      "[187,    85] loss: 15274.559\n",
      "[187,    90] loss: 21381.428\n",
      "[187,    95] loss: 32898.263\n",
      "[187,   100] loss: 30075.079\n",
      "[187,   105] loss: 23731.148\n",
      "[187,   110] loss: 20427.302\n",
      "[187,   115] loss: 38024.643\n",
      "[187,   120] loss: 14995.408\n",
      "[187,   125] loss: 22984.813\n",
      "[187,   130] loss: 19992.130\n",
      "[187,   135] loss: 20246.047\n",
      "[187,   140] loss: 25564.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187,   145] loss: 23148.816\n",
      "[187,   150] loss: 23430.863\n",
      "[187,   155] loss: 37807.143\n",
      "[187,   160] loss: 38684.363\n",
      "[187,   165] loss: 29192.638\n",
      "[187,   170] loss: 25324.071\n",
      "[187,   175] loss: 13265.530\n",
      "[187,   180] loss: 31529.893\n",
      "[187,   185] loss: 28701.091\n",
      "[187,   190] loss: 30357.729\n",
      "[187,   195] loss: 23981.008\n",
      "[187,   200] loss: 20372.189\n",
      "[187,   205] loss: 26843.893\n",
      "[187,   210] loss: 24770.205\n",
      "[187,   215] loss: 38491.494\n",
      "[187,   220] loss: 25331.358\n",
      "[187,   225] loss: 33737.327\n",
      "[187,   230] loss: 22671.914\n",
      "[188,     5] loss: 33793.954\n",
      "[188,    10] loss: 29734.245\n",
      "[188,    15] loss: 20564.265\n",
      "[188,    20] loss: 26736.943\n",
      "[188,    25] loss: 22891.801\n",
      "[188,    30] loss: 31559.289\n",
      "[188,    35] loss: 24634.520\n",
      "[188,    40] loss: 29601.361\n",
      "[188,    45] loss: 22913.466\n",
      "[188,    50] loss: 29409.316\n",
      "[188,    55] loss: 23783.520\n",
      "[188,    60] loss: 33523.150\n",
      "[188,    65] loss: 23859.810\n",
      "[188,    70] loss: 28564.234\n",
      "[188,    75] loss: 26558.352\n",
      "[188,    80] loss: 23833.465\n",
      "[188,    85] loss: 30298.632\n",
      "[188,    90] loss: 28779.274\n",
      "[188,    95] loss: 40023.040\n",
      "[188,   100] loss: 32419.917\n",
      "[188,   105] loss: 21296.038\n",
      "[188,   110] loss: 20910.219\n",
      "[188,   115] loss: 31378.822\n",
      "[188,   120] loss: 21505.779\n",
      "[188,   125] loss: 15950.944\n",
      "[188,   130] loss: 25230.661\n",
      "[188,   135] loss: 25218.887\n",
      "[188,   140] loss: 23631.646\n",
      "[188,   145] loss: 42257.371\n",
      "[188,   150] loss: 17842.042\n",
      "[188,   155] loss: 23490.714\n",
      "[188,   160] loss: 34844.706\n",
      "[188,   165] loss: 19620.123\n",
      "[188,   170] loss: 37261.862\n",
      "[188,   175] loss: 19734.205\n",
      "[188,   180] loss: 18400.175\n",
      "[188,   185] loss: 43373.004\n",
      "[188,   190] loss: 29387.745\n",
      "[188,   195] loss: 17205.458\n",
      "[188,   200] loss: 27825.700\n",
      "[188,   205] loss: 26126.017\n",
      "[188,   210] loss: 22382.326\n",
      "[188,   215] loss: 26211.518\n",
      "[188,   220] loss: 21856.232\n",
      "[188,   225] loss: 22801.957\n",
      "[188,   230] loss: 13987.552\n",
      "[189,     5] loss: 23784.674\n",
      "[189,    10] loss: 33477.089\n",
      "[189,    15] loss: 21235.491\n",
      "[189,    20] loss: 28411.004\n",
      "[189,    25] loss: 22772.563\n",
      "[189,    30] loss: 20473.096\n",
      "[189,    35] loss: 27156.462\n",
      "[189,    40] loss: 14826.491\n",
      "[189,    45] loss: 32343.254\n",
      "[189,    50] loss: 20294.742\n",
      "[189,    55] loss: 33090.180\n",
      "[189,    60] loss: 18360.644\n",
      "[189,    65] loss: 21585.852\n",
      "[189,    70] loss: 54899.468\n",
      "[189,    75] loss: 35676.105\n",
      "[189,    80] loss: 30278.730\n",
      "[189,    85] loss: 30507.481\n",
      "[189,    90] loss: 23093.618\n",
      "[189,    95] loss: 20253.148\n",
      "[189,   100] loss: 19632.613\n",
      "[189,   105] loss: 24586.140\n",
      "[189,   110] loss: 25048.748\n",
      "[189,   115] loss: 40245.803\n",
      "[189,   120] loss: 36720.148\n",
      "[189,   125] loss: 24581.612\n",
      "[189,   130] loss: 22777.582\n",
      "[189,   135] loss: 32335.541\n",
      "[189,   140] loss: 20435.116\n",
      "[189,   145] loss: 23778.670\n",
      "[189,   150] loss: 22221.486\n",
      "[189,   155] loss: 28918.601\n",
      "[189,   160] loss: 30899.237\n",
      "[189,   165] loss: 24356.568\n",
      "[189,   170] loss: 21485.821\n",
      "[189,   175] loss: 27532.619\n",
      "[189,   180] loss: 22658.555\n",
      "[189,   185] loss: 29093.000\n",
      "[189,   190] loss: 25937.325\n",
      "[189,   195] loss: 19813.771\n",
      "[189,   200] loss: 24933.843\n",
      "[189,   205] loss: 35479.540\n",
      "[189,   210] loss: 25923.170\n",
      "[189,   215] loss: 18501.541\n",
      "[189,   220] loss: 27212.986\n",
      "[189,   225] loss: 21662.671\n",
      "[189,   230] loss: 24759.302\n",
      "[190,     5] loss: 26814.001\n",
      "[190,    10] loss: 16203.298\n",
      "[190,    15] loss: 24502.322\n",
      "[190,    20] loss: 30510.430\n",
      "[190,    25] loss: 21622.987\n",
      "[190,    30] loss: 43937.884\n",
      "[190,    35] loss: 38854.412\n",
      "[190,    40] loss: 22773.175\n",
      "[190,    45] loss: 26659.886\n",
      "[190,    50] loss: 21897.013\n",
      "[190,    55] loss: 20085.786\n",
      "[190,    60] loss: 23531.225\n",
      "[190,    65] loss: 28376.293\n",
      "[190,    70] loss: 22442.667\n",
      "[190,    75] loss: 21444.666\n",
      "[190,    80] loss: 48219.099\n",
      "[190,    85] loss: 20889.823\n",
      "[190,    90] loss: 21644.882\n",
      "[190,    95] loss: 21703.714\n",
      "[190,   100] loss: 24173.298\n",
      "[190,   105] loss: 24757.069\n",
      "[190,   110] loss: 23601.768\n",
      "[190,   115] loss: 28957.381\n",
      "[190,   120] loss: 25906.925\n",
      "[190,   125] loss: 19196.887\n",
      "[190,   130] loss: 20259.112\n",
      "[190,   135] loss: 29669.955\n",
      "[190,   140] loss: 18298.483\n",
      "[190,   145] loss: 31399.176\n",
      "[190,   150] loss: 18576.725\n",
      "[190,   155] loss: 26393.311\n",
      "[190,   160] loss: 19616.132\n",
      "[190,   165] loss: 25861.441\n",
      "[190,   170] loss: 25479.420\n",
      "[190,   175] loss: 38041.674\n",
      "[190,   180] loss: 34924.743\n",
      "[190,   185] loss: 36088.486\n",
      "[190,   190] loss: 25614.444\n",
      "[190,   195] loss: 37389.141\n",
      "[190,   200] loss: 26280.195\n",
      "[190,   205] loss: 16956.040\n",
      "[190,   210] loss: 35578.807\n",
      "[190,   215] loss: 30686.722\n",
      "[190,   220] loss: 17411.403\n",
      "[190,   225] loss: 22479.521\n",
      "[190,   230] loss: 27049.126\n",
      "[191,     5] loss: 28892.186\n",
      "[191,    10] loss: 28095.304\n",
      "[191,    15] loss: 23350.076\n",
      "[191,    20] loss: 25309.786\n",
      "[191,    25] loss: 23716.210\n",
      "[191,    30] loss: 31678.300\n",
      "[191,    35] loss: 33142.192\n",
      "[191,    40] loss: 28575.007\n",
      "[191,    45] loss: 29435.356\n",
      "[191,    50] loss: 20920.192\n",
      "[191,    55] loss: 19086.883\n",
      "[191,    60] loss: 27541.580\n",
      "[191,    65] loss: 28896.629\n",
      "[191,    70] loss: 22763.762\n",
      "[191,    75] loss: 23363.378\n",
      "[191,    80] loss: 31752.113\n",
      "[191,    85] loss: 27800.557\n",
      "[191,    90] loss: 21749.875\n",
      "[191,    95] loss: 42435.651\n",
      "[191,   100] loss: 31568.934\n",
      "[191,   105] loss: 26752.759\n",
      "[191,   110] loss: 29682.791\n",
      "[191,   115] loss: 16648.634\n",
      "[191,   120] loss: 18649.966\n",
      "[191,   125] loss: 23294.235\n",
      "[191,   130] loss: 19951.096\n",
      "[191,   135] loss: 27593.264\n",
      "[191,   140] loss: 24477.867\n",
      "[191,   145] loss: 27017.317\n",
      "[191,   150] loss: 22714.737\n",
      "[191,   155] loss: 27809.173\n",
      "[191,   160] loss: 17545.487\n",
      "[191,   165] loss: 18507.215\n",
      "[191,   170] loss: 23945.943\n",
      "[191,   175] loss: 27292.555\n",
      "[191,   180] loss: 44493.371\n",
      "[191,   185] loss: 24661.891\n",
      "[191,   190] loss: 21708.880\n",
      "[191,   195] loss: 23004.954\n",
      "[191,   200] loss: 32340.647\n",
      "[191,   205] loss: 27140.020\n",
      "[191,   210] loss: 23856.484\n",
      "[191,   215] loss: 41118.979\n",
      "[191,   220] loss: 22653.464\n",
      "[191,   225] loss: 29728.805\n",
      "[191,   230] loss: 19093.003\n",
      "[192,     5] loss: 21440.618\n",
      "[192,    10] loss: 20595.791\n",
      "[192,    15] loss: 27135.421\n",
      "[192,    20] loss: 26723.529\n",
      "[192,    25] loss: 26855.433\n",
      "[192,    30] loss: 24074.906\n",
      "[192,    35] loss: 30879.845\n",
      "[192,    40] loss: 27826.318\n",
      "[192,    45] loss: 34222.962\n",
      "[192,    50] loss: 29944.936\n",
      "[192,    55] loss: 18868.351\n",
      "[192,    60] loss: 23930.928\n",
      "[192,    65] loss: 24337.631\n",
      "[192,    70] loss: 18585.365\n",
      "[192,    75] loss: 16864.483\n",
      "[192,    80] loss: 24493.834\n",
      "[192,    85] loss: 22445.615\n",
      "[192,    90] loss: 34227.946\n",
      "[192,    95] loss: 26486.714\n",
      "[192,   100] loss: 24216.420\n",
      "[192,   105] loss: 29147.454\n",
      "[192,   110] loss: 27280.171\n",
      "[192,   115] loss: 30141.374\n",
      "[192,   120] loss: 22550.457\n",
      "[192,   125] loss: 19601.116\n",
      "[192,   130] loss: 24458.944\n",
      "[192,   135] loss: 25644.008\n",
      "[192,   140] loss: 34625.550\n",
      "[192,   145] loss: 27319.303\n",
      "[192,   150] loss: 26264.895\n",
      "[192,   155] loss: 27567.858\n",
      "[192,   160] loss: 23083.703\n",
      "[192,   165] loss: 19805.336\n",
      "[192,   170] loss: 19518.586\n",
      "[192,   175] loss: 25047.637\n",
      "[192,   180] loss: 60257.879\n",
      "[192,   185] loss: 30413.444\n",
      "[192,   190] loss: 37914.402\n",
      "[192,   195] loss: 38590.057\n",
      "[192,   200] loss: 17457.383\n",
      "[192,   205] loss: 16899.129\n",
      "[192,   210] loss: 22823.389\n",
      "[192,   215] loss: 16681.025\n",
      "[192,   220] loss: 25010.126\n",
      "[192,   225] loss: 26151.939\n",
      "[192,   230] loss: 25893.516\n",
      "[193,     5] loss: 29685.588\n",
      "[193,    10] loss: 27699.415\n",
      "[193,    15] loss: 40912.059\n",
      "[193,    20] loss: 19120.873\n",
      "[193,    25] loss: 32222.505\n",
      "[193,    30] loss: 25871.881\n",
      "[193,    35] loss: 26282.096\n",
      "[193,    40] loss: 25855.338\n",
      "[193,    45] loss: 30120.423\n",
      "[193,    50] loss: 24254.339\n",
      "[193,    55] loss: 21053.853\n",
      "[193,    60] loss: 29852.321\n",
      "[193,    65] loss: 32246.592\n",
      "[193,    70] loss: 46187.812\n",
      "[193,    75] loss: 25868.339\n",
      "[193,    80] loss: 26709.202\n",
      "[193,    85] loss: 16518.576\n",
      "[193,    90] loss: 14507.085\n",
      "[193,    95] loss: 21103.827\n",
      "[193,   100] loss: 27630.397\n",
      "[193,   105] loss: 19131.859\n",
      "[193,   110] loss: 23294.921\n",
      "[193,   115] loss: 34720.368\n",
      "[193,   120] loss: 28182.318\n",
      "[193,   125] loss: 21861.643\n",
      "[193,   130] loss: 23236.120\n",
      "[193,   135] loss: 28926.042\n",
      "[193,   140] loss: 47556.534\n",
      "[193,   145] loss: 20960.126\n",
      "[193,   150] loss: 32356.280\n",
      "[193,   155] loss: 25342.951\n",
      "[193,   160] loss: 18985.746\n",
      "[193,   165] loss: 31499.891\n",
      "[193,   170] loss: 22961.071\n",
      "[193,   175] loss: 30928.223\n",
      "[193,   180] loss: 17686.737\n",
      "[193,   185] loss: 28098.931\n",
      "[193,   190] loss: 17422.475\n",
      "[193,   195] loss: 23529.055\n",
      "[193,   200] loss: 34984.975\n",
      "[193,   205] loss: 20840.001\n",
      "[193,   210] loss: 24680.396\n",
      "[193,   215] loss: 24827.256\n",
      "[193,   220] loss: 28441.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193,   225] loss: 17543.586\n",
      "[193,   230] loss: 24512.410\n",
      "[194,     5] loss: 24049.796\n",
      "[194,    10] loss: 21138.392\n",
      "[194,    15] loss: 22147.573\n",
      "[194,    20] loss: 30265.379\n",
      "[194,    25] loss: 30214.008\n",
      "[194,    30] loss: 19269.329\n",
      "[194,    35] loss: 35250.862\n",
      "[194,    40] loss: 21653.616\n",
      "[194,    45] loss: 24208.898\n",
      "[194,    50] loss: 20441.419\n",
      "[194,    55] loss: 30356.812\n",
      "[194,    60] loss: 30375.574\n",
      "[194,    65] loss: 25926.234\n",
      "[194,    70] loss: 27155.545\n",
      "[194,    75] loss: 28495.791\n",
      "[194,    80] loss: 20020.686\n",
      "[194,    85] loss: 28808.837\n",
      "[194,    90] loss: 16050.121\n",
      "[194,    95] loss: 22148.255\n",
      "[194,   100] loss: 49312.491\n",
      "[194,   105] loss: 32280.382\n",
      "[194,   110] loss: 33052.824\n",
      "[194,   115] loss: 21521.937\n",
      "[194,   120] loss: 20466.391\n",
      "[194,   125] loss: 22387.522\n",
      "[194,   130] loss: 24572.168\n",
      "[194,   135] loss: 26669.078\n",
      "[194,   140] loss: 22645.491\n",
      "[194,   145] loss: 23579.311\n",
      "[194,   150] loss: 34388.544\n",
      "[194,   155] loss: 24571.022\n",
      "[194,   160] loss: 27487.819\n",
      "[194,   165] loss: 29524.346\n",
      "[194,   170] loss: 25367.564\n",
      "[194,   175] loss: 24754.588\n",
      "[194,   180] loss: 29735.562\n",
      "[194,   185] loss: 34382.259\n",
      "[194,   190] loss: 21635.894\n",
      "[194,   195] loss: 35591.854\n",
      "[194,   200] loss: 24426.192\n",
      "[194,   205] loss: 24756.965\n",
      "[194,   210] loss: 23679.221\n",
      "[194,   215] loss: 23027.033\n",
      "[194,   220] loss: 19333.506\n",
      "[194,   225] loss: 32960.495\n",
      "[194,   230] loss: 24013.463\n",
      "[195,     5] loss: 35818.174\n",
      "[195,    10] loss: 24333.970\n",
      "[195,    15] loss: 20235.750\n",
      "[195,    20] loss: 31851.861\n",
      "[195,    25] loss: 22455.986\n",
      "[195,    30] loss: 46282.562\n",
      "[195,    35] loss: 37300.487\n",
      "[195,    40] loss: 25511.789\n",
      "[195,    45] loss: 17854.309\n",
      "[195,    50] loss: 30277.083\n",
      "[195,    55] loss: 38415.115\n",
      "[195,    60] loss: 29948.042\n",
      "[195,    65] loss: 28662.670\n",
      "[195,    70] loss: 25947.584\n",
      "[195,    75] loss: 21968.929\n",
      "[195,    80] loss: 23759.278\n",
      "[195,    85] loss: 22908.611\n",
      "[195,    90] loss: 19360.649\n",
      "[195,    95] loss: 31208.484\n",
      "[195,   100] loss: 19208.549\n",
      "[195,   105] loss: 24049.758\n",
      "[195,   110] loss: 26149.300\n",
      "[195,   115] loss: 19225.514\n",
      "[195,   120] loss: 18916.895\n",
      "[195,   125] loss: 35064.989\n",
      "[195,   130] loss: 31023.918\n",
      "[195,   135] loss: 23407.561\n",
      "[195,   140] loss: 18784.554\n",
      "[195,   145] loss: 26199.703\n",
      "[195,   150] loss: 21771.124\n",
      "[195,   155] loss: 29100.863\n",
      "[195,   160] loss: 25994.929\n",
      "[195,   165] loss: 25619.722\n",
      "[195,   170] loss: 28399.025\n",
      "[195,   175] loss: 15132.171\n",
      "[195,   180] loss: 24827.660\n",
      "[195,   185] loss: 22187.720\n",
      "[195,   190] loss: 38786.658\n",
      "[195,   195] loss: 27604.799\n",
      "[195,   200] loss: 24528.587\n",
      "[195,   205] loss: 19392.137\n",
      "[195,   210] loss: 15796.419\n",
      "[195,   215] loss: 31320.083\n",
      "[195,   220] loss: 22613.030\n",
      "[195,   225] loss: 19973.732\n",
      "[195,   230] loss: 34616.360\n",
      "[196,     5] loss: 32695.639\n",
      "[196,    10] loss: 18377.240\n",
      "[196,    15] loss: 26569.164\n",
      "[196,    20] loss: 31650.400\n",
      "[196,    25] loss: 25531.544\n",
      "[196,    30] loss: 23400.232\n",
      "[196,    35] loss: 28204.526\n",
      "[196,    40] loss: 31506.911\n",
      "[196,    45] loss: 25708.365\n",
      "[196,    50] loss: 31029.055\n",
      "[196,    55] loss: 16780.226\n",
      "[196,    60] loss: 28604.115\n",
      "[196,    65] loss: 20738.432\n",
      "[196,    70] loss: 19276.111\n",
      "[196,    75] loss: 23707.740\n",
      "[196,    80] loss: 25346.376\n",
      "[196,    85] loss: 32002.162\n",
      "[196,    90] loss: 26003.931\n",
      "[196,    95] loss: 39013.762\n",
      "[196,   100] loss: 24186.702\n",
      "[196,   105] loss: 21204.634\n",
      "[196,   110] loss: 26299.652\n",
      "[196,   115] loss: 31283.284\n",
      "[196,   120] loss: 30236.970\n",
      "[196,   125] loss: 31649.853\n",
      "[196,   130] loss: 25424.257\n",
      "[196,   135] loss: 22287.923\n",
      "[196,   140] loss: 22211.617\n",
      "[196,   145] loss: 15259.802\n",
      "[196,   150] loss: 44512.016\n",
      "[196,   155] loss: 25413.372\n",
      "[196,   160] loss: 29985.834\n",
      "[196,   165] loss: 30258.584\n",
      "[196,   170] loss: 35553.369\n",
      "[196,   175] loss: 30655.842\n",
      "[196,   180] loss: 32017.416\n",
      "[196,   185] loss: 19344.172\n",
      "[196,   190] loss: 24803.871\n",
      "[196,   195] loss: 25024.886\n",
      "[196,   200] loss: 15565.745\n",
      "[196,   205] loss: 22367.517\n",
      "[196,   210] loss: 24126.999\n",
      "[196,   215] loss: 20705.505\n",
      "[196,   220] loss: 21062.978\n",
      "[196,   225] loss: 23346.330\n",
      "[196,   230] loss: 31773.485\n",
      "[197,     5] loss: 27748.513\n",
      "[197,    10] loss: 20266.864\n",
      "[197,    15] loss: 24602.834\n",
      "[197,    20] loss: 19137.229\n",
      "[197,    25] loss: 28432.353\n",
      "[197,    30] loss: 24893.286\n",
      "[197,    35] loss: 25941.917\n",
      "[197,    40] loss: 19976.769\n",
      "[197,    45] loss: 32482.728\n",
      "[197,    50] loss: 27489.923\n",
      "[197,    55] loss: 16163.153\n",
      "[197,    60] loss: 27125.165\n",
      "[197,    65] loss: 34973.819\n",
      "[197,    70] loss: 27006.517\n",
      "[197,    75] loss: 23002.943\n",
      "[197,    80] loss: 28641.709\n",
      "[197,    85] loss: 32484.304\n",
      "[197,    90] loss: 52445.424\n",
      "[197,    95] loss: 25030.080\n",
      "[197,   100] loss: 27714.940\n",
      "[197,   105] loss: 25791.275\n",
      "[197,   110] loss: 28573.737\n",
      "[197,   115] loss: 21398.323\n",
      "[197,   120] loss: 17699.775\n",
      "[197,   125] loss: 19573.301\n",
      "[197,   130] loss: 30710.031\n",
      "[197,   135] loss: 24358.691\n",
      "[197,   140] loss: 34511.226\n",
      "[197,   145] loss: 27113.658\n",
      "[197,   150] loss: 24735.170\n",
      "[197,   155] loss: 21219.035\n",
      "[197,   160] loss: 19122.590\n",
      "[197,   165] loss: 23559.465\n",
      "[197,   170] loss: 30857.166\n",
      "[197,   175] loss: 22458.949\n",
      "[197,   180] loss: 25547.693\n",
      "[197,   185] loss: 32404.257\n",
      "[197,   190] loss: 30621.403\n",
      "[197,   195] loss: 19505.961\n",
      "[197,   200] loss: 28589.748\n",
      "[197,   205] loss: 29384.518\n",
      "[197,   210] loss: 33125.696\n",
      "[197,   215] loss: 25795.001\n",
      "[197,   220] loss: 20556.357\n",
      "[197,   225] loss: 22481.987\n",
      "[197,   230] loss: 18312.429\n",
      "[198,     5] loss: 22061.695\n",
      "[198,    10] loss: 27682.240\n",
      "[198,    15] loss: 26503.990\n",
      "[198,    20] loss: 23601.090\n",
      "[198,    25] loss: 31515.645\n",
      "[198,    30] loss: 32476.936\n",
      "[198,    35] loss: 22103.955\n",
      "[198,    40] loss: 25185.273\n",
      "[198,    45] loss: 22232.156\n",
      "[198,    50] loss: 26062.735\n",
      "[198,    55] loss: 33600.412\n",
      "[198,    60] loss: 32238.618\n",
      "[198,    65] loss: 20951.103\n",
      "[198,    70] loss: 20438.366\n",
      "[198,    75] loss: 24060.133\n",
      "[198,    80] loss: 20876.333\n",
      "[198,    85] loss: 23945.195\n",
      "[198,    90] loss: 26928.052\n",
      "[198,    95] loss: 23228.754\n",
      "[198,   100] loss: 31821.462\n",
      "[198,   105] loss: 30438.852\n",
      "[198,   110] loss: 25991.676\n",
      "[198,   115] loss: 31557.105\n",
      "[198,   120] loss: 33048.954\n",
      "[198,   125] loss: 19476.755\n",
      "[198,   130] loss: 27044.704\n",
      "[198,   135] loss: 43041.764\n",
      "[198,   140] loss: 22906.193\n",
      "[198,   145] loss: 30981.334\n",
      "[198,   150] loss: 17591.685\n",
      "[198,   155] loss: 28292.727\n",
      "[198,   160] loss: 19428.544\n",
      "[198,   165] loss: 16216.171\n",
      "[198,   170] loss: 20964.406\n",
      "[198,   175] loss: 17962.801\n",
      "[198,   180] loss: 25853.479\n",
      "[198,   185] loss: 24918.169\n",
      "[198,   190] loss: 18540.648\n",
      "[198,   195] loss: 36339.974\n",
      "[198,   200] loss: 23007.678\n",
      "[198,   205] loss: 21969.686\n",
      "[198,   210] loss: 25151.905\n",
      "[198,   215] loss: 17717.291\n",
      "[198,   220] loss: 36428.463\n",
      "[198,   225] loss: 48529.692\n",
      "[198,   230] loss: 32182.984\n",
      "[199,     5] loss: 27704.525\n",
      "[199,    10] loss: 53229.223\n",
      "[199,    15] loss: 40012.718\n",
      "[199,    20] loss: 27856.867\n",
      "[199,    25] loss: 19845.291\n",
      "[199,    30] loss: 17536.477\n",
      "[199,    35] loss: 34947.766\n",
      "[199,    40] loss: 25651.213\n",
      "[199,    45] loss: 38844.839\n",
      "[199,    50] loss: 22077.457\n",
      "[199,    55] loss: 23860.790\n",
      "[199,    60] loss: 18462.179\n",
      "[199,    65] loss: 29114.758\n",
      "[199,    70] loss: 19476.213\n",
      "[199,    75] loss: 16682.133\n",
      "[199,    80] loss: 18342.682\n",
      "[199,    85] loss: 19488.962\n",
      "[199,    90] loss: 33155.270\n",
      "[199,    95] loss: 21048.011\n",
      "[199,   100] loss: 26209.934\n",
      "[199,   105] loss: 33190.813\n",
      "[199,   110] loss: 25287.646\n",
      "[199,   115] loss: 23045.874\n",
      "[199,   120] loss: 23693.903\n",
      "[199,   125] loss: 22026.512\n",
      "[199,   130] loss: 22723.063\n",
      "[199,   135] loss: 34515.559\n",
      "[199,   140] loss: 20312.684\n",
      "[199,   145] loss: 28296.905\n",
      "[199,   150] loss: 32544.129\n",
      "[199,   155] loss: 25239.116\n",
      "[199,   160] loss: 42795.484\n",
      "[199,   165] loss: 25576.855\n",
      "[199,   170] loss: 19300.337\n",
      "[199,   175] loss: 16306.187\n",
      "[199,   180] loss: 20877.422\n",
      "[199,   185] loss: 36881.899\n",
      "[199,   190] loss: 30534.684\n",
      "[199,   195] loss: 31244.340\n",
      "[199,   200] loss: 22326.811\n",
      "[199,   205] loss: 29353.725\n",
      "[199,   210] loss: 22002.448\n",
      "[199,   215] loss: 21014.390\n",
      "[199,   220] loss: 18997.611\n",
      "[199,   225] loss: 18497.088\n",
      "[199,   230] loss: 32921.980\n",
      "[200,     5] loss: 23916.339\n",
      "[200,    10] loss: 23276.180\n",
      "[200,    15] loss: 24358.475\n",
      "[200,    20] loss: 31324.695\n",
      "[200,    25] loss: 23834.847\n",
      "[200,    30] loss: 21133.182\n",
      "[200,    35] loss: 18344.116\n",
      "[200,    40] loss: 32252.191\n",
      "[200,    45] loss: 23732.093\n",
      "[200,    50] loss: 21486.747\n",
      "[200,    55] loss: 36005.668\n",
      "[200,    60] loss: 19189.470\n",
      "[200,    65] loss: 21860.627\n",
      "[200,    70] loss: 54987.248\n",
      "[200,    75] loss: 32259.459\n",
      "[200,    80] loss: 25463.298\n",
      "[200,    85] loss: 31828.611\n",
      "[200,    90] loss: 25281.014\n",
      "[200,    95] loss: 30590.401\n",
      "[200,   100] loss: 24160.105\n",
      "[200,   105] loss: 19162.519\n",
      "[200,   110] loss: 18903.809\n",
      "[200,   115] loss: 41411.659\n",
      "[200,   120] loss: 26194.994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200,   125] loss: 15642.059\n",
      "[200,   130] loss: 31297.252\n",
      "[200,   135] loss: 21732.088\n",
      "[200,   140] loss: 21721.584\n",
      "[200,   145] loss: 38922.198\n",
      "[200,   150] loss: 22698.644\n",
      "[200,   155] loss: 39075.729\n",
      "[200,   160] loss: 23795.349\n",
      "[200,   165] loss: 16414.064\n",
      "[200,   170] loss: 33061.248\n",
      "[200,   175] loss: 25932.742\n",
      "[200,   180] loss: 24252.758\n",
      "[200,   185] loss: 24026.198\n",
      "[200,   190] loss: 25104.433\n",
      "[200,   195] loss: 28049.154\n",
      "[200,   200] loss: 25467.676\n",
      "[200,   205] loss: 23261.351\n",
      "[200,   210] loss: 17031.163\n",
      "[200,   215] loss: 21578.494\n",
      "[200,   220] loss: 22629.953\n",
      "[200,   225] loss: 25802.031\n",
      "[200,   230] loss: 27378.032\n",
      "[201,     5] loss: 28343.380\n",
      "[201,    10] loss: 15370.697\n",
      "[201,    15] loss: 28380.198\n",
      "[201,    20] loss: 28005.823\n",
      "[201,    25] loss: 26506.083\n",
      "[201,    30] loss: 23084.636\n",
      "[201,    35] loss: 55942.512\n",
      "[201,    40] loss: 29613.750\n",
      "[201,    45] loss: 36195.368\n",
      "[201,    50] loss: 33617.655\n",
      "[201,    55] loss: 24326.968\n",
      "[201,    60] loss: 37041.558\n",
      "[201,    65] loss: 15595.341\n",
      "[201,    70] loss: 27129.182\n",
      "[201,    75] loss: 23347.234\n",
      "[201,    80] loss: 28732.509\n",
      "[201,    85] loss: 29308.418\n",
      "[201,    90] loss: 18858.958\n",
      "[201,    95] loss: 20555.360\n",
      "[201,   100] loss: 36487.119\n",
      "[201,   105] loss: 34204.380\n",
      "[201,   110] loss: 26677.141\n",
      "[201,   115] loss: 29359.146\n",
      "[201,   120] loss: 25824.147\n",
      "[201,   125] loss: 32431.880\n",
      "[201,   130] loss: 20127.726\n",
      "[201,   135] loss: 27976.590\n",
      "[201,   140] loss: 32539.131\n",
      "[201,   145] loss: 18653.440\n",
      "[201,   150] loss: 22728.533\n",
      "[201,   155] loss: 19890.764\n",
      "[201,   160] loss: 28833.225\n",
      "[201,   165] loss: 31805.839\n",
      "[201,   170] loss: 23116.180\n",
      "[201,   175] loss: 21568.739\n",
      "[201,   180] loss: 28293.350\n",
      "[201,   185] loss: 17296.958\n",
      "[201,   190] loss: 21318.261\n",
      "[201,   195] loss: 28977.240\n",
      "[201,   200] loss: 16955.013\n",
      "[201,   205] loss: 20211.711\n",
      "[201,   210] loss: 22242.169\n",
      "[201,   215] loss: 23701.535\n",
      "[201,   220] loss: 21355.444\n",
      "[201,   225] loss: 22093.432\n",
      "[201,   230] loss: 17744.532\n",
      "[202,     5] loss: 27736.775\n",
      "[202,    10] loss: 23431.931\n",
      "[202,    15] loss: 29397.485\n",
      "[202,    20] loss: 15524.313\n",
      "[202,    25] loss: 25192.127\n",
      "[202,    30] loss: 21268.751\n",
      "[202,    35] loss: 22420.128\n",
      "[202,    40] loss: 23752.432\n",
      "[202,    45] loss: 29179.812\n",
      "[202,    50] loss: 20621.434\n",
      "[202,    55] loss: 29632.558\n",
      "[202,    60] loss: 25172.168\n",
      "[202,    65] loss: 23352.027\n",
      "[202,    70] loss: 18827.670\n",
      "[202,    75] loss: 14943.858\n",
      "[202,    80] loss: 31048.502\n",
      "[202,    85] loss: 23271.610\n",
      "[202,    90] loss: 24388.040\n",
      "[202,    95] loss: 23318.923\n",
      "[202,   100] loss: 22031.962\n",
      "[202,   105] loss: 47219.627\n",
      "[202,   110] loss: 30725.105\n",
      "[202,   115] loss: 21065.306\n",
      "[202,   120] loss: 21102.060\n",
      "[202,   125] loss: 41714.768\n",
      "[202,   130] loss: 25558.441\n",
      "[202,   135] loss: 35056.387\n",
      "[202,   140] loss: 25633.737\n",
      "[202,   145] loss: 27202.422\n",
      "[202,   150] loss: 21323.191\n",
      "[202,   155] loss: 25353.074\n",
      "[202,   160] loss: 22896.742\n",
      "[202,   165] loss: 18860.066\n",
      "[202,   170] loss: 23516.385\n",
      "[202,   175] loss: 21206.549\n",
      "[202,   180] loss: 34668.415\n",
      "[202,   185] loss: 35710.899\n",
      "[202,   190] loss: 21668.432\n",
      "[202,   195] loss: 23250.925\n",
      "[202,   200] loss: 25495.865\n",
      "[202,   205] loss: 28326.759\n",
      "[202,   210] loss: 37136.989\n",
      "[202,   215] loss: 42729.369\n",
      "[202,   220] loss: 32292.112\n",
      "[202,   225] loss: 25599.806\n",
      "[202,   230] loss: 19064.459\n",
      "[203,     5] loss: 26325.942\n",
      "[203,    10] loss: 28605.799\n",
      "[203,    15] loss: 21778.642\n",
      "[203,    20] loss: 35413.438\n",
      "[203,    25] loss: 30522.536\n",
      "[203,    30] loss: 34715.641\n",
      "[203,    35] loss: 23203.902\n",
      "[203,    40] loss: 25472.623\n",
      "[203,    45] loss: 18420.116\n",
      "[203,    50] loss: 33883.833\n",
      "[203,    55] loss: 30839.718\n",
      "[203,    60] loss: 41619.805\n",
      "[203,    65] loss: 20494.189\n",
      "[203,    70] loss: 23263.962\n",
      "[203,    75] loss: 20395.423\n",
      "[203,    80] loss: 16945.811\n",
      "[203,    85] loss: 28953.301\n",
      "[203,    90] loss: 33645.121\n",
      "[203,    95] loss: 19748.957\n",
      "[203,   100] loss: 21278.547\n",
      "[203,   105] loss: 29950.171\n",
      "[203,   110] loss: 35317.629\n",
      "[203,   115] loss: 27953.242\n",
      "[203,   120] loss: 30864.261\n",
      "[203,   125] loss: 22865.872\n",
      "[203,   130] loss: 24591.705\n",
      "[203,   135] loss: 24938.607\n",
      "[203,   140] loss: 25295.718\n",
      "[203,   145] loss: 26256.077\n",
      "[203,   150] loss: 48349.654\n",
      "[203,   155] loss: 22267.445\n",
      "[203,   160] loss: 20903.362\n",
      "[203,   165] loss: 31181.908\n",
      "[203,   170] loss: 23217.008\n",
      "[203,   175] loss: 16187.772\n",
      "[203,   180] loss: 11974.382\n",
      "[203,   185] loss: 29091.514\n",
      "[203,   190] loss: 20013.319\n",
      "[203,   195] loss: 38026.515\n",
      "[203,   200] loss: 21105.882\n",
      "[203,   205] loss: 25134.290\n",
      "[203,   210] loss: 22645.548\n",
      "[203,   215] loss: 25928.489\n",
      "[203,   220] loss: 18262.526\n",
      "[203,   225] loss: 26955.785\n",
      "[203,   230] loss: 22189.114\n",
      "[204,     5] loss: 28999.281\n",
      "[204,    10] loss: 33389.014\n",
      "[204,    15] loss: 37644.923\n",
      "[204,    20] loss: 18978.185\n",
      "[204,    25] loss: 31019.166\n",
      "[204,    30] loss: 25719.210\n",
      "[204,    35] loss: 29949.301\n",
      "[204,    40] loss: 19516.880\n",
      "[204,    45] loss: 20335.438\n",
      "[204,    50] loss: 26346.654\n",
      "[204,    55] loss: 31529.277\n",
      "[204,    60] loss: 29221.575\n",
      "[204,    65] loss: 23050.298\n",
      "[204,    70] loss: 32278.913\n",
      "[204,    75] loss: 21650.222\n",
      "[204,    80] loss: 25324.922\n",
      "[204,    85] loss: 26408.253\n",
      "[204,    90] loss: 16555.490\n",
      "[204,    95] loss: 33984.398\n",
      "[204,   100] loss: 24316.479\n",
      "[204,   105] loss: 24592.673\n",
      "[204,   110] loss: 18594.180\n",
      "[204,   115] loss: 19974.573\n",
      "[204,   120] loss: 19320.035\n",
      "[204,   125] loss: 24440.855\n",
      "[204,   130] loss: 24359.357\n",
      "[204,   135] loss: 38196.178\n",
      "[204,   140] loss: 23861.679\n",
      "[204,   145] loss: 22156.335\n",
      "[204,   150] loss: 32759.020\n",
      "[204,   155] loss: 27222.169\n",
      "[204,   160] loss: 20883.409\n",
      "[204,   165] loss: 18112.841\n",
      "[204,   170] loss: 42170.757\n",
      "[204,   175] loss: 39825.546\n",
      "[204,   180] loss: 30762.133\n",
      "[204,   185] loss: 19558.610\n",
      "[204,   190] loss: 24672.501\n",
      "[204,   195] loss: 22737.459\n",
      "[204,   200] loss: 24723.812\n",
      "[204,   205] loss: 25975.677\n",
      "[204,   210] loss: 24497.984\n",
      "[204,   215] loss: 19683.878\n",
      "[204,   220] loss: 23922.629\n",
      "[204,   225] loss: 33157.622\n",
      "[204,   230] loss: 24856.164\n",
      "[205,     5] loss: 25312.223\n",
      "[205,    10] loss: 21294.685\n",
      "[205,    15] loss: 36069.462\n",
      "[205,    20] loss: 22168.017\n",
      "[205,    25] loss: 31737.979\n",
      "[205,    30] loss: 17639.020\n",
      "[205,    35] loss: 20285.722\n",
      "[205,    40] loss: 37995.506\n",
      "[205,    45] loss: 15433.882\n",
      "[205,    50] loss: 55732.873\n",
      "[205,    55] loss: 19107.630\n",
      "[205,    60] loss: 32139.239\n",
      "[205,    65] loss: 38882.616\n",
      "[205,    70] loss: 20233.007\n",
      "[205,    75] loss: 27379.880\n",
      "[205,    80] loss: 54855.852\n",
      "[205,    85] loss: 16122.525\n",
      "[205,    90] loss: 25820.161\n",
      "[205,    95] loss: 29320.818\n",
      "[205,   100] loss: 24819.277\n",
      "[205,   105] loss: 20118.885\n",
      "[205,   110] loss: 18350.115\n",
      "[205,   115] loss: 26254.815\n",
      "[205,   120] loss: 24465.950\n",
      "[205,   125] loss: 26760.851\n",
      "[205,   130] loss: 23896.174\n",
      "[205,   135] loss: 20123.643\n",
      "[205,   140] loss: 31855.461\n",
      "[205,   145] loss: 19282.644\n",
      "[205,   150] loss: 20703.510\n",
      "[205,   155] loss: 20370.095\n",
      "[205,   160] loss: 22458.875\n",
      "[205,   165] loss: 22691.470\n",
      "[205,   170] loss: 34243.508\n",
      "[205,   175] loss: 24819.291\n",
      "[205,   180] loss: 25599.232\n",
      "[205,   185] loss: 14271.198\n",
      "[205,   190] loss: 24057.061\n",
      "[205,   195] loss: 29856.011\n",
      "[205,   200] loss: 32584.744\n",
      "[205,   205] loss: 26070.603\n",
      "[205,   210] loss: 18348.334\n",
      "[205,   215] loss: 24290.377\n",
      "[205,   220] loss: 20616.211\n",
      "[205,   225] loss: 30596.334\n",
      "[205,   230] loss: 33380.214\n",
      "[206,     5] loss: 26931.369\n",
      "[206,    10] loss: 26528.783\n",
      "[206,    15] loss: 23379.244\n",
      "[206,    20] loss: 32692.854\n",
      "[206,    25] loss: 24270.537\n",
      "[206,    30] loss: 17673.244\n",
      "[206,    35] loss: 28613.215\n",
      "[206,    40] loss: 30145.782\n",
      "[206,    45] loss: 32850.264\n",
      "[206,    50] loss: 33175.189\n",
      "[206,    55] loss: 24579.363\n",
      "[206,    60] loss: 43213.442\n",
      "[206,    65] loss: 24465.513\n",
      "[206,    70] loss: 20527.570\n",
      "[206,    75] loss: 23782.756\n",
      "[206,    80] loss: 17834.917\n",
      "[206,    85] loss: 34006.853\n",
      "[206,    90] loss: 27571.928\n",
      "[206,    95] loss: 33330.329\n",
      "[206,   100] loss: 22649.457\n",
      "[206,   105] loss: 29130.133\n",
      "[206,   110] loss: 21835.888\n",
      "[206,   115] loss: 30454.431\n",
      "[206,   120] loss: 18526.500\n",
      "[206,   125] loss: 21326.206\n",
      "[206,   130] loss: 17151.843\n",
      "[206,   135] loss: 29823.870\n",
      "[206,   140] loss: 26816.344\n",
      "[206,   145] loss: 55095.534\n",
      "[206,   150] loss: 27995.782\n",
      "[206,   155] loss: 23796.456\n",
      "[206,   160] loss: 29744.261\n",
      "[206,   165] loss: 23477.124\n",
      "[206,   170] loss: 20846.583\n",
      "[206,   175] loss: 22448.052\n",
      "[206,   180] loss: 23469.496\n",
      "[206,   185] loss: 18762.059\n",
      "[206,   190] loss: 20288.995\n",
      "[206,   195] loss: 30844.218\n",
      "[206,   200] loss: 26098.396\n",
      "[206,   205] loss: 27131.995\n",
      "[206,   210] loss: 30464.184\n",
      "[206,   215] loss: 16271.431\n",
      "[206,   220] loss: 26161.994\n",
      "[206,   225] loss: 19366.313\n",
      "[206,   230] loss: 18912.816\n",
      "[207,     5] loss: 26267.309\n",
      "[207,    10] loss: 20759.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207,    15] loss: 25338.381\n",
      "[207,    20] loss: 19878.120\n",
      "[207,    25] loss: 24473.443\n",
      "[207,    30] loss: 25623.837\n",
      "[207,    35] loss: 26589.890\n",
      "[207,    40] loss: 35618.008\n",
      "[207,    45] loss: 17204.763\n",
      "[207,    50] loss: 30340.976\n",
      "[207,    55] loss: 30317.846\n",
      "[207,    60] loss: 40662.005\n",
      "[207,    65] loss: 18919.412\n",
      "[207,    70] loss: 15994.572\n",
      "[207,    75] loss: 20255.670\n",
      "[207,    80] loss: 22910.509\n",
      "[207,    85] loss: 25610.693\n",
      "[207,    90] loss: 22015.011\n",
      "[207,    95] loss: 25217.309\n",
      "[207,   100] loss: 40472.036\n",
      "[207,   105] loss: 25405.272\n",
      "[207,   110] loss: 31810.253\n",
      "[207,   115] loss: 17798.641\n",
      "[207,   120] loss: 23790.141\n",
      "[207,   125] loss: 19895.291\n",
      "[207,   130] loss: 25062.641\n",
      "[207,   135] loss: 25434.461\n",
      "[207,   140] loss: 22985.548\n",
      "[207,   145] loss: 20451.493\n",
      "[207,   150] loss: 25239.372\n",
      "[207,   155] loss: 25731.218\n",
      "[207,   160] loss: 25906.345\n",
      "[207,   165] loss: 53971.914\n",
      "[207,   170] loss: 30286.557\n",
      "[207,   175] loss: 32630.082\n",
      "[207,   180] loss: 19220.558\n",
      "[207,   185] loss: 27054.865\n",
      "[207,   190] loss: 30485.371\n",
      "[207,   195] loss: 26706.757\n",
      "[207,   200] loss: 21521.955\n",
      "[207,   205] loss: 26160.546\n",
      "[207,   210] loss: 30079.633\n",
      "[207,   215] loss: 21137.087\n",
      "[207,   220] loss: 32182.438\n",
      "[207,   225] loss: 24622.402\n",
      "[207,   230] loss: 29980.438\n",
      "[208,     5] loss: 45074.989\n",
      "[208,    10] loss: 23973.486\n",
      "[208,    15] loss: 41930.732\n",
      "[208,    20] loss: 21098.021\n",
      "[208,    25] loss: 30993.375\n",
      "[208,    30] loss: 25438.409\n",
      "[208,    35] loss: 23707.670\n",
      "[208,    40] loss: 29100.060\n",
      "[208,    45] loss: 23283.321\n",
      "[208,    50] loss: 29117.082\n",
      "[208,    55] loss: 21796.585\n",
      "[208,    60] loss: 20634.058\n",
      "[208,    65] loss: 23060.060\n",
      "[208,    70] loss: 27130.408\n",
      "[208,    75] loss: 25342.284\n",
      "[208,    80] loss: 15992.994\n",
      "[208,    85] loss: 35336.951\n",
      "[208,    90] loss: 27986.009\n",
      "[208,    95] loss: 23647.186\n",
      "[208,   100] loss: 25850.096\n",
      "[208,   105] loss: 28316.986\n",
      "[208,   110] loss: 24521.304\n",
      "[208,   115] loss: 15400.121\n",
      "[208,   120] loss: 31393.724\n",
      "[208,   125] loss: 19062.266\n",
      "[208,   130] loss: 23832.774\n",
      "[208,   135] loss: 24273.519\n",
      "[208,   140] loss: 23015.663\n",
      "[208,   145] loss: 25644.850\n",
      "[208,   150] loss: 31744.907\n",
      "[208,   155] loss: 28252.916\n",
      "[208,   160] loss: 35076.246\n",
      "[208,   165] loss: 29256.233\n",
      "[208,   170] loss: 32700.194\n",
      "[208,   175] loss: 19795.841\n",
      "[208,   180] loss: 20163.413\n",
      "[208,   185] loss: 28720.021\n",
      "[208,   190] loss: 22035.330\n",
      "[208,   195] loss: 23164.933\n",
      "[208,   200] loss: 20119.639\n",
      "[208,   205] loss: 21838.924\n",
      "[208,   210] loss: 25833.947\n",
      "[208,   215] loss: 34346.884\n",
      "[208,   220] loss: 20035.266\n",
      "[208,   225] loss: 29190.846\n",
      "[208,   230] loss: 27650.045\n",
      "[209,     5] loss: 23616.482\n",
      "[209,    10] loss: 19781.149\n",
      "[209,    15] loss: 26154.966\n",
      "[209,    20] loss: 32136.714\n",
      "[209,    25] loss: 15602.623\n",
      "[209,    30] loss: 20833.876\n",
      "[209,    35] loss: 24658.798\n",
      "[209,    40] loss: 22938.215\n",
      "[209,    45] loss: 23872.514\n",
      "[209,    50] loss: 33816.618\n",
      "[209,    55] loss: 23311.314\n",
      "[209,    60] loss: 21954.443\n",
      "[209,    65] loss: 21279.285\n",
      "[209,    70] loss: 30416.842\n",
      "[209,    75] loss: 51795.764\n",
      "[209,    80] loss: 21549.262\n",
      "[209,    85] loss: 18703.042\n",
      "[209,    90] loss: 22415.232\n",
      "[209,    95] loss: 25376.462\n",
      "[209,   100] loss: 37586.872\n",
      "[209,   105] loss: 23997.303\n",
      "[209,   110] loss: 22067.687\n",
      "[209,   115] loss: 31839.375\n",
      "[209,   120] loss: 26963.790\n",
      "[209,   125] loss: 30407.233\n",
      "[209,   130] loss: 27032.036\n",
      "[209,   135] loss: 23174.939\n",
      "[209,   140] loss: 25089.116\n",
      "[209,   145] loss: 30588.330\n",
      "[209,   150] loss: 23849.150\n",
      "[209,   155] loss: 27942.930\n",
      "[209,   160] loss: 24898.220\n",
      "[209,   165] loss: 27814.999\n",
      "[209,   170] loss: 33930.945\n",
      "[209,   175] loss: 32054.332\n",
      "[209,   180] loss: 33890.390\n",
      "[209,   185] loss: 16064.030\n",
      "[209,   190] loss: 29798.111\n",
      "[209,   195] loss: 18982.426\n",
      "[209,   200] loss: 38104.648\n",
      "[209,   205] loss: 23840.112\n",
      "[209,   210] loss: 23692.892\n",
      "[209,   215] loss: 23527.710\n",
      "[209,   220] loss: 18776.288\n",
      "[209,   225] loss: 29120.983\n",
      "[209,   230] loss: 21922.327\n",
      "[210,     5] loss: 28617.827\n",
      "[210,    10] loss: 25543.029\n",
      "[210,    15] loss: 25718.566\n",
      "[210,    20] loss: 36878.334\n",
      "[210,    25] loss: 31065.747\n",
      "[210,    30] loss: 20705.336\n",
      "[210,    35] loss: 24870.431\n",
      "[210,    40] loss: 23234.979\n",
      "[210,    45] loss: 21033.490\n",
      "[210,    50] loss: 27903.411\n",
      "[210,    55] loss: 34646.320\n",
      "[210,    60] loss: 19679.388\n",
      "[210,    65] loss: 28637.288\n",
      "[210,    70] loss: 26895.649\n",
      "[210,    75] loss: 26666.824\n",
      "[210,    80] loss: 25140.147\n",
      "[210,    85] loss: 20474.158\n",
      "[210,    90] loss: 22321.770\n",
      "[210,    95] loss: 19524.837\n",
      "[210,   100] loss: 29724.151\n",
      "[210,   105] loss: 24197.014\n",
      "[210,   110] loss: 30853.362\n",
      "[210,   115] loss: 27608.938\n",
      "[210,   120] loss: 27609.231\n",
      "[210,   125] loss: 20890.313\n",
      "[210,   130] loss: 27750.328\n",
      "[210,   135] loss: 28258.399\n",
      "[210,   140] loss: 32901.311\n",
      "[210,   145] loss: 21579.393\n",
      "[210,   150] loss: 24291.482\n",
      "[210,   155] loss: 21630.410\n",
      "[210,   160] loss: 23850.382\n",
      "[210,   165] loss: 21856.636\n",
      "[210,   170] loss: 26472.123\n",
      "[210,   175] loss: 26587.827\n",
      "[210,   180] loss: 16148.574\n",
      "[210,   185] loss: 18860.775\n",
      "[210,   190] loss: 37363.936\n",
      "[210,   195] loss: 18649.676\n",
      "[210,   200] loss: 36687.163\n",
      "[210,   205] loss: 26956.282\n",
      "[210,   210] loss: 45071.947\n",
      "[210,   215] loss: 26397.415\n",
      "[210,   220] loss: 18151.375\n",
      "[210,   225] loss: 22569.886\n",
      "[210,   230] loss: 30532.100\n",
      "[211,     5] loss: 30754.421\n",
      "[211,    10] loss: 21254.453\n",
      "[211,    15] loss: 21701.375\n",
      "[211,    20] loss: 31449.312\n",
      "[211,    25] loss: 18564.266\n",
      "[211,    30] loss: 24486.689\n",
      "[211,    35] loss: 27463.043\n",
      "[211,    40] loss: 28492.562\n",
      "[211,    45] loss: 22482.474\n",
      "[211,    50] loss: 12385.849\n",
      "[211,    55] loss: 21465.450\n",
      "[211,    60] loss: 14913.471\n",
      "[211,    65] loss: 21943.224\n",
      "[211,    70] loss: 33573.204\n",
      "[211,    75] loss: 29880.478\n",
      "[211,    80] loss: 27870.520\n",
      "[211,    85] loss: 32780.402\n",
      "[211,    90] loss: 22785.033\n",
      "[211,    95] loss: 26103.570\n",
      "[211,   100] loss: 25768.774\n",
      "[211,   105] loss: 22024.093\n",
      "[211,   110] loss: 11391.713\n",
      "[211,   115] loss: 23144.324\n",
      "[211,   120] loss: 28032.206\n",
      "[211,   125] loss: 34435.127\n",
      "[211,   130] loss: 23581.784\n",
      "[211,   135] loss: 29711.201\n",
      "[211,   140] loss: 21356.201\n",
      "[211,   145] loss: 23544.111\n",
      "[211,   150] loss: 28069.149\n",
      "[211,   155] loss: 20377.368\n",
      "[211,   160] loss: 21443.942\n",
      "[211,   165] loss: 37476.404\n",
      "[211,   170] loss: 51053.842\n",
      "[211,   175] loss: 35180.760\n",
      "[211,   180] loss: 25866.783\n",
      "[211,   185] loss: 33123.606\n",
      "[211,   190] loss: 22658.887\n",
      "[211,   195] loss: 29041.506\n",
      "[211,   200] loss: 25452.777\n",
      "[211,   205] loss: 22555.624\n",
      "[211,   210] loss: 32366.858\n",
      "[211,   215] loss: 41320.994\n",
      "[211,   220] loss: 21465.961\n",
      "[211,   225] loss: 23125.561\n",
      "[211,   230] loss: 29129.489\n",
      "[212,     5] loss: 29835.305\n",
      "[212,    10] loss: 20424.579\n",
      "[212,    15] loss: 23619.286\n",
      "[212,    20] loss: 25673.846\n",
      "[212,    25] loss: 23517.219\n",
      "[212,    30] loss: 30649.848\n",
      "[212,    35] loss: 25611.716\n",
      "[212,    40] loss: 29039.166\n",
      "[212,    45] loss: 32015.964\n",
      "[212,    50] loss: 45453.164\n",
      "[212,    55] loss: 45460.705\n",
      "[212,    60] loss: 21774.561\n",
      "[212,    65] loss: 26388.050\n",
      "[212,    70] loss: 21794.711\n",
      "[212,    75] loss: 18431.021\n",
      "[212,    80] loss: 23339.292\n",
      "[212,    85] loss: 30113.236\n",
      "[212,    90] loss: 26299.366\n",
      "[212,    95] loss: 21376.552\n",
      "[212,   100] loss: 19925.550\n",
      "[212,   105] loss: 30779.675\n",
      "[212,   110] loss: 34911.184\n",
      "[212,   115] loss: 32449.159\n",
      "[212,   120] loss: 22575.328\n",
      "[212,   125] loss: 21736.926\n",
      "[212,   130] loss: 19504.368\n",
      "[212,   135] loss: 20676.779\n",
      "[212,   140] loss: 26838.465\n",
      "[212,   145] loss: 41914.500\n",
      "[212,   150] loss: 27672.886\n",
      "[212,   155] loss: 29931.710\n",
      "[212,   160] loss: 17851.905\n",
      "[212,   165] loss: 28651.298\n",
      "[212,   170] loss: 25305.589\n",
      "[212,   175] loss: 32992.667\n",
      "[212,   180] loss: 16286.123\n",
      "[212,   185] loss: 22762.162\n",
      "[212,   190] loss: 27488.942\n",
      "[212,   195] loss: 28629.820\n",
      "[212,   200] loss: 21235.155\n",
      "[212,   205] loss: 18061.165\n",
      "[212,   210] loss: 39812.265\n",
      "[212,   215] loss: 20514.684\n",
      "[212,   220] loss: 17950.096\n",
      "[212,   225] loss: 21225.966\n",
      "[212,   230] loss: 17543.705\n",
      "[213,     5] loss: 25376.110\n",
      "[213,    10] loss: 26315.771\n",
      "[213,    15] loss: 29973.306\n",
      "[213,    20] loss: 25421.568\n",
      "[213,    25] loss: 26952.273\n",
      "[213,    30] loss: 21806.558\n",
      "[213,    35] loss: 24626.074\n",
      "[213,    40] loss: 18876.899\n",
      "[213,    45] loss: 22740.541\n",
      "[213,    50] loss: 18713.596\n",
      "[213,    55] loss: 38262.864\n",
      "[213,    60] loss: 29277.996\n",
      "[213,    65] loss: 23839.064\n",
      "[213,    70] loss: 29184.770\n",
      "[213,    75] loss: 27273.047\n",
      "[213,    80] loss: 28024.713\n",
      "[213,    85] loss: 18883.655\n",
      "[213,    90] loss: 17842.330\n",
      "[213,    95] loss: 27079.837\n",
      "[213,   100] loss: 42352.964\n",
      "[213,   105] loss: 23151.492\n",
      "[213,   110] loss: 45295.846\n",
      "[213,   115] loss: 28143.534\n",
      "[213,   120] loss: 24479.933\n",
      "[213,   125] loss: 18856.362\n",
      "[213,   130] loss: 29821.774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[213,   135] loss: 24067.897\n",
      "[213,   140] loss: 28308.491\n",
      "[213,   145] loss: 25714.963\n",
      "[213,   150] loss: 22466.936\n",
      "[213,   155] loss: 29283.427\n",
      "[213,   160] loss: 27103.430\n",
      "[213,   165] loss: 22592.958\n",
      "[213,   170] loss: 19771.412\n",
      "[213,   175] loss: 23544.818\n",
      "[213,   180] loss: 25689.739\n",
      "[213,   185] loss: 24083.417\n",
      "[213,   190] loss: 30603.639\n",
      "[213,   195] loss: 24430.356\n",
      "[213,   200] loss: 33658.299\n",
      "[213,   205] loss: 33903.161\n",
      "[213,   210] loss: 27859.551\n",
      "[213,   215] loss: 21499.862\n",
      "[213,   220] loss: 18748.685\n",
      "[213,   225] loss: 23750.513\n",
      "[213,   230] loss: 28802.838\n",
      "[214,     5] loss: 22611.318\n",
      "[214,    10] loss: 28712.501\n",
      "[214,    15] loss: 20494.647\n",
      "[214,    20] loss: 25252.793\n",
      "[214,    25] loss: 33915.154\n",
      "[214,    30] loss: 25001.569\n",
      "[214,    35] loss: 19416.291\n",
      "[214,    40] loss: 20544.590\n",
      "[214,    45] loss: 27296.570\n",
      "[214,    50] loss: 16746.693\n",
      "[214,    55] loss: 24616.036\n",
      "[214,    60] loss: 27026.982\n",
      "[214,    65] loss: 24888.520\n",
      "[214,    70] loss: 24807.600\n",
      "[214,    75] loss: 23117.270\n",
      "[214,    80] loss: 21005.950\n",
      "[214,    85] loss: 25482.603\n",
      "[214,    90] loss: 23255.667\n",
      "[214,    95] loss: 35364.504\n",
      "[214,   100] loss: 21596.189\n",
      "[214,   105] loss: 24668.150\n",
      "[214,   110] loss: 22270.959\n",
      "[214,   115] loss: 30462.876\n",
      "[214,   120] loss: 22227.303\n",
      "[214,   125] loss: 29054.095\n",
      "[214,   130] loss: 25678.409\n",
      "[214,   135] loss: 32385.088\n",
      "[214,   140] loss: 26287.712\n",
      "[214,   145] loss: 29838.636\n",
      "[214,   150] loss: 19156.026\n",
      "[214,   155] loss: 30636.504\n",
      "[214,   160] loss: 20048.811\n",
      "[214,   165] loss: 57376.818\n",
      "[214,   170] loss: 24075.947\n",
      "[214,   175] loss: 24507.364\n",
      "[214,   180] loss: 22718.042\n",
      "[214,   185] loss: 24074.460\n",
      "[214,   190] loss: 32249.026\n",
      "[214,   195] loss: 29868.771\n",
      "[214,   200] loss: 16576.818\n",
      "[214,   205] loss: 20451.755\n",
      "[214,   210] loss: 31518.173\n",
      "[214,   215] loss: 36970.080\n",
      "[214,   220] loss: 20415.281\n",
      "[214,   225] loss: 21499.665\n",
      "[214,   230] loss: 41195.103\n",
      "[215,     5] loss: 24165.695\n",
      "[215,    10] loss: 25890.932\n",
      "[215,    15] loss: 29138.966\n",
      "[215,    20] loss: 25555.550\n",
      "[215,    25] loss: 44119.304\n",
      "[215,    30] loss: 19048.755\n",
      "[215,    35] loss: 32394.034\n",
      "[215,    40] loss: 21570.271\n",
      "[215,    45] loss: 21828.951\n",
      "[215,    50] loss: 17592.248\n",
      "[215,    55] loss: 27093.438\n",
      "[215,    60] loss: 20998.657\n",
      "[215,    65] loss: 39949.421\n",
      "[215,    70] loss: 19135.932\n",
      "[215,    75] loss: 34586.645\n",
      "[215,    80] loss: 27951.372\n",
      "[215,    85] loss: 24674.463\n",
      "[215,    90] loss: 28809.196\n",
      "[215,    95] loss: 22357.348\n",
      "[215,   100] loss: 23908.097\n",
      "[215,   105] loss: 27106.838\n",
      "[215,   110] loss: 19183.096\n",
      "[215,   115] loss: 25205.816\n",
      "[215,   120] loss: 36439.429\n",
      "[215,   125] loss: 24110.989\n",
      "[215,   130] loss: 28542.050\n",
      "[215,   135] loss: 30410.037\n",
      "[215,   140] loss: 23136.855\n",
      "[215,   145] loss: 25369.805\n",
      "[215,   150] loss: 17715.660\n",
      "[215,   155] loss: 20112.001\n",
      "[215,   160] loss: 24554.264\n",
      "[215,   165] loss: 29513.807\n",
      "[215,   170] loss: 20142.525\n",
      "[215,   175] loss: 23872.563\n",
      "[215,   180] loss: 23831.554\n",
      "[215,   185] loss: 23093.156\n",
      "[215,   190] loss: 28560.876\n",
      "[215,   195] loss: 20549.369\n",
      "[215,   200] loss: 56155.286\n",
      "[215,   205] loss: 18088.189\n",
      "[215,   210] loss: 18032.460\n",
      "[215,   215] loss: 24265.894\n",
      "[215,   220] loss: 43167.536\n",
      "[215,   225] loss: 24279.635\n",
      "[215,   230] loss: 20994.017\n",
      "[216,     5] loss: 25868.745\n",
      "[216,    10] loss: 31501.965\n",
      "[216,    15] loss: 22485.463\n",
      "[216,    20] loss: 22929.896\n",
      "[216,    25] loss: 30271.059\n",
      "[216,    30] loss: 15610.792\n",
      "[216,    35] loss: 23800.979\n",
      "[216,    40] loss: 23370.217\n",
      "[216,    45] loss: 19573.105\n",
      "[216,    50] loss: 23567.799\n",
      "[216,    55] loss: 37167.563\n",
      "[216,    60] loss: 25174.272\n",
      "[216,    65] loss: 16586.199\n",
      "[216,    70] loss: 18153.591\n",
      "[216,    75] loss: 23002.359\n",
      "[216,    80] loss: 28461.087\n",
      "[216,    85] loss: 36416.923\n",
      "[216,    90] loss: 26703.421\n",
      "[216,    95] loss: 56293.955\n",
      "[216,   100] loss: 28586.107\n",
      "[216,   105] loss: 25797.048\n",
      "[216,   110] loss: 28759.904\n",
      "[216,   115] loss: 38096.660\n",
      "[216,   120] loss: 24198.204\n",
      "[216,   125] loss: 19104.970\n",
      "[216,   130] loss: 36353.060\n",
      "[216,   135] loss: 30695.682\n",
      "[216,   140] loss: 26607.057\n",
      "[216,   145] loss: 15747.327\n",
      "[216,   150] loss: 20132.657\n",
      "[216,   155] loss: 26212.086\n",
      "[216,   160] loss: 24756.908\n",
      "[216,   165] loss: 19219.145\n",
      "[216,   170] loss: 38200.687\n",
      "[216,   175] loss: 23683.592\n",
      "[216,   180] loss: 24669.604\n",
      "[216,   185] loss: 40883.168\n",
      "[216,   190] loss: 25299.059\n",
      "[216,   195] loss: 22098.830\n",
      "[216,   200] loss: 19758.056\n",
      "[216,   205] loss: 24772.795\n",
      "[216,   210] loss: 23944.242\n",
      "[216,   215] loss: 24068.804\n",
      "[216,   220] loss: 18729.915\n",
      "[216,   225] loss: 24842.197\n",
      "[216,   230] loss: 22857.582\n",
      "[217,     5] loss: 19145.464\n",
      "[217,    10] loss: 25746.446\n",
      "[217,    15] loss: 16699.940\n",
      "[217,    20] loss: 25237.547\n",
      "[217,    25] loss: 47013.851\n",
      "[217,    30] loss: 31004.634\n",
      "[217,    35] loss: 24910.672\n",
      "[217,    40] loss: 37260.857\n",
      "[217,    45] loss: 23116.043\n",
      "[217,    50] loss: 37868.307\n",
      "[217,    55] loss: 36507.364\n",
      "[217,    60] loss: 22538.223\n",
      "[217,    65] loss: 27258.023\n",
      "[217,    70] loss: 24039.992\n",
      "[217,    75] loss: 18009.962\n",
      "[217,    80] loss: 23736.093\n",
      "[217,    85] loss: 28174.263\n",
      "[217,    90] loss: 20153.772\n",
      "[217,    95] loss: 26566.139\n",
      "[217,   100] loss: 24382.564\n",
      "[217,   105] loss: 34782.724\n",
      "[217,   110] loss: 22507.037\n",
      "[217,   115] loss: 25389.480\n",
      "[217,   120] loss: 23223.116\n",
      "[217,   125] loss: 20976.986\n",
      "[217,   130] loss: 26581.479\n",
      "[217,   135] loss: 20757.165\n",
      "[217,   140] loss: 30525.109\n",
      "[217,   145] loss: 28630.934\n",
      "[217,   150] loss: 20127.287\n",
      "[217,   155] loss: 19514.416\n",
      "[217,   160] loss: 18248.816\n",
      "[217,   165] loss: 27766.591\n",
      "[217,   170] loss: 31450.950\n",
      "[217,   175] loss: 35157.621\n",
      "[217,   180] loss: 25925.520\n",
      "[217,   185] loss: 33144.161\n",
      "[217,   190] loss: 25497.304\n",
      "[217,   195] loss: 18400.045\n",
      "[217,   200] loss: 17936.291\n",
      "[217,   205] loss: 33172.525\n",
      "[217,   210] loss: 36867.989\n",
      "[217,   215] loss: 26118.927\n",
      "[217,   220] loss: 20022.256\n",
      "[217,   225] loss: 22837.321\n",
      "[217,   230] loss: 20789.440\n",
      "[218,     5] loss: 36027.610\n",
      "[218,    10] loss: 19186.161\n",
      "[218,    15] loss: 25079.129\n",
      "[218,    20] loss: 30105.908\n",
      "[218,    25] loss: 23655.834\n",
      "[218,    30] loss: 16062.331\n",
      "[218,    35] loss: 30155.610\n",
      "[218,    40] loss: 29184.320\n",
      "[218,    45] loss: 22098.355\n",
      "[218,    50] loss: 21655.980\n",
      "[218,    55] loss: 26493.272\n",
      "[218,    60] loss: 37062.703\n",
      "[218,    65] loss: 20904.735\n",
      "[218,    70] loss: 22095.451\n",
      "[218,    75] loss: 25151.099\n",
      "[218,    80] loss: 27553.225\n",
      "[218,    85] loss: 18985.267\n",
      "[218,    90] loss: 33135.135\n",
      "[218,    95] loss: 23039.506\n",
      "[218,   100] loss: 27951.005\n",
      "[218,   105] loss: 31193.997\n",
      "[218,   110] loss: 37311.523\n",
      "[218,   115] loss: 20369.200\n",
      "[218,   120] loss: 25287.865\n",
      "[218,   125] loss: 19129.787\n",
      "[218,   130] loss: 14994.059\n",
      "[218,   135] loss: 22934.951\n",
      "[218,   140] loss: 24265.073\n",
      "[218,   145] loss: 35224.313\n",
      "[218,   150] loss: 24790.901\n",
      "[218,   155] loss: 28962.808\n",
      "[218,   160] loss: 28293.793\n",
      "[218,   165] loss: 23604.761\n",
      "[218,   170] loss: 25267.706\n",
      "[218,   175] loss: 22160.502\n",
      "[218,   180] loss: 28657.652\n",
      "[218,   185] loss: 49670.907\n",
      "[218,   190] loss: 21387.592\n",
      "[218,   195] loss: 17201.700\n",
      "[218,   200] loss: 24598.852\n",
      "[218,   205] loss: 22224.108\n",
      "[218,   210] loss: 36815.104\n",
      "[218,   215] loss: 22544.000\n",
      "[218,   220] loss: 26788.122\n",
      "[218,   225] loss: 27635.281\n",
      "[218,   230] loss: 22175.788\n",
      "[219,     5] loss: 32734.026\n",
      "[219,    10] loss: 24956.829\n",
      "[219,    15] loss: 27423.559\n",
      "[219,    20] loss: 26048.145\n",
      "[219,    25] loss: 50140.059\n",
      "[219,    30] loss: 17438.109\n",
      "[219,    35] loss: 22490.330\n",
      "[219,    40] loss: 31671.873\n",
      "[219,    45] loss: 24576.395\n",
      "[219,    50] loss: 30543.774\n",
      "[219,    55] loss: 21242.419\n",
      "[219,    60] loss: 22058.331\n",
      "[219,    65] loss: 24981.523\n",
      "[219,    70] loss: 21888.141\n",
      "[219,    75] loss: 27332.834\n",
      "[219,    80] loss: 29230.959\n",
      "[219,    85] loss: 38933.890\n",
      "[219,    90] loss: 20212.023\n",
      "[219,    95] loss: 17901.724\n",
      "[219,   100] loss: 22463.034\n",
      "[219,   105] loss: 21877.058\n",
      "[219,   110] loss: 29379.148\n",
      "[219,   115] loss: 21027.290\n",
      "[219,   120] loss: 16690.748\n",
      "[219,   125] loss: 31916.277\n",
      "[219,   130] loss: 29829.947\n",
      "[219,   135] loss: 14763.860\n",
      "[219,   140] loss: 22381.854\n",
      "[219,   145] loss: 18826.243\n",
      "[219,   150] loss: 28661.780\n",
      "[219,   155] loss: 18229.250\n",
      "[219,   160] loss: 33218.578\n",
      "[219,   165] loss: 28731.838\n",
      "[219,   170] loss: 28410.418\n",
      "[219,   175] loss: 31545.959\n",
      "[219,   180] loss: 19718.850\n",
      "[219,   185] loss: 29793.167\n",
      "[219,   190] loss: 45104.665\n",
      "[219,   195] loss: 17864.775\n",
      "[219,   200] loss: 21851.140\n",
      "[219,   205] loss: 30605.348\n",
      "[219,   210] loss: 34424.065\n",
      "[219,   215] loss: 27827.731\n",
      "[219,   220] loss: 17445.898\n",
      "[219,   225] loss: 20975.670\n",
      "[219,   230] loss: 31350.077\n",
      "[220,     5] loss: 19079.561\n",
      "[220,    10] loss: 24684.617\n",
      "[220,    15] loss: 23368.463\n",
      "[220,    20] loss: 38228.194\n",
      "[220,    25] loss: 21080.904\n",
      "[220,    30] loss: 21192.912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220,    35] loss: 26925.664\n",
      "[220,    40] loss: 33429.918\n",
      "[220,    45] loss: 16599.449\n",
      "[220,    50] loss: 21758.219\n",
      "[220,    55] loss: 19708.183\n",
      "[220,    60] loss: 26447.617\n",
      "[220,    65] loss: 22712.906\n",
      "[220,    70] loss: 32556.635\n",
      "[220,    75] loss: 19912.091\n",
      "[220,    80] loss: 30757.671\n",
      "[220,    85] loss: 22866.532\n",
      "[220,    90] loss: 18748.052\n",
      "[220,    95] loss: 28792.982\n",
      "[220,   100] loss: 28126.071\n",
      "[220,   105] loss: 33634.637\n",
      "[220,   110] loss: 26554.468\n",
      "[220,   115] loss: 29344.689\n",
      "[220,   120] loss: 21758.780\n",
      "[220,   125] loss: 20368.499\n",
      "[220,   130] loss: 20904.304\n",
      "[220,   135] loss: 22688.774\n",
      "[220,   140] loss: 26531.292\n",
      "[220,   145] loss: 22601.198\n",
      "[220,   150] loss: 22090.250\n",
      "[220,   155] loss: 27213.343\n",
      "[220,   160] loss: 28842.000\n",
      "[220,   165] loss: 34069.256\n",
      "[220,   170] loss: 33306.726\n",
      "[220,   175] loss: 19377.471\n",
      "[220,   180] loss: 29206.990\n",
      "[220,   185] loss: 17922.876\n",
      "[220,   190] loss: 45493.904\n",
      "[220,   195] loss: 18397.490\n",
      "[220,   200] loss: 24102.594\n",
      "[220,   205] loss: 31460.177\n",
      "[220,   210] loss: 28952.408\n",
      "[220,   215] loss: 27488.546\n",
      "[220,   220] loss: 25954.825\n",
      "[220,   225] loss: 31938.427\n",
      "[220,   230] loss: 29221.216\n",
      "[221,     5] loss: 19003.612\n",
      "[221,    10] loss: 23796.159\n",
      "[221,    15] loss: 29078.763\n",
      "[221,    20] loss: 22930.260\n",
      "[221,    25] loss: 26359.512\n",
      "[221,    30] loss: 22508.503\n",
      "[221,    35] loss: 22703.131\n",
      "[221,    40] loss: 27943.239\n",
      "[221,    45] loss: 17823.534\n",
      "[221,    50] loss: 21878.491\n",
      "[221,    55] loss: 21486.603\n",
      "[221,    60] loss: 23421.080\n",
      "[221,    65] loss: 18603.381\n",
      "[221,    70] loss: 23634.343\n",
      "[221,    75] loss: 21367.129\n",
      "[221,    80] loss: 26897.987\n",
      "[221,    85] loss: 21491.660\n",
      "[221,    90] loss: 18568.892\n",
      "[221,    95] loss: 29472.132\n",
      "[221,   100] loss: 28268.888\n",
      "[221,   105] loss: 36469.999\n",
      "[221,   110] loss: 26441.890\n",
      "[221,   115] loss: 22706.708\n",
      "[221,   120] loss: 19895.479\n",
      "[221,   125] loss: 31126.738\n",
      "[221,   130] loss: 29490.631\n",
      "[221,   135] loss: 25810.878\n",
      "[221,   140] loss: 27979.323\n",
      "[221,   145] loss: 23757.279\n",
      "[221,   150] loss: 29815.947\n",
      "[221,   155] loss: 48801.793\n",
      "[221,   160] loss: 34463.954\n",
      "[221,   165] loss: 17481.747\n",
      "[221,   170] loss: 21924.845\n",
      "[221,   175] loss: 30799.439\n",
      "[221,   180] loss: 24855.986\n",
      "[221,   185] loss: 30565.059\n",
      "[221,   190] loss: 32290.184\n",
      "[221,   195] loss: 13898.431\n",
      "[221,   200] loss: 19112.823\n",
      "[221,   205] loss: 35285.203\n",
      "[221,   210] loss: 24964.665\n",
      "[221,   215] loss: 26070.318\n",
      "[221,   220] loss: 28792.995\n",
      "[221,   225] loss: 37596.547\n",
      "[221,   230] loss: 35793.565\n",
      "[222,     5] loss: 23071.490\n",
      "[222,    10] loss: 24645.314\n",
      "[222,    15] loss: 19821.000\n",
      "[222,    20] loss: 30033.104\n",
      "[222,    25] loss: 17843.589\n",
      "[222,    30] loss: 24554.293\n",
      "[222,    35] loss: 20401.511\n",
      "[222,    40] loss: 22103.752\n",
      "[222,    45] loss: 31236.979\n",
      "[222,    50] loss: 26839.619\n",
      "[222,    55] loss: 35069.357\n",
      "[222,    60] loss: 23508.677\n",
      "[222,    65] loss: 22372.692\n",
      "[222,    70] loss: 34837.280\n",
      "[222,    75] loss: 18276.187\n",
      "[222,    80] loss: 19335.201\n",
      "[222,    85] loss: 18783.974\n",
      "[222,    90] loss: 28114.018\n",
      "[222,    95] loss: 16241.386\n",
      "[222,   100] loss: 28070.251\n",
      "[222,   105] loss: 32021.947\n",
      "[222,   110] loss: 29242.004\n",
      "[222,   115] loss: 25862.468\n",
      "[222,   120] loss: 27282.071\n",
      "[222,   125] loss: 23301.246\n",
      "[222,   130] loss: 27229.023\n",
      "[222,   135] loss: 28739.091\n",
      "[222,   140] loss: 25967.163\n",
      "[222,   145] loss: 27546.245\n",
      "[222,   150] loss: 16134.102\n",
      "[222,   155] loss: 26235.298\n",
      "[222,   160] loss: 26132.456\n",
      "[222,   165] loss: 24129.717\n",
      "[222,   170] loss: 17840.456\n",
      "[222,   175] loss: 26762.464\n",
      "[222,   180] loss: 41029.067\n",
      "[222,   185] loss: 31488.727\n",
      "[222,   190] loss: 45363.347\n",
      "[222,   195] loss: 27413.844\n",
      "[222,   200] loss: 14385.531\n",
      "[222,   205] loss: 25321.549\n",
      "[222,   210] loss: 28852.170\n",
      "[222,   215] loss: 23824.434\n",
      "[222,   220] loss: 16519.677\n",
      "[222,   225] loss: 35509.374\n",
      "[222,   230] loss: 42153.555\n",
      "[223,     5] loss: 19058.054\n",
      "[223,    10] loss: 30480.582\n",
      "[223,    15] loss: 32653.571\n",
      "[223,    20] loss: 34216.795\n",
      "[223,    25] loss: 46800.878\n",
      "[223,    30] loss: 26485.960\n",
      "[223,    35] loss: 22814.931\n",
      "[223,    40] loss: 19744.661\n",
      "[223,    45] loss: 23371.314\n",
      "[223,    50] loss: 29154.428\n",
      "[223,    55] loss: 23702.558\n",
      "[223,    60] loss: 28610.120\n",
      "[223,    65] loss: 18737.991\n",
      "[223,    70] loss: 15851.304\n",
      "[223,    75] loss: 16032.658\n",
      "[223,    80] loss: 24038.042\n",
      "[223,    85] loss: 24681.912\n",
      "[223,    90] loss: 29421.308\n",
      "[223,    95] loss: 22996.480\n",
      "[223,   100] loss: 31772.270\n",
      "[223,   105] loss: 24486.177\n",
      "[223,   110] loss: 23595.233\n",
      "[223,   115] loss: 25028.191\n",
      "[223,   120] loss: 21386.583\n",
      "[223,   125] loss: 20470.858\n",
      "[223,   130] loss: 28399.673\n",
      "[223,   135] loss: 21953.403\n",
      "[223,   140] loss: 27336.110\n",
      "[223,   145] loss: 25875.560\n",
      "[223,   150] loss: 23959.127\n",
      "[223,   155] loss: 26499.254\n",
      "[223,   160] loss: 19743.513\n",
      "[223,   165] loss: 29933.603\n",
      "[223,   170] loss: 24609.785\n",
      "[223,   175] loss: 26199.390\n",
      "[223,   180] loss: 34921.580\n",
      "[223,   185] loss: 31216.169\n",
      "[223,   190] loss: 22657.843\n",
      "[223,   195] loss: 25437.201\n",
      "[223,   200] loss: 31276.008\n",
      "[223,   205] loss: 29773.973\n",
      "[223,   210] loss: 29615.714\n",
      "[223,   215] loss: 26976.746\n",
      "[223,   220] loss: 22259.577\n",
      "[223,   225] loss: 22395.301\n",
      "[223,   230] loss: 36209.059\n",
      "[224,     5] loss: 29693.630\n",
      "[224,    10] loss: 34339.507\n",
      "[224,    15] loss: 29514.779\n",
      "[224,    20] loss: 22980.788\n",
      "[224,    25] loss: 32747.912\n",
      "[224,    30] loss: 16692.628\n",
      "[224,    35] loss: 32948.461\n",
      "[224,    40] loss: 22234.106\n",
      "[224,    45] loss: 31683.197\n",
      "[224,    50] loss: 25484.196\n",
      "[224,    55] loss: 31792.307\n",
      "[224,    60] loss: 21796.976\n",
      "[224,    65] loss: 28302.634\n",
      "[224,    70] loss: 20181.964\n",
      "[224,    75] loss: 24261.945\n",
      "[224,    80] loss: 21553.653\n",
      "[224,    85] loss: 25802.819\n",
      "[224,    90] loss: 25366.428\n",
      "[224,    95] loss: 25219.657\n",
      "[224,   100] loss: 27237.975\n",
      "[224,   105] loss: 25868.948\n",
      "[224,   110] loss: 36670.480\n",
      "[224,   115] loss: 47506.779\n",
      "[224,   120] loss: 27921.295\n",
      "[224,   125] loss: 20600.743\n",
      "[224,   130] loss: 18329.449\n",
      "[224,   135] loss: 20005.609\n",
      "[224,   140] loss: 25079.285\n",
      "[224,   145] loss: 24933.771\n",
      "[224,   150] loss: 25995.823\n",
      "[224,   155] loss: 26744.986\n",
      "[224,   160] loss: 20929.446\n",
      "[224,   165] loss: 19222.706\n",
      "[224,   170] loss: 23497.164\n",
      "[224,   175] loss: 23084.060\n",
      "[224,   180] loss: 27852.703\n",
      "[224,   185] loss: 28313.053\n",
      "[224,   190] loss: 32947.812\n",
      "[224,   195] loss: 17654.167\n",
      "[224,   200] loss: 25341.525\n",
      "[224,   205] loss: 27338.394\n",
      "[224,   210] loss: 27046.791\n",
      "[224,   215] loss: 38596.620\n",
      "[224,   220] loss: 28448.338\n",
      "[224,   225] loss: 13865.484\n",
      "[224,   230] loss: 22411.831\n",
      "[225,     5] loss: 16926.500\n",
      "[225,    10] loss: 25588.196\n",
      "[225,    15] loss: 28048.707\n",
      "[225,    20] loss: 25388.482\n",
      "[225,    25] loss: 21301.836\n",
      "[225,    30] loss: 21727.015\n",
      "[225,    35] loss: 23742.530\n",
      "[225,    40] loss: 20821.109\n",
      "[225,    45] loss: 19877.208\n",
      "[225,    50] loss: 14690.396\n",
      "[225,    55] loss: 27575.659\n",
      "[225,    60] loss: 15600.861\n",
      "[225,    65] loss: 32672.460\n",
      "[225,    70] loss: 21052.106\n",
      "[225,    75] loss: 27202.414\n",
      "[225,    80] loss: 27806.880\n",
      "[225,    85] loss: 16821.228\n",
      "[225,    90] loss: 27135.709\n",
      "[225,    95] loss: 29072.396\n",
      "[225,   100] loss: 28525.445\n",
      "[225,   105] loss: 32162.646\n",
      "[225,   110] loss: 37158.409\n",
      "[225,   115] loss: 26493.751\n",
      "[225,   120] loss: 43376.976\n",
      "[225,   125] loss: 31475.884\n",
      "[225,   130] loss: 26604.622\n",
      "[225,   135] loss: 19477.887\n",
      "[225,   140] loss: 26546.477\n",
      "[225,   145] loss: 21888.484\n",
      "[225,   150] loss: 25846.105\n",
      "[225,   155] loss: 22306.800\n",
      "[225,   160] loss: 30476.803\n",
      "[225,   165] loss: 23012.797\n",
      "[225,   170] loss: 30247.287\n",
      "[225,   175] loss: 26644.862\n",
      "[225,   180] loss: 39608.256\n",
      "[225,   185] loss: 24208.607\n",
      "[225,   190] loss: 25756.441\n",
      "[225,   195] loss: 17827.952\n",
      "[225,   200] loss: 31210.517\n",
      "[225,   205] loss: 22791.001\n",
      "[225,   210] loss: 24819.681\n",
      "[225,   215] loss: 52471.162\n",
      "[225,   220] loss: 21587.850\n",
      "[225,   225] loss: 17635.275\n",
      "[225,   230] loss: 22704.796\n",
      "[226,     5] loss: 25988.871\n",
      "[226,    10] loss: 26778.217\n",
      "[226,    15] loss: 26761.708\n",
      "[226,    20] loss: 17724.031\n",
      "[226,    25] loss: 33091.233\n",
      "[226,    30] loss: 28795.816\n",
      "[226,    35] loss: 16029.482\n",
      "[226,    40] loss: 26398.822\n",
      "[226,    45] loss: 29165.352\n",
      "[226,    50] loss: 22216.388\n",
      "[226,    55] loss: 20657.013\n",
      "[226,    60] loss: 43243.521\n",
      "[226,    65] loss: 42352.311\n",
      "[226,    70] loss: 24184.070\n",
      "[226,    75] loss: 20846.001\n",
      "[226,    80] loss: 24246.832\n",
      "[226,    85] loss: 22733.282\n",
      "[226,    90] loss: 30690.639\n",
      "[226,    95] loss: 33097.613\n",
      "[226,   100] loss: 25456.962\n",
      "[226,   105] loss: 21386.713\n",
      "[226,   110] loss: 27399.218\n",
      "[226,   115] loss: 25969.153\n",
      "[226,   120] loss: 19543.577\n",
      "[226,   125] loss: 24110.265\n",
      "[226,   130] loss: 22828.151\n",
      "[226,   135] loss: 27776.525\n",
      "[226,   140] loss: 51951.534\n",
      "[226,   145] loss: 30690.696\n",
      "[226,   150] loss: 20570.667\n",
      "[226,   155] loss: 21718.632\n",
      "[226,   160] loss: 29511.521\n",
      "[226,   165] loss: 21724.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[226,   170] loss: 17158.961\n",
      "[226,   175] loss: 25363.656\n",
      "[226,   180] loss: 39612.509\n",
      "[226,   185] loss: 31169.460\n",
      "[226,   190] loss: 21814.617\n",
      "[226,   195] loss: 20058.286\n",
      "[226,   200] loss: 29990.012\n",
      "[226,   205] loss: 21680.400\n",
      "[226,   210] loss: 20629.651\n",
      "[226,   215] loss: 24327.229\n",
      "[226,   220] loss: 28376.641\n",
      "[226,   225] loss: 17690.547\n",
      "[226,   230] loss: 22630.238\n",
      "[227,     5] loss: 31574.849\n",
      "[227,    10] loss: 18279.979\n",
      "[227,    15] loss: 29419.224\n",
      "[227,    20] loss: 23032.110\n",
      "[227,    25] loss: 40760.229\n",
      "[227,    30] loss: 32022.555\n",
      "[227,    35] loss: 31102.227\n",
      "[227,    40] loss: 21695.500\n",
      "[227,    45] loss: 30020.009\n",
      "[227,    50] loss: 45516.313\n",
      "[227,    55] loss: 35615.493\n",
      "[227,    60] loss: 19002.393\n",
      "[227,    65] loss: 26784.022\n",
      "[227,    70] loss: 29586.994\n",
      "[227,    75] loss: 16413.361\n",
      "[227,    80] loss: 12837.716\n",
      "[227,    85] loss: 26692.549\n",
      "[227,    90] loss: 18222.059\n",
      "[227,    95] loss: 26213.611\n",
      "[227,   100] loss: 31131.191\n",
      "[227,   105] loss: 19802.014\n",
      "[227,   110] loss: 21590.860\n",
      "[227,   115] loss: 24603.970\n",
      "[227,   120] loss: 34151.326\n",
      "[227,   125] loss: 26471.242\n",
      "[227,   130] loss: 29600.192\n",
      "[227,   135] loss: 22050.943\n",
      "[227,   140] loss: 26659.976\n",
      "[227,   145] loss: 28303.354\n",
      "[227,   150] loss: 22337.771\n",
      "[227,   155] loss: 26728.294\n",
      "[227,   160] loss: 31596.553\n",
      "[227,   165] loss: 26002.649\n",
      "[227,   170] loss: 27654.479\n",
      "[227,   175] loss: 34233.355\n",
      "[227,   180] loss: 28385.060\n",
      "[227,   185] loss: 22488.334\n",
      "[227,   190] loss: 24480.474\n",
      "[227,   195] loss: 22037.379\n",
      "[227,   200] loss: 24861.325\n",
      "[227,   205] loss: 31495.377\n",
      "[227,   210] loss: 25175.628\n",
      "[227,   215] loss: 24792.361\n",
      "[227,   220] loss: 21535.886\n",
      "[227,   225] loss: 16747.270\n",
      "[227,   230] loss: 19119.130\n",
      "[228,     5] loss: 28021.360\n",
      "[228,    10] loss: 22854.206\n",
      "[228,    15] loss: 22053.449\n",
      "[228,    20] loss: 18869.764\n",
      "[228,    25] loss: 20950.559\n",
      "[228,    30] loss: 29509.709\n",
      "[228,    35] loss: 32808.207\n",
      "[228,    40] loss: 22526.253\n",
      "[228,    45] loss: 34443.357\n",
      "[228,    50] loss: 27349.010\n",
      "[228,    55] loss: 25561.160\n",
      "[228,    60] loss: 20373.326\n",
      "[228,    65] loss: 23510.646\n",
      "[228,    70] loss: 23564.971\n",
      "[228,    75] loss: 22974.303\n",
      "[228,    80] loss: 22365.992\n",
      "[228,    85] loss: 47610.041\n",
      "[228,    90] loss: 22722.088\n",
      "[228,    95] loss: 23003.469\n",
      "[228,   100] loss: 29173.852\n",
      "[228,   105] loss: 30980.723\n",
      "[228,   110] loss: 25775.621\n",
      "[228,   115] loss: 17051.954\n",
      "[228,   120] loss: 31988.843\n",
      "[228,   125] loss: 33487.718\n",
      "[228,   130] loss: 30283.837\n",
      "[228,   135] loss: 23909.044\n",
      "[228,   140] loss: 26445.662\n",
      "[228,   145] loss: 23075.702\n",
      "[228,   150] loss: 20894.037\n",
      "[228,   155] loss: 22555.723\n",
      "[228,   160] loss: 26223.918\n",
      "[228,   165] loss: 22587.496\n",
      "[228,   170] loss: 26676.716\n",
      "[228,   175] loss: 28004.355\n",
      "[228,   180] loss: 20941.245\n",
      "[228,   185] loss: 22855.281\n",
      "[228,   190] loss: 17174.473\n",
      "[228,   195] loss: 19308.444\n",
      "[228,   200] loss: 16103.296\n",
      "[228,   205] loss: 20355.769\n",
      "[228,   210] loss: 46229.129\n",
      "[228,   215] loss: 43983.933\n",
      "[228,   220] loss: 27036.276\n",
      "[228,   225] loss: 41350.061\n",
      "[228,   230] loss: 20462.229\n",
      "[229,     5] loss: 23388.023\n",
      "[229,    10] loss: 22800.833\n",
      "[229,    15] loss: 18517.329\n",
      "[229,    20] loss: 22813.153\n",
      "[229,    25] loss: 22980.566\n",
      "[229,    30] loss: 30087.424\n",
      "[229,    35] loss: 20833.692\n",
      "[229,    40] loss: 27303.128\n",
      "[229,    45] loss: 25904.863\n",
      "[229,    50] loss: 35866.455\n",
      "[229,    55] loss: 29921.549\n",
      "[229,    60] loss: 14427.542\n",
      "[229,    65] loss: 27979.592\n",
      "[229,    70] loss: 25185.834\n",
      "[229,    75] loss: 29152.596\n",
      "[229,    80] loss: 21473.065\n",
      "[229,    85] loss: 21628.878\n",
      "[229,    90] loss: 26475.909\n",
      "[229,    95] loss: 31168.761\n",
      "[229,   100] loss: 29282.887\n",
      "[229,   105] loss: 32005.802\n",
      "[229,   110] loss: 26594.321\n",
      "[229,   115] loss: 25870.548\n",
      "[229,   120] loss: 17996.722\n",
      "[229,   125] loss: 21998.218\n",
      "[229,   130] loss: 23459.416\n",
      "[229,   135] loss: 19076.873\n",
      "[229,   140] loss: 28082.242\n",
      "[229,   145] loss: 31291.215\n",
      "[229,   150] loss: 22184.197\n",
      "[229,   155] loss: 25902.022\n",
      "[229,   160] loss: 44107.001\n",
      "[229,   165] loss: 26683.339\n",
      "[229,   170] loss: 33757.722\n",
      "[229,   175] loss: 18745.217\n",
      "[229,   180] loss: 19224.343\n",
      "[229,   185] loss: 40543.122\n",
      "[229,   190] loss: 18853.426\n",
      "[229,   195] loss: 20167.927\n",
      "[229,   200] loss: 24014.700\n",
      "[229,   205] loss: 27166.993\n",
      "[229,   210] loss: 19503.021\n",
      "[229,   215] loss: 44226.559\n",
      "[229,   220] loss: 23193.534\n",
      "[229,   225] loss: 29263.108\n",
      "[229,   230] loss: 31197.344\n",
      "[230,     5] loss: 27994.020\n",
      "[230,    10] loss: 22674.299\n",
      "[230,    15] loss: 32193.937\n",
      "[230,    20] loss: 20564.893\n",
      "[230,    25] loss: 25827.208\n",
      "[230,    30] loss: 25963.907\n",
      "[230,    35] loss: 23459.577\n",
      "[230,    40] loss: 30528.367\n",
      "[230,    45] loss: 15863.164\n",
      "[230,    50] loss: 18955.622\n",
      "[230,    55] loss: 30592.480\n",
      "[230,    60] loss: 25925.762\n",
      "[230,    65] loss: 22110.118\n",
      "[230,    70] loss: 36714.075\n",
      "[230,    75] loss: 31168.739\n",
      "[230,    80] loss: 29234.396\n",
      "[230,    85] loss: 18807.404\n",
      "[230,    90] loss: 17464.343\n",
      "[230,    95] loss: 24161.970\n",
      "[230,   100] loss: 20734.023\n",
      "[230,   105] loss: 24962.474\n",
      "[230,   110] loss: 25838.712\n",
      "[230,   115] loss: 30714.349\n",
      "[230,   120] loss: 18531.408\n",
      "[230,   125] loss: 22819.121\n",
      "[230,   130] loss: 36379.858\n",
      "[230,   135] loss: 31471.836\n",
      "[230,   140] loss: 23295.176\n",
      "[230,   145] loss: 27599.893\n",
      "[230,   150] loss: 37042.448\n",
      "[230,   155] loss: 20931.297\n",
      "[230,   160] loss: 32505.906\n",
      "[230,   165] loss: 19605.558\n",
      "[230,   170] loss: 45099.929\n",
      "[230,   175] loss: 25121.666\n",
      "[230,   180] loss: 22932.263\n",
      "[230,   185] loss: 26655.010\n",
      "[230,   190] loss: 23890.592\n",
      "[230,   195] loss: 16020.360\n",
      "[230,   200] loss: 28726.076\n",
      "[230,   205] loss: 32380.559\n",
      "[230,   210] loss: 26036.798\n",
      "[230,   215] loss: 18203.165\n",
      "[230,   220] loss: 25088.841\n",
      "[230,   225] loss: 25834.863\n",
      "[230,   230] loss: 35891.489\n",
      "[231,     5] loss: 26952.858\n",
      "[231,    10] loss: 23837.739\n",
      "[231,    15] loss: 18173.709\n",
      "[231,    20] loss: 18880.872\n",
      "[231,    25] loss: 25061.879\n",
      "[231,    30] loss: 31520.689\n",
      "[231,    35] loss: 20486.590\n",
      "[231,    40] loss: 35492.109\n",
      "[231,    45] loss: 15410.584\n",
      "[231,    50] loss: 26826.272\n",
      "[231,    55] loss: 24705.016\n",
      "[231,    60] loss: 30541.930\n",
      "[231,    65] loss: 27315.713\n",
      "[231,    70] loss: 25796.137\n",
      "[231,    75] loss: 22633.984\n",
      "[231,    80] loss: 34369.384\n",
      "[231,    85] loss: 25896.783\n",
      "[231,    90] loss: 23232.556\n",
      "[231,    95] loss: 28306.819\n",
      "[231,   100] loss: 27120.255\n",
      "[231,   105] loss: 24495.224\n",
      "[231,   110] loss: 21820.650\n",
      "[231,   115] loss: 40305.890\n",
      "[231,   120] loss: 18518.262\n",
      "[231,   125] loss: 16451.241\n",
      "[231,   130] loss: 36194.113\n",
      "[231,   135] loss: 29156.992\n",
      "[231,   140] loss: 21478.125\n",
      "[231,   145] loss: 30112.882\n",
      "[231,   150] loss: 26550.184\n",
      "[231,   155] loss: 24672.889\n",
      "[231,   160] loss: 21317.037\n",
      "[231,   165] loss: 26689.724\n",
      "[231,   170] loss: 24394.359\n",
      "[231,   175] loss: 19825.176\n",
      "[231,   180] loss: 29311.621\n",
      "[231,   185] loss: 21708.691\n",
      "[231,   190] loss: 29080.705\n",
      "[231,   195] loss: 25163.552\n",
      "[231,   200] loss: 23263.316\n",
      "[231,   205] loss: 32423.446\n",
      "[231,   210] loss: 27289.632\n",
      "[231,   215] loss: 40208.146\n",
      "[231,   220] loss: 29003.479\n",
      "[231,   225] loss: 28254.564\n",
      "[231,   230] loss: 21606.362\n",
      "[232,     5] loss: 30523.759\n",
      "[232,    10] loss: 29673.498\n",
      "[232,    15] loss: 22695.233\n",
      "[232,    20] loss: 22425.262\n",
      "[232,    25] loss: 28009.585\n",
      "[232,    30] loss: 26973.635\n",
      "[232,    35] loss: 26114.544\n",
      "[232,    40] loss: 27734.349\n",
      "[232,    45] loss: 20521.056\n",
      "[232,    50] loss: 25442.169\n",
      "[232,    55] loss: 37062.711\n",
      "[232,    60] loss: 21764.220\n",
      "[232,    65] loss: 39493.157\n",
      "[232,    70] loss: 26408.957\n",
      "[232,    75] loss: 24472.189\n",
      "[232,    80] loss: 21258.098\n",
      "[232,    85] loss: 23622.667\n",
      "[232,    90] loss: 39907.826\n",
      "[232,    95] loss: 28735.839\n",
      "[232,   100] loss: 36671.277\n",
      "[232,   105] loss: 31178.744\n",
      "[232,   110] loss: 23881.515\n",
      "[232,   115] loss: 13614.232\n",
      "[232,   120] loss: 23653.764\n",
      "[232,   125] loss: 24904.895\n",
      "[232,   130] loss: 19413.993\n",
      "[232,   135] loss: 18443.934\n",
      "[232,   140] loss: 32142.365\n",
      "[232,   145] loss: 27106.377\n",
      "[232,   150] loss: 30481.050\n",
      "[232,   155] loss: 27319.238\n",
      "[232,   160] loss: 33038.794\n",
      "[232,   165] loss: 18245.597\n",
      "[232,   170] loss: 18415.809\n",
      "[232,   175] loss: 58221.236\n",
      "[232,   180] loss: 15538.797\n",
      "[232,   185] loss: 26001.366\n",
      "[232,   190] loss: 19370.404\n",
      "[232,   195] loss: 18925.629\n",
      "[232,   200] loss: 21460.187\n",
      "[232,   205] loss: 18135.653\n",
      "[232,   210] loss: 28896.875\n",
      "[232,   215] loss: 27893.799\n",
      "[232,   220] loss: 17300.441\n",
      "[232,   225] loss: 26038.279\n",
      "[232,   230] loss: 21566.218\n",
      "[233,     5] loss: 49140.180\n",
      "[233,    10] loss: 25443.888\n",
      "[233,    15] loss: 18473.075\n",
      "[233,    20] loss: 23271.079\n",
      "[233,    25] loss: 25476.385\n",
      "[233,    30] loss: 15881.911\n",
      "[233,    35] loss: 36416.408\n",
      "[233,    40] loss: 35033.210\n",
      "[233,    45] loss: 29969.345\n",
      "[233,    50] loss: 18342.757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233,    55] loss: 22061.247\n",
      "[233,    60] loss: 25815.257\n",
      "[233,    65] loss: 20194.864\n",
      "[233,    70] loss: 21054.177\n",
      "[233,    75] loss: 31682.666\n",
      "[233,    80] loss: 20592.703\n",
      "[233,    85] loss: 26503.356\n",
      "[233,    90] loss: 27084.859\n",
      "[233,    95] loss: 24485.510\n",
      "[233,   100] loss: 25165.938\n",
      "[233,   105] loss: 20432.970\n",
      "[233,   110] loss: 25917.916\n",
      "[233,   115] loss: 32130.573\n",
      "[233,   120] loss: 20058.084\n",
      "[233,   125] loss: 17230.829\n",
      "[233,   130] loss: 27616.305\n",
      "[233,   135] loss: 27246.511\n",
      "[233,   140] loss: 14923.202\n",
      "[233,   145] loss: 39233.743\n",
      "[233,   150] loss: 31468.910\n",
      "[233,   155] loss: 39299.025\n",
      "[233,   160] loss: 27805.882\n",
      "[233,   165] loss: 26206.728\n",
      "[233,   170] loss: 19464.968\n",
      "[233,   175] loss: 39220.824\n",
      "[233,   180] loss: 22642.999\n",
      "[233,   185] loss: 30782.090\n",
      "[233,   190] loss: 30000.867\n",
      "[233,   195] loss: 28836.676\n",
      "[233,   200] loss: 26359.796\n",
      "[233,   205] loss: 16494.209\n",
      "[233,   210] loss: 33423.111\n",
      "[233,   215] loss: 11032.168\n",
      "[233,   220] loss: 16730.376\n",
      "[233,   225] loss: 30513.960\n",
      "[233,   230] loss: 22669.268\n",
      "[234,     5] loss: 34872.282\n",
      "[234,    10] loss: 26557.242\n",
      "[234,    15] loss: 19753.871\n",
      "[234,    20] loss: 22693.721\n",
      "[234,    25] loss: 20119.420\n",
      "[234,    30] loss: 23463.722\n",
      "[234,    35] loss: 22141.562\n",
      "[234,    40] loss: 27537.822\n",
      "[234,    45] loss: 27250.727\n",
      "[234,    50] loss: 16831.841\n",
      "[234,    55] loss: 47845.451\n",
      "[234,    60] loss: 32633.287\n",
      "[234,    65] loss: 26541.737\n",
      "[234,    70] loss: 18894.300\n",
      "[234,    75] loss: 22209.842\n",
      "[234,    80] loss: 31828.410\n",
      "[234,    85] loss: 24257.884\n",
      "[234,    90] loss: 22023.663\n",
      "[234,    95] loss: 38699.307\n",
      "[234,   100] loss: 32377.126\n",
      "[234,   105] loss: 24798.004\n",
      "[234,   110] loss: 17623.326\n",
      "[234,   115] loss: 20190.320\n",
      "[234,   120] loss: 24885.719\n",
      "[234,   125] loss: 28101.316\n",
      "[234,   130] loss: 24515.727\n",
      "[234,   135] loss: 32003.549\n",
      "[234,   140] loss: 24760.082\n",
      "[234,   145] loss: 29457.426\n",
      "[234,   150] loss: 22567.455\n",
      "[234,   155] loss: 23947.005\n",
      "[234,   160] loss: 29654.093\n",
      "[234,   165] loss: 16739.146\n",
      "[234,   170] loss: 22567.331\n",
      "[234,   175] loss: 17835.917\n",
      "[234,   180] loss: 26862.702\n",
      "[234,   185] loss: 17984.246\n",
      "[234,   190] loss: 31442.251\n",
      "[234,   195] loss: 29461.088\n",
      "[234,   200] loss: 22054.089\n",
      "[234,   205] loss: 33357.647\n",
      "[234,   210] loss: 26081.339\n",
      "[234,   215] loss: 29469.132\n",
      "[234,   220] loss: 30808.587\n",
      "[234,   225] loss: 32425.032\n",
      "[234,   230] loss: 24737.155\n",
      "[235,     5] loss: 26268.516\n",
      "[235,    10] loss: 30162.160\n",
      "[235,    15] loss: 23984.342\n",
      "[235,    20] loss: 28239.822\n",
      "[235,    25] loss: 23251.032\n",
      "[235,    30] loss: 32268.112\n",
      "[235,    35] loss: 21617.919\n",
      "[235,    40] loss: 21571.400\n",
      "[235,    45] loss: 37161.515\n",
      "[235,    50] loss: 22726.548\n",
      "[235,    55] loss: 28600.729\n",
      "[235,    60] loss: 25127.414\n",
      "[235,    65] loss: 24536.808\n",
      "[235,    70] loss: 29912.385\n",
      "[235,    75] loss: 29803.638\n",
      "[235,    80] loss: 33185.289\n",
      "[235,    85] loss: 20482.445\n",
      "[235,    90] loss: 22127.786\n",
      "[235,    95] loss: 23400.897\n",
      "[235,   100] loss: 27502.097\n",
      "[235,   105] loss: 20464.936\n",
      "[235,   110] loss: 31292.767\n",
      "[235,   115] loss: 31718.912\n",
      "[235,   120] loss: 22713.415\n",
      "[235,   125] loss: 24777.232\n",
      "[235,   130] loss: 21364.424\n",
      "[235,   135] loss: 24485.564\n",
      "[235,   140] loss: 25204.359\n",
      "[235,   145] loss: 24116.357\n",
      "[235,   150] loss: 24463.866\n",
      "[235,   155] loss: 27685.712\n",
      "[235,   160] loss: 48772.189\n",
      "[235,   165] loss: 22760.364\n",
      "[235,   170] loss: 19201.994\n",
      "[235,   175] loss: 30838.148\n",
      "[235,   180] loss: 25977.199\n",
      "[235,   185] loss: 21542.124\n",
      "[235,   190] loss: 21317.565\n",
      "[235,   195] loss: 20249.103\n",
      "[235,   200] loss: 29266.948\n",
      "[235,   205] loss: 19433.795\n",
      "[235,   210] loss: 42532.686\n",
      "[235,   215] loss: 23610.318\n",
      "[235,   220] loss: 20758.082\n",
      "[235,   225] loss: 22556.043\n",
      "[235,   230] loss: 23773.399\n",
      "[236,     5] loss: 20568.942\n",
      "[236,    10] loss: 19223.653\n",
      "[236,    15] loss: 23651.516\n",
      "[236,    20] loss: 26337.472\n",
      "[236,    25] loss: 26043.163\n",
      "[236,    30] loss: 30968.500\n",
      "[236,    35] loss: 35529.357\n",
      "[236,    40] loss: 20644.852\n",
      "[236,    45] loss: 25388.698\n",
      "[236,    50] loss: 19011.341\n",
      "[236,    55] loss: 31220.980\n",
      "[236,    60] loss: 22062.680\n",
      "[236,    65] loss: 24908.447\n",
      "[236,    70] loss: 26867.383\n",
      "[236,    75] loss: 30443.853\n",
      "[236,    80] loss: 23281.214\n",
      "[236,    85] loss: 28408.097\n",
      "[236,    90] loss: 12636.152\n",
      "[236,    95] loss: 33155.394\n",
      "[236,   100] loss: 17811.054\n",
      "[236,   105] loss: 30926.799\n",
      "[236,   110] loss: 31467.368\n",
      "[236,   115] loss: 45054.019\n",
      "[236,   120] loss: 19236.691\n",
      "[236,   125] loss: 26309.545\n",
      "[236,   130] loss: 21986.472\n",
      "[236,   135] loss: 38550.690\n",
      "[236,   140] loss: 19747.733\n",
      "[236,   145] loss: 20783.265\n",
      "[236,   150] loss: 21353.637\n",
      "[236,   155] loss: 16817.656\n",
      "[236,   160] loss: 43055.550\n",
      "[236,   165] loss: 36728.189\n",
      "[236,   170] loss: 26774.985\n",
      "[236,   175] loss: 26908.070\n",
      "[236,   180] loss: 20910.335\n",
      "[236,   185] loss: 28769.152\n",
      "[236,   190] loss: 15792.676\n",
      "[236,   195] loss: 28424.939\n",
      "[236,   200] loss: 29532.609\n",
      "[236,   205] loss: 25819.666\n",
      "[236,   210] loss: 24934.928\n",
      "[236,   215] loss: 25581.405\n",
      "[236,   220] loss: 19724.592\n",
      "[236,   225] loss: 33651.368\n",
      "[236,   230] loss: 29187.238\n",
      "[237,     5] loss: 32714.529\n",
      "[237,    10] loss: 21698.544\n",
      "[237,    15] loss: 21016.085\n",
      "[237,    20] loss: 22368.225\n",
      "[237,    25] loss: 30494.403\n",
      "[237,    30] loss: 35862.941\n",
      "[237,    35] loss: 24096.175\n",
      "[237,    40] loss: 42679.749\n",
      "[237,    45] loss: 13674.506\n",
      "[237,    50] loss: 16985.992\n",
      "[237,    55] loss: 19286.263\n",
      "[237,    60] loss: 21290.018\n",
      "[237,    65] loss: 26266.544\n",
      "[237,    70] loss: 24508.029\n",
      "[237,    75] loss: 21957.385\n",
      "[237,    80] loss: 27168.454\n",
      "[237,    85] loss: 27346.350\n",
      "[237,    90] loss: 24870.561\n",
      "[237,    95] loss: 24305.024\n",
      "[237,   100] loss: 21636.132\n",
      "[237,   105] loss: 24120.181\n",
      "[237,   110] loss: 23066.770\n",
      "[237,   115] loss: 22738.143\n",
      "[237,   120] loss: 26276.357\n",
      "[237,   125] loss: 22007.686\n",
      "[237,   130] loss: 24773.324\n",
      "[237,   135] loss: 23969.904\n",
      "[237,   140] loss: 29643.668\n",
      "[237,   145] loss: 24537.936\n",
      "[237,   150] loss: 22682.722\n",
      "[237,   155] loss: 35317.461\n",
      "[237,   160] loss: 21497.994\n",
      "[237,   165] loss: 20284.779\n",
      "[237,   170] loss: 24501.313\n",
      "[237,   175] loss: 22693.268\n",
      "[237,   180] loss: 31168.196\n",
      "[237,   185] loss: 43532.115\n",
      "[237,   190] loss: 25519.289\n",
      "[237,   195] loss: 32482.076\n",
      "[237,   200] loss: 23769.034\n",
      "[237,   205] loss: 25071.528\n",
      "[237,   210] loss: 32067.047\n",
      "[237,   215] loss: 27976.358\n",
      "[237,   220] loss: 23931.996\n",
      "[237,   225] loss: 25551.930\n",
      "[237,   230] loss: 39823.795\n",
      "[238,     5] loss: 19812.642\n",
      "[238,    10] loss: 22057.103\n",
      "[238,    15] loss: 17338.860\n",
      "[238,    20] loss: 30599.909\n",
      "[238,    25] loss: 23931.159\n",
      "[238,    30] loss: 23740.182\n",
      "[238,    35] loss: 31193.421\n",
      "[238,    40] loss: 33453.589\n",
      "[238,    45] loss: 20781.399\n",
      "[238,    50] loss: 23853.529\n",
      "[238,    55] loss: 23625.882\n",
      "[238,    60] loss: 28336.565\n",
      "[238,    65] loss: 27460.156\n",
      "[238,    70] loss: 24906.906\n",
      "[238,    75] loss: 29111.692\n",
      "[238,    80] loss: 32944.114\n",
      "[238,    85] loss: 34182.960\n",
      "[238,    90] loss: 22151.924\n",
      "[238,    95] loss: 19651.409\n",
      "[238,   100] loss: 18978.408\n",
      "[238,   105] loss: 29904.776\n",
      "[238,   110] loss: 21747.770\n",
      "[238,   115] loss: 22557.131\n",
      "[238,   120] loss: 47214.144\n",
      "[238,   125] loss: 36521.809\n",
      "[238,   130] loss: 26842.915\n",
      "[238,   135] loss: 20776.027\n",
      "[238,   140] loss: 25404.698\n",
      "[238,   145] loss: 27853.380\n",
      "[238,   150] loss: 26752.602\n",
      "[238,   155] loss: 23712.522\n",
      "[238,   160] loss: 24984.106\n",
      "[238,   165] loss: 36359.681\n",
      "[238,   170] loss: 25830.218\n",
      "[238,   175] loss: 23297.580\n",
      "[238,   180] loss: 19250.037\n",
      "[238,   185] loss: 45668.936\n",
      "[238,   190] loss: 23110.434\n",
      "[238,   195] loss: 25853.421\n",
      "[238,   200] loss: 18925.287\n",
      "[238,   205] loss: 18533.664\n",
      "[238,   210] loss: 26047.927\n",
      "[238,   215] loss: 14205.650\n",
      "[238,   220] loss: 28154.082\n",
      "[238,   225] loss: 18786.067\n",
      "[238,   230] loss: 29476.575\n",
      "[239,     5] loss: 25155.740\n",
      "[239,    10] loss: 24869.805\n",
      "[239,    15] loss: 22002.077\n",
      "[239,    20] loss: 26195.850\n",
      "[239,    25] loss: 26205.149\n",
      "[239,    30] loss: 20434.029\n",
      "[239,    35] loss: 22759.795\n",
      "[239,    40] loss: 27998.294\n",
      "[239,    45] loss: 25700.432\n",
      "[239,    50] loss: 28639.500\n",
      "[239,    55] loss: 19636.987\n",
      "[239,    60] loss: 39696.084\n",
      "[239,    65] loss: 16095.174\n",
      "[239,    70] loss: 26167.973\n",
      "[239,    75] loss: 30031.887\n",
      "[239,    80] loss: 27834.125\n",
      "[239,    85] loss: 16803.678\n",
      "[239,    90] loss: 23751.656\n",
      "[239,    95] loss: 24800.193\n",
      "[239,   100] loss: 22905.955\n",
      "[239,   105] loss: 27718.009\n",
      "[239,   110] loss: 31211.423\n",
      "[239,   115] loss: 23359.333\n",
      "[239,   120] loss: 30086.970\n",
      "[239,   125] loss: 22176.657\n",
      "[239,   130] loss: 12024.007\n",
      "[239,   135] loss: 19599.246\n",
      "[239,   140] loss: 54794.571\n",
      "[239,   145] loss: 24615.185\n",
      "[239,   150] loss: 22397.061\n",
      "[239,   155] loss: 24249.651\n",
      "[239,   160] loss: 28288.495\n",
      "[239,   165] loss: 24718.333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239,   170] loss: 33806.478\n",
      "[239,   175] loss: 21715.548\n",
      "[239,   180] loss: 17657.891\n",
      "[239,   185] loss: 33777.185\n",
      "[239,   190] loss: 16754.998\n",
      "[239,   195] loss: 21752.423\n",
      "[239,   200] loss: 29191.665\n",
      "[239,   205] loss: 33863.052\n",
      "[239,   210] loss: 22592.800\n",
      "[239,   215] loss: 26031.047\n",
      "[239,   220] loss: 26790.581\n",
      "[239,   225] loss: 23727.889\n",
      "[239,   230] loss: 38569.625\n",
      "[240,     5] loss: 18183.326\n",
      "[240,    10] loss: 23441.362\n",
      "[240,    15] loss: 20953.686\n",
      "[240,    20] loss: 30280.435\n",
      "[240,    25] loss: 18845.032\n",
      "[240,    30] loss: 16976.608\n",
      "[240,    35] loss: 20713.397\n",
      "[240,    40] loss: 16627.883\n",
      "[240,    45] loss: 20632.059\n",
      "[240,    50] loss: 22233.840\n",
      "[240,    55] loss: 28175.000\n",
      "[240,    60] loss: 28047.237\n",
      "[240,    65] loss: 28625.925\n",
      "[240,    70] loss: 20874.566\n",
      "[240,    75] loss: 30729.338\n",
      "[240,    80] loss: 23298.718\n",
      "[240,    85] loss: 25895.238\n",
      "[240,    90] loss: 34189.177\n",
      "[240,    95] loss: 25940.106\n",
      "[240,   100] loss: 27091.702\n",
      "[240,   105] loss: 28388.797\n",
      "[240,   110] loss: 26221.027\n",
      "[240,   115] loss: 32319.206\n",
      "[240,   120] loss: 23363.986\n",
      "[240,   125] loss: 27346.393\n",
      "[240,   130] loss: 30772.611\n",
      "[240,   135] loss: 31280.686\n",
      "[240,   140] loss: 41745.326\n",
      "[240,   145] loss: 50856.900\n",
      "[240,   150] loss: 31273.349\n",
      "[240,   155] loss: 33902.913\n",
      "[240,   160] loss: 15050.927\n",
      "[240,   165] loss: 25125.183\n",
      "[240,   170] loss: 32024.419\n",
      "[240,   175] loss: 13176.899\n",
      "[240,   180] loss: 24978.539\n",
      "[240,   185] loss: 25931.986\n",
      "[240,   190] loss: 26396.705\n",
      "[240,   195] loss: 23879.137\n",
      "[240,   200] loss: 24601.806\n",
      "[240,   205] loss: 19918.802\n",
      "[240,   210] loss: 30045.137\n",
      "[240,   215] loss: 24102.647\n",
      "[240,   220] loss: 19556.521\n",
      "[240,   225] loss: 27651.746\n",
      "[240,   230] loss: 26096.733\n",
      "[241,     5] loss: 31232.244\n",
      "[241,    10] loss: 36641.942\n",
      "[241,    15] loss: 27880.314\n",
      "[241,    20] loss: 19412.911\n",
      "[241,    25] loss: 20375.162\n",
      "[241,    30] loss: 25573.479\n",
      "[241,    35] loss: 18611.236\n",
      "[241,    40] loss: 29819.537\n",
      "[241,    45] loss: 36989.293\n",
      "[241,    50] loss: 21467.998\n",
      "[241,    55] loss: 25279.160\n",
      "[241,    60] loss: 28498.624\n",
      "[241,    65] loss: 22156.338\n",
      "[241,    70] loss: 25774.834\n",
      "[241,    75] loss: 25257.229\n",
      "[241,    80] loss: 20515.983\n",
      "[241,    85] loss: 30126.475\n",
      "[241,    90] loss: 22277.603\n",
      "[241,    95] loss: 18356.526\n",
      "[241,   100] loss: 20576.167\n",
      "[241,   105] loss: 23438.120\n",
      "[241,   110] loss: 45522.524\n",
      "[241,   115] loss: 29306.995\n",
      "[241,   120] loss: 25760.966\n",
      "[241,   125] loss: 33665.374\n",
      "[241,   130] loss: 27069.986\n",
      "[241,   135] loss: 20630.570\n",
      "[241,   140] loss: 31613.817\n",
      "[241,   145] loss: 27407.314\n",
      "[241,   150] loss: 19399.735\n",
      "[241,   155] loss: 18939.447\n",
      "[241,   160] loss: 34187.807\n",
      "[241,   165] loss: 20295.802\n",
      "[241,   170] loss: 24489.353\n",
      "[241,   175] loss: 20900.849\n",
      "[241,   180] loss: 19660.778\n",
      "[241,   185] loss: 56050.086\n",
      "[241,   190] loss: 16151.069\n",
      "[241,   195] loss: 22747.910\n",
      "[241,   200] loss: 31156.230\n",
      "[241,   205] loss: 21634.533\n",
      "[241,   210] loss: 18624.084\n",
      "[241,   215] loss: 25934.638\n",
      "[241,   220] loss: 17753.762\n",
      "[241,   225] loss: 22910.645\n",
      "[241,   230] loss: 32936.726\n",
      "[242,     5] loss: 24669.196\n",
      "[242,    10] loss: 14420.730\n",
      "[242,    15] loss: 27554.538\n",
      "[242,    20] loss: 30369.992\n",
      "[242,    25] loss: 28631.250\n",
      "[242,    30] loss: 27013.728\n",
      "[242,    35] loss: 29896.491\n",
      "[242,    40] loss: 18133.193\n",
      "[242,    45] loss: 23904.623\n",
      "[242,    50] loss: 30147.901\n",
      "[242,    55] loss: 23765.259\n",
      "[242,    60] loss: 23452.499\n",
      "[242,    65] loss: 19826.889\n",
      "[242,    70] loss: 18204.719\n",
      "[242,    75] loss: 36003.685\n",
      "[242,    80] loss: 26506.517\n",
      "[242,    85] loss: 21993.640\n",
      "[242,    90] loss: 22216.170\n",
      "[242,    95] loss: 20276.188\n",
      "[242,   100] loss: 17896.421\n",
      "[242,   105] loss: 22048.561\n",
      "[242,   110] loss: 29274.696\n",
      "[242,   115] loss: 38031.753\n",
      "[242,   120] loss: 36924.254\n",
      "[242,   125] loss: 23124.975\n",
      "[242,   130] loss: 26692.838\n",
      "[242,   135] loss: 24846.311\n",
      "[242,   140] loss: 49149.910\n",
      "[242,   145] loss: 25183.192\n",
      "[242,   150] loss: 38631.673\n",
      "[242,   155] loss: 19068.474\n",
      "[242,   160] loss: 24790.370\n",
      "[242,   165] loss: 18389.545\n",
      "[242,   170] loss: 27004.835\n",
      "[242,   175] loss: 17271.938\n",
      "[242,   180] loss: 28295.943\n",
      "[242,   185] loss: 26337.403\n",
      "[242,   190] loss: 25603.448\n",
      "[242,   195] loss: 21401.463\n",
      "[242,   200] loss: 29748.376\n",
      "[242,   205] loss: 27004.925\n",
      "[242,   210] loss: 27324.310\n",
      "[242,   215] loss: 27601.708\n",
      "[242,   220] loss: 36054.896\n",
      "[242,   225] loss: 24970.535\n",
      "[242,   230] loss: 23478.121\n",
      "[243,     5] loss: 28990.659\n",
      "[243,    10] loss: 13759.688\n",
      "[243,    15] loss: 26320.850\n",
      "[243,    20] loss: 30211.336\n",
      "[243,    25] loss: 16004.549\n",
      "[243,    30] loss: 35397.779\n",
      "[243,    35] loss: 27360.832\n",
      "[243,    40] loss: 24171.457\n",
      "[243,    45] loss: 20751.355\n",
      "[243,    50] loss: 30051.346\n",
      "[243,    55] loss: 29602.641\n",
      "[243,    60] loss: 20591.857\n",
      "[243,    65] loss: 27209.076\n",
      "[243,    70] loss: 22267.162\n",
      "[243,    75] loss: 27676.595\n",
      "[243,    80] loss: 19129.798\n",
      "[243,    85] loss: 23977.101\n",
      "[243,    90] loss: 23005.861\n",
      "[243,    95] loss: 28790.805\n",
      "[243,   100] loss: 29197.527\n",
      "[243,   105] loss: 19553.260\n",
      "[243,   110] loss: 25881.211\n",
      "[243,   115] loss: 27877.706\n",
      "[243,   120] loss: 25333.394\n",
      "[243,   125] loss: 20334.193\n",
      "[243,   130] loss: 23705.070\n",
      "[243,   135] loss: 33121.996\n",
      "[243,   140] loss: 25363.384\n",
      "[243,   145] loss: 15660.348\n",
      "[243,   150] loss: 20886.993\n",
      "[243,   155] loss: 26273.478\n",
      "[243,   160] loss: 28813.035\n",
      "[243,   165] loss: 22948.151\n",
      "[243,   170] loss: 25398.230\n",
      "[243,   175] loss: 28014.415\n",
      "[243,   180] loss: 22764.775\n",
      "[243,   185] loss: 37191.119\n",
      "[243,   190] loss: 25032.133\n",
      "[243,   195] loss: 26196.666\n",
      "[243,   200] loss: 22647.468\n",
      "[243,   205] loss: 62691.847\n",
      "[243,   210] loss: 28719.697\n",
      "[243,   215] loss: 29970.762\n",
      "[243,   220] loss: 28830.396\n",
      "[243,   225] loss: 21986.002\n",
      "[243,   230] loss: 24871.623\n",
      "[244,     5] loss: 25211.191\n",
      "[244,    10] loss: 22109.152\n",
      "[244,    15] loss: 19374.310\n",
      "[244,    20] loss: 25345.596\n",
      "[244,    25] loss: 31376.827\n",
      "[244,    30] loss: 22697.362\n",
      "[244,    35] loss: 31263.488\n",
      "[244,    40] loss: 29636.830\n",
      "[244,    45] loss: 59729.298\n",
      "[244,    50] loss: 21014.565\n",
      "[244,    55] loss: 16785.285\n",
      "[244,    60] loss: 31944.059\n",
      "[244,    65] loss: 18758.382\n",
      "[244,    70] loss: 23658.654\n",
      "[244,    75] loss: 21331.118\n",
      "[244,    80] loss: 32233.475\n",
      "[244,    85] loss: 25407.502\n",
      "[244,    90] loss: 18818.958\n",
      "[244,    95] loss: 21923.010\n",
      "[244,   100] loss: 24809.460\n",
      "[244,   105] loss: 36193.555\n",
      "[244,   110] loss: 25899.863\n",
      "[244,   115] loss: 22990.510\n",
      "[244,   120] loss: 31226.630\n",
      "[244,   125] loss: 24319.094\n",
      "[244,   130] loss: 23611.187\n",
      "[244,   135] loss: 27466.738\n",
      "[244,   140] loss: 20949.612\n",
      "[244,   145] loss: 17873.694\n",
      "[244,   150] loss: 22267.018\n",
      "[244,   155] loss: 19735.791\n",
      "[244,   160] loss: 18058.233\n",
      "[244,   165] loss: 30829.560\n",
      "[244,   170] loss: 33720.427\n",
      "[244,   175] loss: 22752.302\n",
      "[244,   180] loss: 31332.456\n",
      "[244,   185] loss: 23428.269\n",
      "[244,   190] loss: 19587.938\n",
      "[244,   195] loss: 20985.649\n",
      "[244,   200] loss: 36591.619\n",
      "[244,   205] loss: 33676.390\n",
      "[244,   210] loss: 20343.911\n",
      "[244,   215] loss: 23484.263\n",
      "[244,   220] loss: 26534.130\n",
      "[244,   225] loss: 34774.720\n",
      "[244,   230] loss: 26085.541\n",
      "[245,     5] loss: 17092.097\n",
      "[245,    10] loss: 27347.985\n",
      "[245,    15] loss: 20573.574\n",
      "[245,    20] loss: 15688.933\n",
      "[245,    25] loss: 33347.390\n",
      "[245,    30] loss: 33830.092\n",
      "[245,    35] loss: 25558.963\n",
      "[245,    40] loss: 29946.713\n",
      "[245,    45] loss: 25698.948\n",
      "[245,    50] loss: 19356.712\n",
      "[245,    55] loss: 25925.207\n",
      "[245,    60] loss: 32575.255\n",
      "[245,    65] loss: 25250.158\n",
      "[245,    70] loss: 16837.708\n",
      "[245,    75] loss: 22044.355\n",
      "[245,    80] loss: 18705.259\n",
      "[245,    85] loss: 28091.697\n",
      "[245,    90] loss: 24992.412\n",
      "[245,    95] loss: 19560.621\n",
      "[245,   100] loss: 24431.779\n",
      "[245,   105] loss: 25604.273\n",
      "[245,   110] loss: 24159.402\n",
      "[245,   115] loss: 30268.503\n",
      "[245,   120] loss: 25664.806\n",
      "[245,   125] loss: 21590.364\n",
      "[245,   130] loss: 27413.295\n",
      "[245,   135] loss: 48202.810\n",
      "[245,   140] loss: 26614.773\n",
      "[245,   145] loss: 19018.126\n",
      "[245,   150] loss: 19426.010\n",
      "[245,   155] loss: 21746.755\n",
      "[245,   160] loss: 34364.269\n",
      "[245,   165] loss: 17417.428\n",
      "[245,   170] loss: 25547.482\n",
      "[245,   175] loss: 20669.622\n",
      "[245,   180] loss: 24040.838\n",
      "[245,   185] loss: 23443.884\n",
      "[245,   190] loss: 27478.582\n",
      "[245,   195] loss: 21991.936\n",
      "[245,   200] loss: 18406.740\n",
      "[245,   205] loss: 39097.782\n",
      "[245,   210] loss: 63707.571\n",
      "[245,   215] loss: 29612.287\n",
      "[245,   220] loss: 23288.629\n",
      "[245,   225] loss: 23496.362\n",
      "[245,   230] loss: 27942.662\n",
      "[246,     5] loss: 14730.625\n",
      "[246,    10] loss: 24425.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[246,    15] loss: 26478.713\n",
      "[246,    20] loss: 22552.076\n",
      "[246,    25] loss: 30635.771\n",
      "[246,    30] loss: 28794.033\n",
      "[246,    35] loss: 34123.759\n",
      "[246,    40] loss: 25072.667\n",
      "[246,    45] loss: 18453.492\n",
      "[246,    50] loss: 33501.017\n",
      "[246,    55] loss: 30068.010\n",
      "[246,    60] loss: 24675.489\n",
      "[246,    65] loss: 26439.020\n",
      "[246,    70] loss: 19324.586\n",
      "[246,    75] loss: 43208.657\n",
      "[246,    80] loss: 18799.845\n",
      "[246,    85] loss: 24394.558\n",
      "[246,    90] loss: 17949.046\n",
      "[246,    95] loss: 18728.276\n",
      "[246,   100] loss: 29315.352\n",
      "[246,   105] loss: 27141.138\n",
      "[246,   110] loss: 22986.999\n",
      "[246,   115] loss: 16750.468\n",
      "[246,   120] loss: 28623.296\n",
      "[246,   125] loss: 24246.772\n",
      "[246,   130] loss: 34370.522\n",
      "[246,   135] loss: 20685.229\n",
      "[246,   140] loss: 26519.862\n",
      "[246,   145] loss: 23078.059\n",
      "[246,   150] loss: 40399.482\n",
      "[246,   155] loss: 38211.662\n",
      "[246,   160] loss: 33118.839\n",
      "[246,   165] loss: 22811.313\n",
      "[246,   170] loss: 19400.029\n",
      "[246,   175] loss: 26921.850\n",
      "[246,   180] loss: 22151.217\n",
      "[246,   185] loss: 25698.611\n",
      "[246,   190] loss: 36010.765\n",
      "[246,   195] loss: 15599.888\n",
      "[246,   200] loss: 26812.874\n",
      "[246,   205] loss: 24948.900\n",
      "[246,   210] loss: 30880.823\n",
      "[246,   215] loss: 27260.866\n",
      "[246,   220] loss: 28415.482\n",
      "[246,   225] loss: 23383.052\n",
      "[246,   230] loss: 20494.551\n",
      "[247,     5] loss: 19980.224\n",
      "[247,    10] loss: 29685.175\n",
      "[247,    15] loss: 23667.723\n",
      "[247,    20] loss: 22710.694\n",
      "[247,    25] loss: 21993.589\n",
      "[247,    30] loss: 21959.618\n",
      "[247,    35] loss: 20260.309\n",
      "[247,    40] loss: 34769.670\n",
      "[247,    45] loss: 30036.055\n",
      "[247,    50] loss: 29497.567\n",
      "[247,    55] loss: 20681.733\n",
      "[247,    60] loss: 55902.842\n",
      "[247,    65] loss: 21322.622\n",
      "[247,    70] loss: 33791.196\n",
      "[247,    75] loss: 21123.225\n",
      "[247,    80] loss: 21959.251\n",
      "[247,    85] loss: 22184.927\n",
      "[247,    90] loss: 31165.872\n",
      "[247,    95] loss: 24039.075\n",
      "[247,   100] loss: 31946.521\n",
      "[247,   105] loss: 26740.871\n",
      "[247,   110] loss: 13687.932\n",
      "[247,   115] loss: 39732.009\n",
      "[247,   120] loss: 24362.648\n",
      "[247,   125] loss: 36878.747\n",
      "[247,   130] loss: 25216.581\n",
      "[247,   135] loss: 18990.511\n",
      "[247,   140] loss: 23934.303\n",
      "[247,   145] loss: 20791.870\n",
      "[247,   150] loss: 15871.960\n",
      "[247,   155] loss: 31793.129\n",
      "[247,   160] loss: 34134.048\n",
      "[247,   165] loss: 21163.405\n",
      "[247,   170] loss: 30689.516\n",
      "[247,   175] loss: 24451.919\n",
      "[247,   180] loss: 18990.668\n",
      "[247,   185] loss: 25002.564\n",
      "[247,   190] loss: 27682.375\n",
      "[247,   195] loss: 27686.657\n",
      "[247,   200] loss: 24784.271\n",
      "[247,   205] loss: 23883.930\n",
      "[247,   210] loss: 21492.673\n",
      "[247,   215] loss: 22613.181\n",
      "[247,   220] loss: 38380.904\n",
      "[247,   225] loss: 22026.826\n",
      "[247,   230] loss: 19685.251\n",
      "[248,     5] loss: 18303.132\n",
      "[248,    10] loss: 27430.333\n",
      "[248,    15] loss: 31362.968\n",
      "[248,    20] loss: 23816.168\n",
      "[248,    25] loss: 24762.541\n",
      "[248,    30] loss: 20070.285\n",
      "[248,    35] loss: 56230.425\n",
      "[248,    40] loss: 21110.325\n",
      "[248,    45] loss: 28420.412\n",
      "[248,    50] loss: 33712.652\n",
      "[248,    55] loss: 28789.978\n",
      "[248,    60] loss: 16528.591\n",
      "[248,    65] loss: 19574.896\n",
      "[248,    70] loss: 24583.553\n",
      "[248,    75] loss: 34097.585\n",
      "[248,    80] loss: 19878.238\n",
      "[248,    85] loss: 23830.145\n",
      "[248,    90] loss: 24666.654\n",
      "[248,    95] loss: 21012.734\n",
      "[248,   100] loss: 24780.017\n",
      "[248,   105] loss: 32056.632\n",
      "[248,   110] loss: 31444.659\n",
      "[248,   115] loss: 32511.885\n",
      "[248,   120] loss: 18504.920\n",
      "[248,   125] loss: 28240.943\n",
      "[248,   130] loss: 22585.312\n",
      "[248,   135] loss: 25166.712\n",
      "[248,   140] loss: 26969.061\n",
      "[248,   145] loss: 27969.812\n",
      "[248,   150] loss: 22963.912\n",
      "[248,   155] loss: 23845.940\n",
      "[248,   160] loss: 20935.853\n",
      "[248,   165] loss: 19736.853\n",
      "[248,   170] loss: 20639.299\n",
      "[248,   175] loss: 26392.287\n",
      "[248,   180] loss: 33249.822\n",
      "[248,   185] loss: 24088.305\n",
      "[248,   190] loss: 29701.338\n",
      "[248,   195] loss: 25737.097\n",
      "[248,   200] loss: 47823.799\n",
      "[248,   205] loss: 23236.213\n",
      "[248,   210] loss: 18374.287\n",
      "[248,   215] loss: 28737.755\n",
      "[248,   220] loss: 22836.642\n",
      "[248,   225] loss: 28548.939\n",
      "[248,   230] loss: 15654.229\n",
      "[249,     5] loss: 14898.016\n",
      "[249,    10] loss: 41788.469\n",
      "[249,    15] loss: 13863.994\n",
      "[249,    20] loss: 26244.797\n",
      "[249,    25] loss: 27481.857\n",
      "[249,    30] loss: 36732.361\n",
      "[249,    35] loss: 21145.101\n",
      "[249,    40] loss: 21566.194\n",
      "[249,    45] loss: 19720.344\n",
      "[249,    50] loss: 29885.794\n",
      "[249,    55] loss: 28375.555\n",
      "[249,    60] loss: 18204.088\n",
      "[249,    65] loss: 31234.452\n",
      "[249,    70] loss: 19979.904\n",
      "[249,    75] loss: 25438.613\n",
      "[249,    80] loss: 20466.614\n",
      "[249,    85] loss: 24677.670\n",
      "[249,    90] loss: 33245.793\n",
      "[249,    95] loss: 19259.349\n",
      "[249,   100] loss: 22622.474\n",
      "[249,   105] loss: 32520.304\n",
      "[249,   110] loss: 27434.948\n",
      "[249,   115] loss: 21366.408\n",
      "[249,   120] loss: 13578.606\n",
      "[249,   125] loss: 36367.168\n",
      "[249,   130] loss: 32419.483\n",
      "[249,   135] loss: 35270.221\n",
      "[249,   140] loss: 28412.077\n",
      "[249,   145] loss: 19629.126\n",
      "[249,   150] loss: 20936.820\n",
      "[249,   155] loss: 24880.551\n",
      "[249,   160] loss: 27420.588\n",
      "[249,   165] loss: 26013.744\n",
      "[249,   170] loss: 24873.277\n",
      "[249,   175] loss: 25307.513\n",
      "[249,   180] loss: 24258.039\n",
      "[249,   185] loss: 22284.292\n",
      "[249,   190] loss: 38540.543\n",
      "[249,   195] loss: 27574.789\n",
      "[249,   200] loss: 29440.138\n",
      "[249,   205] loss: 23259.065\n",
      "[249,   210] loss: 43959.224\n",
      "[249,   215] loss: 22710.653\n",
      "[249,   220] loss: 27971.369\n",
      "[249,   225] loss: 27878.193\n",
      "[249,   230] loss: 18242.885\n",
      "[250,     5] loss: 12581.507\n",
      "[250,    10] loss: 24831.612\n",
      "[250,    15] loss: 21711.081\n",
      "[250,    20] loss: 21118.964\n",
      "[250,    25] loss: 22587.364\n",
      "[250,    30] loss: 25161.902\n",
      "[250,    35] loss: 34799.546\n",
      "[250,    40] loss: 26499.696\n",
      "[250,    45] loss: 26777.472\n",
      "[250,    50] loss: 47743.822\n",
      "[250,    55] loss: 29294.662\n",
      "[250,    60] loss: 23801.382\n",
      "[250,    65] loss: 28619.429\n",
      "[250,    70] loss: 24292.263\n",
      "[250,    75] loss: 41850.418\n",
      "[250,    80] loss: 31041.662\n",
      "[250,    85] loss: 30604.334\n",
      "[250,    90] loss: 23704.691\n",
      "[250,    95] loss: 20929.753\n",
      "[250,   100] loss: 34501.023\n",
      "[250,   105] loss: 29755.385\n",
      "[250,   110] loss: 19647.206\n",
      "[250,   115] loss: 20346.828\n",
      "[250,   120] loss: 30246.561\n",
      "[250,   125] loss: 18089.873\n",
      "[250,   130] loss: 26856.352\n",
      "[250,   135] loss: 27359.673\n",
      "[250,   140] loss: 17998.330\n",
      "[250,   145] loss: 28751.809\n",
      "[250,   150] loss: 29626.641\n",
      "[250,   155] loss: 24309.672\n",
      "[250,   160] loss: 31068.448\n",
      "[250,   165] loss: 23876.608\n",
      "[250,   170] loss: 14608.774\n",
      "[250,   175] loss: 21830.581\n",
      "[250,   180] loss: 23397.006\n",
      "[250,   185] loss: 20167.560\n",
      "[250,   190] loss: 22221.708\n",
      "[250,   195] loss: 22203.703\n",
      "[250,   200] loss: 43421.463\n",
      "[250,   205] loss: 25258.893\n",
      "[250,   210] loss: 22045.147\n",
      "[250,   215] loss: 33651.507\n",
      "[250,   220] loss: 24260.380\n",
      "[250,   225] loss: 27166.691\n",
      "[250,   230] loss: 21376.983\n",
      "[251,     5] loss: 25140.803\n",
      "[251,    10] loss: 26879.920\n",
      "[251,    15] loss: 27837.479\n",
      "[251,    20] loss: 31665.636\n",
      "[251,    25] loss: 19179.675\n",
      "[251,    30] loss: 24124.655\n",
      "[251,    35] loss: 20293.232\n",
      "[251,    40] loss: 32825.794\n",
      "[251,    45] loss: 43279.610\n",
      "[251,    50] loss: 26074.899\n",
      "[251,    55] loss: 25499.716\n",
      "[251,    60] loss: 25412.681\n",
      "[251,    65] loss: 26203.597\n",
      "[251,    70] loss: 23993.965\n",
      "[251,    75] loss: 35398.800\n",
      "[251,    80] loss: 20663.007\n",
      "[251,    85] loss: 24567.199\n",
      "[251,    90] loss: 26230.491\n",
      "[251,    95] loss: 19200.059\n",
      "[251,   100] loss: 27659.627\n",
      "[251,   105] loss: 24956.210\n",
      "[251,   110] loss: 20202.821\n",
      "[251,   115] loss: 28305.750\n",
      "[251,   120] loss: 25927.180\n",
      "[251,   125] loss: 32663.969\n",
      "[251,   130] loss: 24650.846\n",
      "[251,   135] loss: 22526.366\n",
      "[251,   140] loss: 25218.827\n",
      "[251,   145] loss: 16021.307\n",
      "[251,   150] loss: 27854.050\n",
      "[251,   155] loss: 35671.428\n",
      "[251,   160] loss: 29525.057\n",
      "[251,   165] loss: 26040.417\n",
      "[251,   170] loss: 27153.544\n",
      "[251,   175] loss: 21441.902\n",
      "[251,   180] loss: 22753.936\n",
      "[251,   185] loss: 33868.019\n",
      "[251,   190] loss: 20570.854\n",
      "[251,   195] loss: 46471.582\n",
      "[251,   200] loss: 28306.111\n",
      "[251,   205] loss: 20685.381\n",
      "[251,   210] loss: 18515.186\n",
      "[251,   215] loss: 24442.245\n",
      "[251,   220] loss: 18249.580\n",
      "[251,   225] loss: 16310.553\n",
      "[251,   230] loss: 27587.525\n",
      "[252,     5] loss: 23447.631\n",
      "[252,    10] loss: 31362.029\n",
      "[252,    15] loss: 16376.073\n",
      "[252,    20] loss: 25602.741\n",
      "[252,    25] loss: 26779.164\n",
      "[252,    30] loss: 24096.819\n",
      "[252,    35] loss: 21052.180\n",
      "[252,    40] loss: 21490.497\n",
      "[252,    45] loss: 29025.257\n",
      "[252,    50] loss: 28558.759\n",
      "[252,    55] loss: 22180.313\n",
      "[252,    60] loss: 33566.758\n",
      "[252,    65] loss: 25821.100\n",
      "[252,    70] loss: 25498.108\n",
      "[252,    75] loss: 22414.843\n",
      "[252,    80] loss: 29856.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[252,    85] loss: 37727.367\n",
      "[252,    90] loss: 27462.205\n",
      "[252,    95] loss: 23841.820\n",
      "[252,   100] loss: 26895.050\n",
      "[252,   105] loss: 30980.097\n",
      "[252,   110] loss: 18990.120\n",
      "[252,   115] loss: 28308.007\n",
      "[252,   120] loss: 19483.014\n",
      "[252,   125] loss: 31515.591\n",
      "[252,   130] loss: 14143.787\n",
      "[252,   135] loss: 22270.288\n",
      "[252,   140] loss: 29220.096\n",
      "[252,   145] loss: 23402.406\n",
      "[252,   150] loss: 27485.523\n",
      "[252,   155] loss: 29420.948\n",
      "[252,   160] loss: 28514.349\n",
      "[252,   165] loss: 22823.023\n",
      "[252,   170] loss: 30952.998\n",
      "[252,   175] loss: 27779.904\n",
      "[252,   180] loss: 28911.156\n",
      "[252,   185] loss: 46624.591\n",
      "[252,   190] loss: 25161.655\n",
      "[252,   195] loss: 24751.330\n",
      "[252,   200] loss: 22515.978\n",
      "[252,   205] loss: 21615.417\n",
      "[252,   210] loss: 19023.150\n",
      "[252,   215] loss: 21418.833\n",
      "[252,   220] loss: 17353.069\n",
      "[252,   225] loss: 24270.312\n",
      "[252,   230] loss: 34251.578\n",
      "[253,     5] loss: 38511.520\n",
      "[253,    10] loss: 33217.816\n",
      "[253,    15] loss: 18143.636\n",
      "[253,    20] loss: 20568.987\n",
      "[253,    25] loss: 24776.908\n",
      "[253,    30] loss: 24227.098\n",
      "[253,    35] loss: 27888.452\n",
      "[253,    40] loss: 24797.554\n",
      "[253,    45] loss: 27294.288\n",
      "[253,    50] loss: 24395.874\n",
      "[253,    55] loss: 20323.244\n",
      "[253,    60] loss: 25549.809\n",
      "[253,    65] loss: 24103.999\n",
      "[253,    70] loss: 20111.965\n",
      "[253,    75] loss: 35936.110\n",
      "[253,    80] loss: 33507.230\n",
      "[253,    85] loss: 28519.055\n",
      "[253,    90] loss: 23752.608\n",
      "[253,    95] loss: 24907.877\n",
      "[253,   100] loss: 35046.523\n",
      "[253,   105] loss: 25739.673\n",
      "[253,   110] loss: 13118.763\n",
      "[253,   115] loss: 23738.786\n",
      "[253,   120] loss: 18554.556\n",
      "[253,   125] loss: 22606.701\n",
      "[253,   130] loss: 24383.116\n",
      "[253,   135] loss: 20402.554\n",
      "[253,   140] loss: 25394.353\n",
      "[253,   145] loss: 27979.530\n",
      "[253,   150] loss: 22209.581\n",
      "[253,   155] loss: 24874.457\n",
      "[253,   160] loss: 24405.236\n",
      "[253,   165] loss: 21521.512\n",
      "[253,   170] loss: 51783.287\n",
      "[253,   175] loss: 26444.237\n",
      "[253,   180] loss: 28240.156\n",
      "[253,   185] loss: 26522.816\n",
      "[253,   190] loss: 17580.994\n",
      "[253,   195] loss: 19591.700\n",
      "[253,   200] loss: 40561.116\n",
      "[253,   205] loss: 19714.059\n",
      "[253,   210] loss: 21320.758\n",
      "[253,   215] loss: 32019.880\n",
      "[253,   220] loss: 30036.708\n",
      "[253,   225] loss: 24872.997\n",
      "[253,   230] loss: 21219.690\n",
      "[254,     5] loss: 27009.572\n",
      "[254,    10] loss: 28391.665\n",
      "[254,    15] loss: 29616.829\n",
      "[254,    20] loss: 29122.069\n",
      "[254,    25] loss: 27825.837\n",
      "[254,    30] loss: 20409.922\n",
      "[254,    35] loss: 24102.819\n",
      "[254,    40] loss: 20948.306\n",
      "[254,    45] loss: 25893.316\n",
      "[254,    50] loss: 24064.033\n",
      "[254,    55] loss: 28487.693\n",
      "[254,    60] loss: 22793.458\n",
      "[254,    65] loss: 30080.767\n",
      "[254,    70] loss: 32429.421\n",
      "[254,    75] loss: 29844.020\n",
      "[254,    80] loss: 22360.733\n",
      "[254,    85] loss: 23900.280\n",
      "[254,    90] loss: 41977.832\n",
      "[254,    95] loss: 17314.915\n",
      "[254,   100] loss: 23115.508\n",
      "[254,   105] loss: 19472.103\n",
      "[254,   110] loss: 21588.886\n",
      "[254,   115] loss: 24821.795\n",
      "[254,   120] loss: 20506.049\n",
      "[254,   125] loss: 16598.745\n",
      "[254,   130] loss: 21369.402\n",
      "[254,   135] loss: 24355.045\n",
      "[254,   140] loss: 21400.475\n",
      "[254,   145] loss: 23848.489\n",
      "[254,   150] loss: 14620.355\n",
      "[254,   155] loss: 30767.230\n",
      "[254,   160] loss: 30124.780\n",
      "[254,   165] loss: 21419.995\n",
      "[254,   170] loss: 47426.192\n",
      "[254,   175] loss: 25770.908\n",
      "[254,   180] loss: 42881.586\n",
      "[254,   185] loss: 26496.888\n",
      "[254,   190] loss: 33370.520\n",
      "[254,   195] loss: 25344.487\n",
      "[254,   200] loss: 29618.595\n",
      "[254,   205] loss: 20324.318\n",
      "[254,   210] loss: 27517.114\n",
      "[254,   215] loss: 25570.103\n",
      "[254,   220] loss: 21871.030\n",
      "[254,   225] loss: 25427.553\n",
      "[254,   230] loss: 22152.680\n",
      "[255,     5] loss: 21886.516\n",
      "[255,    10] loss: 23560.291\n",
      "[255,    15] loss: 20981.096\n",
      "[255,    20] loss: 27844.890\n",
      "[255,    25] loss: 29421.637\n",
      "[255,    30] loss: 28493.775\n",
      "[255,    35] loss: 21351.919\n",
      "[255,    40] loss: 20019.162\n",
      "[255,    45] loss: 37054.725\n",
      "[255,    50] loss: 31496.304\n",
      "[255,    55] loss: 23689.151\n",
      "[255,    60] loss: 29554.296\n",
      "[255,    65] loss: 19691.572\n",
      "[255,    70] loss: 23168.034\n",
      "[255,    75] loss: 24538.523\n",
      "[255,    80] loss: 27888.861\n",
      "[255,    85] loss: 24238.500\n",
      "[255,    90] loss: 24197.777\n",
      "[255,    95] loss: 23759.172\n",
      "[255,   100] loss: 24820.489\n",
      "[255,   105] loss: 32852.531\n",
      "[255,   110] loss: 15405.286\n",
      "[255,   115] loss: 25930.804\n",
      "[255,   120] loss: 17049.215\n",
      "[255,   125] loss: 22831.103\n",
      "[255,   130] loss: 33533.393\n",
      "[255,   135] loss: 19422.356\n",
      "[255,   140] loss: 46878.271\n",
      "[255,   145] loss: 19775.469\n",
      "[255,   150] loss: 25172.930\n",
      "[255,   155] loss: 27475.761\n",
      "[255,   160] loss: 25156.592\n",
      "[255,   165] loss: 25889.437\n",
      "[255,   170] loss: 42635.766\n",
      "[255,   175] loss: 26706.044\n",
      "[255,   180] loss: 20124.833\n",
      "[255,   185] loss: 23933.269\n",
      "[255,   190] loss: 28468.466\n",
      "[255,   195] loss: 21437.091\n",
      "[255,   200] loss: 21094.130\n",
      "[255,   205] loss: 30992.724\n",
      "[255,   210] loss: 31387.024\n",
      "[255,   215] loss: 24924.802\n",
      "[255,   220] loss: 31330.739\n",
      "[255,   225] loss: 29378.728\n",
      "[255,   230] loss: 25525.365\n",
      "[256,     5] loss: 31467.860\n",
      "[256,    10] loss: 27791.483\n",
      "[256,    15] loss: 22387.351\n",
      "[256,    20] loss: 31022.774\n",
      "[256,    25] loss: 31227.689\n",
      "[256,    30] loss: 25071.202\n",
      "[256,    35] loss: 17965.824\n",
      "[256,    40] loss: 22546.898\n",
      "[256,    45] loss: 20091.388\n",
      "[256,    50] loss: 20095.448\n",
      "[256,    55] loss: 35484.373\n",
      "[256,    60] loss: 31630.384\n",
      "[256,    65] loss: 23674.648\n",
      "[256,    70] loss: 27238.997\n",
      "[256,    75] loss: 19475.969\n",
      "[256,    80] loss: 23026.265\n",
      "[256,    85] loss: 16201.163\n",
      "[256,    90] loss: 26373.303\n",
      "[256,    95] loss: 21744.028\n",
      "[256,   100] loss: 54246.844\n",
      "[256,   105] loss: 26971.220\n",
      "[256,   110] loss: 36657.346\n",
      "[256,   115] loss: 21403.095\n",
      "[256,   120] loss: 22535.951\n",
      "[256,   125] loss: 21214.476\n",
      "[256,   130] loss: 22793.526\n",
      "[256,   135] loss: 31244.039\n",
      "[256,   140] loss: 37832.829\n",
      "[256,   145] loss: 22708.007\n",
      "[256,   150] loss: 30029.551\n",
      "[256,   155] loss: 18833.106\n",
      "[256,   160] loss: 14980.745\n",
      "[256,   165] loss: 27098.074\n",
      "[256,   170] loss: 26500.434\n",
      "[256,   175] loss: 29901.806\n",
      "[256,   180] loss: 28353.015\n",
      "[256,   185] loss: 20066.727\n",
      "[256,   190] loss: 25910.731\n",
      "[256,   195] loss: 33665.707\n",
      "[256,   200] loss: 22584.011\n",
      "[256,   205] loss: 25844.339\n",
      "[256,   210] loss: 29732.465\n",
      "[256,   215] loss: 16302.012\n",
      "[256,   220] loss: 22460.565\n",
      "[256,   225] loss: 36317.505\n",
      "[256,   230] loss: 17879.254\n",
      "[257,     5] loss: 20603.747\n",
      "[257,    10] loss: 24598.595\n",
      "[257,    15] loss: 26218.582\n",
      "[257,    20] loss: 20855.780\n",
      "[257,    25] loss: 29504.991\n",
      "[257,    30] loss: 17731.290\n",
      "[257,    35] loss: 17720.166\n",
      "[257,    40] loss: 30669.730\n",
      "[257,    45] loss: 30814.737\n",
      "[257,    50] loss: 24103.837\n",
      "[257,    55] loss: 24201.405\n",
      "[257,    60] loss: 20008.041\n",
      "[257,    65] loss: 39294.962\n",
      "[257,    70] loss: 36946.678\n",
      "[257,    75] loss: 27569.841\n",
      "[257,    80] loss: 34104.670\n",
      "[257,    85] loss: 15509.357\n",
      "[257,    90] loss: 22723.975\n",
      "[257,    95] loss: 25922.002\n",
      "[257,   100] loss: 27782.880\n",
      "[257,   105] loss: 32065.390\n",
      "[257,   110] loss: 31606.682\n",
      "[257,   115] loss: 22317.812\n",
      "[257,   120] loss: 16523.979\n",
      "[257,   125] loss: 29722.585\n",
      "[257,   130] loss: 15563.115\n",
      "[257,   135] loss: 30246.938\n",
      "[257,   140] loss: 27791.988\n",
      "[257,   145] loss: 31340.096\n",
      "[257,   150] loss: 21893.825\n",
      "[257,   155] loss: 23874.300\n",
      "[257,   160] loss: 28043.976\n",
      "[257,   165] loss: 38129.507\n",
      "[257,   170] loss: 25007.444\n",
      "[257,   175] loss: 26192.920\n",
      "[257,   180] loss: 19222.351\n",
      "[257,   185] loss: 28960.984\n",
      "[257,   190] loss: 55433.397\n",
      "[257,   195] loss: 21954.738\n",
      "[257,   200] loss: 19533.417\n",
      "[257,   205] loss: 22325.224\n",
      "[257,   210] loss: 26420.401\n",
      "[257,   215] loss: 23407.298\n",
      "[257,   220] loss: 20022.849\n",
      "[257,   225] loss: 21458.549\n",
      "[257,   230] loss: 26360.466\n",
      "[258,     5] loss: 17773.949\n",
      "[258,    10] loss: 28555.525\n",
      "[258,    15] loss: 32918.689\n",
      "[258,    20] loss: 22350.575\n",
      "[258,    25] loss: 20084.075\n",
      "[258,    30] loss: 23684.829\n",
      "[258,    35] loss: 18575.422\n",
      "[258,    40] loss: 24209.271\n",
      "[258,    45] loss: 42082.096\n",
      "[258,    50] loss: 22819.983\n",
      "[258,    55] loss: 14183.151\n",
      "[258,    60] loss: 30355.805\n",
      "[258,    65] loss: 25763.323\n",
      "[258,    70] loss: 35544.424\n",
      "[258,    75] loss: 29080.298\n",
      "[258,    80] loss: 32028.960\n",
      "[258,    85] loss: 31895.688\n",
      "[258,    90] loss: 17730.401\n",
      "[258,    95] loss: 31392.877\n",
      "[258,   100] loss: 32822.189\n",
      "[258,   105] loss: 32354.140\n",
      "[258,   110] loss: 19222.756\n",
      "[258,   115] loss: 39953.475\n",
      "[258,   120] loss: 28868.948\n",
      "[258,   125] loss: 34581.914\n",
      "[258,   130] loss: 18919.475\n",
      "[258,   135] loss: 20884.197\n",
      "[258,   140] loss: 26334.933\n",
      "[258,   145] loss: 20752.426\n",
      "[258,   150] loss: 23547.493\n",
      "[258,   155] loss: 26641.138\n",
      "[258,   160] loss: 19346.512\n",
      "[258,   165] loss: 25028.530\n",
      "[258,   170] loss: 21390.919\n",
      "[258,   175] loss: 25009.823\n",
      "[258,   180] loss: 30263.631\n",
      "[258,   185] loss: 25788.875\n",
      "[258,   190] loss: 28048.073\n",
      "[258,   195] loss: 39072.782\n",
      "[258,   200] loss: 36657.534\n",
      "[258,   205] loss: 23479.516\n",
      "[258,   210] loss: 24758.961\n",
      "[258,   215] loss: 18130.945\n",
      "[258,   220] loss: 16595.528\n",
      "[258,   225] loss: 24123.200\n",
      "[258,   230] loss: 14590.833\n",
      "[259,     5] loss: 26674.486\n",
      "[259,    10] loss: 29474.472\n",
      "[259,    15] loss: 23224.316\n",
      "[259,    20] loss: 28325.886\n",
      "[259,    25] loss: 37652.410\n",
      "[259,    30] loss: 25110.473\n",
      "[259,    35] loss: 29925.896\n",
      "[259,    40] loss: 26434.400\n",
      "[259,    45] loss: 23918.421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259,    50] loss: 16236.364\n",
      "[259,    55] loss: 24357.548\n",
      "[259,    60] loss: 16867.159\n",
      "[259,    65] loss: 30842.901\n",
      "[259,    70] loss: 20748.040\n",
      "[259,    75] loss: 48174.995\n",
      "[259,    80] loss: 21952.077\n",
      "[259,    85] loss: 36089.334\n",
      "[259,    90] loss: 19082.832\n",
      "[259,    95] loss: 23229.228\n",
      "[259,   100] loss: 34185.955\n",
      "[259,   105] loss: 28733.709\n",
      "[259,   110] loss: 22507.412\n",
      "[259,   115] loss: 25625.954\n",
      "[259,   120] loss: 24621.913\n",
      "[259,   125] loss: 28946.746\n",
      "[259,   130] loss: 23995.784\n",
      "[259,   135] loss: 33027.266\n",
      "[259,   140] loss: 29554.078\n",
      "[259,   145] loss: 23828.206\n",
      "[259,   150] loss: 24397.910\n",
      "[259,   155] loss: 24425.912\n",
      "[259,   160] loss: 28550.564\n",
      "[259,   165] loss: 20366.423\n",
      "[259,   170] loss: 14076.207\n",
      "[259,   175] loss: 26957.671\n",
      "[259,   180] loss: 23910.193\n",
      "[259,   185] loss: 21965.913\n",
      "[259,   190] loss: 23307.618\n",
      "[259,   195] loss: 32217.687\n",
      "[259,   200] loss: 26854.835\n",
      "[259,   205] loss: 18090.013\n",
      "[259,   210] loss: 24069.887\n",
      "[259,   215] loss: 23115.367\n",
      "[259,   220] loss: 29504.990\n",
      "[259,   225] loss: 19014.004\n",
      "[259,   230] loss: 26220.512\n",
      "[260,     5] loss: 28112.606\n",
      "[260,    10] loss: 17364.880\n",
      "[260,    15] loss: 20066.430\n",
      "[260,    20] loss: 31031.142\n",
      "[260,    25] loss: 19248.519\n",
      "[260,    30] loss: 18512.451\n",
      "[260,    35] loss: 19910.910\n",
      "[260,    40] loss: 21361.749\n",
      "[260,    45] loss: 21769.074\n",
      "[260,    50] loss: 31910.230\n",
      "[260,    55] loss: 24468.455\n",
      "[260,    60] loss: 27263.714\n",
      "[260,    65] loss: 21288.429\n",
      "[260,    70] loss: 29916.810\n",
      "[260,    75] loss: 40585.138\n",
      "[260,    80] loss: 35187.604\n",
      "[260,    85] loss: 20473.293\n",
      "[260,    90] loss: 23455.790\n",
      "[260,    95] loss: 38691.446\n",
      "[260,   100] loss: 25814.916\n",
      "[260,   105] loss: 18600.624\n",
      "[260,   110] loss: 19804.597\n",
      "[260,   115] loss: 24671.185\n",
      "[260,   120] loss: 32403.664\n",
      "[260,   125] loss: 37084.347\n",
      "[260,   130] loss: 20432.590\n",
      "[260,   135] loss: 20910.714\n",
      "[260,   140] loss: 29342.596\n",
      "[260,   145] loss: 24293.336\n",
      "[260,   150] loss: 26853.985\n",
      "[260,   155] loss: 28964.383\n",
      "[260,   160] loss: 26075.850\n",
      "[260,   165] loss: 33105.559\n",
      "[260,   170] loss: 23343.517\n",
      "[260,   175] loss: 22397.162\n",
      "[260,   180] loss: 26051.747\n",
      "[260,   185] loss: 25294.178\n",
      "[260,   190] loss: 27141.531\n",
      "[260,   195] loss: 25695.730\n",
      "[260,   200] loss: 28160.562\n",
      "[260,   205] loss: 25367.839\n",
      "[260,   210] loss: 29471.827\n",
      "[260,   215] loss: 21059.494\n",
      "[260,   220] loss: 22260.161\n",
      "[260,   225] loss: 25883.728\n",
      "[260,   230] loss: 27890.906\n",
      "[261,     5] loss: 23819.067\n",
      "[261,    10] loss: 20355.550\n",
      "[261,    15] loss: 27267.146\n",
      "[261,    20] loss: 19582.568\n",
      "[261,    25] loss: 26949.567\n",
      "[261,    30] loss: 20071.564\n",
      "[261,    35] loss: 47025.214\n",
      "[261,    40] loss: 31323.293\n",
      "[261,    45] loss: 21648.195\n",
      "[261,    50] loss: 26424.768\n",
      "[261,    55] loss: 21943.353\n",
      "[261,    60] loss: 32240.105\n",
      "[261,    65] loss: 20873.248\n",
      "[261,    70] loss: 25432.435\n",
      "[261,    75] loss: 23002.974\n",
      "[261,    80] loss: 39776.557\n",
      "[261,    85] loss: 23661.017\n",
      "[261,    90] loss: 17974.671\n",
      "[261,    95] loss: 24158.139\n",
      "[261,   100] loss: 18944.085\n",
      "[261,   105] loss: 30870.297\n",
      "[261,   110] loss: 21497.800\n",
      "[261,   115] loss: 24765.902\n",
      "[261,   120] loss: 16344.191\n",
      "[261,   125] loss: 30831.931\n",
      "[261,   130] loss: 17372.769\n",
      "[261,   135] loss: 36645.052\n",
      "[261,   140] loss: 36821.786\n",
      "[261,   145] loss: 30661.603\n",
      "[261,   150] loss: 25750.500\n",
      "[261,   155] loss: 38612.219\n",
      "[261,   160] loss: 30611.642\n",
      "[261,   165] loss: 24768.075\n",
      "[261,   170] loss: 24147.697\n",
      "[261,   175] loss: 24662.905\n",
      "[261,   180] loss: 14191.914\n",
      "[261,   185] loss: 23451.421\n",
      "[261,   190] loss: 17773.833\n",
      "[261,   195] loss: 25567.707\n",
      "[261,   200] loss: 28469.921\n",
      "[261,   205] loss: 24875.167\n",
      "[261,   210] loss: 26313.961\n",
      "[261,   215] loss: 30684.819\n",
      "[261,   220] loss: 25541.225\n",
      "[261,   225] loss: 34254.229\n",
      "[261,   230] loss: 24298.910\n",
      "[262,     5] loss: 33664.297\n",
      "[262,    10] loss: 23332.788\n",
      "[262,    15] loss: 29623.761\n",
      "[262,    20] loss: 18593.723\n",
      "[262,    25] loss: 20893.665\n",
      "[262,    30] loss: 19871.857\n",
      "[262,    35] loss: 16966.955\n",
      "[262,    40] loss: 19531.255\n",
      "[262,    45] loss: 20502.235\n",
      "[262,    50] loss: 24952.382\n",
      "[262,    55] loss: 28289.362\n",
      "[262,    60] loss: 23218.239\n",
      "[262,    65] loss: 27890.400\n",
      "[262,    70] loss: 19367.436\n",
      "[262,    75] loss: 19862.495\n",
      "[262,    80] loss: 23173.037\n",
      "[262,    85] loss: 31160.506\n",
      "[262,    90] loss: 30466.118\n",
      "[262,    95] loss: 24665.842\n",
      "[262,   100] loss: 29925.673\n",
      "[262,   105] loss: 23807.198\n",
      "[262,   110] loss: 24851.434\n",
      "[262,   115] loss: 22103.378\n",
      "[262,   120] loss: 38344.476\n",
      "[262,   125] loss: 15139.593\n",
      "[262,   130] loss: 21709.519\n",
      "[262,   135] loss: 26635.317\n",
      "[262,   140] loss: 26183.852\n",
      "[262,   145] loss: 24562.692\n",
      "[262,   150] loss: 43065.266\n",
      "[262,   155] loss: 23287.443\n",
      "[262,   160] loss: 19513.698\n",
      "[262,   165] loss: 31794.253\n",
      "[262,   170] loss: 29375.998\n",
      "[262,   175] loss: 26848.574\n",
      "[262,   180] loss: 28943.522\n",
      "[262,   185] loss: 22973.128\n",
      "[262,   190] loss: 30503.057\n",
      "[262,   195] loss: 41109.002\n",
      "[262,   200] loss: 26651.369\n",
      "[262,   205] loss: 21767.983\n",
      "[262,   210] loss: 49670.329\n",
      "[262,   215] loss: 22949.211\n",
      "[262,   220] loss: 26425.543\n",
      "[262,   225] loss: 24149.157\n",
      "[262,   230] loss: 19559.196\n",
      "[263,     5] loss: 23511.295\n",
      "[263,    10] loss: 28992.903\n",
      "[263,    15] loss: 25938.620\n",
      "[263,    20] loss: 59392.841\n",
      "[263,    25] loss: 22234.288\n",
      "[263,    30] loss: 27628.445\n",
      "[263,    35] loss: 29007.813\n",
      "[263,    40] loss: 34022.018\n",
      "[263,    45] loss: 22806.558\n",
      "[263,    50] loss: 31362.523\n",
      "[263,    55] loss: 20728.498\n",
      "[263,    60] loss: 16572.684\n",
      "[263,    65] loss: 31028.483\n",
      "[263,    70] loss: 35564.517\n",
      "[263,    75] loss: 22756.125\n",
      "[263,    80] loss: 18249.746\n",
      "[263,    85] loss: 22607.592\n",
      "[263,    90] loss: 22986.335\n",
      "[263,    95] loss: 25222.480\n",
      "[263,   100] loss: 35350.639\n",
      "[263,   105] loss: 27082.233\n",
      "[263,   110] loss: 31258.174\n",
      "[263,   115] loss: 25139.750\n",
      "[263,   120] loss: 16884.836\n",
      "[263,   125] loss: 29951.784\n",
      "[263,   130] loss: 30569.350\n",
      "[263,   135] loss: 16486.957\n",
      "[263,   140] loss: 19072.802\n",
      "[263,   145] loss: 23898.975\n",
      "[263,   150] loss: 30680.412\n",
      "[263,   155] loss: 24536.989\n",
      "[263,   160] loss: 17286.036\n",
      "[263,   165] loss: 28719.746\n",
      "[263,   170] loss: 26960.035\n",
      "[263,   175] loss: 18876.419\n",
      "[263,   180] loss: 20333.576\n",
      "[263,   185] loss: 28430.440\n",
      "[263,   190] loss: 24953.266\n",
      "[263,   195] loss: 39644.446\n",
      "[263,   200] loss: 25548.719\n",
      "[263,   205] loss: 13529.273\n",
      "[263,   210] loss: 23004.264\n",
      "[263,   215] loss: 30712.523\n",
      "[263,   220] loss: 22606.004\n",
      "[263,   225] loss: 29759.688\n",
      "[263,   230] loss: 17749.416\n",
      "[264,     5] loss: 26241.717\n",
      "[264,    10] loss: 17911.907\n",
      "[264,    15] loss: 19034.895\n",
      "[264,    20] loss: 23661.456\n",
      "[264,    25] loss: 24556.657\n",
      "[264,    30] loss: 31033.343\n",
      "[264,    35] loss: 15582.434\n",
      "[264,    40] loss: 23325.313\n",
      "[264,    45] loss: 21384.331\n",
      "[264,    50] loss: 23973.662\n",
      "[264,    55] loss: 30178.339\n",
      "[264,    60] loss: 30386.288\n",
      "[264,    65] loss: 24773.454\n",
      "[264,    70] loss: 23286.160\n",
      "[264,    75] loss: 25805.200\n",
      "[264,    80] loss: 27687.732\n",
      "[264,    85] loss: 25871.225\n",
      "[264,    90] loss: 24175.445\n",
      "[264,    95] loss: 28147.948\n",
      "[264,   100] loss: 28662.900\n",
      "[264,   105] loss: 21562.681\n",
      "[264,   110] loss: 23088.733\n",
      "[264,   115] loss: 21997.076\n",
      "[264,   120] loss: 29556.967\n",
      "[264,   125] loss: 19900.245\n",
      "[264,   130] loss: 18954.616\n",
      "[264,   135] loss: 17258.122\n",
      "[264,   140] loss: 22282.613\n",
      "[264,   145] loss: 20752.809\n",
      "[264,   150] loss: 26713.494\n",
      "[264,   155] loss: 25665.132\n",
      "[264,   160] loss: 20414.216\n",
      "[264,   165] loss: 20620.797\n",
      "[264,   170] loss: 38633.596\n",
      "[264,   175] loss: 34401.570\n",
      "[264,   180] loss: 19269.012\n",
      "[264,   185] loss: 36789.818\n",
      "[264,   190] loss: 20550.911\n",
      "[264,   195] loss: 30518.784\n",
      "[264,   200] loss: 21426.531\n",
      "[264,   205] loss: 32624.300\n",
      "[264,   210] loss: 51919.154\n",
      "[264,   215] loss: 43525.536\n",
      "[264,   220] loss: 20727.464\n",
      "[264,   225] loss: 29169.040\n",
      "[264,   230] loss: 31346.101\n",
      "[265,     5] loss: 23944.169\n",
      "[265,    10] loss: 32277.314\n",
      "[265,    15] loss: 32021.131\n",
      "[265,    20] loss: 26511.306\n",
      "[265,    25] loss: 23914.275\n",
      "[265,    30] loss: 24592.814\n",
      "[265,    35] loss: 38257.396\n",
      "[265,    40] loss: 21946.640\n",
      "[265,    45] loss: 28700.521\n",
      "[265,    50] loss: 27508.342\n",
      "[265,    55] loss: 20601.239\n",
      "[265,    60] loss: 19289.467\n",
      "[265,    65] loss: 14733.320\n",
      "[265,    70] loss: 15076.279\n",
      "[265,    75] loss: 24542.087\n",
      "[265,    80] loss: 26802.703\n",
      "[265,    85] loss: 22169.382\n",
      "[265,    90] loss: 22706.388\n",
      "[265,    95] loss: 26422.166\n",
      "[265,   100] loss: 30001.877\n",
      "[265,   105] loss: 25580.992\n",
      "[265,   110] loss: 26298.724\n",
      "[265,   115] loss: 22774.902\n",
      "[265,   120] loss: 24430.347\n",
      "[265,   125] loss: 27021.381\n",
      "[265,   130] loss: 18001.853\n",
      "[265,   135] loss: 22735.002\n",
      "[265,   140] loss: 21248.712\n",
      "[265,   145] loss: 16903.725\n",
      "[265,   150] loss: 21029.811\n",
      "[265,   155] loss: 31039.833\n",
      "[265,   160] loss: 21098.281\n",
      "[265,   165] loss: 23063.176\n",
      "[265,   170] loss: 29969.348\n",
      "[265,   175] loss: 27726.598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[265,   180] loss: 19308.213\n",
      "[265,   185] loss: 22619.375\n",
      "[265,   190] loss: 27318.620\n",
      "[265,   195] loss: 39391.986\n",
      "[265,   200] loss: 49332.605\n",
      "[265,   205] loss: 37658.288\n",
      "[265,   210] loss: 21882.711\n",
      "[265,   215] loss: 23516.958\n",
      "[265,   220] loss: 17739.210\n",
      "[265,   225] loss: 35695.646\n",
      "[265,   230] loss: 27270.629\n",
      "[266,     5] loss: 41901.710\n",
      "[266,    10] loss: 22436.360\n",
      "[266,    15] loss: 21298.198\n",
      "[266,    20] loss: 23804.845\n",
      "[266,    25] loss: 26840.424\n",
      "[266,    30] loss: 27646.417\n",
      "[266,    35] loss: 18537.109\n",
      "[266,    40] loss: 16880.696\n",
      "[266,    45] loss: 28044.175\n",
      "[266,    50] loss: 21775.475\n",
      "[266,    55] loss: 33970.173\n",
      "[266,    60] loss: 25574.568\n",
      "[266,    65] loss: 22689.107\n",
      "[266,    70] loss: 32273.409\n",
      "[266,    75] loss: 22025.864\n",
      "[266,    80] loss: 16434.538\n",
      "[266,    85] loss: 23090.408\n",
      "[266,    90] loss: 26658.504\n",
      "[266,    95] loss: 24405.962\n",
      "[266,   100] loss: 33754.535\n",
      "[266,   105] loss: 21221.411\n",
      "[266,   110] loss: 27484.706\n",
      "[266,   115] loss: 21436.449\n",
      "[266,   120] loss: 22850.843\n",
      "[266,   125] loss: 44861.521\n",
      "[266,   130] loss: 24586.316\n",
      "[266,   135] loss: 39194.376\n",
      "[266,   140] loss: 22631.772\n",
      "[266,   145] loss: 32460.152\n",
      "[266,   150] loss: 21580.791\n",
      "[266,   155] loss: 19098.135\n",
      "[266,   160] loss: 15648.642\n",
      "[266,   165] loss: 28300.984\n",
      "[266,   170] loss: 30415.871\n",
      "[266,   175] loss: 24966.631\n",
      "[266,   180] loss: 34100.740\n",
      "[266,   185] loss: 27671.675\n",
      "[266,   190] loss: 26556.317\n",
      "[266,   195] loss: 29804.979\n",
      "[266,   200] loss: 24311.170\n",
      "[266,   205] loss: 22390.113\n",
      "[266,   210] loss: 25673.808\n",
      "[266,   215] loss: 20963.462\n",
      "[266,   220] loss: 33372.109\n",
      "[266,   225] loss: 26210.741\n",
      "[266,   230] loss: 24166.110\n",
      "[267,     5] loss: 22625.246\n",
      "[267,    10] loss: 37499.511\n",
      "[267,    15] loss: 16487.440\n",
      "[267,    20] loss: 22746.799\n",
      "[267,    25] loss: 30759.503\n",
      "[267,    30] loss: 30881.584\n",
      "[267,    35] loss: 23181.783\n",
      "[267,    40] loss: 24062.726\n",
      "[267,    45] loss: 24679.238\n",
      "[267,    50] loss: 25330.974\n",
      "[267,    55] loss: 21525.992\n",
      "[267,    60] loss: 25817.821\n",
      "[267,    65] loss: 24624.145\n",
      "[267,    70] loss: 30643.137\n",
      "[267,    75] loss: 34338.790\n",
      "[267,    80] loss: 22342.275\n",
      "[267,    85] loss: 35094.768\n",
      "[267,    90] loss: 27851.892\n",
      "[267,    95] loss: 20183.068\n",
      "[267,   100] loss: 20933.027\n",
      "[267,   105] loss: 25069.425\n",
      "[267,   110] loss: 23339.717\n",
      "[267,   115] loss: 23988.294\n",
      "[267,   120] loss: 43070.241\n",
      "[267,   125] loss: 25975.630\n",
      "[267,   130] loss: 34684.395\n",
      "[267,   135] loss: 26026.433\n",
      "[267,   140] loss: 28699.371\n",
      "[267,   145] loss: 22815.699\n",
      "[267,   150] loss: 22758.796\n",
      "[267,   155] loss: 34942.537\n",
      "[267,   160] loss: 25186.766\n",
      "[267,   165] loss: 20885.867\n",
      "[267,   170] loss: 20395.373\n",
      "[267,   175] loss: 27649.485\n",
      "[267,   180] loss: 18186.157\n",
      "[267,   185] loss: 19413.702\n",
      "[267,   190] loss: 19610.242\n",
      "[267,   195] loss: 21826.981\n",
      "[267,   200] loss: 26928.863\n",
      "[267,   205] loss: 21187.508\n",
      "[267,   210] loss: 21343.234\n",
      "[267,   215] loss: 21984.967\n",
      "[267,   220] loss: 25312.103\n",
      "[267,   225] loss: 21049.638\n",
      "[267,   230] loss: 51171.300\n",
      "[268,     5] loss: 21931.499\n",
      "[268,    10] loss: 17059.673\n",
      "[268,    15] loss: 28203.182\n",
      "[268,    20] loss: 15992.926\n",
      "[268,    25] loss: 22925.279\n",
      "[268,    30] loss: 23771.935\n",
      "[268,    35] loss: 15493.342\n",
      "[268,    40] loss: 23646.671\n",
      "[268,    45] loss: 29175.169\n",
      "[268,    50] loss: 24099.651\n",
      "[268,    55] loss: 24711.418\n",
      "[268,    60] loss: 28325.562\n",
      "[268,    65] loss: 36662.592\n",
      "[268,    70] loss: 20202.296\n",
      "[268,    75] loss: 22958.858\n",
      "[268,    80] loss: 20815.392\n",
      "[268,    85] loss: 31873.886\n",
      "[268,    90] loss: 20372.517\n",
      "[268,    95] loss: 27067.775\n",
      "[268,   100] loss: 23480.835\n",
      "[268,   105] loss: 16037.103\n",
      "[268,   110] loss: 20617.762\n",
      "[268,   115] loss: 32432.274\n",
      "[268,   120] loss: 22322.111\n",
      "[268,   125] loss: 35522.373\n",
      "[268,   130] loss: 22557.277\n",
      "[268,   135] loss: 15822.718\n",
      "[268,   140] loss: 27693.232\n",
      "[268,   145] loss: 21325.757\n",
      "[268,   150] loss: 39236.977\n",
      "[268,   155] loss: 24326.024\n",
      "[268,   160] loss: 25968.008\n",
      "[268,   165] loss: 29381.468\n",
      "[268,   170] loss: 43216.897\n",
      "[268,   175] loss: 50627.403\n",
      "[268,   180] loss: 41445.408\n",
      "[268,   185] loss: 23133.585\n",
      "[268,   190] loss: 32158.050\n",
      "[268,   195] loss: 23995.058\n",
      "[268,   200] loss: 17998.265\n",
      "[268,   205] loss: 28065.809\n",
      "[268,   210] loss: 24842.558\n",
      "[268,   215] loss: 26737.820\n",
      "[268,   220] loss: 18101.327\n",
      "[268,   225] loss: 34745.727\n",
      "[268,   230] loss: 20712.919\n",
      "[269,     5] loss: 23364.370\n",
      "[269,    10] loss: 20629.063\n",
      "[269,    15] loss: 33934.224\n",
      "[269,    20] loss: 22942.896\n",
      "[269,    25] loss: 32880.703\n",
      "[269,    30] loss: 23456.214\n",
      "[269,    35] loss: 26992.570\n",
      "[269,    40] loss: 34457.054\n",
      "[269,    45] loss: 32678.760\n",
      "[269,    50] loss: 30747.348\n",
      "[269,    55] loss: 32786.062\n",
      "[269,    60] loss: 23306.425\n",
      "[269,    65] loss: 28833.902\n",
      "[269,    70] loss: 22951.164\n",
      "[269,    75] loss: 27553.181\n",
      "[269,    80] loss: 17123.580\n",
      "[269,    85] loss: 26310.001\n",
      "[269,    90] loss: 23263.889\n",
      "[269,    95] loss: 22933.943\n",
      "[269,   100] loss: 18814.465\n",
      "[269,   105] loss: 28627.316\n",
      "[269,   110] loss: 24621.888\n",
      "[269,   115] loss: 27504.250\n",
      "[269,   120] loss: 17828.276\n",
      "[269,   125] loss: 20881.155\n",
      "[269,   130] loss: 25022.747\n",
      "[269,   135] loss: 34581.458\n",
      "[269,   140] loss: 24022.899\n",
      "[269,   145] loss: 16311.260\n",
      "[269,   150] loss: 19172.194\n",
      "[269,   155] loss: 22857.685\n",
      "[269,   160] loss: 20206.735\n",
      "[269,   165] loss: 31683.971\n",
      "[269,   170] loss: 39751.949\n",
      "[269,   175] loss: 20841.625\n",
      "[269,   180] loss: 31393.245\n",
      "[269,   185] loss: 30543.391\n",
      "[269,   190] loss: 23996.095\n",
      "[269,   195] loss: 19857.908\n",
      "[269,   200] loss: 21118.423\n",
      "[269,   205] loss: 50568.119\n",
      "[269,   210] loss: 24503.243\n",
      "[269,   215] loss: 25873.471\n",
      "[269,   220] loss: 27428.897\n",
      "[269,   225] loss: 25191.504\n",
      "[269,   230] loss: 22635.027\n",
      "[270,     5] loss: 26480.452\n",
      "[270,    10] loss: 20054.310\n",
      "[270,    15] loss: 29047.223\n",
      "[270,    20] loss: 25476.662\n",
      "[270,    25] loss: 20491.049\n",
      "[270,    30] loss: 27837.737\n",
      "[270,    35] loss: 39484.500\n",
      "[270,    40] loss: 19205.465\n",
      "[270,    45] loss: 24952.538\n",
      "[270,    50] loss: 39736.091\n",
      "[270,    55] loss: 24877.002\n",
      "[270,    60] loss: 25971.052\n",
      "[270,    65] loss: 21980.653\n",
      "[270,    70] loss: 20024.952\n",
      "[270,    75] loss: 29973.079\n",
      "[270,    80] loss: 20590.962\n",
      "[270,    85] loss: 21909.642\n",
      "[270,    90] loss: 39172.464\n",
      "[270,    95] loss: 30022.731\n",
      "[270,   100] loss: 20039.964\n",
      "[270,   105] loss: 38198.862\n",
      "[270,   110] loss: 25900.479\n",
      "[270,   115] loss: 52100.404\n",
      "[270,   120] loss: 28539.189\n",
      "[270,   125] loss: 21753.058\n",
      "[270,   130] loss: 18675.971\n",
      "[270,   135] loss: 28013.622\n",
      "[270,   140] loss: 32884.656\n",
      "[270,   145] loss: 27784.773\n",
      "[270,   150] loss: 18864.988\n",
      "[270,   155] loss: 31512.592\n",
      "[270,   160] loss: 25526.874\n",
      "[270,   165] loss: 31504.857\n",
      "[270,   170] loss: 15247.723\n",
      "[270,   175] loss: 18475.688\n",
      "[270,   180] loss: 25372.595\n",
      "[270,   185] loss: 19268.538\n",
      "[270,   190] loss: 16542.666\n",
      "[270,   195] loss: 27446.016\n",
      "[270,   200] loss: 24240.194\n",
      "[270,   205] loss: 11452.450\n",
      "[270,   210] loss: 24884.670\n",
      "[270,   215] loss: 27344.650\n",
      "[270,   220] loss: 25028.644\n",
      "[270,   225] loss: 26237.489\n",
      "[270,   230] loss: 20664.162\n",
      "[271,     5] loss: 18115.248\n",
      "[271,    10] loss: 31297.449\n",
      "[271,    15] loss: 27461.017\n",
      "[271,    20] loss: 32622.355\n",
      "[271,    25] loss: 20439.108\n",
      "[271,    30] loss: 26667.654\n",
      "[271,    35] loss: 30783.311\n",
      "[271,    40] loss: 27572.983\n",
      "[271,    45] loss: 25376.748\n",
      "[271,    50] loss: 21615.029\n",
      "[271,    55] loss: 24224.092\n",
      "[271,    60] loss: 15634.714\n",
      "[271,    65] loss: 38108.109\n",
      "[271,    70] loss: 30457.502\n",
      "[271,    75] loss: 18635.984\n",
      "[271,    80] loss: 21862.258\n",
      "[271,    85] loss: 26208.614\n",
      "[271,    90] loss: 31555.295\n",
      "[271,    95] loss: 26985.521\n",
      "[271,   100] loss: 33427.774\n",
      "[271,   105] loss: 22550.156\n",
      "[271,   110] loss: 19904.438\n",
      "[271,   115] loss: 23579.441\n",
      "[271,   120] loss: 16865.566\n",
      "[271,   125] loss: 20597.547\n",
      "[271,   130] loss: 29386.177\n",
      "[271,   135] loss: 23599.642\n",
      "[271,   140] loss: 27693.688\n",
      "[271,   145] loss: 24503.222\n",
      "[271,   150] loss: 23365.327\n",
      "[271,   155] loss: 20540.000\n",
      "[271,   160] loss: 67741.546\n",
      "[271,   165] loss: 20686.520\n",
      "[271,   170] loss: 28940.175\n",
      "[271,   175] loss: 11666.228\n",
      "[271,   180] loss: 32629.438\n",
      "[271,   185] loss: 14177.306\n",
      "[271,   190] loss: 22137.654\n",
      "[271,   195] loss: 22048.683\n",
      "[271,   200] loss: 34081.039\n",
      "[271,   205] loss: 19177.880\n",
      "[271,   210] loss: 28216.721\n",
      "[271,   215] loss: 30313.358\n",
      "[271,   220] loss: 33753.030\n",
      "[271,   225] loss: 22420.337\n",
      "[271,   230] loss: 22320.703\n",
      "[272,     5] loss: 22477.209\n",
      "[272,    10] loss: 25600.696\n",
      "[272,    15] loss: 15066.911\n",
      "[272,    20] loss: 25648.948\n",
      "[272,    25] loss: 19635.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272,    30] loss: 23979.046\n",
      "[272,    35] loss: 21327.741\n",
      "[272,    40] loss: 32381.742\n",
      "[272,    45] loss: 24271.424\n",
      "[272,    50] loss: 19999.416\n",
      "[272,    55] loss: 29408.176\n",
      "[272,    60] loss: 23707.083\n",
      "[272,    65] loss: 29367.564\n",
      "[272,    70] loss: 26638.286\n",
      "[272,    75] loss: 25015.513\n",
      "[272,    80] loss: 22439.290\n",
      "[272,    85] loss: 28070.933\n",
      "[272,    90] loss: 21548.918\n",
      "[272,    95] loss: 32097.593\n",
      "[272,   100] loss: 25765.405\n",
      "[272,   105] loss: 21362.364\n",
      "[272,   110] loss: 25809.895\n",
      "[272,   115] loss: 28079.627\n",
      "[272,   120] loss: 18241.463\n",
      "[272,   125] loss: 28830.923\n",
      "[272,   130] loss: 30263.589\n",
      "[272,   135] loss: 29387.398\n",
      "[272,   140] loss: 28597.873\n",
      "[272,   145] loss: 13482.840\n",
      "[272,   150] loss: 25681.056\n",
      "[272,   155] loss: 34384.621\n",
      "[272,   160] loss: 31478.873\n",
      "[272,   165] loss: 46929.920\n",
      "[272,   170] loss: 23758.292\n",
      "[272,   175] loss: 23325.349\n",
      "[272,   180] loss: 34758.420\n",
      "[272,   185] loss: 32464.231\n",
      "[272,   190] loss: 26194.936\n",
      "[272,   195] loss: 36052.300\n",
      "[272,   200] loss: 23224.604\n",
      "[272,   205] loss: 21007.595\n",
      "[272,   210] loss: 29711.045\n",
      "[272,   215] loss: 17048.414\n",
      "[272,   220] loss: 27136.432\n",
      "[272,   225] loss: 19733.611\n",
      "[272,   230] loss: 25328.245\n",
      "[273,     5] loss: 27251.492\n",
      "[273,    10] loss: 21395.426\n",
      "[273,    15] loss: 23088.297\n",
      "[273,    20] loss: 22158.800\n",
      "[273,    25] loss: 20985.025\n",
      "[273,    30] loss: 29590.936\n",
      "[273,    35] loss: 35954.480\n",
      "[273,    40] loss: 21650.050\n",
      "[273,    45] loss: 27388.627\n",
      "[273,    50] loss: 31445.943\n",
      "[273,    55] loss: 17461.593\n",
      "[273,    60] loss: 22767.259\n",
      "[273,    65] loss: 30757.691\n",
      "[273,    70] loss: 49968.912\n",
      "[273,    75] loss: 23498.397\n",
      "[273,    80] loss: 28286.562\n",
      "[273,    85] loss: 41773.984\n",
      "[273,    90] loss: 18620.745\n",
      "[273,    95] loss: 24311.818\n",
      "[273,   100] loss: 28062.097\n",
      "[273,   105] loss: 26311.764\n",
      "[273,   110] loss: 23463.823\n",
      "[273,   115] loss: 39990.699\n",
      "[273,   120] loss: 18541.775\n",
      "[273,   125] loss: 29500.438\n",
      "[273,   130] loss: 31207.669\n",
      "[273,   135] loss: 35880.255\n",
      "[273,   140] loss: 21665.804\n",
      "[273,   145] loss: 29384.876\n",
      "[273,   150] loss: 15539.622\n",
      "[273,   155] loss: 24456.426\n",
      "[273,   160] loss: 17348.141\n",
      "[273,   165] loss: 23357.860\n",
      "[273,   170] loss: 30110.771\n",
      "[273,   175] loss: 29400.922\n",
      "[273,   180] loss: 24221.454\n",
      "[273,   185] loss: 16623.942\n",
      "[273,   190] loss: 23749.329\n",
      "[273,   195] loss: 26604.674\n",
      "[273,   200] loss: 25660.769\n",
      "[273,   205] loss: 17439.438\n",
      "[273,   210] loss: 31727.466\n",
      "[273,   215] loss: 18792.248\n",
      "[273,   220] loss: 27367.141\n",
      "[273,   225] loss: 21689.815\n",
      "[273,   230] loss: 25403.178\n",
      "[274,     5] loss: 23484.672\n",
      "[274,    10] loss: 16479.031\n",
      "[274,    15] loss: 24184.373\n",
      "[274,    20] loss: 21261.368\n",
      "[274,    25] loss: 16772.576\n",
      "[274,    30] loss: 45347.670\n",
      "[274,    35] loss: 18537.939\n",
      "[274,    40] loss: 27278.629\n",
      "[274,    45] loss: 28238.245\n",
      "[274,    50] loss: 31920.451\n",
      "[274,    55] loss: 18636.597\n",
      "[274,    60] loss: 30548.050\n",
      "[274,    65] loss: 22183.844\n",
      "[274,    70] loss: 27065.933\n",
      "[274,    75] loss: 21776.992\n",
      "[274,    80] loss: 32356.027\n",
      "[274,    85] loss: 21320.627\n",
      "[274,    90] loss: 16714.750\n",
      "[274,    95] loss: 17053.815\n",
      "[274,   100] loss: 26582.337\n",
      "[274,   105] loss: 26192.385\n",
      "[274,   110] loss: 29710.009\n",
      "[274,   115] loss: 31529.932\n",
      "[274,   120] loss: 34615.830\n",
      "[274,   125] loss: 26251.170\n",
      "[274,   130] loss: 34005.707\n",
      "[274,   135] loss: 38665.068\n",
      "[274,   140] loss: 25573.509\n",
      "[274,   145] loss: 17864.707\n",
      "[274,   150] loss: 21127.219\n",
      "[274,   155] loss: 30964.330\n",
      "[274,   160] loss: 24073.480\n",
      "[274,   165] loss: 28054.431\n",
      "[274,   170] loss: 24697.409\n",
      "[274,   175] loss: 23953.569\n",
      "[274,   180] loss: 24712.170\n",
      "[274,   185] loss: 26327.763\n",
      "[274,   190] loss: 23997.884\n",
      "[274,   195] loss: 23771.566\n",
      "[274,   200] loss: 28254.701\n",
      "[274,   205] loss: 23708.343\n",
      "[274,   210] loss: 27101.893\n",
      "[274,   215] loss: 25433.505\n",
      "[274,   220] loss: 15952.792\n",
      "[274,   225] loss: 40616.719\n",
      "[274,   230] loss: 28134.694\n",
      "[275,     5] loss: 18234.345\n",
      "[275,    10] loss: 52490.852\n",
      "[275,    15] loss: 26680.653\n",
      "[275,    20] loss: 22661.603\n",
      "[275,    25] loss: 24049.381\n",
      "[275,    30] loss: 18919.932\n",
      "[275,    35] loss: 24559.057\n",
      "[275,    40] loss: 25359.621\n",
      "[275,    45] loss: 29700.126\n",
      "[275,    50] loss: 25940.747\n",
      "[275,    55] loss: 26306.850\n",
      "[275,    60] loss: 24646.852\n",
      "[275,    65] loss: 15463.802\n",
      "[275,    70] loss: 27325.732\n",
      "[275,    75] loss: 29436.683\n",
      "[275,    80] loss: 20494.025\n",
      "[275,    85] loss: 22199.395\n",
      "[275,    90] loss: 25755.657\n",
      "[275,    95] loss: 21100.505\n",
      "[275,   100] loss: 17620.195\n",
      "[275,   105] loss: 27183.489\n",
      "[275,   110] loss: 25358.662\n",
      "[275,   115] loss: 16989.450\n",
      "[275,   120] loss: 23976.786\n",
      "[275,   125] loss: 25431.253\n",
      "[275,   130] loss: 31950.693\n",
      "[275,   135] loss: 22489.100\n",
      "[275,   140] loss: 26602.536\n",
      "[275,   145] loss: 22049.193\n",
      "[275,   150] loss: 34302.113\n",
      "[275,   155] loss: 26378.357\n",
      "[275,   160] loss: 31259.338\n",
      "[275,   165] loss: 28277.047\n",
      "[275,   170] loss: 31223.088\n",
      "[275,   175] loss: 21167.939\n",
      "[275,   180] loss: 24399.259\n",
      "[275,   185] loss: 29198.831\n",
      "[275,   190] loss: 24124.126\n",
      "[275,   195] loss: 22449.662\n",
      "[275,   200] loss: 34326.677\n",
      "[275,   205] loss: 20991.637\n",
      "[275,   210] loss: 33458.586\n",
      "[275,   215] loss: 17160.005\n",
      "[275,   220] loss: 43278.588\n",
      "[275,   225] loss: 24421.080\n",
      "[275,   230] loss: 25266.718\n",
      "[276,     5] loss: 26097.890\n",
      "[276,    10] loss: 20559.585\n",
      "[276,    15] loss: 15135.816\n",
      "[276,    20] loss: 22584.823\n",
      "[276,    25] loss: 23821.538\n",
      "[276,    30] loss: 44888.798\n",
      "[276,    35] loss: 20730.689\n",
      "[276,    40] loss: 19127.020\n",
      "[276,    45] loss: 23118.239\n",
      "[276,    50] loss: 28421.871\n",
      "[276,    55] loss: 21491.839\n",
      "[276,    60] loss: 31219.904\n",
      "[276,    65] loss: 30190.989\n",
      "[276,    70] loss: 19980.808\n",
      "[276,    75] loss: 25364.649\n",
      "[276,    80] loss: 27988.635\n",
      "[276,    85] loss: 25246.071\n",
      "[276,    90] loss: 25144.108\n",
      "[276,    95] loss: 29855.953\n",
      "[276,   100] loss: 69171.895\n",
      "[276,   105] loss: 24106.909\n",
      "[276,   110] loss: 22553.616\n",
      "[276,   115] loss: 22481.584\n",
      "[276,   120] loss: 27070.000\n",
      "[276,   125] loss: 33249.184\n",
      "[276,   130] loss: 22279.897\n",
      "[276,   135] loss: 26887.154\n",
      "[276,   140] loss: 17176.109\n",
      "[276,   145] loss: 22149.521\n",
      "[276,   150] loss: 26915.195\n",
      "[276,   155] loss: 29614.433\n",
      "[276,   160] loss: 24472.829\n",
      "[276,   165] loss: 32549.066\n",
      "[276,   170] loss: 15532.014\n",
      "[276,   175] loss: 24616.563\n",
      "[276,   180] loss: 21973.987\n",
      "[276,   185] loss: 24963.661\n",
      "[276,   190] loss: 21535.832\n",
      "[276,   195] loss: 25278.549\n",
      "[276,   200] loss: 25149.503\n",
      "[276,   205] loss: 27665.724\n",
      "[276,   210] loss: 23728.566\n",
      "[276,   215] loss: 42032.871\n",
      "[276,   220] loss: 18969.787\n",
      "[276,   225] loss: 20746.330\n",
      "[276,   230] loss: 24146.535\n",
      "[277,     5] loss: 28514.976\n",
      "[277,    10] loss: 26090.269\n",
      "[277,    15] loss: 27029.129\n",
      "[277,    20] loss: 20109.997\n",
      "[277,    25] loss: 32013.841\n",
      "[277,    30] loss: 14506.946\n",
      "[277,    35] loss: 27074.650\n",
      "[277,    40] loss: 44700.290\n",
      "[277,    45] loss: 22800.234\n",
      "[277,    50] loss: 28835.340\n",
      "[277,    55] loss: 22311.058\n",
      "[277,    60] loss: 29668.964\n",
      "[277,    65] loss: 13391.070\n",
      "[277,    70] loss: 35567.948\n",
      "[277,    75] loss: 26318.279\n",
      "[277,    80] loss: 34309.416\n",
      "[277,    85] loss: 24406.200\n",
      "[277,    90] loss: 27509.667\n",
      "[277,    95] loss: 27417.147\n",
      "[277,   100] loss: 22372.476\n",
      "[277,   105] loss: 17117.922\n",
      "[277,   110] loss: 19587.311\n",
      "[277,   115] loss: 23081.999\n",
      "[277,   120] loss: 22224.732\n",
      "[277,   125] loss: 27306.827\n",
      "[277,   130] loss: 32175.194\n",
      "[277,   135] loss: 46161.305\n",
      "[277,   140] loss: 28433.019\n",
      "[277,   145] loss: 22458.290\n",
      "[277,   150] loss: 14701.621\n",
      "[277,   155] loss: 22688.520\n",
      "[277,   160] loss: 30503.125\n",
      "[277,   165] loss: 24818.885\n",
      "[277,   170] loss: 25788.384\n",
      "[277,   175] loss: 22349.633\n",
      "[277,   180] loss: 23993.522\n",
      "[277,   185] loss: 24731.982\n",
      "[277,   190] loss: 30074.283\n",
      "[277,   195] loss: 16031.629\n",
      "[277,   200] loss: 27378.792\n",
      "[277,   205] loss: 20209.826\n",
      "[277,   210] loss: 19502.844\n",
      "[277,   215] loss: 23730.772\n",
      "[277,   220] loss: 32772.533\n",
      "[277,   225] loss: 32363.970\n",
      "[277,   230] loss: 31416.394\n",
      "[278,     5] loss: 24053.160\n",
      "[278,    10] loss: 28003.404\n",
      "[278,    15] loss: 19235.556\n",
      "[278,    20] loss: 32534.449\n",
      "[278,    25] loss: 21031.571\n",
      "[278,    30] loss: 44941.733\n",
      "[278,    35] loss: 17157.938\n",
      "[278,    40] loss: 37486.747\n",
      "[278,    45] loss: 15981.914\n",
      "[278,    50] loss: 27459.259\n",
      "[278,    55] loss: 23138.005\n",
      "[278,    60] loss: 29024.740\n",
      "[278,    65] loss: 23101.078\n",
      "[278,    70] loss: 20423.854\n",
      "[278,    75] loss: 27629.139\n",
      "[278,    80] loss: 23336.084\n",
      "[278,    85] loss: 27498.130\n",
      "[278,    90] loss: 27682.092\n",
      "[278,    95] loss: 25629.206\n",
      "[278,   100] loss: 26438.188\n",
      "[278,   105] loss: 18723.397\n",
      "[278,   110] loss: 22003.909\n",
      "[278,   115] loss: 24543.396\n",
      "[278,   120] loss: 31320.626\n",
      "[278,   125] loss: 27518.693\n",
      "[278,   130] loss: 17186.129\n",
      "[278,   135] loss: 31820.985\n",
      "[278,   140] loss: 27629.870\n",
      "[278,   145] loss: 30483.076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[278,   150] loss: 24177.095\n",
      "[278,   155] loss: 20791.298\n",
      "[278,   160] loss: 16373.678\n",
      "[278,   165] loss: 22928.699\n",
      "[278,   170] loss: 21531.641\n",
      "[278,   175] loss: 34206.408\n",
      "[278,   180] loss: 37680.448\n",
      "[278,   185] loss: 34929.629\n",
      "[278,   190] loss: 28945.939\n",
      "[278,   195] loss: 21734.451\n",
      "[278,   200] loss: 23672.861\n",
      "[278,   205] loss: 33339.280\n",
      "[278,   210] loss: 38354.061\n",
      "[278,   215] loss: 18138.511\n",
      "[278,   220] loss: 18846.964\n",
      "[278,   225] loss: 28473.825\n",
      "[278,   230] loss: 24025.729\n",
      "[279,     5] loss: 17993.994\n",
      "[279,    10] loss: 22969.966\n",
      "[279,    15] loss: 19343.075\n",
      "[279,    20] loss: 31571.981\n",
      "[279,    25] loss: 31143.237\n",
      "[279,    30] loss: 24176.927\n",
      "[279,    35] loss: 30952.128\n",
      "[279,    40] loss: 15101.613\n",
      "[279,    45] loss: 26672.848\n",
      "[279,    50] loss: 17309.571\n",
      "[279,    55] loss: 23094.231\n",
      "[279,    60] loss: 26081.516\n",
      "[279,    65] loss: 17490.870\n",
      "[279,    70] loss: 34140.053\n",
      "[279,    75] loss: 27932.104\n",
      "[279,    80] loss: 19147.622\n",
      "[279,    85] loss: 24556.922\n",
      "[279,    90] loss: 27982.840\n",
      "[279,    95] loss: 17159.617\n",
      "[279,   100] loss: 22375.282\n",
      "[279,   105] loss: 32928.535\n",
      "[279,   110] loss: 27186.432\n",
      "[279,   115] loss: 24357.207\n",
      "[279,   120] loss: 23361.217\n",
      "[279,   125] loss: 35333.376\n",
      "[279,   130] loss: 18214.068\n",
      "[279,   135] loss: 28757.645\n",
      "[279,   140] loss: 29823.661\n",
      "[279,   145] loss: 28474.925\n",
      "[279,   150] loss: 33725.117\n",
      "[279,   155] loss: 18955.331\n",
      "[279,   160] loss: 27125.162\n",
      "[279,   165] loss: 22312.650\n",
      "[279,   170] loss: 46451.637\n",
      "[279,   175] loss: 24331.126\n",
      "[279,   180] loss: 20432.074\n",
      "[279,   185] loss: 24233.683\n",
      "[279,   190] loss: 24122.804\n",
      "[279,   195] loss: 28677.698\n",
      "[279,   200] loss: 26699.685\n",
      "[279,   205] loss: 24472.906\n",
      "[279,   210] loss: 25575.356\n",
      "[279,   215] loss: 39124.175\n",
      "[279,   220] loss: 19446.155\n",
      "[279,   225] loss: 25320.643\n",
      "[279,   230] loss: 25539.261\n",
      "[280,     5] loss: 25482.291\n",
      "[280,    10] loss: 23182.142\n",
      "[280,    15] loss: 22525.338\n",
      "[280,    20] loss: 23035.075\n",
      "[280,    25] loss: 30262.088\n",
      "[280,    30] loss: 26718.986\n",
      "[280,    35] loss: 28803.140\n",
      "[280,    40] loss: 23613.471\n",
      "[280,    45] loss: 21700.747\n",
      "[280,    50] loss: 56271.043\n",
      "[280,    55] loss: 23632.818\n",
      "[280,    60] loss: 28151.556\n",
      "[280,    65] loss: 18787.679\n",
      "[280,    70] loss: 27032.256\n",
      "[280,    75] loss: 26483.270\n",
      "[280,    80] loss: 20854.118\n",
      "[280,    85] loss: 26053.699\n",
      "[280,    90] loss: 27337.050\n",
      "[280,    95] loss: 21791.496\n",
      "[280,   100] loss: 39466.540\n",
      "[280,   105] loss: 22193.851\n",
      "[280,   110] loss: 20003.785\n",
      "[280,   115] loss: 25053.447\n",
      "[280,   120] loss: 20214.912\n",
      "[280,   125] loss: 29134.194\n",
      "[280,   130] loss: 21297.376\n",
      "[280,   135] loss: 18028.049\n",
      "[280,   140] loss: 23828.073\n",
      "[280,   145] loss: 26755.587\n",
      "[280,   150] loss: 20303.368\n",
      "[280,   155] loss: 22410.546\n",
      "[280,   160] loss: 30752.408\n",
      "[280,   165] loss: 22513.249\n",
      "[280,   170] loss: 23512.090\n",
      "[280,   175] loss: 47457.137\n",
      "[280,   180] loss: 19808.050\n",
      "[280,   185] loss: 32541.650\n",
      "[280,   190] loss: 23177.931\n",
      "[280,   195] loss: 30992.578\n",
      "[280,   200] loss: 21150.233\n",
      "[280,   205] loss: 18876.270\n",
      "[280,   210] loss: 17782.026\n",
      "[280,   215] loss: 28008.782\n",
      "[280,   220] loss: 29358.157\n",
      "[280,   225] loss: 26491.837\n",
      "[280,   230] loss: 31869.172\n",
      "[281,     5] loss: 25914.040\n",
      "[281,    10] loss: 25990.352\n",
      "[281,    15] loss: 30488.139\n",
      "[281,    20] loss: 22386.464\n",
      "[281,    25] loss: 27365.490\n",
      "[281,    30] loss: 24601.133\n",
      "[281,    35] loss: 29730.665\n",
      "[281,    40] loss: 29381.676\n",
      "[281,    45] loss: 21685.607\n",
      "[281,    50] loss: 29607.346\n",
      "[281,    55] loss: 45417.759\n",
      "[281,    60] loss: 23296.350\n",
      "[281,    65] loss: 23208.309\n",
      "[281,    70] loss: 23537.332\n",
      "[281,    75] loss: 23660.221\n",
      "[281,    80] loss: 26535.521\n",
      "[281,    85] loss: 18147.833\n",
      "[281,    90] loss: 31697.410\n",
      "[281,    95] loss: 15965.434\n",
      "[281,   100] loss: 25261.882\n",
      "[281,   105] loss: 41359.267\n",
      "[281,   110] loss: 20944.877\n",
      "[281,   115] loss: 20401.518\n",
      "[281,   120] loss: 27014.010\n",
      "[281,   125] loss: 18573.822\n",
      "[281,   130] loss: 30983.811\n",
      "[281,   135] loss: 23609.601\n",
      "[281,   140] loss: 40333.316\n",
      "[281,   145] loss: 25488.572\n",
      "[281,   150] loss: 25309.971\n",
      "[281,   155] loss: 54820.018\n",
      "[281,   160] loss: 21083.963\n",
      "[281,   165] loss: 19391.189\n",
      "[281,   170] loss: 17507.138\n",
      "[281,   175] loss: 21786.408\n",
      "[281,   180] loss: 25468.870\n",
      "[281,   185] loss: 15982.231\n",
      "[281,   190] loss: 23980.246\n",
      "[281,   195] loss: 29990.188\n",
      "[281,   200] loss: 23642.123\n",
      "[281,   205] loss: 24549.934\n",
      "[281,   210] loss: 33112.521\n",
      "[281,   215] loss: 21272.813\n",
      "[281,   220] loss: 16398.189\n",
      "[281,   225] loss: 22696.046\n",
      "[281,   230] loss: 23716.695\n",
      "[282,     5] loss: 19743.616\n",
      "[282,    10] loss: 32304.500\n",
      "[282,    15] loss: 20744.960\n",
      "[282,    20] loss: 28083.796\n",
      "[282,    25] loss: 20441.894\n",
      "[282,    30] loss: 33260.670\n",
      "[282,    35] loss: 22446.879\n",
      "[282,    40] loss: 21100.703\n",
      "[282,    45] loss: 22493.404\n",
      "[282,    50] loss: 16126.995\n",
      "[282,    55] loss: 17425.855\n",
      "[282,    60] loss: 13387.606\n",
      "[282,    65] loss: 22723.939\n",
      "[282,    70] loss: 31950.601\n",
      "[282,    75] loss: 33207.782\n",
      "[282,    80] loss: 29478.926\n",
      "[282,    85] loss: 19833.530\n",
      "[282,    90] loss: 23596.221\n",
      "[282,    95] loss: 33738.683\n",
      "[282,   100] loss: 28088.386\n",
      "[282,   105] loss: 25809.875\n",
      "[282,   110] loss: 25425.502\n",
      "[282,   115] loss: 22645.798\n",
      "[282,   120] loss: 30102.604\n",
      "[282,   125] loss: 35381.398\n",
      "[282,   130] loss: 27098.646\n",
      "[282,   135] loss: 23404.430\n",
      "[282,   140] loss: 23100.070\n",
      "[282,   145] loss: 25489.320\n",
      "[282,   150] loss: 25914.442\n",
      "[282,   155] loss: 16698.999\n",
      "[282,   160] loss: 30502.500\n",
      "[282,   165] loss: 32411.696\n",
      "[282,   170] loss: 52121.451\n",
      "[282,   175] loss: 21990.478\n",
      "[282,   180] loss: 28588.672\n",
      "[282,   185] loss: 29144.323\n",
      "[282,   190] loss: 17410.820\n",
      "[282,   195] loss: 22816.259\n",
      "[282,   200] loss: 29880.407\n",
      "[282,   205] loss: 29919.747\n",
      "[282,   210] loss: 20449.546\n",
      "[282,   215] loss: 17960.881\n",
      "[282,   220] loss: 23558.390\n",
      "[282,   225] loss: 23485.048\n",
      "[282,   230] loss: 33625.593\n",
      "[283,     5] loss: 27526.058\n",
      "[283,    10] loss: 18369.062\n",
      "[283,    15] loss: 30737.963\n",
      "[283,    20] loss: 21096.698\n",
      "[283,    25] loss: 17651.544\n",
      "[283,    30] loss: 29479.014\n",
      "[283,    35] loss: 21875.217\n",
      "[283,    40] loss: 26540.308\n",
      "[283,    45] loss: 18565.250\n",
      "[283,    50] loss: 31812.929\n",
      "[283,    55] loss: 29068.575\n",
      "[283,    60] loss: 22653.183\n",
      "[283,    65] loss: 29679.519\n",
      "[283,    70] loss: 32294.664\n",
      "[283,    75] loss: 27532.988\n",
      "[283,    80] loss: 27288.181\n",
      "[283,    85] loss: 19886.662\n",
      "[283,    90] loss: 25673.089\n",
      "[283,    95] loss: 34938.677\n",
      "[283,   100] loss: 26654.288\n",
      "[283,   105] loss: 32611.548\n",
      "[283,   110] loss: 30832.747\n",
      "[283,   115] loss: 32974.274\n",
      "[283,   120] loss: 25313.229\n",
      "[283,   125] loss: 27483.627\n",
      "[283,   130] loss: 18744.423\n",
      "[283,   135] loss: 31641.034\n",
      "[283,   140] loss: 18801.043\n",
      "[283,   145] loss: 31311.591\n",
      "[283,   150] loss: 32996.810\n",
      "[283,   155] loss: 24349.607\n",
      "[283,   160] loss: 31587.189\n",
      "[283,   165] loss: 30186.889\n",
      "[283,   170] loss: 29900.704\n",
      "[283,   175] loss: 28933.946\n",
      "[283,   180] loss: 15915.079\n",
      "[283,   185] loss: 18454.513\n",
      "[283,   190] loss: 46022.682\n",
      "[283,   195] loss: 19515.731\n",
      "[283,   200] loss: 17313.386\n",
      "[283,   205] loss: 23239.879\n",
      "[283,   210] loss: 18378.274\n",
      "[283,   215] loss: 18305.988\n",
      "[283,   220] loss: 27025.191\n",
      "[283,   225] loss: 23282.991\n",
      "[283,   230] loss: 17331.771\n",
      "[284,     5] loss: 27660.323\n",
      "[284,    10] loss: 32632.974\n",
      "[284,    15] loss: 25729.872\n",
      "[284,    20] loss: 28868.184\n",
      "[284,    25] loss: 21377.162\n",
      "[284,    30] loss: 30628.056\n",
      "[284,    35] loss: 21330.619\n",
      "[284,    40] loss: 29029.264\n",
      "[284,    45] loss: 30780.359\n",
      "[284,    50] loss: 24444.180\n",
      "[284,    55] loss: 21914.732\n",
      "[284,    60] loss: 12252.781\n",
      "[284,    65] loss: 24164.852\n",
      "[284,    70] loss: 37879.934\n",
      "[284,    75] loss: 18912.085\n",
      "[284,    80] loss: 19868.627\n",
      "[284,    85] loss: 25461.296\n",
      "[284,    90] loss: 14988.851\n",
      "[284,    95] loss: 23105.760\n",
      "[284,   100] loss: 26774.538\n",
      "[284,   105] loss: 27781.115\n",
      "[284,   110] loss: 25300.786\n",
      "[284,   115] loss: 24619.877\n",
      "[284,   120] loss: 35963.783\n",
      "[284,   125] loss: 25743.692\n",
      "[284,   130] loss: 28107.714\n",
      "[284,   135] loss: 27309.200\n",
      "[284,   140] loss: 35455.203\n",
      "[284,   145] loss: 25879.616\n",
      "[284,   150] loss: 35263.034\n",
      "[284,   155] loss: 49464.668\n",
      "[284,   160] loss: 24759.015\n",
      "[284,   165] loss: 16715.343\n",
      "[284,   170] loss: 22191.285\n",
      "[284,   175] loss: 22226.425\n",
      "[284,   180] loss: 24452.702\n",
      "[284,   185] loss: 19335.143\n",
      "[284,   190] loss: 29686.875\n",
      "[284,   195] loss: 20955.450\n",
      "[284,   200] loss: 33611.999\n",
      "[284,   205] loss: 26553.043\n",
      "[284,   210] loss: 20716.093\n",
      "[284,   215] loss: 23081.923\n",
      "[284,   220] loss: 33156.168\n",
      "[284,   225] loss: 20713.697\n",
      "[284,   230] loss: 20075.630\n",
      "[285,     5] loss: 21127.093\n",
      "[285,    10] loss: 34275.718\n",
      "[285,    15] loss: 20284.864\n",
      "[285,    20] loss: 23872.482\n",
      "[285,    25] loss: 26407.225\n",
      "[285,    30] loss: 19533.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285,    35] loss: 22633.704\n",
      "[285,    40] loss: 33460.110\n",
      "[285,    45] loss: 30749.564\n",
      "[285,    50] loss: 40299.832\n",
      "[285,    55] loss: 29794.037\n",
      "[285,    60] loss: 21322.351\n",
      "[285,    65] loss: 19875.641\n",
      "[285,    70] loss: 29778.194\n",
      "[285,    75] loss: 52745.422\n",
      "[285,    80] loss: 29551.971\n",
      "[285,    85] loss: 17489.330\n",
      "[285,    90] loss: 30899.395\n",
      "[285,    95] loss: 31227.829\n",
      "[285,   100] loss: 22165.842\n",
      "[285,   105] loss: 19031.900\n",
      "[285,   110] loss: 28196.083\n",
      "[285,   115] loss: 27856.047\n",
      "[285,   120] loss: 18414.279\n",
      "[285,   125] loss: 30641.694\n",
      "[285,   130] loss: 22191.548\n",
      "[285,   135] loss: 24284.958\n",
      "[285,   140] loss: 25709.430\n",
      "[285,   145] loss: 28146.415\n",
      "[285,   150] loss: 22120.337\n",
      "[285,   155] loss: 20444.811\n",
      "[285,   160] loss: 36951.160\n",
      "[285,   165] loss: 18942.121\n",
      "[285,   170] loss: 18419.159\n",
      "[285,   175] loss: 12964.754\n",
      "[285,   180] loss: 31388.015\n",
      "[285,   185] loss: 32643.567\n",
      "[285,   190] loss: 17305.675\n",
      "[285,   195] loss: 29631.442\n",
      "[285,   200] loss: 25533.776\n",
      "[285,   205] loss: 30063.455\n",
      "[285,   210] loss: 23783.644\n",
      "[285,   215] loss: 19926.571\n",
      "[285,   220] loss: 31229.960\n",
      "[285,   225] loss: 21130.195\n",
      "[285,   230] loss: 18645.080\n",
      "[286,     5] loss: 23766.428\n",
      "[286,    10] loss: 20689.116\n",
      "[286,    15] loss: 26316.250\n",
      "[286,    20] loss: 22707.722\n",
      "[286,    25] loss: 24429.437\n",
      "[286,    30] loss: 23235.948\n",
      "[286,    35] loss: 46142.754\n",
      "[286,    40] loss: 21986.654\n",
      "[286,    45] loss: 27187.321\n",
      "[286,    50] loss: 15417.445\n",
      "[286,    55] loss: 19867.468\n",
      "[286,    60] loss: 31301.782\n",
      "[286,    65] loss: 21739.434\n",
      "[286,    70] loss: 19063.252\n",
      "[286,    75] loss: 22407.589\n",
      "[286,    80] loss: 33441.188\n",
      "[286,    85] loss: 31145.203\n",
      "[286,    90] loss: 26677.596\n",
      "[286,    95] loss: 28094.991\n",
      "[286,   100] loss: 44360.498\n",
      "[286,   105] loss: 26127.048\n",
      "[286,   110] loss: 26561.440\n",
      "[286,   115] loss: 28086.645\n",
      "[286,   120] loss: 32823.989\n",
      "[286,   125] loss: 25036.151\n",
      "[286,   130] loss: 28689.900\n",
      "[286,   135] loss: 33369.950\n",
      "[286,   140] loss: 26399.254\n",
      "[286,   145] loss: 21209.390\n",
      "[286,   150] loss: 26546.619\n",
      "[286,   155] loss: 20139.651\n",
      "[286,   160] loss: 35909.322\n",
      "[286,   165] loss: 17849.128\n",
      "[286,   170] loss: 27510.013\n",
      "[286,   175] loss: 20341.004\n",
      "[286,   180] loss: 39338.761\n",
      "[286,   185] loss: 21727.944\n",
      "[286,   190] loss: 18870.948\n",
      "[286,   195] loss: 28315.323\n",
      "[286,   200] loss: 19779.311\n",
      "[286,   205] loss: 21694.638\n",
      "[286,   210] loss: 23024.090\n",
      "[286,   215] loss: 24299.975\n",
      "[286,   220] loss: 15346.617\n",
      "[286,   225] loss: 22040.450\n",
      "[286,   230] loss: 25473.563\n",
      "[287,     5] loss: 27403.910\n",
      "[287,    10] loss: 28156.107\n",
      "[287,    15] loss: 22849.737\n",
      "[287,    20] loss: 24654.426\n",
      "[287,    25] loss: 23311.421\n",
      "[287,    30] loss: 26298.126\n",
      "[287,    35] loss: 35280.596\n",
      "[287,    40] loss: 21160.220\n",
      "[287,    45] loss: 35821.416\n",
      "[287,    50] loss: 21887.958\n",
      "[287,    55] loss: 20011.956\n",
      "[287,    60] loss: 17034.402\n",
      "[287,    65] loss: 18660.559\n",
      "[287,    70] loss: 31081.171\n",
      "[287,    75] loss: 31941.708\n",
      "[287,    80] loss: 25140.662\n",
      "[287,    85] loss: 22262.539\n",
      "[287,    90] loss: 30284.880\n",
      "[287,    95] loss: 24381.600\n",
      "[287,   100] loss: 18947.360\n",
      "[287,   105] loss: 22134.256\n",
      "[287,   110] loss: 21290.621\n",
      "[287,   115] loss: 29819.542\n",
      "[287,   120] loss: 30204.275\n",
      "[287,   125] loss: 24171.853\n",
      "[287,   130] loss: 23477.973\n",
      "[287,   135] loss: 23043.054\n",
      "[287,   140] loss: 57030.353\n",
      "[287,   145] loss: 18203.740\n",
      "[287,   150] loss: 23979.464\n",
      "[287,   155] loss: 46666.139\n",
      "[287,   160] loss: 22196.557\n",
      "[287,   165] loss: 19343.202\n",
      "[287,   170] loss: 22478.357\n",
      "[287,   175] loss: 15413.909\n",
      "[287,   180] loss: 19669.707\n",
      "[287,   185] loss: 18151.830\n",
      "[287,   190] loss: 26852.943\n",
      "[287,   195] loss: 22533.031\n",
      "[287,   200] loss: 22197.850\n",
      "[287,   205] loss: 22832.847\n",
      "[287,   210] loss: 23682.746\n",
      "[287,   215] loss: 33235.325\n",
      "[287,   220] loss: 30460.393\n",
      "[287,   225] loss: 30064.496\n",
      "[287,   230] loss: 25441.933\n",
      "[288,     5] loss: 32281.584\n",
      "[288,    10] loss: 15009.123\n",
      "[288,    15] loss: 19709.221\n",
      "[288,    20] loss: 28423.104\n",
      "[288,    25] loss: 28217.065\n",
      "[288,    30] loss: 16713.634\n",
      "[288,    35] loss: 29447.652\n",
      "[288,    40] loss: 16359.503\n",
      "[288,    45] loss: 19125.083\n",
      "[288,    50] loss: 21758.793\n",
      "[288,    55] loss: 25598.044\n",
      "[288,    60] loss: 17549.572\n",
      "[288,    65] loss: 30227.554\n",
      "[288,    70] loss: 28795.616\n",
      "[288,    75] loss: 32875.824\n",
      "[288,    80] loss: 15075.706\n",
      "[288,    85] loss: 36905.370\n",
      "[288,    90] loss: 27018.344\n",
      "[288,    95] loss: 34966.191\n",
      "[288,   100] loss: 25069.856\n",
      "[288,   105] loss: 36458.001\n",
      "[288,   110] loss: 22517.790\n",
      "[288,   115] loss: 20926.861\n",
      "[288,   120] loss: 16178.064\n",
      "[288,   125] loss: 24286.266\n",
      "[288,   130] loss: 20256.948\n",
      "[288,   135] loss: 25580.727\n",
      "[288,   140] loss: 27533.552\n",
      "[288,   145] loss: 27641.707\n",
      "[288,   150] loss: 74221.164\n",
      "[288,   155] loss: 33509.062\n",
      "[288,   160] loss: 22287.652\n",
      "[288,   165] loss: 23149.022\n",
      "[288,   170] loss: 21143.845\n",
      "[288,   175] loss: 20791.176\n",
      "[288,   180] loss: 30606.026\n",
      "[288,   185] loss: 31413.589\n",
      "[288,   190] loss: 25781.153\n",
      "[288,   195] loss: 29957.161\n",
      "[288,   200] loss: 16086.746\n",
      "[288,   205] loss: 21866.358\n",
      "[288,   210] loss: 24450.922\n",
      "[288,   215] loss: 23310.405\n",
      "[288,   220] loss: 28254.347\n",
      "[288,   225] loss: 16283.231\n",
      "[288,   230] loss: 26408.730\n",
      "[289,     5] loss: 17973.697\n",
      "[289,    10] loss: 20083.196\n",
      "[289,    15] loss: 23804.004\n",
      "[289,    20] loss: 27961.757\n",
      "[289,    25] loss: 24579.798\n",
      "[289,    30] loss: 30159.377\n",
      "[289,    35] loss: 26660.635\n",
      "[289,    40] loss: 23454.561\n",
      "[289,    45] loss: 28463.612\n",
      "[289,    50] loss: 25026.048\n",
      "[289,    55] loss: 34598.273\n",
      "[289,    60] loss: 26531.338\n",
      "[289,    65] loss: 35542.482\n",
      "[289,    70] loss: 19401.378\n",
      "[289,    75] loss: 20516.314\n",
      "[289,    80] loss: 22866.746\n",
      "[289,    85] loss: 21304.579\n",
      "[289,    90] loss: 21398.207\n",
      "[289,    95] loss: 24094.305\n",
      "[289,   100] loss: 23687.665\n",
      "[289,   105] loss: 16547.633\n",
      "[289,   110] loss: 28487.376\n",
      "[289,   115] loss: 18811.976\n",
      "[289,   120] loss: 21836.715\n",
      "[289,   125] loss: 34209.103\n",
      "[289,   130] loss: 25810.365\n",
      "[289,   135] loss: 34285.494\n",
      "[289,   140] loss: 20001.570\n",
      "[289,   145] loss: 28306.630\n",
      "[289,   150] loss: 36639.160\n",
      "[289,   155] loss: 29659.268\n",
      "[289,   160] loss: 21929.096\n",
      "[289,   165] loss: 20579.742\n",
      "[289,   170] loss: 25267.617\n",
      "[289,   175] loss: 15996.054\n",
      "[289,   180] loss: 29857.885\n",
      "[289,   185] loss: 21320.815\n",
      "[289,   190] loss: 25085.237\n",
      "[289,   195] loss: 29388.577\n",
      "[289,   200] loss: 47326.380\n",
      "[289,   205] loss: 17481.197\n",
      "[289,   210] loss: 24067.115\n",
      "[289,   215] loss: 36171.364\n",
      "[289,   220] loss: 22956.751\n",
      "[289,   225] loss: 28071.774\n",
      "[289,   230] loss: 29243.491\n",
      "[290,     5] loss: 25260.358\n",
      "[290,    10] loss: 25810.402\n",
      "[290,    15] loss: 16312.889\n",
      "[290,    20] loss: 20587.364\n",
      "[290,    25] loss: 21843.169\n",
      "[290,    30] loss: 19312.941\n",
      "[290,    35] loss: 30167.502\n",
      "[290,    40] loss: 23022.516\n",
      "[290,    45] loss: 20028.187\n",
      "[290,    50] loss: 26821.524\n",
      "[290,    55] loss: 18644.221\n",
      "[290,    60] loss: 21187.967\n",
      "[290,    65] loss: 22674.620\n",
      "[290,    70] loss: 26625.709\n",
      "[290,    75] loss: 33036.776\n",
      "[290,    80] loss: 23209.993\n",
      "[290,    85] loss: 23889.752\n",
      "[290,    90] loss: 24184.837\n",
      "[290,    95] loss: 44164.383\n",
      "[290,   100] loss: 31895.961\n",
      "[290,   105] loss: 37793.279\n",
      "[290,   110] loss: 34392.820\n",
      "[290,   115] loss: 34826.991\n",
      "[290,   120] loss: 31646.053\n",
      "[290,   125] loss: 17786.438\n",
      "[290,   130] loss: 21112.467\n",
      "[290,   135] loss: 18407.149\n",
      "[290,   140] loss: 33083.760\n",
      "[290,   145] loss: 23517.524\n",
      "[290,   150] loss: 21977.081\n",
      "[290,   155] loss: 24924.983\n",
      "[290,   160] loss: 22303.677\n",
      "[290,   165] loss: 18118.188\n",
      "[290,   170] loss: 19673.503\n",
      "[290,   175] loss: 21717.355\n",
      "[290,   180] loss: 33939.912\n",
      "[290,   185] loss: 24149.289\n",
      "[290,   190] loss: 28659.842\n",
      "[290,   195] loss: 32004.753\n",
      "[290,   200] loss: 15571.756\n",
      "[290,   205] loss: 17364.324\n",
      "[290,   210] loss: 28236.499\n",
      "[290,   215] loss: 26336.075\n",
      "[290,   220] loss: 25477.174\n",
      "[290,   225] loss: 40913.920\n",
      "[290,   230] loss: 28692.699\n",
      "[291,     5] loss: 30704.991\n",
      "[291,    10] loss: 26203.554\n",
      "[291,    15] loss: 34318.366\n",
      "[291,    20] loss: 25389.300\n",
      "[291,    25] loss: 20490.436\n",
      "[291,    30] loss: 19220.764\n",
      "[291,    35] loss: 35805.260\n",
      "[291,    40] loss: 42155.132\n",
      "[291,    45] loss: 28229.916\n",
      "[291,    50] loss: 18914.643\n",
      "[291,    55] loss: 20133.354\n",
      "[291,    60] loss: 31623.799\n",
      "[291,    65] loss: 27370.270\n",
      "[291,    70] loss: 26721.990\n",
      "[291,    75] loss: 21114.584\n",
      "[291,    80] loss: 45085.147\n",
      "[291,    85] loss: 27876.889\n",
      "[291,    90] loss: 17385.045\n",
      "[291,    95] loss: 24139.688\n",
      "[291,   100] loss: 22629.268\n",
      "[291,   105] loss: 14595.950\n",
      "[291,   110] loss: 21770.802\n",
      "[291,   115] loss: 23862.882\n",
      "[291,   120] loss: 27796.135\n",
      "[291,   125] loss: 26131.433\n",
      "[291,   130] loss: 25636.871\n",
      "[291,   135] loss: 14353.081\n",
      "[291,   140] loss: 24388.549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291,   145] loss: 14529.714\n",
      "[291,   150] loss: 17623.810\n",
      "[291,   155] loss: 38693.964\n",
      "[291,   160] loss: 21325.674\n",
      "[291,   165] loss: 34245.088\n",
      "[291,   170] loss: 21483.483\n",
      "[291,   175] loss: 29412.555\n",
      "[291,   180] loss: 32600.497\n",
      "[291,   185] loss: 35336.812\n",
      "[291,   190] loss: 14122.673\n",
      "[291,   195] loss: 21634.532\n",
      "[291,   200] loss: 21284.206\n",
      "[291,   205] loss: 28297.584\n",
      "[291,   210] loss: 23877.432\n",
      "[291,   215] loss: 23737.859\n",
      "[291,   220] loss: 18333.246\n",
      "[291,   225] loss: 31199.903\n",
      "[291,   230] loss: 38347.846\n",
      "[292,     5] loss: 26018.122\n",
      "[292,    10] loss: 28270.504\n",
      "[292,    15] loss: 38872.499\n",
      "[292,    20] loss: 18893.593\n",
      "[292,    25] loss: 29763.533\n",
      "[292,    30] loss: 19283.841\n",
      "[292,    35] loss: 36681.888\n",
      "[292,    40] loss: 22380.961\n",
      "[292,    45] loss: 27646.317\n",
      "[292,    50] loss: 23251.563\n",
      "[292,    55] loss: 23336.032\n",
      "[292,    60] loss: 19160.354\n",
      "[292,    65] loss: 23380.091\n",
      "[292,    70] loss: 32316.873\n",
      "[292,    75] loss: 27664.466\n",
      "[292,    80] loss: 25706.152\n",
      "[292,    85] loss: 26403.305\n",
      "[292,    90] loss: 21713.440\n",
      "[292,    95] loss: 23523.277\n",
      "[292,   100] loss: 45207.305\n",
      "[292,   105] loss: 32302.370\n",
      "[292,   110] loss: 25036.212\n",
      "[292,   115] loss: 27648.093\n",
      "[292,   120] loss: 31500.361\n",
      "[292,   125] loss: 17274.627\n",
      "[292,   130] loss: 25055.289\n",
      "[292,   135] loss: 28054.054\n",
      "[292,   140] loss: 19817.260\n",
      "[292,   145] loss: 28328.577\n",
      "[292,   150] loss: 30640.307\n",
      "[292,   155] loss: 22733.354\n",
      "[292,   160] loss: 26759.563\n",
      "[292,   165] loss: 26951.584\n",
      "[292,   170] loss: 16454.326\n",
      "[292,   175] loss: 12082.571\n",
      "[292,   180] loss: 27131.116\n",
      "[292,   185] loss: 26055.271\n",
      "[292,   190] loss: 27225.387\n",
      "[292,   195] loss: 22303.267\n",
      "[292,   200] loss: 19307.696\n",
      "[292,   205] loss: 25190.760\n",
      "[292,   210] loss: 19463.787\n",
      "[292,   215] loss: 24496.145\n",
      "[292,   220] loss: 21496.921\n",
      "[292,   225] loss: 31216.970\n",
      "[292,   230] loss: 34652.288\n",
      "[293,     5] loss: 26056.977\n",
      "[293,    10] loss: 22911.131\n",
      "[293,    15] loss: 29768.092\n",
      "[293,    20] loss: 21484.619\n",
      "[293,    25] loss: 23070.791\n",
      "[293,    30] loss: 28338.103\n",
      "[293,    35] loss: 21314.925\n",
      "[293,    40] loss: 22726.053\n",
      "[293,    45] loss: 27234.765\n",
      "[293,    50] loss: 36841.737\n",
      "[293,    55] loss: 30596.159\n",
      "[293,    60] loss: 23285.204\n",
      "[293,    65] loss: 32879.931\n",
      "[293,    70] loss: 26477.381\n",
      "[293,    75] loss: 24455.342\n",
      "[293,    80] loss: 14564.880\n",
      "[293,    85] loss: 28159.612\n",
      "[293,    90] loss: 49656.735\n",
      "[293,    95] loss: 21103.012\n",
      "[293,   100] loss: 37821.791\n",
      "[293,   105] loss: 22242.763\n",
      "[293,   110] loss: 24775.304\n",
      "[293,   115] loss: 20444.809\n",
      "[293,   120] loss: 23593.286\n",
      "[293,   125] loss: 19248.098\n",
      "[293,   130] loss: 21536.684\n",
      "[293,   135] loss: 31362.652\n",
      "[293,   140] loss: 26671.625\n",
      "[293,   145] loss: 29483.586\n",
      "[293,   150] loss: 31070.436\n",
      "[293,   155] loss: 28492.209\n",
      "[293,   160] loss: 33578.818\n",
      "[293,   165] loss: 19338.683\n",
      "[293,   170] loss: 30733.923\n",
      "[293,   175] loss: 25702.906\n",
      "[293,   180] loss: 22282.786\n",
      "[293,   185] loss: 26906.584\n",
      "[293,   190] loss: 23832.914\n",
      "[293,   195] loss: 23934.717\n",
      "[293,   200] loss: 17120.692\n",
      "[293,   205] loss: 13033.547\n",
      "[293,   210] loss: 39211.957\n",
      "[293,   215] loss: 29109.439\n",
      "[293,   220] loss: 16276.860\n",
      "[293,   225] loss: 20970.036\n",
      "[293,   230] loss: 17143.172\n",
      "[294,     5] loss: 22008.647\n",
      "[294,    10] loss: 30038.094\n",
      "[294,    15] loss: 28601.733\n",
      "[294,    20] loss: 35128.875\n",
      "[294,    25] loss: 21559.416\n",
      "[294,    30] loss: 22578.125\n",
      "[294,    35] loss: 25591.213\n",
      "[294,    40] loss: 17069.524\n",
      "[294,    45] loss: 23712.597\n",
      "[294,    50] loss: 19938.731\n",
      "[294,    55] loss: 27187.060\n",
      "[294,    60] loss: 23064.254\n",
      "[294,    65] loss: 17514.143\n",
      "[294,    70] loss: 35020.504\n",
      "[294,    75] loss: 33790.632\n",
      "[294,    80] loss: 15606.127\n",
      "[294,    85] loss: 28196.071\n",
      "[294,    90] loss: 24677.754\n",
      "[294,    95] loss: 21896.857\n",
      "[294,   100] loss: 22866.689\n",
      "[294,   105] loss: 48499.560\n",
      "[294,   110] loss: 28836.649\n",
      "[294,   115] loss: 17488.379\n",
      "[294,   120] loss: 30969.540\n",
      "[294,   125] loss: 22333.657\n",
      "[294,   130] loss: 25681.685\n",
      "[294,   135] loss: 24184.162\n",
      "[294,   140] loss: 26065.682\n",
      "[294,   145] loss: 23331.172\n",
      "[294,   150] loss: 27597.062\n",
      "[294,   155] loss: 30881.875\n",
      "[294,   160] loss: 23329.663\n",
      "[294,   165] loss: 40918.623\n",
      "[294,   170] loss: 25805.343\n",
      "[294,   175] loss: 26746.328\n",
      "[294,   180] loss: 21389.086\n",
      "[294,   185] loss: 20620.868\n",
      "[294,   190] loss: 24476.412\n",
      "[294,   195] loss: 22118.276\n",
      "[294,   200] loss: 28509.916\n",
      "[294,   205] loss: 21751.480\n",
      "[294,   210] loss: 23178.964\n",
      "[294,   215] loss: 25960.198\n",
      "[294,   220] loss: 36982.996\n",
      "[294,   225] loss: 18161.736\n",
      "[294,   230] loss: 26206.688\n",
      "[295,     5] loss: 26606.122\n",
      "[295,    10] loss: 18565.007\n",
      "[295,    15] loss: 20944.899\n",
      "[295,    20] loss: 25167.849\n",
      "[295,    25] loss: 24391.796\n",
      "[295,    30] loss: 26267.858\n",
      "[295,    35] loss: 26674.153\n",
      "[295,    40] loss: 18334.787\n",
      "[295,    45] loss: 22443.777\n",
      "[295,    50] loss: 19566.074\n",
      "[295,    55] loss: 22107.837\n",
      "[295,    60] loss: 26465.484\n",
      "[295,    65] loss: 29702.907\n",
      "[295,    70] loss: 25336.239\n",
      "[295,    75] loss: 26788.685\n",
      "[295,    80] loss: 22499.161\n",
      "[295,    85] loss: 26792.870\n",
      "[295,    90] loss: 27579.580\n",
      "[295,    95] loss: 51767.983\n",
      "[295,   100] loss: 24481.819\n",
      "[295,   105] loss: 29382.816\n",
      "[295,   110] loss: 43510.337\n",
      "[295,   115] loss: 28036.327\n",
      "[295,   120] loss: 27800.257\n",
      "[295,   125] loss: 24979.232\n",
      "[295,   130] loss: 30148.192\n",
      "[295,   135] loss: 24316.457\n",
      "[295,   140] loss: 26697.888\n",
      "[295,   145] loss: 22176.905\n",
      "[295,   150] loss: 26661.604\n",
      "[295,   155] loss: 18622.185\n",
      "[295,   160] loss: 20487.000\n",
      "[295,   165] loss: 28865.606\n",
      "[295,   170] loss: 16357.870\n",
      "[295,   175] loss: 25408.488\n",
      "[295,   180] loss: 21648.233\n",
      "[295,   185] loss: 25116.166\n",
      "[295,   190] loss: 19808.155\n",
      "[295,   195] loss: 20501.780\n",
      "[295,   200] loss: 23682.511\n",
      "[295,   205] loss: 22286.626\n",
      "[295,   210] loss: 28433.755\n",
      "[295,   215] loss: 50368.761\n",
      "[295,   220] loss: 22241.497\n",
      "[295,   225] loss: 22607.440\n",
      "[295,   230] loss: 31280.591\n",
      "[296,     5] loss: 27433.871\n",
      "[296,    10] loss: 24780.045\n",
      "[296,    15] loss: 24025.431\n",
      "[296,    20] loss: 31047.372\n",
      "[296,    25] loss: 18736.183\n",
      "[296,    30] loss: 27637.158\n",
      "[296,    35] loss: 27571.696\n",
      "[296,    40] loss: 16652.139\n",
      "[296,    45] loss: 23112.419\n",
      "[296,    50] loss: 17423.425\n",
      "[296,    55] loss: 40990.807\n",
      "[296,    60] loss: 20301.704\n",
      "[296,    65] loss: 41470.447\n",
      "[296,    70] loss: 27827.743\n",
      "[296,    75] loss: 59296.738\n",
      "[296,    80] loss: 24353.185\n",
      "[296,    85] loss: 26523.587\n",
      "[296,    90] loss: 26564.951\n",
      "[296,    95] loss: 27995.980\n",
      "[296,   100] loss: 25187.750\n",
      "[296,   105] loss: 24921.889\n",
      "[296,   110] loss: 20829.179\n",
      "[296,   115] loss: 24339.458\n",
      "[296,   120] loss: 24551.856\n",
      "[296,   125] loss: 21820.921\n",
      "[296,   130] loss: 15725.742\n",
      "[296,   135] loss: 22140.856\n",
      "[296,   140] loss: 21567.075\n",
      "[296,   145] loss: 15518.232\n",
      "[296,   150] loss: 19464.836\n",
      "[296,   155] loss: 25365.955\n",
      "[296,   160] loss: 23085.615\n",
      "[296,   165] loss: 27217.398\n",
      "[296,   170] loss: 31054.615\n",
      "[296,   175] loss: 17775.401\n",
      "[296,   180] loss: 25155.084\n",
      "[296,   185] loss: 17954.695\n",
      "[296,   190] loss: 28378.957\n",
      "[296,   195] loss: 39236.765\n",
      "[296,   200] loss: 27555.883\n",
      "[296,   205] loss: 20571.009\n",
      "[296,   210] loss: 17363.020\n",
      "[296,   215] loss: 31680.864\n",
      "[296,   220] loss: 20685.627\n",
      "[296,   225] loss: 19978.387\n",
      "[296,   230] loss: 32755.183\n",
      "[297,     5] loss: 37378.724\n",
      "[297,    10] loss: 17145.999\n",
      "[297,    15] loss: 14693.077\n",
      "[297,    20] loss: 24360.365\n",
      "[297,    25] loss: 22034.903\n",
      "[297,    30] loss: 29866.100\n",
      "[297,    35] loss: 28334.799\n",
      "[297,    40] loss: 32111.109\n",
      "[297,    45] loss: 16866.881\n",
      "[297,    50] loss: 36871.522\n",
      "[297,    55] loss: 19543.960\n",
      "[297,    60] loss: 27085.748\n",
      "[297,    65] loss: 19474.448\n",
      "[297,    70] loss: 24446.979\n",
      "[297,    75] loss: 21684.498\n",
      "[297,    80] loss: 29007.581\n",
      "[297,    85] loss: 22686.485\n",
      "[297,    90] loss: 27873.846\n",
      "[297,    95] loss: 28267.383\n",
      "[297,   100] loss: 24875.272\n",
      "[297,   105] loss: 25289.136\n",
      "[297,   110] loss: 26844.143\n",
      "[297,   115] loss: 27676.916\n",
      "[297,   120] loss: 21984.360\n",
      "[297,   125] loss: 23612.235\n",
      "[297,   130] loss: 29606.400\n",
      "[297,   135] loss: 25346.723\n",
      "[297,   140] loss: 45959.124\n",
      "[297,   145] loss: 20793.095\n",
      "[297,   150] loss: 22205.848\n",
      "[297,   155] loss: 33082.239\n",
      "[297,   160] loss: 21011.840\n",
      "[297,   165] loss: 14701.738\n",
      "[297,   170] loss: 24565.325\n",
      "[297,   175] loss: 28675.791\n",
      "[297,   180] loss: 33650.816\n",
      "[297,   185] loss: 33696.593\n",
      "[297,   190] loss: 18206.630\n",
      "[297,   195] loss: 36254.834\n",
      "[297,   200] loss: 21590.039\n",
      "[297,   205] loss: 38209.212\n",
      "[297,   210] loss: 16167.477\n",
      "[297,   215] loss: 27239.155\n",
      "[297,   220] loss: 19556.481\n",
      "[297,   225] loss: 27565.373\n",
      "[297,   230] loss: 24877.632\n",
      "[298,     5] loss: 17823.588\n",
      "[298,    10] loss: 28976.604\n",
      "[298,    15] loss: 15398.454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[298,    20] loss: 27048.544\n",
      "[298,    25] loss: 19338.781\n",
      "[298,    30] loss: 37532.897\n",
      "[298,    35] loss: 24521.961\n",
      "[298,    40] loss: 25841.024\n",
      "[298,    45] loss: 23884.508\n",
      "[298,    50] loss: 19298.638\n",
      "[298,    55] loss: 21934.800\n",
      "[298,    60] loss: 22134.513\n",
      "[298,    65] loss: 17663.798\n",
      "[298,    70] loss: 28858.737\n",
      "[298,    75] loss: 26813.444\n",
      "[298,    80] loss: 17575.217\n",
      "[298,    85] loss: 21886.310\n",
      "[298,    90] loss: 25478.441\n",
      "[298,    95] loss: 19707.949\n",
      "[298,   100] loss: 58054.528\n",
      "[298,   105] loss: 44674.852\n",
      "[298,   110] loss: 27624.909\n",
      "[298,   115] loss: 24277.138\n",
      "[298,   120] loss: 21721.960\n",
      "[298,   125] loss: 29343.748\n",
      "[298,   130] loss: 25504.803\n",
      "[298,   135] loss: 24737.980\n",
      "[298,   140] loss: 21921.161\n",
      "[298,   145] loss: 18736.574\n",
      "[298,   150] loss: 19215.117\n",
      "[298,   155] loss: 35783.711\n",
      "[298,   160] loss: 26591.896\n",
      "[298,   165] loss: 30731.109\n",
      "[298,   170] loss: 24793.666\n",
      "[298,   175] loss: 27073.640\n",
      "[298,   180] loss: 34090.047\n",
      "[298,   185] loss: 27282.274\n",
      "[298,   190] loss: 31053.421\n",
      "[298,   195] loss: 32162.701\n",
      "[298,   200] loss: 27168.122\n",
      "[298,   205] loss: 19533.339\n",
      "[298,   210] loss: 17629.614\n",
      "[298,   215] loss: 15460.417\n",
      "[298,   220] loss: 27330.532\n",
      "[298,   225] loss: 23838.639\n",
      "[298,   230] loss: 35925.589\n",
      "[299,     5] loss: 29865.736\n",
      "[299,    10] loss: 18136.634\n",
      "[299,    15] loss: 19283.385\n",
      "[299,    20] loss: 22422.814\n",
      "[299,    25] loss: 20806.948\n",
      "[299,    30] loss: 29122.705\n",
      "[299,    35] loss: 21082.875\n",
      "[299,    40] loss: 20156.612\n",
      "[299,    45] loss: 24710.627\n",
      "[299,    50] loss: 22024.789\n",
      "[299,    55] loss: 19111.676\n",
      "[299,    60] loss: 23220.513\n",
      "[299,    65] loss: 17186.451\n",
      "[299,    70] loss: 38854.066\n",
      "[299,    75] loss: 23546.012\n",
      "[299,    80] loss: 31443.558\n",
      "[299,    85] loss: 22330.866\n",
      "[299,    90] loss: 19051.323\n",
      "[299,    95] loss: 18168.338\n",
      "[299,   100] loss: 28841.097\n",
      "[299,   105] loss: 26913.176\n",
      "[299,   110] loss: 24882.868\n",
      "[299,   115] loss: 26705.367\n",
      "[299,   120] loss: 44429.145\n",
      "[299,   125] loss: 25334.863\n",
      "[299,   130] loss: 26048.729\n",
      "[299,   135] loss: 32699.069\n",
      "[299,   140] loss: 25036.563\n",
      "[299,   145] loss: 20256.282\n",
      "[299,   150] loss: 18906.155\n",
      "[299,   155] loss: 33835.611\n",
      "[299,   160] loss: 22930.065\n",
      "[299,   165] loss: 27214.234\n",
      "[299,   170] loss: 23245.060\n",
      "[299,   175] loss: 21277.002\n",
      "[299,   180] loss: 20450.391\n",
      "[299,   185] loss: 22742.646\n",
      "[299,   190] loss: 29830.518\n",
      "[299,   195] loss: 23695.594\n",
      "[299,   200] loss: 22489.758\n",
      "[299,   205] loss: 24453.956\n",
      "[299,   210] loss: 25063.321\n",
      "[299,   215] loss: 31974.808\n",
      "[299,   220] loss: 40532.734\n",
      "[299,   225] loss: 21682.866\n",
      "[299,   230] loss: 31449.665\n",
      "[300,     5] loss: 33228.400\n",
      "[300,    10] loss: 25718.576\n",
      "[300,    15] loss: 23766.114\n",
      "[300,    20] loss: 23140.120\n",
      "[300,    25] loss: 18796.379\n",
      "[300,    30] loss: 23392.898\n",
      "[300,    35] loss: 32964.289\n",
      "[300,    40] loss: 21053.487\n",
      "[300,    45] loss: 25941.139\n",
      "[300,    50] loss: 22186.041\n",
      "[300,    55] loss: 18127.917\n",
      "[300,    60] loss: 27786.741\n",
      "[300,    65] loss: 28456.530\n",
      "[300,    70] loss: 26409.773\n",
      "[300,    75] loss: 23225.560\n",
      "[300,    80] loss: 19931.366\n",
      "[300,    85] loss: 21718.097\n",
      "[300,    90] loss: 32116.540\n",
      "[300,    95] loss: 24447.096\n",
      "[300,   100] loss: 40801.273\n",
      "[300,   105] loss: 20694.040\n",
      "[300,   110] loss: 29735.089\n",
      "[300,   115] loss: 30416.834\n",
      "[300,   120] loss: 21061.137\n",
      "[300,   125] loss: 28692.431\n",
      "[300,   130] loss: 22532.163\n",
      "[300,   135] loss: 24888.326\n",
      "[300,   140] loss: 28046.108\n",
      "[300,   145] loss: 25425.927\n",
      "[300,   150] loss: 17907.896\n",
      "[300,   155] loss: 25272.958\n",
      "[300,   160] loss: 32746.469\n",
      "[300,   165] loss: 19305.075\n",
      "[300,   170] loss: 33402.106\n",
      "[300,   175] loss: 36269.273\n",
      "[300,   180] loss: 18121.044\n",
      "[300,   185] loss: 25071.131\n",
      "[300,   190] loss: 17629.883\n",
      "[300,   195] loss: 24857.413\n",
      "[300,   200] loss: 28596.947\n",
      "[300,   205] loss: 27755.905\n",
      "[300,   210] loss: 27298.277\n",
      "[300,   215] loss: 25278.537\n",
      "[300,   220] loss: 32047.545\n",
      "[300,   225] loss: 25589.125\n",
      "[300,   230] loss: 34273.649\n",
      "[301,     5] loss: 30603.139\n",
      "[301,    10] loss: 21173.616\n",
      "[301,    15] loss: 26240.266\n",
      "[301,    20] loss: 23452.225\n",
      "[301,    25] loss: 25041.011\n",
      "[301,    30] loss: 19745.361\n",
      "[301,    35] loss: 34389.793\n",
      "[301,    40] loss: 18510.374\n",
      "[301,    45] loss: 17979.658\n",
      "[301,    50] loss: 29361.043\n",
      "[301,    55] loss: 58653.990\n",
      "[301,    60] loss: 25066.913\n",
      "[301,    65] loss: 21955.393\n",
      "[301,    70] loss: 19695.290\n",
      "[301,    75] loss: 29105.095\n",
      "[301,    80] loss: 17809.045\n",
      "[301,    85] loss: 25235.680\n",
      "[301,    90] loss: 30842.984\n",
      "[301,    95] loss: 21079.286\n",
      "[301,   100] loss: 21157.381\n",
      "[301,   105] loss: 29153.276\n",
      "[301,   110] loss: 21750.788\n",
      "[301,   115] loss: 16596.278\n",
      "[301,   120] loss: 18696.960\n",
      "[301,   125] loss: 23666.682\n",
      "[301,   130] loss: 13975.232\n",
      "[301,   135] loss: 33847.337\n",
      "[301,   140] loss: 35920.365\n",
      "[301,   145] loss: 22291.681\n",
      "[301,   150] loss: 24490.960\n",
      "[301,   155] loss: 15989.819\n",
      "[301,   160] loss: 25955.494\n",
      "[301,   165] loss: 34085.727\n",
      "[301,   170] loss: 20358.920\n",
      "[301,   175] loss: 24049.746\n",
      "[301,   180] loss: 29161.215\n",
      "[301,   185] loss: 25494.378\n",
      "[301,   190] loss: 30126.294\n",
      "[301,   195] loss: 34593.043\n",
      "[301,   200] loss: 26375.715\n",
      "[301,   205] loss: 30769.873\n",
      "[301,   210] loss: 22134.197\n",
      "[301,   215] loss: 19146.925\n",
      "[301,   220] loss: 39994.249\n",
      "[301,   225] loss: 31420.832\n",
      "[301,   230] loss: 21941.378\n",
      "[302,     5] loss: 21275.729\n",
      "[302,    10] loss: 21225.232\n",
      "[302,    15] loss: 32169.684\n",
      "[302,    20] loss: 30323.615\n",
      "[302,    25] loss: 25367.336\n",
      "[302,    30] loss: 29826.381\n",
      "[302,    35] loss: 26382.729\n",
      "[302,    40] loss: 34144.515\n",
      "[302,    45] loss: 18275.507\n",
      "[302,    50] loss: 27854.153\n",
      "[302,    55] loss: 17497.577\n",
      "[302,    60] loss: 19630.751\n",
      "[302,    65] loss: 23452.951\n",
      "[302,    70] loss: 50302.628\n",
      "[302,    75] loss: 17294.744\n",
      "[302,    80] loss: 36184.929\n",
      "[302,    85] loss: 34037.246\n",
      "[302,    90] loss: 21618.596\n",
      "[302,    95] loss: 25471.578\n",
      "[302,   100] loss: 21742.834\n",
      "[302,   105] loss: 21120.192\n",
      "[302,   110] loss: 33477.713\n",
      "[302,   115] loss: 20155.376\n",
      "[302,   120] loss: 35680.969\n",
      "[302,   125] loss: 19471.140\n",
      "[302,   130] loss: 23441.149\n",
      "[302,   135] loss: 26686.277\n",
      "[302,   140] loss: 15278.729\n",
      "[302,   145] loss: 26519.528\n",
      "[302,   150] loss: 22320.146\n",
      "[302,   155] loss: 30417.312\n",
      "[302,   160] loss: 30383.608\n",
      "[302,   165] loss: 21846.293\n",
      "[302,   170] loss: 25259.152\n",
      "[302,   175] loss: 19245.673\n",
      "[302,   180] loss: 34973.641\n",
      "[302,   185] loss: 21601.797\n",
      "[302,   190] loss: 28843.245\n",
      "[302,   195] loss: 23815.375\n",
      "[302,   200] loss: 34413.512\n",
      "[302,   205] loss: 18982.578\n",
      "[302,   210] loss: 23439.717\n",
      "[302,   215] loss: 21531.883\n",
      "[302,   220] loss: 24451.359\n",
      "[302,   225] loss: 28652.348\n",
      "[302,   230] loss: 25758.391\n",
      "[303,     5] loss: 20382.267\n",
      "[303,    10] loss: 33208.841\n",
      "[303,    15] loss: 26315.543\n",
      "[303,    20] loss: 24386.805\n",
      "[303,    25] loss: 31926.929\n",
      "[303,    30] loss: 33504.876\n",
      "[303,    35] loss: 26687.837\n",
      "[303,    40] loss: 27837.780\n",
      "[303,    45] loss: 20475.201\n",
      "[303,    50] loss: 23356.297\n",
      "[303,    55] loss: 26047.972\n",
      "[303,    60] loss: 24245.438\n",
      "[303,    65] loss: 27003.294\n",
      "[303,    70] loss: 21680.106\n",
      "[303,    75] loss: 20633.436\n",
      "[303,    80] loss: 31664.892\n",
      "[303,    85] loss: 31291.170\n",
      "[303,    90] loss: 28889.704\n",
      "[303,    95] loss: 26691.231\n",
      "[303,   100] loss: 21781.586\n",
      "[303,   105] loss: 22619.996\n",
      "[303,   110] loss: 18801.107\n",
      "[303,   115] loss: 16031.062\n",
      "[303,   120] loss: 25123.464\n",
      "[303,   125] loss: 24970.099\n",
      "[303,   130] loss: 45302.763\n",
      "[303,   135] loss: 16587.943\n",
      "[303,   140] loss: 25870.676\n",
      "[303,   145] loss: 25520.340\n",
      "[303,   150] loss: 32041.537\n",
      "[303,   155] loss: 52266.930\n",
      "[303,   160] loss: 29124.452\n",
      "[303,   165] loss: 20023.071\n",
      "[303,   170] loss: 21961.983\n",
      "[303,   175] loss: 22184.751\n",
      "[303,   180] loss: 17964.562\n",
      "[303,   185] loss: 26519.486\n",
      "[303,   190] loss: 24475.735\n",
      "[303,   195] loss: 16822.200\n",
      "[303,   200] loss: 25395.205\n",
      "[303,   205] loss: 53887.088\n",
      "[303,   210] loss: 20912.918\n",
      "[303,   215] loss: 17197.861\n",
      "[303,   220] loss: 25911.380\n",
      "[303,   225] loss: 19067.805\n",
      "[303,   230] loss: 23362.429\n",
      "[304,     5] loss: 39762.908\n",
      "[304,    10] loss: 21916.100\n",
      "[304,    15] loss: 27614.340\n",
      "[304,    20] loss: 19815.217\n",
      "[304,    25] loss: 29051.327\n",
      "[304,    30] loss: 19367.173\n",
      "[304,    35] loss: 27666.732\n",
      "[304,    40] loss: 19140.085\n",
      "[304,    45] loss: 20951.450\n",
      "[304,    50] loss: 26007.026\n",
      "[304,    55] loss: 28169.064\n",
      "[304,    60] loss: 21496.441\n",
      "[304,    65] loss: 16011.194\n",
      "[304,    70] loss: 25054.250\n",
      "[304,    75] loss: 29488.811\n",
      "[304,    80] loss: 28986.999\n",
      "[304,    85] loss: 35903.737\n",
      "[304,    90] loss: 24958.273\n",
      "[304,    95] loss: 29888.291\n",
      "[304,   100] loss: 24070.943\n",
      "[304,   105] loss: 20223.921\n",
      "[304,   110] loss: 31430.320\n",
      "[304,   115] loss: 19577.858\n",
      "[304,   120] loss: 18867.599\n",
      "[304,   125] loss: 18196.229\n",
      "[304,   130] loss: 27965.593\n",
      "[304,   135] loss: 21999.304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304,   140] loss: 40406.510\n",
      "[304,   145] loss: 16377.545\n",
      "[304,   150] loss: 19314.419\n",
      "[304,   155] loss: 31220.989\n",
      "[304,   160] loss: 40992.089\n",
      "[304,   165] loss: 25215.055\n",
      "[304,   170] loss: 27738.134\n",
      "[304,   175] loss: 38691.559\n",
      "[304,   180] loss: 26638.662\n",
      "[304,   185] loss: 22582.610\n",
      "[304,   190] loss: 32359.671\n",
      "[304,   195] loss: 22586.407\n",
      "[304,   200] loss: 27016.542\n",
      "[304,   205] loss: 30483.564\n",
      "[304,   210] loss: 17205.675\n",
      "[304,   215] loss: 27575.015\n",
      "[304,   220] loss: 22130.511\n",
      "[304,   225] loss: 25726.445\n",
      "[304,   230] loss: 20591.646\n",
      "[305,     5] loss: 24690.864\n",
      "[305,    10] loss: 22981.831\n",
      "[305,    15] loss: 31221.737\n",
      "[305,    20] loss: 29356.938\n",
      "[305,    25] loss: 24176.002\n",
      "[305,    30] loss: 21044.087\n",
      "[305,    35] loss: 24609.459\n",
      "[305,    40] loss: 18918.645\n",
      "[305,    45] loss: 27044.738\n",
      "[305,    50] loss: 24085.717\n",
      "[305,    55] loss: 26766.434\n",
      "[305,    60] loss: 28805.755\n",
      "[305,    65] loss: 22876.421\n",
      "[305,    70] loss: 30218.170\n",
      "[305,    75] loss: 21317.738\n",
      "[305,    80] loss: 28853.388\n",
      "[305,    85] loss: 29751.237\n",
      "[305,    90] loss: 26956.116\n",
      "[305,    95] loss: 25422.316\n",
      "[305,   100] loss: 51151.897\n",
      "[305,   105] loss: 45768.096\n",
      "[305,   110] loss: 34802.633\n",
      "[305,   115] loss: 23995.482\n",
      "[305,   120] loss: 23852.114\n",
      "[305,   125] loss: 18255.320\n",
      "[305,   130] loss: 22403.735\n",
      "[305,   135] loss: 21895.173\n",
      "[305,   140] loss: 18942.492\n",
      "[305,   145] loss: 16522.862\n",
      "[305,   150] loss: 37275.962\n",
      "[305,   155] loss: 23969.041\n",
      "[305,   160] loss: 22990.569\n",
      "[305,   165] loss: 28859.796\n",
      "[305,   170] loss: 24173.798\n",
      "[305,   175] loss: 22734.106\n",
      "[305,   180] loss: 24343.544\n",
      "[305,   185] loss: 32780.407\n",
      "[305,   190] loss: 20379.540\n",
      "[305,   195] loss: 24479.596\n",
      "[305,   200] loss: 21407.014\n",
      "[305,   205] loss: 26300.765\n",
      "[305,   210] loss: 18810.071\n",
      "[305,   215] loss: 16774.938\n",
      "[305,   220] loss: 27798.799\n",
      "[305,   225] loss: 24794.553\n",
      "[305,   230] loss: 26375.839\n",
      "[306,     5] loss: 33631.120\n",
      "[306,    10] loss: 20514.720\n",
      "[306,    15] loss: 22304.748\n",
      "[306,    20] loss: 28671.116\n",
      "[306,    25] loss: 21475.025\n",
      "[306,    30] loss: 23904.844\n",
      "[306,    35] loss: 24957.942\n",
      "[306,    40] loss: 23353.068\n",
      "[306,    45] loss: 21535.178\n",
      "[306,    50] loss: 17933.332\n",
      "[306,    55] loss: 25525.364\n",
      "[306,    60] loss: 25689.175\n",
      "[306,    65] loss: 16904.505\n",
      "[306,    70] loss: 24253.407\n",
      "[306,    75] loss: 29585.953\n",
      "[306,    80] loss: 24354.619\n",
      "[306,    85] loss: 15760.597\n",
      "[306,    90] loss: 23530.164\n",
      "[306,    95] loss: 41856.877\n",
      "[306,   100] loss: 18912.669\n",
      "[306,   105] loss: 46013.516\n",
      "[306,   110] loss: 27816.768\n",
      "[306,   115] loss: 40273.059\n",
      "[306,   120] loss: 29805.108\n",
      "[306,   125] loss: 20559.028\n",
      "[306,   130] loss: 27600.634\n",
      "[306,   135] loss: 22801.424\n",
      "[306,   140] loss: 36265.627\n",
      "[306,   145] loss: 23936.015\n",
      "[306,   150] loss: 23355.471\n",
      "[306,   155] loss: 31616.762\n",
      "[306,   160] loss: 29686.916\n",
      "[306,   165] loss: 25681.878\n",
      "[306,   170] loss: 25416.510\n",
      "[306,   175] loss: 29954.330\n",
      "[306,   180] loss: 21778.484\n",
      "[306,   185] loss: 18398.633\n",
      "[306,   190] loss: 25354.041\n",
      "[306,   195] loss: 19658.943\n",
      "[306,   200] loss: 24367.992\n",
      "[306,   205] loss: 41865.230\n",
      "[306,   210] loss: 27104.115\n",
      "[306,   215] loss: 19925.052\n",
      "[306,   220] loss: 19141.843\n",
      "[306,   225] loss: 40579.982\n",
      "[306,   230] loss: 13438.463\n",
      "[307,     5] loss: 24169.327\n",
      "[307,    10] loss: 21397.238\n",
      "[307,    15] loss: 24136.162\n",
      "[307,    20] loss: 28643.478\n",
      "[307,    25] loss: 23295.389\n",
      "[307,    30] loss: 21735.258\n",
      "[307,    35] loss: 35649.330\n",
      "[307,    40] loss: 35830.221\n",
      "[307,    45] loss: 19832.045\n",
      "[307,    50] loss: 19963.841\n",
      "[307,    55] loss: 33458.891\n",
      "[307,    60] loss: 29841.771\n",
      "[307,    65] loss: 19678.688\n",
      "[307,    70] loss: 21263.350\n",
      "[307,    75] loss: 23507.379\n",
      "[307,    80] loss: 29860.831\n",
      "[307,    85] loss: 17946.775\n",
      "[307,    90] loss: 23091.026\n",
      "[307,    95] loss: 21439.777\n",
      "[307,   100] loss: 25471.974\n",
      "[307,   105] loss: 21580.994\n",
      "[307,   110] loss: 25311.252\n",
      "[307,   115] loss: 15530.855\n",
      "[307,   120] loss: 22760.783\n",
      "[307,   125] loss: 25976.593\n",
      "[307,   130] loss: 22295.794\n",
      "[307,   135] loss: 23066.422\n",
      "[307,   140] loss: 24964.083\n",
      "[307,   145] loss: 22007.344\n",
      "[307,   150] loss: 23582.537\n",
      "[307,   155] loss: 42091.831\n",
      "[307,   160] loss: 51574.877\n",
      "[307,   165] loss: 36370.534\n",
      "[307,   170] loss: 25075.038\n",
      "[307,   175] loss: 21214.010\n",
      "[307,   180] loss: 31384.127\n",
      "[307,   185] loss: 19758.903\n",
      "[307,   190] loss: 16533.053\n",
      "[307,   195] loss: 20978.590\n",
      "[307,   200] loss: 16735.804\n",
      "[307,   205] loss: 31517.118\n",
      "[307,   210] loss: 35261.844\n",
      "[307,   215] loss: 30459.337\n",
      "[307,   220] loss: 31108.648\n",
      "[307,   225] loss: 31530.193\n",
      "[307,   230] loss: 26565.716\n",
      "[308,     5] loss: 13243.857\n",
      "[308,    10] loss: 29432.103\n",
      "[308,    15] loss: 34402.647\n",
      "[308,    20] loss: 27795.508\n",
      "[308,    25] loss: 30022.414\n",
      "[308,    30] loss: 32510.898\n",
      "[308,    35] loss: 25774.902\n",
      "[308,    40] loss: 21800.708\n",
      "[308,    45] loss: 18759.471\n",
      "[308,    50] loss: 27334.371\n",
      "[308,    55] loss: 24867.345\n",
      "[308,    60] loss: 20157.998\n",
      "[308,    65] loss: 19749.320\n",
      "[308,    70] loss: 22439.986\n",
      "[308,    75] loss: 25890.216\n",
      "[308,    80] loss: 21721.576\n",
      "[308,    85] loss: 25955.733\n",
      "[308,    90] loss: 23081.582\n",
      "[308,    95] loss: 17012.116\n",
      "[308,   100] loss: 58843.584\n",
      "[308,   105] loss: 27266.774\n",
      "[308,   110] loss: 19833.623\n",
      "[308,   115] loss: 17152.163\n",
      "[308,   120] loss: 21662.946\n",
      "[308,   125] loss: 39181.975\n",
      "[308,   130] loss: 21827.384\n",
      "[308,   135] loss: 39536.839\n",
      "[308,   140] loss: 21987.737\n",
      "[308,   145] loss: 21090.966\n",
      "[308,   150] loss: 21778.034\n",
      "[308,   155] loss: 25945.172\n",
      "[308,   160] loss: 24619.763\n",
      "[308,   165] loss: 22617.053\n",
      "[308,   170] loss: 24833.444\n",
      "[308,   175] loss: 22186.688\n",
      "[308,   180] loss: 24575.109\n",
      "[308,   185] loss: 29004.530\n",
      "[308,   190] loss: 37407.207\n",
      "[308,   195] loss: 26780.210\n",
      "[308,   200] loss: 36005.584\n",
      "[308,   205] loss: 25005.278\n",
      "[308,   210] loss: 20878.273\n",
      "[308,   215] loss: 23404.298\n",
      "[308,   220] loss: 25307.438\n",
      "[308,   225] loss: 18951.731\n",
      "[308,   230] loss: 34847.973\n",
      "[309,     5] loss: 36568.614\n",
      "[309,    10] loss: 23837.483\n",
      "[309,    15] loss: 23257.721\n",
      "[309,    20] loss: 18699.998\n",
      "[309,    25] loss: 26180.620\n",
      "[309,    30] loss: 24910.468\n",
      "[309,    35] loss: 27317.148\n",
      "[309,    40] loss: 22469.259\n",
      "[309,    45] loss: 17232.721\n",
      "[309,    50] loss: 22921.261\n",
      "[309,    55] loss: 24517.941\n",
      "[309,    60] loss: 23341.002\n",
      "[309,    65] loss: 24387.079\n",
      "[309,    70] loss: 18481.131\n",
      "[309,    75] loss: 20466.103\n",
      "[309,    80] loss: 49775.304\n",
      "[309,    85] loss: 28904.190\n",
      "[309,    90] loss: 23976.144\n",
      "[309,    95] loss: 21219.454\n",
      "[309,   100] loss: 25459.690\n",
      "[309,   105] loss: 33785.149\n",
      "[309,   110] loss: 46490.055\n",
      "[309,   115] loss: 26261.041\n",
      "[309,   120] loss: 30136.138\n",
      "[309,   125] loss: 16630.574\n",
      "[309,   130] loss: 19588.643\n",
      "[309,   135] loss: 24571.361\n",
      "[309,   140] loss: 28921.951\n",
      "[309,   145] loss: 22479.976\n",
      "[309,   150] loss: 24380.477\n",
      "[309,   155] loss: 27939.893\n",
      "[309,   160] loss: 26305.029\n",
      "[309,   165] loss: 24746.798\n",
      "[309,   170] loss: 26350.425\n",
      "[309,   175] loss: 21964.120\n",
      "[309,   180] loss: 38427.677\n",
      "[309,   185] loss: 23957.891\n",
      "[309,   190] loss: 23445.294\n",
      "[309,   195] loss: 25058.265\n",
      "[309,   200] loss: 28253.282\n",
      "[309,   205] loss: 17109.085\n",
      "[309,   210] loss: 34593.112\n",
      "[309,   215] loss: 20429.658\n",
      "[309,   220] loss: 25910.902\n",
      "[309,   225] loss: 23311.825\n",
      "[309,   230] loss: 18814.198\n",
      "[310,     5] loss: 34945.712\n",
      "[310,    10] loss: 14675.500\n",
      "[310,    15] loss: 24598.287\n",
      "[310,    20] loss: 30444.184\n",
      "[310,    25] loss: 17381.533\n",
      "[310,    30] loss: 17349.975\n",
      "[310,    35] loss: 25395.311\n",
      "[310,    40] loss: 29269.388\n",
      "[310,    45] loss: 16135.673\n",
      "[310,    50] loss: 23032.525\n",
      "[310,    55] loss: 17462.117\n",
      "[310,    60] loss: 29030.937\n",
      "[310,    65] loss: 17133.778\n",
      "[310,    70] loss: 31604.640\n",
      "[310,    75] loss: 24545.536\n",
      "[310,    80] loss: 24070.617\n",
      "[310,    85] loss: 21147.468\n",
      "[310,    90] loss: 23469.284\n",
      "[310,    95] loss: 26961.708\n",
      "[310,   100] loss: 23788.542\n",
      "[310,   105] loss: 20568.111\n",
      "[310,   110] loss: 24797.628\n",
      "[310,   115] loss: 31787.572\n",
      "[310,   120] loss: 19472.375\n",
      "[310,   125] loss: 49599.588\n",
      "[310,   130] loss: 18051.355\n",
      "[310,   135] loss: 18484.186\n",
      "[310,   140] loss: 38488.100\n",
      "[310,   145] loss: 29387.309\n",
      "[310,   150] loss: 22652.539\n",
      "[310,   155] loss: 37865.612\n",
      "[310,   160] loss: 32204.105\n",
      "[310,   165] loss: 24000.700\n",
      "[310,   170] loss: 23549.470\n",
      "[310,   175] loss: 33061.211\n",
      "[310,   180] loss: 32575.773\n",
      "[310,   185] loss: 19226.031\n",
      "[310,   190] loss: 16561.377\n",
      "[310,   195] loss: 31987.605\n",
      "[310,   200] loss: 33958.319\n",
      "[310,   205] loss: 22320.218\n",
      "[310,   210] loss: 24677.737\n",
      "[310,   215] loss: 37475.028\n",
      "[310,   220] loss: 23443.178\n",
      "[310,   225] loss: 20603.727\n",
      "[310,   230] loss: 31919.121\n",
      "[311,     5] loss: 16823.302\n",
      "[311,    10] loss: 25284.196\n",
      "[311,    15] loss: 23001.091\n",
      "[311,    20] loss: 35778.046\n",
      "[311,    25] loss: 24499.486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[311,    30] loss: 29676.279\n",
      "[311,    35] loss: 16285.751\n",
      "[311,    40] loss: 24389.942\n",
      "[311,    45] loss: 29698.081\n",
      "[311,    50] loss: 15911.976\n",
      "[311,    55] loss: 20331.581\n",
      "[311,    60] loss: 34778.467\n",
      "[311,    65] loss: 24812.629\n",
      "[311,    70] loss: 27371.818\n",
      "[311,    75] loss: 23552.273\n",
      "[311,    80] loss: 31228.213\n",
      "[311,    85] loss: 18571.141\n",
      "[311,    90] loss: 23024.148\n",
      "[311,    95] loss: 29330.436\n",
      "[311,   100] loss: 25260.031\n",
      "[311,   105] loss: 26558.025\n",
      "[311,   110] loss: 46511.715\n",
      "[311,   115] loss: 19149.738\n",
      "[311,   120] loss: 23342.508\n",
      "[311,   125] loss: 27729.771\n",
      "[311,   130] loss: 27974.702\n",
      "[311,   135] loss: 33740.780\n",
      "[311,   140] loss: 15830.673\n",
      "[311,   145] loss: 27298.854\n",
      "[311,   150] loss: 20401.400\n",
      "[311,   155] loss: 22493.062\n",
      "[311,   160] loss: 26011.645\n",
      "[311,   165] loss: 28564.643\n",
      "[311,   170] loss: 49929.581\n",
      "[311,   175] loss: 26918.282\n",
      "[311,   180] loss: 26205.923\n",
      "[311,   185] loss: 27754.144\n",
      "[311,   190] loss: 26904.763\n",
      "[311,   195] loss: 26892.047\n",
      "[311,   200] loss: 20038.149\n",
      "[311,   205] loss: 34822.280\n",
      "[311,   210] loss: 18643.703\n",
      "[311,   215] loss: 19859.254\n",
      "[311,   220] loss: 25499.568\n",
      "[311,   225] loss: 21681.379\n",
      "[311,   230] loss: 16381.890\n",
      "[312,     5] loss: 29224.228\n",
      "[312,    10] loss: 35174.918\n",
      "[312,    15] loss: 24544.328\n",
      "[312,    20] loss: 32198.338\n",
      "[312,    25] loss: 30099.763\n",
      "[312,    30] loss: 30263.357\n",
      "[312,    35] loss: 27110.293\n",
      "[312,    40] loss: 31834.576\n",
      "[312,    45] loss: 20003.879\n",
      "[312,    50] loss: 28511.480\n",
      "[312,    55] loss: 15265.468\n",
      "[312,    60] loss: 22397.440\n",
      "[312,    65] loss: 15832.037\n",
      "[312,    70] loss: 22714.675\n",
      "[312,    75] loss: 24618.877\n",
      "[312,    80] loss: 17504.256\n",
      "[312,    85] loss: 20547.355\n",
      "[312,    90] loss: 22549.242\n",
      "[312,    95] loss: 36523.985\n",
      "[312,   100] loss: 28912.027\n",
      "[312,   105] loss: 25445.564\n",
      "[312,   110] loss: 37472.864\n",
      "[312,   115] loss: 25209.048\n",
      "[312,   120] loss: 28852.768\n",
      "[312,   125] loss: 26330.425\n",
      "[312,   130] loss: 24808.586\n",
      "[312,   135] loss: 17939.717\n",
      "[312,   140] loss: 19435.152\n",
      "[312,   145] loss: 23494.843\n",
      "[312,   150] loss: 24699.074\n",
      "[312,   155] loss: 22968.747\n",
      "[312,   160] loss: 22277.020\n",
      "[312,   165] loss: 19373.164\n",
      "[312,   170] loss: 21176.258\n",
      "[312,   175] loss: 24506.079\n",
      "[312,   180] loss: 20191.758\n",
      "[312,   185] loss: 22218.254\n",
      "[312,   190] loss: 22620.927\n",
      "[312,   195] loss: 30647.589\n",
      "[312,   200] loss: 34739.885\n",
      "[312,   205] loss: 22299.268\n",
      "[312,   210] loss: 37854.089\n",
      "[312,   215] loss: 30636.863\n",
      "[312,   220] loss: 14521.984\n",
      "[312,   225] loss: 15624.383\n",
      "[312,   230] loss: 31459.224\n",
      "[313,     5] loss: 45881.132\n",
      "[313,    10] loss: 23107.268\n",
      "[313,    15] loss: 19794.176\n",
      "[313,    20] loss: 29701.786\n",
      "[313,    25] loss: 14504.983\n",
      "[313,    30] loss: 26053.300\n",
      "[313,    35] loss: 23640.487\n",
      "[313,    40] loss: 28142.314\n",
      "[313,    45] loss: 22030.307\n",
      "[313,    50] loss: 26590.505\n",
      "[313,    55] loss: 24028.924\n",
      "[313,    60] loss: 25322.971\n",
      "[313,    65] loss: 22519.355\n",
      "[313,    70] loss: 20603.187\n",
      "[313,    75] loss: 24747.920\n",
      "[313,    80] loss: 20457.201\n",
      "[313,    85] loss: 25619.805\n",
      "[313,    90] loss: 26176.034\n",
      "[313,    95] loss: 25771.485\n",
      "[313,   100] loss: 30786.292\n",
      "[313,   105] loss: 25064.126\n",
      "[313,   110] loss: 12514.413\n",
      "[313,   115] loss: 18588.522\n",
      "[313,   120] loss: 24043.947\n",
      "[313,   125] loss: 35289.944\n",
      "[313,   130] loss: 27388.987\n",
      "[313,   135] loss: 28737.481\n",
      "[313,   140] loss: 20956.856\n",
      "[313,   145] loss: 28698.726\n",
      "[313,   150] loss: 24302.996\n",
      "[313,   155] loss: 17818.274\n",
      "[313,   160] loss: 24676.642\n",
      "[313,   165] loss: 34260.063\n",
      "[313,   170] loss: 22901.438\n",
      "[313,   175] loss: 37395.700\n",
      "[313,   180] loss: 31529.388\n",
      "[313,   185] loss: 33328.393\n",
      "[313,   190] loss: 27201.721\n",
      "[313,   195] loss: 27063.927\n",
      "[313,   200] loss: 22997.003\n",
      "[313,   205] loss: 25937.204\n",
      "[313,   210] loss: 22054.659\n",
      "[313,   215] loss: 25931.017\n",
      "[313,   220] loss: 20428.543\n",
      "[313,   225] loss: 33529.409\n",
      "[313,   230] loss: 34247.968\n",
      "[314,     5] loss: 25261.755\n",
      "[314,    10] loss: 28906.876\n",
      "[314,    15] loss: 19066.630\n",
      "[314,    20] loss: 33191.492\n",
      "[314,    25] loss: 25825.857\n",
      "[314,    30] loss: 42775.135\n",
      "[314,    35] loss: 25148.251\n",
      "[314,    40] loss: 19362.774\n",
      "[314,    45] loss: 17946.229\n",
      "[314,    50] loss: 27107.539\n",
      "[314,    55] loss: 21819.656\n",
      "[314,    60] loss: 27408.241\n",
      "[314,    65] loss: 36949.389\n",
      "[314,    70] loss: 26330.500\n",
      "[314,    75] loss: 26434.930\n",
      "[314,    80] loss: 20882.718\n",
      "[314,    85] loss: 18347.734\n",
      "[314,    90] loss: 17791.409\n",
      "[314,    95] loss: 30964.110\n",
      "[314,   100] loss: 21789.362\n",
      "[314,   105] loss: 24733.149\n",
      "[314,   110] loss: 25001.355\n",
      "[314,   115] loss: 29318.954\n",
      "[314,   120] loss: 26789.196\n",
      "[314,   125] loss: 23743.489\n",
      "[314,   130] loss: 21298.875\n",
      "[314,   135] loss: 29868.233\n",
      "[314,   140] loss: 18805.022\n",
      "[314,   145] loss: 30130.340\n",
      "[314,   150] loss: 13900.292\n",
      "[314,   155] loss: 27857.764\n",
      "[314,   160] loss: 28936.666\n",
      "[314,   165] loss: 26879.353\n",
      "[314,   170] loss: 23226.448\n",
      "[314,   175] loss: 27628.027\n",
      "[314,   180] loss: 16449.996\n",
      "[314,   185] loss: 15227.064\n",
      "[314,   190] loss: 14913.647\n",
      "[314,   195] loss: 35725.037\n",
      "[314,   200] loss: 16864.440\n",
      "[314,   205] loss: 55020.293\n",
      "[314,   210] loss: 27696.039\n",
      "[314,   215] loss: 26906.177\n",
      "[314,   220] loss: 30130.397\n",
      "[314,   225] loss: 29857.704\n",
      "[314,   230] loss: 32691.124\n",
      "[315,     5] loss: 18402.785\n",
      "[315,    10] loss: 24948.763\n",
      "[315,    15] loss: 24034.697\n",
      "[315,    20] loss: 29878.063\n",
      "[315,    25] loss: 21733.404\n",
      "[315,    30] loss: 18759.396\n",
      "[315,    35] loss: 27296.693\n",
      "[315,    40] loss: 27424.423\n",
      "[315,    45] loss: 33776.718\n",
      "[315,    50] loss: 26637.683\n",
      "[315,    55] loss: 25552.854\n",
      "[315,    60] loss: 21845.080\n",
      "[315,    65] loss: 21307.767\n",
      "[315,    70] loss: 22596.799\n",
      "[315,    75] loss: 51975.066\n",
      "[315,    80] loss: 30590.354\n",
      "[315,    85] loss: 19794.777\n",
      "[315,    90] loss: 31793.706\n",
      "[315,    95] loss: 21810.179\n",
      "[315,   100] loss: 23755.497\n",
      "[315,   105] loss: 31357.872\n",
      "[315,   110] loss: 25569.564\n",
      "[315,   115] loss: 28057.218\n",
      "[315,   120] loss: 32783.623\n",
      "[315,   125] loss: 25851.768\n",
      "[315,   130] loss: 27039.322\n",
      "[315,   135] loss: 27673.779\n",
      "[315,   140] loss: 29760.757\n",
      "[315,   145] loss: 17977.398\n",
      "[315,   150] loss: 25589.761\n",
      "[315,   155] loss: 18082.815\n",
      "[315,   160] loss: 25075.374\n",
      "[315,   165] loss: 28822.447\n",
      "[315,   170] loss: 10184.523\n",
      "[315,   175] loss: 21774.466\n",
      "[315,   180] loss: 17930.314\n",
      "[315,   185] loss: 29553.643\n",
      "[315,   190] loss: 31150.446\n",
      "[315,   195] loss: 21933.517\n",
      "[315,   200] loss: 29260.811\n",
      "[315,   205] loss: 23440.462\n",
      "[315,   210] loss: 22064.526\n",
      "[315,   215] loss: 41270.825\n",
      "[315,   220] loss: 17788.328\n",
      "[315,   225] loss: 29137.681\n",
      "[315,   230] loss: 27466.398\n",
      "[316,     5] loss: 23572.114\n",
      "[316,    10] loss: 30028.617\n",
      "[316,    15] loss: 20197.238\n",
      "[316,    20] loss: 24304.814\n",
      "[316,    25] loss: 26024.321\n",
      "[316,    30] loss: 23394.123\n",
      "[316,    35] loss: 18741.337\n",
      "[316,    40] loss: 17544.977\n",
      "[316,    45] loss: 28182.948\n",
      "[316,    50] loss: 25947.180\n",
      "[316,    55] loss: 33050.781\n",
      "[316,    60] loss: 33077.675\n",
      "[316,    65] loss: 21594.713\n",
      "[316,    70] loss: 30262.437\n",
      "[316,    75] loss: 45788.143\n",
      "[316,    80] loss: 17962.625\n",
      "[316,    85] loss: 18406.615\n",
      "[316,    90] loss: 18503.805\n",
      "[316,    95] loss: 22534.029\n",
      "[316,   100] loss: 27462.290\n",
      "[316,   105] loss: 19859.111\n",
      "[316,   110] loss: 25682.397\n",
      "[316,   115] loss: 25542.733\n",
      "[316,   120] loss: 31151.492\n",
      "[316,   125] loss: 24630.174\n",
      "[316,   130] loss: 15755.630\n",
      "[316,   135] loss: 28523.026\n",
      "[316,   140] loss: 22218.289\n",
      "[316,   145] loss: 29136.622\n",
      "[316,   150] loss: 32065.153\n",
      "[316,   155] loss: 37480.336\n",
      "[316,   160] loss: 29441.542\n",
      "[316,   165] loss: 22801.044\n",
      "[316,   170] loss: 23848.605\n",
      "[316,   175] loss: 34854.371\n",
      "[316,   180] loss: 20018.512\n",
      "[316,   185] loss: 30685.854\n",
      "[316,   190] loss: 32830.256\n",
      "[316,   195] loss: 27439.129\n",
      "[316,   200] loss: 25431.761\n",
      "[316,   205] loss: 22453.765\n",
      "[316,   210] loss: 17862.860\n",
      "[316,   215] loss: 24772.225\n",
      "[316,   220] loss: 27670.081\n",
      "[316,   225] loss: 28578.176\n",
      "[316,   230] loss: 17044.862\n",
      "[317,     5] loss: 25047.073\n",
      "[317,    10] loss: 29426.703\n",
      "[317,    15] loss: 28573.023\n",
      "[317,    20] loss: 30757.747\n",
      "[317,    25] loss: 31842.081\n",
      "[317,    30] loss: 29509.646\n",
      "[317,    35] loss: 25274.733\n",
      "[317,    40] loss: 28092.052\n",
      "[317,    45] loss: 19395.535\n",
      "[317,    50] loss: 21400.013\n",
      "[317,    55] loss: 26327.740\n",
      "[317,    60] loss: 17219.223\n",
      "[317,    65] loss: 27193.930\n",
      "[317,    70] loss: 38605.804\n",
      "[317,    75] loss: 22529.507\n",
      "[317,    80] loss: 24975.552\n",
      "[317,    85] loss: 26095.284\n",
      "[317,    90] loss: 38782.242\n",
      "[317,    95] loss: 29628.506\n",
      "[317,   100] loss: 12679.092\n",
      "[317,   105] loss: 27431.368\n",
      "[317,   110] loss: 29143.274\n",
      "[317,   115] loss: 45399.652\n",
      "[317,   120] loss: 29531.858\n",
      "[317,   125] loss: 17899.371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[317,   130] loss: 28960.172\n",
      "[317,   135] loss: 19694.666\n",
      "[317,   140] loss: 22049.045\n",
      "[317,   145] loss: 19696.293\n",
      "[317,   150] loss: 22293.546\n",
      "[317,   155] loss: 26355.098\n",
      "[317,   160] loss: 21335.336\n",
      "[317,   165] loss: 17092.136\n",
      "[317,   170] loss: 20274.621\n",
      "[317,   175] loss: 20053.560\n",
      "[317,   180] loss: 24242.830\n",
      "[317,   185] loss: 23992.612\n",
      "[317,   190] loss: 23836.051\n",
      "[317,   195] loss: 32268.042\n",
      "[317,   200] loss: 20552.573\n",
      "[317,   205] loss: 23803.180\n",
      "[317,   210] loss: 17812.728\n",
      "[317,   215] loss: 26997.263\n",
      "[317,   220] loss: 31747.627\n",
      "[317,   225] loss: 24174.343\n",
      "[317,   230] loss: 32373.935\n",
      "[318,     5] loss: 23611.650\n",
      "[318,    10] loss: 25058.018\n",
      "[318,    15] loss: 27014.213\n",
      "[318,    20] loss: 30590.178\n",
      "[318,    25] loss: 20134.133\n",
      "[318,    30] loss: 38147.957\n",
      "[318,    35] loss: 18758.449\n",
      "[318,    40] loss: 22462.523\n",
      "[318,    45] loss: 23461.283\n",
      "[318,    50] loss: 26291.552\n",
      "[318,    55] loss: 25008.053\n",
      "[318,    60] loss: 31436.109\n",
      "[318,    65] loss: 17398.180\n",
      "[318,    70] loss: 26327.181\n",
      "[318,    75] loss: 19188.951\n",
      "[318,    80] loss: 20265.393\n",
      "[318,    85] loss: 30283.469\n",
      "[318,    90] loss: 24826.661\n",
      "[318,    95] loss: 28422.520\n",
      "[318,   100] loss: 35093.243\n",
      "[318,   105] loss: 23178.625\n",
      "[318,   110] loss: 25899.517\n",
      "[318,   115] loss: 28615.516\n",
      "[318,   120] loss: 17105.906\n",
      "[318,   125] loss: 24835.593\n",
      "[318,   130] loss: 21187.161\n",
      "[318,   135] loss: 33311.691\n",
      "[318,   140] loss: 24746.875\n",
      "[318,   145] loss: 24538.278\n",
      "[318,   150] loss: 29036.648\n",
      "[318,   155] loss: 24259.529\n",
      "[318,   160] loss: 23360.685\n",
      "[318,   165] loss: 23425.712\n",
      "[318,   170] loss: 26839.864\n",
      "[318,   175] loss: 35094.326\n",
      "[318,   180] loss: 23491.617\n",
      "[318,   185] loss: 22435.500\n",
      "[318,   190] loss: 34507.219\n",
      "[318,   195] loss: 25551.696\n",
      "[318,   200] loss: 27542.351\n",
      "[318,   205] loss: 21417.811\n",
      "[318,   210] loss: 13630.433\n",
      "[318,   215] loss: 48520.407\n",
      "[318,   220] loss: 23710.110\n",
      "[318,   225] loss: 22059.276\n",
      "[318,   230] loss: 32202.364\n",
      "[319,     5] loss: 21837.572\n",
      "[319,    10] loss: 22336.407\n",
      "[319,    15] loss: 30286.715\n",
      "[319,    20] loss: 32211.663\n",
      "[319,    25] loss: 26397.450\n",
      "[319,    30] loss: 27142.121\n",
      "[319,    35] loss: 36469.663\n",
      "[319,    40] loss: 31143.022\n",
      "[319,    45] loss: 19949.411\n",
      "[319,    50] loss: 25672.763\n",
      "[319,    55] loss: 22035.550\n",
      "[319,    60] loss: 34259.681\n",
      "[319,    65] loss: 15248.647\n",
      "[319,    70] loss: 26759.262\n",
      "[319,    75] loss: 31210.103\n",
      "[319,    80] loss: 28988.699\n",
      "[319,    85] loss: 49646.254\n",
      "[319,    90] loss: 22413.177\n",
      "[319,    95] loss: 19539.552\n",
      "[319,   100] loss: 26649.661\n",
      "[319,   105] loss: 29125.180\n",
      "[319,   110] loss: 21789.161\n",
      "[319,   115] loss: 27639.452\n",
      "[319,   120] loss: 24298.659\n",
      "[319,   125] loss: 19362.911\n",
      "[319,   130] loss: 23758.022\n",
      "[319,   135] loss: 22932.891\n",
      "[319,   140] loss: 25758.983\n",
      "[319,   145] loss: 25344.352\n",
      "[319,   150] loss: 17693.535\n",
      "[319,   155] loss: 17377.496\n",
      "[319,   160] loss: 22389.066\n",
      "[319,   165] loss: 13033.143\n",
      "[319,   170] loss: 29839.359\n",
      "[319,   175] loss: 24999.536\n",
      "[319,   180] loss: 25096.899\n",
      "[319,   185] loss: 38686.696\n",
      "[319,   190] loss: 28015.080\n",
      "[319,   195] loss: 20072.228\n",
      "[319,   200] loss: 28958.386\n",
      "[319,   205] loss: 28197.940\n",
      "[319,   210] loss: 27123.246\n",
      "[319,   215] loss: 17272.023\n",
      "[319,   220] loss: 25293.661\n",
      "[319,   225] loss: 21355.294\n",
      "[319,   230] loss: 29787.009\n",
      "[320,     5] loss: 19807.833\n",
      "[320,    10] loss: 28423.530\n",
      "[320,    15] loss: 32105.234\n",
      "[320,    20] loss: 22700.295\n",
      "[320,    25] loss: 24385.903\n",
      "[320,    30] loss: 19817.282\n",
      "[320,    35] loss: 20535.112\n",
      "[320,    40] loss: 28481.867\n",
      "[320,    45] loss: 30300.318\n",
      "[320,    50] loss: 31870.028\n",
      "[320,    55] loss: 21474.003\n",
      "[320,    60] loss: 15437.097\n",
      "[320,    65] loss: 24406.612\n",
      "[320,    70] loss: 31537.931\n",
      "[320,    75] loss: 27858.968\n",
      "[320,    80] loss: 23965.956\n",
      "[320,    85] loss: 23017.486\n",
      "[320,    90] loss: 17151.316\n",
      "[320,    95] loss: 19253.920\n",
      "[320,   100] loss: 29167.687\n",
      "[320,   105] loss: 37887.581\n",
      "[320,   110] loss: 26924.645\n",
      "[320,   115] loss: 32530.735\n",
      "[320,   120] loss: 29237.071\n",
      "[320,   125] loss: 22643.524\n",
      "[320,   130] loss: 25684.804\n",
      "[320,   135] loss: 24304.294\n",
      "[320,   140] loss: 23236.236\n",
      "[320,   145] loss: 46263.752\n",
      "[320,   150] loss: 23411.637\n",
      "[320,   155] loss: 27898.553\n",
      "[320,   160] loss: 19057.921\n",
      "[320,   165] loss: 23707.196\n",
      "[320,   170] loss: 30266.427\n",
      "[320,   175] loss: 25171.575\n",
      "[320,   180] loss: 24208.290\n",
      "[320,   185] loss: 23813.366\n",
      "[320,   190] loss: 21808.525\n",
      "[320,   195] loss: 27009.781\n",
      "[320,   200] loss: 24693.721\n",
      "[320,   205] loss: 23829.604\n",
      "[320,   210] loss: 23305.937\n",
      "[320,   215] loss: 44382.132\n",
      "[320,   220] loss: 30374.513\n",
      "[320,   225] loss: 20709.083\n",
      "[320,   230] loss: 20053.209\n",
      "[321,     5] loss: 31546.853\n",
      "[321,    10] loss: 22025.272\n",
      "[321,    15] loss: 24605.211\n",
      "[321,    20] loss: 29614.829\n",
      "[321,    25] loss: 22084.453\n",
      "[321,    30] loss: 14574.602\n",
      "[321,    35] loss: 34019.920\n",
      "[321,    40] loss: 16701.478\n",
      "[321,    45] loss: 19330.141\n",
      "[321,    50] loss: 22792.600\n",
      "[321,    55] loss: 26466.488\n",
      "[321,    60] loss: 17490.616\n",
      "[321,    65] loss: 19915.984\n",
      "[321,    70] loss: 30128.920\n",
      "[321,    75] loss: 27020.670\n",
      "[321,    80] loss: 26460.373\n",
      "[321,    85] loss: 26999.222\n",
      "[321,    90] loss: 29327.334\n",
      "[321,    95] loss: 49512.305\n",
      "[321,   100] loss: 19993.632\n",
      "[321,   105] loss: 28697.290\n",
      "[321,   110] loss: 21624.031\n",
      "[321,   115] loss: 20378.028\n",
      "[321,   120] loss: 26736.356\n",
      "[321,   125] loss: 30062.585\n",
      "[321,   130] loss: 23302.265\n",
      "[321,   135] loss: 25497.838\n",
      "[321,   140] loss: 26668.670\n",
      "[321,   145] loss: 26482.804\n",
      "[321,   150] loss: 24919.049\n",
      "[321,   155] loss: 27348.888\n",
      "[321,   160] loss: 26773.097\n",
      "[321,   165] loss: 36342.050\n",
      "[321,   170] loss: 16101.047\n",
      "[321,   175] loss: 26798.600\n",
      "[321,   180] loss: 18937.708\n",
      "[321,   185] loss: 38824.368\n",
      "[321,   190] loss: 19188.338\n",
      "[321,   195] loss: 27192.125\n",
      "[321,   200] loss: 22071.773\n",
      "[321,   205] loss: 27739.203\n",
      "[321,   210] loss: 18506.308\n",
      "[321,   215] loss: 27377.969\n",
      "[321,   220] loss: 29352.119\n",
      "[321,   225] loss: 31578.114\n",
      "[321,   230] loss: 30696.442\n",
      "[322,     5] loss: 21255.857\n",
      "[322,    10] loss: 24829.260\n",
      "[322,    15] loss: 22073.673\n",
      "[322,    20] loss: 12035.668\n",
      "[322,    25] loss: 17839.849\n",
      "[322,    30] loss: 30538.359\n",
      "[322,    35] loss: 27466.510\n",
      "[322,    40] loss: 20333.724\n",
      "[322,    45] loss: 26720.297\n",
      "[322,    50] loss: 19603.092\n",
      "[322,    55] loss: 18953.108\n",
      "[322,    60] loss: 21795.536\n",
      "[322,    65] loss: 40745.280\n",
      "[322,    70] loss: 31646.856\n",
      "[322,    75] loss: 18526.643\n",
      "[322,    80] loss: 17903.306\n",
      "[322,    85] loss: 28749.869\n",
      "[322,    90] loss: 18859.074\n",
      "[322,    95] loss: 27412.553\n",
      "[322,   100] loss: 24282.650\n",
      "[322,   105] loss: 25611.318\n",
      "[322,   110] loss: 22359.381\n",
      "[322,   115] loss: 27371.104\n",
      "[322,   120] loss: 27838.010\n",
      "[322,   125] loss: 22586.293\n",
      "[322,   130] loss: 26679.129\n",
      "[322,   135] loss: 28203.969\n",
      "[322,   140] loss: 23484.159\n",
      "[322,   145] loss: 31977.437\n",
      "[322,   150] loss: 24986.829\n",
      "[322,   155] loss: 30445.809\n",
      "[322,   160] loss: 24055.316\n",
      "[322,   165] loss: 28254.332\n",
      "[322,   170] loss: 43399.759\n",
      "[322,   175] loss: 43733.902\n",
      "[322,   180] loss: 19637.988\n",
      "[322,   185] loss: 24628.626\n",
      "[322,   190] loss: 21893.562\n",
      "[322,   195] loss: 41569.890\n",
      "[322,   200] loss: 25189.419\n",
      "[322,   205] loss: 22192.908\n",
      "[322,   210] loss: 25676.021\n",
      "[322,   215] loss: 29041.668\n",
      "[322,   220] loss: 23856.211\n",
      "[322,   225] loss: 22980.624\n",
      "[322,   230] loss: 26971.132\n",
      "[323,     5] loss: 31673.331\n",
      "[323,    10] loss: 35085.738\n",
      "[323,    15] loss: 17783.728\n",
      "[323,    20] loss: 23836.234\n",
      "[323,    25] loss: 30992.846\n",
      "[323,    30] loss: 19057.900\n",
      "[323,    35] loss: 47073.446\n",
      "[323,    40] loss: 21336.588\n",
      "[323,    45] loss: 26731.075\n",
      "[323,    50] loss: 27531.106\n",
      "[323,    55] loss: 33339.410\n",
      "[323,    60] loss: 20484.485\n",
      "[323,    65] loss: 17788.518\n",
      "[323,    70] loss: 25468.432\n",
      "[323,    75] loss: 30559.381\n",
      "[323,    80] loss: 15623.371\n",
      "[323,    85] loss: 16259.782\n",
      "[323,    90] loss: 22311.636\n",
      "[323,    95] loss: 24525.272\n",
      "[323,   100] loss: 27221.833\n",
      "[323,   105] loss: 25006.941\n",
      "[323,   110] loss: 25783.252\n",
      "[323,   115] loss: 25508.863\n",
      "[323,   120] loss: 15998.704\n",
      "[323,   125] loss: 12965.747\n",
      "[323,   130] loss: 23877.154\n",
      "[323,   135] loss: 19771.373\n",
      "[323,   140] loss: 21874.075\n",
      "[323,   145] loss: 36577.904\n",
      "[323,   150] loss: 36390.271\n",
      "[323,   155] loss: 33305.924\n",
      "[323,   160] loss: 27680.734\n",
      "[323,   165] loss: 24785.895\n",
      "[323,   170] loss: 34563.288\n",
      "[323,   175] loss: 17151.941\n",
      "[323,   180] loss: 29873.023\n",
      "[323,   185] loss: 27500.184\n",
      "[323,   190] loss: 17018.504\n",
      "[323,   195] loss: 18806.395\n",
      "[323,   200] loss: 29710.178\n",
      "[323,   205] loss: 29279.966\n",
      "[323,   210] loss: 25655.664\n",
      "[323,   215] loss: 24476.383\n",
      "[323,   220] loss: 33168.338\n",
      "[323,   225] loss: 37406.389\n",
      "[323,   230] loss: 17480.338\n",
      "[324,     5] loss: 23571.713\n",
      "[324,    10] loss: 29643.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[324,    15] loss: 29401.234\n",
      "[324,    20] loss: 30506.802\n",
      "[324,    25] loss: 25369.330\n",
      "[324,    30] loss: 28518.035\n",
      "[324,    35] loss: 26147.868\n",
      "[324,    40] loss: 19042.916\n",
      "[324,    45] loss: 36848.912\n",
      "[324,    50] loss: 16542.593\n",
      "[324,    55] loss: 23628.923\n",
      "[324,    60] loss: 24632.586\n",
      "[324,    65] loss: 27115.098\n",
      "[324,    70] loss: 28150.273\n",
      "[324,    75] loss: 20726.422\n",
      "[324,    80] loss: 26329.933\n",
      "[324,    85] loss: 41966.693\n",
      "[324,    90] loss: 23847.833\n",
      "[324,    95] loss: 23775.798\n",
      "[324,   100] loss: 31840.152\n",
      "[324,   105] loss: 21030.549\n",
      "[324,   110] loss: 18222.874\n",
      "[324,   115] loss: 14741.872\n",
      "[324,   120] loss: 20882.886\n",
      "[324,   125] loss: 49064.595\n",
      "[324,   130] loss: 21369.509\n",
      "[324,   135] loss: 22993.857\n",
      "[324,   140] loss: 26056.341\n",
      "[324,   145] loss: 39732.996\n",
      "[324,   150] loss: 27112.238\n",
      "[324,   155] loss: 28437.551\n",
      "[324,   160] loss: 23366.815\n",
      "[324,   165] loss: 18307.394\n",
      "[324,   170] loss: 21823.492\n",
      "[324,   175] loss: 20464.833\n",
      "[324,   180] loss: 21328.538\n",
      "[324,   185] loss: 24405.450\n",
      "[324,   190] loss: 26086.105\n",
      "[324,   195] loss: 17199.903\n",
      "[324,   200] loss: 27016.257\n",
      "[324,   205] loss: 33204.356\n",
      "[324,   210] loss: 30849.577\n",
      "[324,   215] loss: 26207.234\n",
      "[324,   220] loss: 24899.009\n",
      "[324,   225] loss: 25687.267\n",
      "[324,   230] loss: 17149.049\n",
      "[325,     5] loss: 26698.348\n",
      "[325,    10] loss: 22986.890\n",
      "[325,    15] loss: 16429.204\n",
      "[325,    20] loss: 29548.368\n",
      "[325,    25] loss: 25789.103\n",
      "[325,    30] loss: 28569.737\n",
      "[325,    35] loss: 24666.713\n",
      "[325,    40] loss: 29193.480\n",
      "[325,    45] loss: 22404.757\n",
      "[325,    50] loss: 44702.265\n",
      "[325,    55] loss: 23551.236\n",
      "[325,    60] loss: 21803.489\n",
      "[325,    65] loss: 25191.186\n",
      "[325,    70] loss: 22732.794\n",
      "[325,    75] loss: 21170.937\n",
      "[325,    80] loss: 32167.061\n",
      "[325,    85] loss: 27189.746\n",
      "[325,    90] loss: 24828.038\n",
      "[325,    95] loss: 24099.508\n",
      "[325,   100] loss: 21655.866\n",
      "[325,   105] loss: 26204.599\n",
      "[325,   110] loss: 15668.493\n",
      "[325,   115] loss: 20993.840\n",
      "[325,   120] loss: 10526.607\n",
      "[325,   125] loss: 24713.788\n",
      "[325,   130] loss: 24098.747\n",
      "[325,   135] loss: 26360.187\n",
      "[325,   140] loss: 28968.057\n",
      "[325,   145] loss: 31893.764\n",
      "[325,   150] loss: 31191.113\n",
      "[325,   155] loss: 22918.635\n",
      "[325,   160] loss: 34697.459\n",
      "[325,   165] loss: 26610.399\n",
      "[325,   170] loss: 26674.870\n",
      "[325,   175] loss: 26151.763\n",
      "[325,   180] loss: 23876.807\n",
      "[325,   185] loss: 38001.278\n",
      "[325,   190] loss: 25573.337\n",
      "[325,   195] loss: 21529.785\n",
      "[325,   200] loss: 25292.858\n",
      "[325,   205] loss: 34974.523\n",
      "[325,   210] loss: 20310.616\n",
      "[325,   215] loss: 25377.997\n",
      "[325,   220] loss: 22165.400\n",
      "[325,   225] loss: 26029.792\n",
      "[325,   230] loss: 32005.101\n",
      "[326,     5] loss: 35714.905\n",
      "[326,    10] loss: 18074.796\n",
      "[326,    15] loss: 28113.375\n",
      "[326,    20] loss: 25036.314\n",
      "[326,    25] loss: 15679.331\n",
      "[326,    30] loss: 22819.534\n",
      "[326,    35] loss: 28001.611\n",
      "[326,    40] loss: 28419.942\n",
      "[326,    45] loss: 29725.027\n",
      "[326,    50] loss: 24464.798\n",
      "[326,    55] loss: 21006.820\n",
      "[326,    60] loss: 22627.573\n",
      "[326,    65] loss: 26812.124\n",
      "[326,    70] loss: 19556.304\n",
      "[326,    75] loss: 18642.459\n",
      "[326,    80] loss: 24519.255\n",
      "[326,    85] loss: 17829.086\n",
      "[326,    90] loss: 35846.327\n",
      "[326,    95] loss: 17578.861\n",
      "[326,   100] loss: 22609.621\n",
      "[326,   105] loss: 23043.112\n",
      "[326,   110] loss: 24345.224\n",
      "[326,   115] loss: 22265.675\n",
      "[326,   120] loss: 23754.516\n",
      "[326,   125] loss: 28578.137\n",
      "[326,   130] loss: 47853.014\n",
      "[326,   135] loss: 21661.169\n",
      "[326,   140] loss: 27903.087\n",
      "[326,   145] loss: 22291.357\n",
      "[326,   150] loss: 29022.098\n",
      "[326,   155] loss: 29787.453\n",
      "[326,   160] loss: 36226.649\n",
      "[326,   165] loss: 28122.833\n",
      "[326,   170] loss: 26745.059\n",
      "[326,   175] loss: 24069.172\n",
      "[326,   180] loss: 18718.537\n",
      "[326,   185] loss: 18072.421\n",
      "[326,   190] loss: 23508.462\n",
      "[326,   195] loss: 30946.607\n",
      "[326,   200] loss: 24987.758\n",
      "[326,   205] loss: 31424.568\n",
      "[326,   210] loss: 24555.580\n",
      "[326,   215] loss: 17673.664\n",
      "[326,   220] loss: 43394.209\n",
      "[326,   225] loss: 25596.251\n",
      "[326,   230] loss: 17427.139\n",
      "[327,     5] loss: 28174.414\n",
      "[327,    10] loss: 24823.460\n",
      "[327,    15] loss: 24112.386\n",
      "[327,    20] loss: 26435.247\n",
      "[327,    25] loss: 17770.070\n",
      "[327,    30] loss: 24218.993\n",
      "[327,    35] loss: 15329.863\n",
      "[327,    40] loss: 20157.966\n",
      "[327,    45] loss: 22541.410\n",
      "[327,    50] loss: 20855.377\n",
      "[327,    55] loss: 26239.107\n",
      "[327,    60] loss: 16403.170\n",
      "[327,    65] loss: 16034.786\n",
      "[327,    70] loss: 29071.902\n",
      "[327,    75] loss: 28881.280\n",
      "[327,    80] loss: 24394.017\n",
      "[327,    85] loss: 20142.222\n",
      "[327,    90] loss: 27049.282\n",
      "[327,    95] loss: 61770.541\n",
      "[327,   100] loss: 26585.914\n",
      "[327,   105] loss: 32132.199\n",
      "[327,   110] loss: 24241.689\n",
      "[327,   115] loss: 18467.016\n",
      "[327,   120] loss: 23104.925\n",
      "[327,   125] loss: 26852.504\n",
      "[327,   130] loss: 36149.001\n",
      "[327,   135] loss: 23856.119\n",
      "[327,   140] loss: 36204.243\n",
      "[327,   145] loss: 25689.559\n",
      "[327,   150] loss: 20083.138\n",
      "[327,   155] loss: 26232.752\n",
      "[327,   160] loss: 23079.143\n",
      "[327,   165] loss: 28268.588\n",
      "[327,   170] loss: 21633.344\n",
      "[327,   175] loss: 29302.523\n",
      "[327,   180] loss: 25483.587\n",
      "[327,   185] loss: 20669.187\n",
      "[327,   190] loss: 31393.855\n",
      "[327,   195] loss: 26260.320\n",
      "[327,   200] loss: 29545.255\n",
      "[327,   205] loss: 27694.318\n",
      "[327,   210] loss: 24551.940\n",
      "[327,   215] loss: 21198.013\n",
      "[327,   220] loss: 33362.976\n",
      "[327,   225] loss: 27807.397\n",
      "[327,   230] loss: 21570.994\n",
      "[328,     5] loss: 22216.384\n",
      "[328,    10] loss: 17053.821\n",
      "[328,    15] loss: 19743.638\n",
      "[328,    20] loss: 23354.395\n",
      "[328,    25] loss: 23602.858\n",
      "[328,    30] loss: 23113.439\n",
      "[328,    35] loss: 34855.028\n",
      "[328,    40] loss: 31034.947\n",
      "[328,    45] loss: 29419.699\n",
      "[328,    50] loss: 21284.023\n",
      "[328,    55] loss: 24763.638\n",
      "[328,    60] loss: 13455.386\n",
      "[328,    65] loss: 33786.875\n",
      "[328,    70] loss: 21319.643\n",
      "[328,    75] loss: 31918.529\n",
      "[328,    80] loss: 29463.455\n",
      "[328,    85] loss: 26397.910\n",
      "[328,    90] loss: 19849.508\n",
      "[328,    95] loss: 30623.286\n",
      "[328,   100] loss: 21317.550\n",
      "[328,   105] loss: 38702.167\n",
      "[328,   110] loss: 32376.557\n",
      "[328,   115] loss: 15841.268\n",
      "[328,   120] loss: 19165.298\n",
      "[328,   125] loss: 33893.934\n",
      "[328,   130] loss: 22120.255\n",
      "[328,   135] loss: 28110.231\n",
      "[328,   140] loss: 27564.307\n",
      "[328,   145] loss: 20565.873\n",
      "[328,   150] loss: 33597.383\n",
      "[328,   155] loss: 26917.413\n",
      "[328,   160] loss: 21424.353\n",
      "[328,   165] loss: 17581.276\n",
      "[328,   170] loss: 32929.328\n",
      "[328,   175] loss: 36209.670\n",
      "[328,   180] loss: 26306.984\n",
      "[328,   185] loss: 21906.480\n",
      "[328,   190] loss: 23769.575\n",
      "[328,   195] loss: 24355.099\n",
      "[328,   200] loss: 24018.252\n",
      "[328,   205] loss: 19058.112\n",
      "[328,   210] loss: 31619.206\n",
      "[328,   215] loss: 20965.287\n",
      "[328,   220] loss: 49488.310\n",
      "[328,   225] loss: 19263.124\n",
      "[328,   230] loss: 18313.239\n",
      "[329,     5] loss: 21338.195\n",
      "[329,    10] loss: 19280.328\n",
      "[329,    15] loss: 14946.642\n",
      "[329,    20] loss: 24510.630\n",
      "[329,    25] loss: 30582.306\n",
      "[329,    30] loss: 20690.520\n",
      "[329,    35] loss: 32825.071\n",
      "[329,    40] loss: 20472.245\n",
      "[329,    45] loss: 29587.854\n",
      "[329,    50] loss: 31157.881\n",
      "[329,    55] loss: 19692.953\n",
      "[329,    60] loss: 30588.386\n",
      "[329,    65] loss: 31212.856\n",
      "[329,    70] loss: 17363.524\n",
      "[329,    75] loss: 22828.699\n",
      "[329,    80] loss: 23746.418\n",
      "[329,    85] loss: 18024.022\n",
      "[329,    90] loss: 28956.251\n",
      "[329,    95] loss: 24062.661\n",
      "[329,   100] loss: 24540.079\n",
      "[329,   105] loss: 31288.279\n",
      "[329,   110] loss: 18646.333\n",
      "[329,   115] loss: 31707.219\n",
      "[329,   120] loss: 22548.100\n",
      "[329,   125] loss: 25192.003\n",
      "[329,   130] loss: 21088.268\n",
      "[329,   135] loss: 20052.902\n",
      "[329,   140] loss: 29606.012\n",
      "[329,   145] loss: 21726.971\n",
      "[329,   150] loss: 22927.289\n",
      "[329,   155] loss: 35288.546\n",
      "[329,   160] loss: 31549.503\n",
      "[329,   165] loss: 54134.203\n",
      "[329,   170] loss: 23693.725\n",
      "[329,   175] loss: 26963.761\n",
      "[329,   180] loss: 24825.936\n",
      "[329,   185] loss: 22754.748\n",
      "[329,   190] loss: 32085.793\n",
      "[329,   195] loss: 43605.843\n",
      "[329,   200] loss: 15343.156\n",
      "[329,   205] loss: 24958.186\n",
      "[329,   210] loss: 21842.751\n",
      "[329,   215] loss: 33246.141\n",
      "[329,   220] loss: 25018.962\n",
      "[329,   225] loss: 19721.788\n",
      "[329,   230] loss: 22373.193\n",
      "[330,     5] loss: 33705.601\n",
      "[330,    10] loss: 26991.045\n",
      "[330,    15] loss: 20859.704\n",
      "[330,    20] loss: 25036.259\n",
      "[330,    25] loss: 26804.805\n",
      "[330,    30] loss: 28645.998\n",
      "[330,    35] loss: 17787.545\n",
      "[330,    40] loss: 25323.755\n",
      "[330,    45] loss: 23909.893\n",
      "[330,    50] loss: 25438.393\n",
      "[330,    55] loss: 26116.484\n",
      "[330,    60] loss: 20250.629\n",
      "[330,    65] loss: 28695.301\n",
      "[330,    70] loss: 27073.529\n",
      "[330,    75] loss: 34978.770\n",
      "[330,    80] loss: 17354.361\n",
      "[330,    85] loss: 18489.950\n",
      "[330,    90] loss: 25838.509\n",
      "[330,    95] loss: 27707.776\n",
      "[330,   100] loss: 52193.129\n",
      "[330,   105] loss: 24209.391\n",
      "[330,   110] loss: 26210.190\n",
      "[330,   115] loss: 18478.062\n",
      "[330,   120] loss: 29212.287\n",
      "[330,   125] loss: 34158.681\n",
      "[330,   130] loss: 28818.452\n",
      "[330,   135] loss: 17571.944\n",
      "[330,   140] loss: 29196.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[330,   145] loss: 34180.433\n",
      "[330,   150] loss: 23836.071\n",
      "[330,   155] loss: 27435.342\n",
      "[330,   160] loss: 23296.370\n",
      "[330,   165] loss: 17499.842\n",
      "[330,   170] loss: 21912.326\n",
      "[330,   175] loss: 22851.527\n",
      "[330,   180] loss: 19376.332\n",
      "[330,   185] loss: 23043.468\n",
      "[330,   190] loss: 25159.184\n",
      "[330,   195] loss: 28102.084\n",
      "[330,   200] loss: 48925.120\n",
      "[330,   205] loss: 20889.269\n",
      "[330,   210] loss: 18058.201\n",
      "[330,   215] loss: 19896.114\n",
      "[330,   220] loss: 36803.119\n",
      "[330,   225] loss: 15912.046\n",
      "[330,   230] loss: 18518.480\n",
      "[331,     5] loss: 21237.784\n",
      "[331,    10] loss: 36169.583\n",
      "[331,    15] loss: 15974.234\n",
      "[331,    20] loss: 17027.269\n",
      "[331,    25] loss: 31664.305\n",
      "[331,    30] loss: 20305.688\n",
      "[331,    35] loss: 33711.330\n",
      "[331,    40] loss: 24760.021\n",
      "[331,    45] loss: 30309.569\n",
      "[331,    50] loss: 20522.368\n",
      "[331,    55] loss: 23095.931\n",
      "[331,    60] loss: 25710.695\n",
      "[331,    65] loss: 16677.903\n",
      "[331,    70] loss: 26034.122\n",
      "[331,    75] loss: 38742.345\n",
      "[331,    80] loss: 20906.224\n",
      "[331,    85] loss: 22423.989\n",
      "[331,    90] loss: 27771.162\n",
      "[331,    95] loss: 44242.002\n",
      "[331,   100] loss: 24602.284\n",
      "[331,   105] loss: 20098.386\n",
      "[331,   110] loss: 40447.445\n",
      "[331,   115] loss: 19753.583\n",
      "[331,   120] loss: 26167.979\n",
      "[331,   125] loss: 24357.646\n",
      "[331,   130] loss: 20518.322\n",
      "[331,   135] loss: 23327.367\n",
      "[331,   140] loss: 26020.318\n",
      "[331,   145] loss: 34520.355\n",
      "[331,   150] loss: 28971.177\n",
      "[331,   155] loss: 31100.416\n",
      "[331,   160] loss: 26413.751\n",
      "[331,   165] loss: 26498.410\n",
      "[331,   170] loss: 23169.316\n",
      "[331,   175] loss: 27198.336\n",
      "[331,   180] loss: 18381.767\n",
      "[331,   185] loss: 18288.932\n",
      "[331,   190] loss: 17060.724\n",
      "[331,   195] loss: 30111.866\n",
      "[331,   200] loss: 18968.624\n",
      "[331,   205] loss: 24411.876\n",
      "[331,   210] loss: 30336.964\n",
      "[331,   215] loss: 28258.149\n",
      "[331,   220] loss: 28066.606\n",
      "[331,   225] loss: 22731.143\n",
      "[331,   230] loss: 23717.079\n",
      "[332,     5] loss: 26349.860\n",
      "[332,    10] loss: 49555.529\n",
      "[332,    15] loss: 20733.355\n",
      "[332,    20] loss: 15664.415\n",
      "[332,    25] loss: 15713.172\n",
      "[332,    30] loss: 23810.283\n",
      "[332,    35] loss: 21902.981\n",
      "[332,    40] loss: 27204.622\n",
      "[332,    45] loss: 26976.125\n",
      "[332,    50] loss: 25054.430\n",
      "[332,    55] loss: 31278.231\n",
      "[332,    60] loss: 17925.569\n",
      "[332,    65] loss: 22156.483\n",
      "[332,    70] loss: 21191.512\n",
      "[332,    75] loss: 30331.650\n",
      "[332,    80] loss: 36719.625\n",
      "[332,    85] loss: 40353.883\n",
      "[332,    90] loss: 15552.246\n",
      "[332,    95] loss: 23119.347\n",
      "[332,   100] loss: 23217.068\n",
      "[332,   105] loss: 36284.724\n",
      "[332,   110] loss: 17818.918\n",
      "[332,   115] loss: 35569.674\n",
      "[332,   120] loss: 31786.184\n",
      "[332,   125] loss: 21289.366\n",
      "[332,   130] loss: 23461.937\n",
      "[332,   135] loss: 27768.162\n",
      "[332,   140] loss: 22647.014\n",
      "[332,   145] loss: 20486.562\n",
      "[332,   150] loss: 23921.625\n",
      "[332,   155] loss: 28946.182\n",
      "[332,   160] loss: 16898.307\n",
      "[332,   165] loss: 46837.351\n",
      "[332,   170] loss: 17126.291\n",
      "[332,   175] loss: 21523.199\n",
      "[332,   180] loss: 17338.942\n",
      "[332,   185] loss: 26808.738\n",
      "[332,   190] loss: 30170.713\n",
      "[332,   195] loss: 27456.898\n",
      "[332,   200] loss: 27774.306\n",
      "[332,   205] loss: 24637.010\n",
      "[332,   210] loss: 26600.739\n",
      "[332,   215] loss: 18299.562\n",
      "[332,   220] loss: 19807.445\n",
      "[332,   225] loss: 27975.067\n",
      "[332,   230] loss: 27252.375\n",
      "[333,     5] loss: 30737.626\n",
      "[333,    10] loss: 24952.823\n",
      "[333,    15] loss: 39777.902\n",
      "[333,    20] loss: 21919.509\n",
      "[333,    25] loss: 19361.286\n",
      "[333,    30] loss: 34498.092\n",
      "[333,    35] loss: 17852.283\n",
      "[333,    40] loss: 29472.730\n",
      "[333,    45] loss: 25834.910\n",
      "[333,    50] loss: 32938.200\n",
      "[333,    55] loss: 27231.965\n",
      "[333,    60] loss: 20120.474\n",
      "[333,    65] loss: 22109.847\n",
      "[333,    70] loss: 34344.559\n",
      "[333,    75] loss: 26462.612\n",
      "[333,    80] loss: 26052.469\n",
      "[333,    85] loss: 23016.696\n",
      "[333,    90] loss: 22488.593\n",
      "[333,    95] loss: 27598.303\n",
      "[333,   100] loss: 34942.876\n",
      "[333,   105] loss: 16522.295\n",
      "[333,   110] loss: 15778.986\n",
      "[333,   115] loss: 24036.999\n",
      "[333,   120] loss: 19564.209\n",
      "[333,   125] loss: 37046.631\n",
      "[333,   130] loss: 22658.011\n",
      "[333,   135] loss: 27190.166\n",
      "[333,   140] loss: 18468.605\n",
      "[333,   145] loss: 18072.993\n",
      "[333,   150] loss: 24427.067\n",
      "[333,   155] loss: 30223.179\n",
      "[333,   160] loss: 19746.643\n",
      "[333,   165] loss: 26931.478\n",
      "[333,   170] loss: 23004.882\n",
      "[333,   175] loss: 23142.100\n",
      "[333,   180] loss: 26309.305\n",
      "[333,   185] loss: 24701.127\n",
      "[333,   190] loss: 44436.469\n",
      "[333,   195] loss: 25242.868\n",
      "[333,   200] loss: 33024.781\n",
      "[333,   205] loss: 18223.283\n",
      "[333,   210] loss: 35472.194\n",
      "[333,   215] loss: 22187.738\n",
      "[333,   220] loss: 19378.060\n",
      "[333,   225] loss: 19751.708\n",
      "[333,   230] loss: 25949.792\n",
      "[334,     5] loss: 23731.301\n",
      "[334,    10] loss: 37189.372\n",
      "[334,    15] loss: 35402.081\n",
      "[334,    20] loss: 24164.693\n",
      "[334,    25] loss: 35511.648\n",
      "[334,    30] loss: 23769.746\n",
      "[334,    35] loss: 20668.393\n",
      "[334,    40] loss: 32969.724\n",
      "[334,    45] loss: 26573.465\n",
      "[334,    50] loss: 22631.479\n",
      "[334,    55] loss: 28199.269\n",
      "[334,    60] loss: 38127.916\n",
      "[334,    65] loss: 18819.394\n",
      "[334,    70] loss: 13183.566\n",
      "[334,    75] loss: 20839.928\n",
      "[334,    80] loss: 22445.135\n",
      "[334,    85] loss: 23521.728\n",
      "[334,    90] loss: 24484.506\n",
      "[334,    95] loss: 21879.713\n",
      "[334,   100] loss: 23542.228\n",
      "[334,   105] loss: 27043.568\n",
      "[334,   110] loss: 28738.064\n",
      "[334,   115] loss: 25117.323\n",
      "[334,   120] loss: 28224.350\n",
      "[334,   125] loss: 22703.362\n",
      "[334,   130] loss: 15882.183\n",
      "[334,   135] loss: 29222.061\n",
      "[334,   140] loss: 16225.819\n",
      "[334,   145] loss: 23515.922\n",
      "[334,   150] loss: 35309.480\n",
      "[334,   155] loss: 25559.141\n",
      "[334,   160] loss: 17223.660\n",
      "[334,   165] loss: 23942.934\n",
      "[334,   170] loss: 23136.672\n",
      "[334,   175] loss: 31126.961\n",
      "[334,   180] loss: 27857.529\n",
      "[334,   185] loss: 25252.496\n",
      "[334,   190] loss: 22302.631\n",
      "[334,   195] loss: 15521.611\n",
      "[334,   200] loss: 22958.158\n",
      "[334,   205] loss: 44153.589\n",
      "[334,   210] loss: 20276.515\n",
      "[334,   215] loss: 24250.340\n",
      "[334,   220] loss: 21297.164\n",
      "[334,   225] loss: 26870.475\n",
      "[334,   230] loss: 37127.410\n",
      "[335,     5] loss: 23818.416\n",
      "[335,    10] loss: 26063.024\n",
      "[335,    15] loss: 22113.825\n",
      "[335,    20] loss: 32473.632\n",
      "[335,    25] loss: 27644.129\n",
      "[335,    30] loss: 25882.895\n",
      "[335,    35] loss: 20194.451\n",
      "[335,    40] loss: 29177.737\n",
      "[335,    45] loss: 25526.142\n",
      "[335,    50] loss: 20766.795\n",
      "[335,    55] loss: 19283.924\n",
      "[335,    60] loss: 35970.496\n",
      "[335,    65] loss: 20918.198\n",
      "[335,    70] loss: 22297.035\n",
      "[335,    75] loss: 23031.344\n",
      "[335,    80] loss: 28349.133\n",
      "[335,    85] loss: 23071.005\n",
      "[335,    90] loss: 32374.691\n",
      "[335,    95] loss: 19849.300\n",
      "[335,   100] loss: 27763.355\n",
      "[335,   105] loss: 29187.572\n",
      "[335,   110] loss: 24125.753\n",
      "[335,   115] loss: 20679.932\n",
      "[335,   120] loss: 19428.695\n",
      "[335,   125] loss: 24344.447\n",
      "[335,   130] loss: 30180.359\n",
      "[335,   135] loss: 24784.022\n",
      "[335,   140] loss: 11415.924\n",
      "[335,   145] loss: 28335.948\n",
      "[335,   150] loss: 16470.857\n",
      "[335,   155] loss: 22145.351\n",
      "[335,   160] loss: 27430.625\n",
      "[335,   165] loss: 21175.608\n",
      "[335,   170] loss: 15232.415\n",
      "[335,   175] loss: 29218.303\n",
      "[335,   180] loss: 27483.708\n",
      "[335,   185] loss: 19944.822\n",
      "[335,   190] loss: 34140.638\n",
      "[335,   195] loss: 20604.236\n",
      "[335,   200] loss: 27643.561\n",
      "[335,   205] loss: 30911.230\n",
      "[335,   210] loss: 25335.581\n",
      "[335,   215] loss: 27780.829\n",
      "[335,   220] loss: 35545.531\n",
      "[335,   225] loss: 31287.204\n",
      "[335,   230] loss: 21375.213\n",
      "[336,     5] loss: 25806.206\n",
      "[336,    10] loss: 16654.877\n",
      "[336,    15] loss: 20160.221\n",
      "[336,    20] loss: 20422.108\n",
      "[336,    25] loss: 28587.861\n",
      "[336,    30] loss: 31395.543\n",
      "[336,    35] loss: 34440.241\n",
      "[336,    40] loss: 26344.930\n",
      "[336,    45] loss: 18030.254\n",
      "[336,    50] loss: 23363.633\n",
      "[336,    55] loss: 26490.872\n",
      "[336,    60] loss: 30819.265\n",
      "[336,    65] loss: 36489.276\n",
      "[336,    70] loss: 30342.667\n",
      "[336,    75] loss: 22140.685\n",
      "[336,    80] loss: 39951.010\n",
      "[336,    85] loss: 26133.792\n",
      "[336,    90] loss: 30238.887\n",
      "[336,    95] loss: 15025.883\n",
      "[336,   100] loss: 39260.994\n",
      "[336,   105] loss: 32534.356\n",
      "[336,   110] loss: 22869.425\n",
      "[336,   115] loss: 31041.929\n",
      "[336,   120] loss: 20341.782\n",
      "[336,   125] loss: 27394.091\n",
      "[336,   130] loss: 24283.559\n",
      "[336,   135] loss: 19734.708\n",
      "[336,   140] loss: 18928.786\n",
      "[336,   145] loss: 30428.192\n",
      "[336,   150] loss: 31259.671\n",
      "[336,   155] loss: 18761.222\n",
      "[336,   160] loss: 20052.030\n",
      "[336,   165] loss: 24719.881\n",
      "[336,   170] loss: 24136.354\n",
      "[336,   175] loss: 24864.741\n",
      "[336,   180] loss: 26948.004\n",
      "[336,   185] loss: 26656.503\n",
      "[336,   190] loss: 27293.487\n",
      "[336,   195] loss: 30045.918\n",
      "[336,   200] loss: 30505.982\n",
      "[336,   205] loss: 20911.409\n",
      "[336,   210] loss: 22241.703\n",
      "[336,   215] loss: 27941.196\n",
      "[336,   220] loss: 22793.468\n",
      "[336,   225] loss: 19459.367\n",
      "[336,   230] loss: 21600.412\n",
      "[337,     5] loss: 30772.465\n",
      "[337,    10] loss: 28877.796\n",
      "[337,    15] loss: 21658.121\n",
      "[337,    20] loss: 18119.318\n",
      "[337,    25] loss: 27219.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[337,    30] loss: 25756.590\n",
      "[337,    35] loss: 20453.223\n",
      "[337,    40] loss: 28192.339\n",
      "[337,    45] loss: 21194.374\n",
      "[337,    50] loss: 31383.870\n",
      "[337,    55] loss: 27437.470\n",
      "[337,    60] loss: 36667.370\n",
      "[337,    65] loss: 27571.642\n",
      "[337,    70] loss: 18970.466\n",
      "[337,    75] loss: 15813.885\n",
      "[337,    80] loss: 24583.886\n",
      "[337,    85] loss: 39450.288\n",
      "[337,    90] loss: 25886.467\n",
      "[337,    95] loss: 38452.718\n",
      "[337,   100] loss: 30064.195\n",
      "[337,   105] loss: 27116.021\n",
      "[337,   110] loss: 21024.255\n",
      "[337,   115] loss: 35593.975\n",
      "[337,   120] loss: 35959.056\n",
      "[337,   125] loss: 33524.575\n",
      "[337,   130] loss: 19513.075\n",
      "[337,   135] loss: 19462.617\n",
      "[337,   140] loss: 18420.936\n",
      "[337,   145] loss: 19332.691\n",
      "[337,   150] loss: 21098.311\n",
      "[337,   155] loss: 27389.940\n",
      "[337,   160] loss: 21155.845\n",
      "[337,   165] loss: 21787.113\n",
      "[337,   170] loss: 25666.267\n",
      "[337,   175] loss: 26293.797\n",
      "[337,   180] loss: 18328.190\n",
      "[337,   185] loss: 18295.552\n",
      "[337,   190] loss: 21056.559\n",
      "[337,   195] loss: 40952.251\n",
      "[337,   200] loss: 18279.202\n",
      "[337,   205] loss: 22555.215\n",
      "[337,   210] loss: 18554.436\n",
      "[337,   215] loss: 19629.034\n",
      "[337,   220] loss: 43094.406\n",
      "[337,   225] loss: 22073.799\n",
      "[337,   230] loss: 27363.763\n",
      "[338,     5] loss: 25474.935\n",
      "[338,    10] loss: 28791.857\n",
      "[338,    15] loss: 27343.699\n",
      "[338,    20] loss: 21628.395\n",
      "[338,    25] loss: 24179.682\n",
      "[338,    30] loss: 39812.548\n",
      "[338,    35] loss: 25733.657\n",
      "[338,    40] loss: 17511.623\n",
      "[338,    45] loss: 20189.246\n",
      "[338,    50] loss: 17928.227\n",
      "[338,    55] loss: 23024.969\n",
      "[338,    60] loss: 17858.777\n",
      "[338,    65] loss: 26115.366\n",
      "[338,    70] loss: 26768.812\n",
      "[338,    75] loss: 36151.255\n",
      "[338,    80] loss: 26883.352\n",
      "[338,    85] loss: 24696.408\n",
      "[338,    90] loss: 31222.838\n",
      "[338,    95] loss: 15539.690\n",
      "[338,   100] loss: 20301.133\n",
      "[338,   105] loss: 26432.851\n",
      "[338,   110] loss: 23204.942\n",
      "[338,   115] loss: 22825.927\n",
      "[338,   120] loss: 31068.332\n",
      "[338,   125] loss: 18619.738\n",
      "[338,   130] loss: 18687.688\n",
      "[338,   135] loss: 32749.758\n",
      "[338,   140] loss: 33246.636\n",
      "[338,   145] loss: 24861.714\n",
      "[338,   150] loss: 23289.031\n",
      "[338,   155] loss: 28992.566\n",
      "[338,   160] loss: 22382.399\n",
      "[338,   165] loss: 22355.551\n",
      "[338,   170] loss: 25279.289\n",
      "[338,   175] loss: 22948.454\n",
      "[338,   180] loss: 24545.124\n",
      "[338,   185] loss: 22296.644\n",
      "[338,   190] loss: 37730.249\n",
      "[338,   195] loss: 23755.256\n",
      "[338,   200] loss: 36741.358\n",
      "[338,   205] loss: 24646.482\n",
      "[338,   210] loss: 20962.627\n",
      "[338,   215] loss: 37213.340\n",
      "[338,   220] loss: 33071.729\n",
      "[338,   225] loss: 24733.608\n",
      "[338,   230] loss: 24671.857\n",
      "[339,     5] loss: 24539.390\n",
      "[339,    10] loss: 24614.421\n",
      "[339,    15] loss: 29054.178\n",
      "[339,    20] loss: 21448.953\n",
      "[339,    25] loss: 19126.798\n",
      "[339,    30] loss: 25797.402\n",
      "[339,    35] loss: 23000.112\n",
      "[339,    40] loss: 20873.950\n",
      "[339,    45] loss: 19826.276\n",
      "[339,    50] loss: 22814.290\n",
      "[339,    55] loss: 33362.582\n",
      "[339,    60] loss: 17951.544\n",
      "[339,    65] loss: 32404.358\n",
      "[339,    70] loss: 26965.919\n",
      "[339,    75] loss: 21006.530\n",
      "[339,    80] loss: 24265.912\n",
      "[339,    85] loss: 21383.317\n",
      "[339,    90] loss: 22262.376\n",
      "[339,    95] loss: 21649.443\n",
      "[339,   100] loss: 23187.969\n",
      "[339,   105] loss: 35013.438\n",
      "[339,   110] loss: 21722.032\n",
      "[339,   115] loss: 23401.260\n",
      "[339,   120] loss: 21761.439\n",
      "[339,   125] loss: 25218.117\n",
      "[339,   130] loss: 26281.076\n",
      "[339,   135] loss: 25841.016\n",
      "[339,   140] loss: 25211.047\n",
      "[339,   145] loss: 20964.949\n",
      "[339,   150] loss: 24849.044\n",
      "[339,   155] loss: 29466.005\n",
      "[339,   160] loss: 23411.498\n",
      "[339,   165] loss: 29728.885\n",
      "[339,   170] loss: 27735.406\n",
      "[339,   175] loss: 28191.581\n",
      "[339,   180] loss: 29208.379\n",
      "[339,   185] loss: 32953.239\n",
      "[339,   190] loss: 31721.336\n",
      "[339,   195] loss: 14742.615\n",
      "[339,   200] loss: 28175.854\n",
      "[339,   205] loss: 25901.546\n",
      "[339,   210] loss: 44732.829\n",
      "[339,   215] loss: 28952.939\n",
      "[339,   220] loss: 20247.581\n",
      "[339,   225] loss: 22051.731\n",
      "[339,   230] loss: 23796.421\n",
      "[340,     5] loss: 31568.895\n",
      "[340,    10] loss: 23692.225\n",
      "[340,    15] loss: 30106.359\n",
      "[340,    20] loss: 20382.603\n",
      "[340,    25] loss: 18814.284\n",
      "[340,    30] loss: 27216.378\n",
      "[340,    35] loss: 27033.192\n",
      "[340,    40] loss: 30642.691\n",
      "[340,    45] loss: 22534.449\n",
      "[340,    50] loss: 22360.625\n",
      "[340,    55] loss: 20777.536\n",
      "[340,    60] loss: 21570.904\n",
      "[340,    65] loss: 19077.224\n",
      "[340,    70] loss: 24959.075\n",
      "[340,    75] loss: 33434.829\n",
      "[340,    80] loss: 23252.054\n",
      "[340,    85] loss: 27598.341\n",
      "[340,    90] loss: 17176.718\n",
      "[340,    95] loss: 33226.813\n",
      "[340,   100] loss: 28549.614\n",
      "[340,   105] loss: 31447.082\n",
      "[340,   110] loss: 26108.659\n",
      "[340,   115] loss: 30968.471\n",
      "[340,   120] loss: 22682.441\n",
      "[340,   125] loss: 23158.681\n",
      "[340,   130] loss: 18314.163\n",
      "[340,   135] loss: 24905.819\n",
      "[340,   140] loss: 21813.146\n",
      "[340,   145] loss: 19832.068\n",
      "[340,   150] loss: 27631.255\n",
      "[340,   155] loss: 31980.675\n",
      "[340,   160] loss: 52022.190\n",
      "[340,   165] loss: 24288.043\n",
      "[340,   170] loss: 16483.059\n",
      "[340,   175] loss: 18760.922\n",
      "[340,   180] loss: 27078.409\n",
      "[340,   185] loss: 17244.830\n",
      "[340,   190] loss: 19472.765\n",
      "[340,   195] loss: 27401.631\n",
      "[340,   200] loss: 22179.562\n",
      "[340,   205] loss: 20983.112\n",
      "[340,   210] loss: 25788.948\n",
      "[340,   215] loss: 27695.720\n",
      "[340,   220] loss: 23968.424\n",
      "[340,   225] loss: 31038.385\n",
      "[340,   230] loss: 44957.116\n",
      "[341,     5] loss: 24039.895\n",
      "[341,    10] loss: 28025.152\n",
      "[341,    15] loss: 29013.993\n",
      "[341,    20] loss: 22007.738\n",
      "[341,    25] loss: 20230.192\n",
      "[341,    30] loss: 42861.475\n",
      "[341,    35] loss: 31114.246\n",
      "[341,    40] loss: 23255.108\n",
      "[341,    45] loss: 26414.303\n",
      "[341,    50] loss: 15330.858\n",
      "[341,    55] loss: 22795.662\n",
      "[341,    60] loss: 32976.965\n",
      "[341,    65] loss: 34190.867\n",
      "[341,    70] loss: 39783.026\n",
      "[341,    75] loss: 21890.222\n",
      "[341,    80] loss: 32036.075\n",
      "[341,    85] loss: 31117.916\n",
      "[341,    90] loss: 25351.365\n",
      "[341,    95] loss: 17219.077\n",
      "[341,   100] loss: 22589.951\n",
      "[341,   105] loss: 20716.773\n",
      "[341,   110] loss: 18306.401\n",
      "[341,   115] loss: 31927.536\n",
      "[341,   120] loss: 21296.578\n",
      "[341,   125] loss: 20007.753\n",
      "[341,   130] loss: 23732.034\n",
      "[341,   135] loss: 25184.053\n",
      "[341,   140] loss: 24086.413\n",
      "[341,   145] loss: 23782.713\n",
      "[341,   150] loss: 27581.125\n",
      "[341,   155] loss: 26060.910\n",
      "[341,   160] loss: 30747.094\n",
      "[341,   165] loss: 27010.660\n",
      "[341,   170] loss: 21393.630\n",
      "[341,   175] loss: 24188.058\n",
      "[341,   180] loss: 21591.776\n",
      "[341,   185] loss: 24833.946\n",
      "[341,   190] loss: 25570.261\n",
      "[341,   195] loss: 18567.156\n",
      "[341,   200] loss: 31918.821\n",
      "[341,   205] loss: 17433.733\n",
      "[341,   210] loss: 24585.129\n",
      "[341,   215] loss: 28067.834\n",
      "[341,   220] loss: 28448.794\n",
      "[341,   225] loss: 30362.124\n",
      "[341,   230] loss: 24082.086\n",
      "[342,     5] loss: 16817.307\n",
      "[342,    10] loss: 44067.407\n",
      "[342,    15] loss: 27376.902\n",
      "[342,    20] loss: 34546.134\n",
      "[342,    25] loss: 27190.928\n",
      "[342,    30] loss: 25439.427\n",
      "[342,    35] loss: 23871.091\n",
      "[342,    40] loss: 22753.470\n",
      "[342,    45] loss: 24815.792\n",
      "[342,    50] loss: 21576.012\n",
      "[342,    55] loss: 25393.256\n",
      "[342,    60] loss: 27477.089\n",
      "[342,    65] loss: 24338.415\n",
      "[342,    70] loss: 31877.691\n",
      "[342,    75] loss: 25023.980\n",
      "[342,    80] loss: 19342.352\n",
      "[342,    85] loss: 15982.698\n",
      "[342,    90] loss: 25339.544\n",
      "[342,    95] loss: 26150.164\n",
      "[342,   100] loss: 44661.601\n",
      "[342,   105] loss: 24690.253\n",
      "[342,   110] loss: 25923.045\n",
      "[342,   115] loss: 26438.020\n",
      "[342,   120] loss: 19620.440\n",
      "[342,   125] loss: 19453.596\n",
      "[342,   130] loss: 29875.835\n",
      "[342,   135] loss: 28090.930\n",
      "[342,   140] loss: 18447.271\n",
      "[342,   145] loss: 30389.952\n",
      "[342,   150] loss: 26669.138\n",
      "[342,   155] loss: 27763.035\n",
      "[342,   160] loss: 24444.577\n",
      "[342,   165] loss: 21019.878\n",
      "[342,   170] loss: 21990.372\n",
      "[342,   175] loss: 24835.017\n",
      "[342,   180] loss: 27639.625\n",
      "[342,   185] loss: 33762.433\n",
      "[342,   190] loss: 18594.030\n",
      "[342,   195] loss: 23412.394\n",
      "[342,   200] loss: 19187.546\n",
      "[342,   205] loss: 34604.112\n",
      "[342,   210] loss: 18320.058\n",
      "[342,   215] loss: 35700.192\n",
      "[342,   220] loss: 33777.288\n",
      "[342,   225] loss: 24465.647\n",
      "[342,   230] loss: 16511.819\n",
      "[343,     5] loss: 49381.205\n",
      "[343,    10] loss: 17121.159\n",
      "[343,    15] loss: 29679.139\n",
      "[343,    20] loss: 25053.932\n",
      "[343,    25] loss: 14886.026\n",
      "[343,    30] loss: 26104.841\n",
      "[343,    35] loss: 22591.515\n",
      "[343,    40] loss: 29224.751\n",
      "[343,    45] loss: 19670.977\n",
      "[343,    50] loss: 22320.738\n",
      "[343,    55] loss: 26958.517\n",
      "[343,    60] loss: 18510.975\n",
      "[343,    65] loss: 30663.590\n",
      "[343,    70] loss: 37522.452\n",
      "[343,    75] loss: 28924.512\n",
      "[343,    80] loss: 15735.082\n",
      "[343,    85] loss: 25924.200\n",
      "[343,    90] loss: 35139.588\n",
      "[343,    95] loss: 28248.959\n",
      "[343,   100] loss: 21258.141\n",
      "[343,   105] loss: 29380.720\n",
      "[343,   110] loss: 29300.289\n",
      "[343,   115] loss: 20984.298\n",
      "[343,   120] loss: 22984.330\n",
      "[343,   125] loss: 23025.942\n",
      "[343,   130] loss: 20377.456\n",
      "[343,   135] loss: 22541.381\n",
      "[343,   140] loss: 25384.502\n",
      "[343,   145] loss: 21087.576\n",
      "[343,   150] loss: 19199.804\n",
      "[343,   155] loss: 30680.695\n",
      "[343,   160] loss: 24441.200\n",
      "[343,   165] loss: 33073.075\n",
      "[343,   170] loss: 26536.737\n",
      "[343,   175] loss: 35954.740\n",
      "[343,   180] loss: 29144.814\n",
      "[343,   185] loss: 27346.923\n",
      "[343,   190] loss: 21754.021\n",
      "[343,   195] loss: 26918.277\n",
      "[343,   200] loss: 25093.627\n",
      "[343,   205] loss: 21740.633\n",
      "[343,   210] loss: 20969.399\n",
      "[343,   215] loss: 26590.540\n",
      "[343,   220] loss: 20984.791\n",
      "[343,   225] loss: 28370.952\n",
      "[343,   230] loss: 27244.989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[344,     5] loss: 27246.312\n",
      "[344,    10] loss: 24348.824\n",
      "[344,    15] loss: 19228.682\n",
      "[344,    20] loss: 24492.503\n",
      "[344,    25] loss: 24842.015\n",
      "[344,    30] loss: 26410.569\n",
      "[344,    35] loss: 27305.648\n",
      "[344,    40] loss: 30192.466\n",
      "[344,    45] loss: 33676.887\n",
      "[344,    50] loss: 28632.186\n",
      "[344,    55] loss: 23925.648\n",
      "[344,    60] loss: 45069.234\n",
      "[344,    65] loss: 27360.755\n",
      "[344,    70] loss: 23571.545\n",
      "[344,    75] loss: 12947.645\n",
      "[344,    80] loss: 42078.777\n",
      "[344,    85] loss: 21444.331\n",
      "[344,    90] loss: 27238.282\n",
      "[344,    95] loss: 26685.480\n",
      "[344,   100] loss: 20678.854\n",
      "[344,   105] loss: 23104.600\n",
      "[344,   110] loss: 22000.172\n",
      "[344,   115] loss: 26871.180\n",
      "[344,   120] loss: 37146.412\n",
      "[344,   125] loss: 35359.625\n",
      "[344,   130] loss: 31328.408\n",
      "[344,   135] loss: 37082.111\n",
      "[344,   140] loss: 27529.044\n",
      "[344,   145] loss: 18508.091\n",
      "[344,   150] loss: 17008.735\n",
      "[344,   155] loss: 28655.544\n",
      "[344,   160] loss: 22144.461\n",
      "[344,   165] loss: 27854.944\n",
      "[344,   170] loss: 24122.203\n",
      "[344,   175] loss: 19494.752\n",
      "[344,   180] loss: 21390.693\n",
      "[344,   185] loss: 23581.620\n",
      "[344,   190] loss: 18893.941\n",
      "[344,   195] loss: 23751.440\n",
      "[344,   200] loss: 21909.209\n",
      "[344,   205] loss: 22759.944\n",
      "[344,   210] loss: 25039.009\n",
      "[344,   215] loss: 20677.923\n",
      "[344,   220] loss: 26871.526\n",
      "[344,   225] loss: 21504.770\n",
      "[344,   230] loss: 23059.970\n",
      "[345,     5] loss: 27708.869\n",
      "[345,    10] loss: 43305.222\n",
      "[345,    15] loss: 18364.981\n",
      "[345,    20] loss: 29679.068\n",
      "[345,    25] loss: 32153.926\n",
      "[345,    30] loss: 22756.350\n",
      "[345,    35] loss: 29505.737\n",
      "[345,    40] loss: 23331.964\n",
      "[345,    45] loss: 22318.220\n",
      "[345,    50] loss: 35182.682\n",
      "[345,    55] loss: 24370.882\n",
      "[345,    60] loss: 27398.944\n",
      "[345,    65] loss: 22561.363\n",
      "[345,    70] loss: 34098.135\n",
      "[345,    75] loss: 25698.777\n",
      "[345,    80] loss: 18194.381\n",
      "[345,    85] loss: 18956.226\n",
      "[345,    90] loss: 22880.921\n",
      "[345,    95] loss: 28383.266\n",
      "[345,   100] loss: 25241.086\n",
      "[345,   105] loss: 31960.266\n",
      "[345,   110] loss: 24942.040\n",
      "[345,   115] loss: 23962.336\n",
      "[345,   120] loss: 16726.879\n",
      "[345,   125] loss: 25217.429\n",
      "[345,   130] loss: 25645.940\n",
      "[345,   135] loss: 35702.793\n",
      "[345,   140] loss: 26458.609\n",
      "[345,   145] loss: 20957.395\n",
      "[345,   150] loss: 25274.801\n",
      "[345,   155] loss: 21713.675\n",
      "[345,   160] loss: 30068.327\n",
      "[345,   165] loss: 16613.860\n",
      "[345,   170] loss: 16685.487\n",
      "[345,   175] loss: 26611.213\n",
      "[345,   180] loss: 25819.946\n",
      "[345,   185] loss: 19531.089\n",
      "[345,   190] loss: 24862.918\n",
      "[345,   195] loss: 30619.505\n",
      "[345,   200] loss: 22818.782\n",
      "[345,   205] loss: 21875.864\n",
      "[345,   210] loss: 15871.226\n",
      "[345,   215] loss: 26564.545\n",
      "[345,   220] loss: 40304.894\n",
      "[345,   225] loss: 30318.619\n",
      "[345,   230] loss: 24884.370\n",
      "[346,     5] loss: 31525.541\n",
      "[346,    10] loss: 21793.999\n",
      "[346,    15] loss: 29763.655\n",
      "[346,    20] loss: 16121.375\n",
      "[346,    25] loss: 23449.664\n",
      "[346,    30] loss: 21947.096\n",
      "[346,    35] loss: 24524.059\n",
      "[346,    40] loss: 20176.381\n",
      "[346,    45] loss: 30566.165\n",
      "[346,    50] loss: 15178.485\n",
      "[346,    55] loss: 25610.632\n",
      "[346,    60] loss: 24766.292\n",
      "[346,    65] loss: 26652.308\n",
      "[346,    70] loss: 24810.555\n",
      "[346,    75] loss: 40369.037\n",
      "[346,    80] loss: 23450.514\n",
      "[346,    85] loss: 22829.585\n",
      "[346,    90] loss: 42589.949\n",
      "[346,    95] loss: 25146.257\n",
      "[346,   100] loss: 29592.448\n",
      "[346,   105] loss: 45071.480\n",
      "[346,   110] loss: 19774.873\n",
      "[346,   115] loss: 27756.849\n",
      "[346,   120] loss: 37927.907\n",
      "[346,   125] loss: 28905.327\n",
      "[346,   130] loss: 25440.671\n",
      "[346,   135] loss: 23290.177\n",
      "[346,   140] loss: 26120.404\n",
      "[346,   145] loss: 18401.677\n",
      "[346,   150] loss: 19287.639\n",
      "[346,   155] loss: 20560.043\n",
      "[346,   160] loss: 28755.560\n",
      "[346,   165] loss: 24930.981\n",
      "[346,   170] loss: 28490.320\n",
      "[346,   175] loss: 26353.039\n",
      "[346,   180] loss: 23885.291\n",
      "[346,   185] loss: 15264.561\n",
      "[346,   190] loss: 29778.365\n",
      "[346,   195] loss: 24908.535\n",
      "[346,   200] loss: 34251.120\n",
      "[346,   205] loss: 18796.602\n",
      "[346,   210] loss: 19096.049\n",
      "[346,   215] loss: 18523.030\n",
      "[346,   220] loss: 24295.764\n",
      "[346,   225] loss: 26517.306\n",
      "[346,   230] loss: 21807.618\n",
      "[347,     5] loss: 37510.265\n",
      "[347,    10] loss: 27052.310\n",
      "[347,    15] loss: 20176.928\n",
      "[347,    20] loss: 33426.353\n",
      "[347,    25] loss: 27938.339\n",
      "[347,    30] loss: 19356.021\n",
      "[347,    35] loss: 26538.947\n",
      "[347,    40] loss: 29104.361\n",
      "[347,    45] loss: 23249.345\n",
      "[347,    50] loss: 21557.351\n",
      "[347,    55] loss: 24573.830\n",
      "[347,    60] loss: 24833.521\n",
      "[347,    65] loss: 27088.830\n",
      "[347,    70] loss: 23587.207\n",
      "[347,    75] loss: 23977.137\n",
      "[347,    80] loss: 29259.118\n",
      "[347,    85] loss: 22555.860\n",
      "[347,    90] loss: 17772.584\n",
      "[347,    95] loss: 23177.371\n",
      "[347,   100] loss: 27730.756\n",
      "[347,   105] loss: 17864.483\n",
      "[347,   110] loss: 24894.963\n",
      "[347,   115] loss: 35474.403\n",
      "[347,   120] loss: 24460.551\n",
      "[347,   125] loss: 18799.978\n",
      "[347,   130] loss: 20329.702\n",
      "[347,   135] loss: 26861.289\n",
      "[347,   140] loss: 27827.527\n",
      "[347,   145] loss: 20486.123\n",
      "[347,   150] loss: 36731.562\n",
      "[347,   155] loss: 54078.315\n",
      "[347,   160] loss: 15472.509\n",
      "[347,   165] loss: 20381.739\n",
      "[347,   170] loss: 26985.484\n",
      "[347,   175] loss: 21803.840\n",
      "[347,   180] loss: 22900.542\n",
      "[347,   185] loss: 28172.877\n",
      "[347,   190] loss: 35542.720\n",
      "[347,   195] loss: 27147.330\n",
      "[347,   200] loss: 23943.128\n",
      "[347,   205] loss: 29469.399\n",
      "[347,   210] loss: 23371.001\n",
      "[347,   215] loss: 18224.408\n",
      "[347,   220] loss: 29580.136\n",
      "[347,   225] loss: 24196.958\n",
      "[347,   230] loss: 20212.710\n",
      "[348,     5] loss: 15259.126\n",
      "[348,    10] loss: 29912.804\n",
      "[348,    15] loss: 20626.357\n",
      "[348,    20] loss: 29582.534\n",
      "[348,    25] loss: 28142.667\n",
      "[348,    30] loss: 28556.587\n",
      "[348,    35] loss: 25282.972\n",
      "[348,    40] loss: 24624.117\n",
      "[348,    45] loss: 33137.143\n",
      "[348,    50] loss: 22086.371\n",
      "[348,    55] loss: 28704.963\n",
      "[348,    60] loss: 21278.198\n",
      "[348,    65] loss: 18426.272\n",
      "[348,    70] loss: 33031.540\n",
      "[348,    75] loss: 24946.256\n",
      "[348,    80] loss: 21096.095\n",
      "[348,    85] loss: 28167.025\n",
      "[348,    90] loss: 30822.905\n",
      "[348,    95] loss: 18176.905\n",
      "[348,   100] loss: 21700.067\n",
      "[348,   105] loss: 23390.805\n",
      "[348,   110] loss: 18998.701\n",
      "[348,   115] loss: 24433.074\n",
      "[348,   120] loss: 29833.144\n",
      "[348,   125] loss: 22505.154\n",
      "[348,   130] loss: 26955.207\n",
      "[348,   135] loss: 37788.487\n",
      "[348,   140] loss: 23329.414\n",
      "[348,   145] loss: 15602.301\n",
      "[348,   150] loss: 37078.566\n",
      "[348,   155] loss: 27177.542\n",
      "[348,   160] loss: 18055.242\n",
      "[348,   165] loss: 26421.595\n",
      "[348,   170] loss: 15713.463\n",
      "[348,   175] loss: 47051.715\n",
      "[348,   180] loss: 20353.027\n",
      "[348,   185] loss: 23036.168\n",
      "[348,   190] loss: 23953.934\n",
      "[348,   195] loss: 26225.508\n",
      "[348,   200] loss: 33781.946\n",
      "[348,   205] loss: 39920.904\n",
      "[348,   210] loss: 23643.472\n",
      "[348,   215] loss: 22575.963\n",
      "[348,   220] loss: 27426.221\n",
      "[348,   225] loss: 24998.992\n",
      "[348,   230] loss: 20519.178\n",
      "[349,     5] loss: 15919.807\n",
      "[349,    10] loss: 26738.003\n",
      "[349,    15] loss: 21265.195\n",
      "[349,    20] loss: 21503.129\n",
      "[349,    25] loss: 22488.028\n",
      "[349,    30] loss: 23730.790\n",
      "[349,    35] loss: 25852.405\n",
      "[349,    40] loss: 24434.174\n",
      "[349,    45] loss: 26322.658\n",
      "[349,    50] loss: 24935.587\n",
      "[349,    55] loss: 12663.672\n",
      "[349,    60] loss: 33916.909\n",
      "[349,    65] loss: 17308.394\n",
      "[349,    70] loss: 28037.562\n",
      "[349,    75] loss: 23465.122\n",
      "[349,    80] loss: 26551.055\n",
      "[349,    85] loss: 21789.820\n",
      "[349,    90] loss: 23989.710\n",
      "[349,    95] loss: 33079.061\n",
      "[349,   100] loss: 33201.473\n",
      "[349,   105] loss: 34190.148\n",
      "[349,   110] loss: 34972.785\n",
      "[349,   115] loss: 19984.158\n",
      "[349,   120] loss: 23402.629\n",
      "[349,   125] loss: 28675.518\n",
      "[349,   130] loss: 44511.370\n",
      "[349,   135] loss: 25358.162\n",
      "[349,   140] loss: 22380.426\n",
      "[349,   145] loss: 30478.716\n",
      "[349,   150] loss: 25202.471\n",
      "[349,   155] loss: 32774.421\n",
      "[349,   160] loss: 15449.992\n",
      "[349,   165] loss: 27998.770\n",
      "[349,   170] loss: 28245.262\n",
      "[349,   175] loss: 15843.640\n",
      "[349,   180] loss: 20818.057\n",
      "[349,   185] loss: 28174.433\n",
      "[349,   190] loss: 15569.881\n",
      "[349,   195] loss: 19127.219\n",
      "[349,   200] loss: 31393.927\n",
      "[349,   205] loss: 21176.271\n",
      "[349,   210] loss: 36187.265\n",
      "[349,   215] loss: 43654.952\n",
      "[349,   220] loss: 17660.516\n",
      "[349,   225] loss: 18635.311\n",
      "[349,   230] loss: 27291.558\n",
      "[350,     5] loss: 17286.413\n",
      "[350,    10] loss: 22575.301\n",
      "[350,    15] loss: 35314.139\n",
      "[350,    20] loss: 40158.471\n",
      "[350,    25] loss: 21391.846\n",
      "[350,    30] loss: 19734.285\n",
      "[350,    35] loss: 24848.195\n",
      "[350,    40] loss: 27938.974\n",
      "[350,    45] loss: 22803.351\n",
      "[350,    50] loss: 16386.569\n",
      "[350,    55] loss: 22795.291\n",
      "[350,    60] loss: 31811.620\n",
      "[350,    65] loss: 22020.520\n",
      "[350,    70] loss: 20837.661\n",
      "[350,    75] loss: 43391.663\n",
      "[350,    80] loss: 22039.065\n",
      "[350,    85] loss: 20713.303\n",
      "[350,    90] loss: 27178.794\n",
      "[350,    95] loss: 23153.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350,   100] loss: 28637.877\n",
      "[350,   105] loss: 23539.992\n",
      "[350,   110] loss: 22375.637\n",
      "[350,   115] loss: 28006.800\n",
      "[350,   120] loss: 30656.950\n",
      "[350,   125] loss: 29479.992\n",
      "[350,   130] loss: 22705.997\n",
      "[350,   135] loss: 20827.405\n",
      "[350,   140] loss: 33570.810\n",
      "[350,   145] loss: 22509.022\n",
      "[350,   150] loss: 26148.162\n",
      "[350,   155] loss: 24824.544\n",
      "[350,   160] loss: 20186.755\n",
      "[350,   165] loss: 35631.094\n",
      "[350,   170] loss: 17735.235\n",
      "[350,   175] loss: 22103.001\n",
      "[350,   180] loss: 27690.366\n",
      "[350,   185] loss: 24349.102\n",
      "[350,   190] loss: 32518.017\n",
      "[350,   195] loss: 25889.616\n",
      "[350,   200] loss: 21374.773\n",
      "[350,   205] loss: 21841.704\n",
      "[350,   210] loss: 27903.604\n",
      "[350,   215] loss: 26395.265\n",
      "[350,   220] loss: 26326.173\n",
      "[350,   225] loss: 20994.066\n",
      "[350,   230] loss: 38818.600\n",
      "[351,     5] loss: 22020.680\n",
      "[351,    10] loss: 21248.478\n",
      "[351,    15] loss: 29352.411\n",
      "[351,    20] loss: 22646.829\n",
      "[351,    25] loss: 40173.021\n",
      "[351,    30] loss: 37149.886\n",
      "[351,    35] loss: 23324.090\n",
      "[351,    40] loss: 23238.214\n",
      "[351,    45] loss: 18799.906\n",
      "[351,    50] loss: 27415.809\n",
      "[351,    55] loss: 25222.851\n",
      "[351,    60] loss: 27572.773\n",
      "[351,    65] loss: 23711.418\n",
      "[351,    70] loss: 20111.325\n",
      "[351,    75] loss: 25082.353\n",
      "[351,    80] loss: 25116.281\n",
      "[351,    85] loss: 34047.231\n",
      "[351,    90] loss: 58321.152\n",
      "[351,    95] loss: 25012.070\n",
      "[351,   100] loss: 33527.505\n",
      "[351,   105] loss: 18670.160\n",
      "[351,   110] loss: 15844.400\n",
      "[351,   115] loss: 35121.105\n",
      "[351,   120] loss: 20412.280\n",
      "[351,   125] loss: 21688.386\n",
      "[351,   130] loss: 30315.356\n",
      "[351,   135] loss: 24189.247\n",
      "[351,   140] loss: 16864.416\n",
      "[351,   145] loss: 17064.706\n",
      "[351,   150] loss: 20034.195\n",
      "[351,   155] loss: 31509.742\n",
      "[351,   160] loss: 17779.057\n",
      "[351,   165] loss: 17836.311\n",
      "[351,   170] loss: 22160.664\n",
      "[351,   175] loss: 20878.567\n",
      "[351,   180] loss: 24245.812\n",
      "[351,   185] loss: 19682.599\n",
      "[351,   190] loss: 29573.621\n",
      "[351,   195] loss: 23317.998\n",
      "[351,   200] loss: 15421.137\n",
      "[351,   205] loss: 41149.736\n",
      "[351,   210] loss: 30714.457\n",
      "[351,   215] loss: 21535.181\n",
      "[351,   220] loss: 33061.418\n",
      "[351,   225] loss: 19844.405\n",
      "[351,   230] loss: 30059.562\n",
      "[352,     5] loss: 17769.634\n",
      "[352,    10] loss: 27388.781\n",
      "[352,    15] loss: 27571.014\n",
      "[352,    20] loss: 31185.954\n",
      "[352,    25] loss: 22894.750\n",
      "[352,    30] loss: 23292.309\n",
      "[352,    35] loss: 28385.858\n",
      "[352,    40] loss: 25567.930\n",
      "[352,    45] loss: 17032.169\n",
      "[352,    50] loss: 29579.141\n",
      "[352,    55] loss: 29277.110\n",
      "[352,    60] loss: 15084.417\n",
      "[352,    65] loss: 27112.321\n",
      "[352,    70] loss: 26687.845\n",
      "[352,    75] loss: 20471.829\n",
      "[352,    80] loss: 15752.649\n",
      "[352,    85] loss: 26718.499\n",
      "[352,    90] loss: 23006.379\n",
      "[352,    95] loss: 26294.263\n",
      "[352,   100] loss: 23407.916\n",
      "[352,   105] loss: 30688.173\n",
      "[352,   110] loss: 25979.740\n",
      "[352,   115] loss: 21534.714\n",
      "[352,   120] loss: 24622.763\n",
      "[352,   125] loss: 30711.251\n",
      "[352,   130] loss: 23692.544\n",
      "[352,   135] loss: 17438.814\n",
      "[352,   140] loss: 33754.883\n",
      "[352,   145] loss: 48538.128\n",
      "[352,   150] loss: 29590.649\n",
      "[352,   155] loss: 22414.761\n",
      "[352,   160] loss: 20025.280\n",
      "[352,   165] loss: 25815.050\n",
      "[352,   170] loss: 26263.222\n",
      "[352,   175] loss: 31986.090\n",
      "[352,   180] loss: 21595.518\n",
      "[352,   185] loss: 24816.844\n",
      "[352,   190] loss: 30651.456\n",
      "[352,   195] loss: 17992.730\n",
      "[352,   200] loss: 26776.595\n",
      "[352,   205] loss: 27366.534\n",
      "[352,   210] loss: 27341.577\n",
      "[352,   215] loss: 45672.796\n",
      "[352,   220] loss: 25914.531\n",
      "[352,   225] loss: 19594.450\n",
      "[352,   230] loss: 22637.399\n",
      "[353,     5] loss: 19909.556\n",
      "[353,    10] loss: 19688.708\n",
      "[353,    15] loss: 18883.206\n",
      "[353,    20] loss: 23263.114\n",
      "[353,    25] loss: 25822.779\n",
      "[353,    30] loss: 25088.370\n",
      "[353,    35] loss: 30624.923\n",
      "[353,    40] loss: 35923.467\n",
      "[353,    45] loss: 28634.823\n",
      "[353,    50] loss: 23477.562\n",
      "[353,    55] loss: 28652.821\n",
      "[353,    60] loss: 36387.197\n",
      "[353,    65] loss: 19357.271\n",
      "[353,    70] loss: 23329.249\n",
      "[353,    75] loss: 18442.975\n",
      "[353,    80] loss: 24846.981\n",
      "[353,    85] loss: 27912.130\n",
      "[353,    90] loss: 34329.502\n",
      "[353,    95] loss: 25200.878\n",
      "[353,   100] loss: 16608.409\n",
      "[353,   105] loss: 21641.424\n",
      "[353,   110] loss: 20331.070\n",
      "[353,   115] loss: 33864.810\n",
      "[353,   120] loss: 19581.820\n",
      "[353,   125] loss: 25127.081\n",
      "[353,   130] loss: 30116.221\n",
      "[353,   135] loss: 25693.417\n",
      "[353,   140] loss: 25748.780\n",
      "[353,   145] loss: 21546.340\n",
      "[353,   150] loss: 26201.987\n",
      "[353,   155] loss: 26833.838\n",
      "[353,   160] loss: 28047.927\n",
      "[353,   165] loss: 22146.407\n",
      "[353,   170] loss: 34466.323\n",
      "[353,   175] loss: 24878.472\n",
      "[353,   180] loss: 14926.691\n",
      "[353,   185] loss: 29807.895\n",
      "[353,   190] loss: 35111.536\n",
      "[353,   195] loss: 51906.854\n",
      "[353,   200] loss: 23029.128\n",
      "[353,   205] loss: 19408.782\n",
      "[353,   210] loss: 22371.769\n",
      "[353,   215] loss: 28473.948\n",
      "[353,   220] loss: 25936.878\n",
      "[353,   225] loss: 23131.619\n",
      "[353,   230] loss: 22112.202\n",
      "[354,     5] loss: 17194.029\n",
      "[354,    10] loss: 27431.909\n",
      "[354,    15] loss: 28154.149\n",
      "[354,    20] loss: 18068.182\n",
      "[354,    25] loss: 26603.970\n",
      "[354,    30] loss: 23932.131\n",
      "[354,    35] loss: 29261.537\n",
      "[354,    40] loss: 18281.596\n",
      "[354,    45] loss: 20356.526\n",
      "[354,    50] loss: 24238.276\n",
      "[354,    55] loss: 46986.686\n",
      "[354,    60] loss: 30903.303\n",
      "[354,    65] loss: 23333.113\n",
      "[354,    70] loss: 20515.390\n",
      "[354,    75] loss: 31128.219\n",
      "[354,    80] loss: 22948.283\n",
      "[354,    85] loss: 30414.548\n",
      "[354,    90] loss: 25963.181\n",
      "[354,    95] loss: 25782.667\n",
      "[354,   100] loss: 26759.528\n",
      "[354,   105] loss: 22374.211\n",
      "[354,   110] loss: 25892.927\n",
      "[354,   115] loss: 26065.532\n",
      "[354,   120] loss: 30204.760\n",
      "[354,   125] loss: 17717.180\n",
      "[354,   130] loss: 38084.604\n",
      "[354,   135] loss: 19232.188\n",
      "[354,   140] loss: 29399.070\n",
      "[354,   145] loss: 22732.044\n",
      "[354,   150] loss: 19191.274\n",
      "[354,   155] loss: 32040.931\n",
      "[354,   160] loss: 32272.202\n",
      "[354,   165] loss: 32875.102\n",
      "[354,   170] loss: 23441.896\n",
      "[354,   175] loss: 21879.368\n",
      "[354,   180] loss: 25331.476\n",
      "[354,   185] loss: 23984.385\n",
      "[354,   190] loss: 27659.918\n",
      "[354,   195] loss: 21713.636\n",
      "[354,   200] loss: 20545.487\n",
      "[354,   205] loss: 20432.471\n",
      "[354,   210] loss: 26672.878\n",
      "[354,   215] loss: 17102.047\n",
      "[354,   220] loss: 31625.789\n",
      "[354,   225] loss: 36658.484\n",
      "[354,   230] loss: 20020.096\n",
      "[355,     5] loss: 22193.762\n",
      "[355,    10] loss: 31651.558\n",
      "[355,    15] loss: 25320.393\n",
      "[355,    20] loss: 25948.734\n",
      "[355,    25] loss: 23070.572\n",
      "[355,    30] loss: 20956.437\n",
      "[355,    35] loss: 23744.111\n",
      "[355,    40] loss: 20513.579\n",
      "[355,    45] loss: 24225.787\n",
      "[355,    50] loss: 33782.723\n",
      "[355,    55] loss: 15480.001\n",
      "[355,    60] loss: 38134.949\n",
      "[355,    65] loss: 22981.229\n",
      "[355,    70] loss: 32380.289\n",
      "[355,    75] loss: 22670.085\n",
      "[355,    80] loss: 62003.980\n",
      "[355,    85] loss: 21497.017\n",
      "[355,    90] loss: 30000.406\n",
      "[355,    95] loss: 21464.338\n",
      "[355,   100] loss: 25214.723\n",
      "[355,   105] loss: 15972.797\n",
      "[355,   110] loss: 17569.342\n",
      "[355,   115] loss: 26857.871\n",
      "[355,   120] loss: 28886.270\n",
      "[355,   125] loss: 23859.967\n",
      "[355,   130] loss: 23055.370\n",
      "[355,   135] loss: 26735.831\n",
      "[355,   140] loss: 18608.572\n",
      "[355,   145] loss: 25969.933\n",
      "[355,   150] loss: 24097.184\n",
      "[355,   155] loss: 30574.527\n",
      "[355,   160] loss: 42081.699\n",
      "[355,   165] loss: 32020.182\n",
      "[355,   170] loss: 24718.671\n",
      "[355,   175] loss: 24601.321\n",
      "[355,   180] loss: 32322.611\n",
      "[355,   185] loss: 19558.874\n",
      "[355,   190] loss: 25906.225\n",
      "[355,   195] loss: 21100.541\n",
      "[355,   200] loss: 16171.553\n",
      "[355,   205] loss: 22817.767\n",
      "[355,   210] loss: 20599.347\n",
      "[355,   215] loss: 23362.572\n",
      "[355,   220] loss: 15629.193\n",
      "[355,   225] loss: 27992.701\n",
      "[355,   230] loss: 28151.962\n",
      "[356,     5] loss: 34223.246\n",
      "[356,    10] loss: 33691.418\n",
      "[356,    15] loss: 38088.731\n",
      "[356,    20] loss: 37586.023\n",
      "[356,    25] loss: 34124.793\n",
      "[356,    30] loss: 29688.974\n",
      "[356,    35] loss: 20637.770\n",
      "[356,    40] loss: 15712.237\n",
      "[356,    45] loss: 19634.371\n",
      "[356,    50] loss: 29638.939\n",
      "[356,    55] loss: 24658.974\n",
      "[356,    60] loss: 33012.508\n",
      "[356,    65] loss: 27554.428\n",
      "[356,    70] loss: 22798.170\n",
      "[356,    75] loss: 15660.124\n",
      "[356,    80] loss: 23246.921\n",
      "[356,    85] loss: 19528.328\n",
      "[356,    90] loss: 29262.452\n",
      "[356,    95] loss: 24643.683\n",
      "[356,   100] loss: 27206.030\n",
      "[356,   105] loss: 26047.302\n",
      "[356,   110] loss: 31262.095\n",
      "[356,   115] loss: 23563.146\n",
      "[356,   120] loss: 27331.978\n",
      "[356,   125] loss: 13524.249\n",
      "[356,   130] loss: 36662.046\n",
      "[356,   135] loss: 28580.551\n",
      "[356,   140] loss: 20080.069\n",
      "[356,   145] loss: 19620.451\n",
      "[356,   150] loss: 21865.551\n",
      "[356,   155] loss: 45441.413\n",
      "[356,   160] loss: 18592.622\n",
      "[356,   165] loss: 25529.054\n",
      "[356,   170] loss: 19018.940\n",
      "[356,   175] loss: 16154.063\n",
      "[356,   180] loss: 22878.271\n",
      "[356,   185] loss: 30454.149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[356,   190] loss: 32128.812\n",
      "[356,   195] loss: 14732.459\n",
      "[356,   200] loss: 19791.491\n",
      "[356,   205] loss: 19399.418\n",
      "[356,   210] loss: 24702.952\n",
      "[356,   215] loss: 26403.719\n",
      "[356,   220] loss: 27839.388\n",
      "[356,   225] loss: 29331.062\n",
      "[356,   230] loss: 20356.423\n",
      "[357,     5] loss: 45391.229\n",
      "[357,    10] loss: 18019.625\n",
      "[357,    15] loss: 21876.471\n",
      "[357,    20] loss: 17959.933\n",
      "[357,    25] loss: 48380.615\n",
      "[357,    30] loss: 23929.748\n",
      "[357,    35] loss: 24135.087\n",
      "[357,    40] loss: 24987.909\n",
      "[357,    45] loss: 32712.140\n",
      "[357,    50] loss: 26085.859\n",
      "[357,    55] loss: 25357.389\n",
      "[357,    60] loss: 24129.398\n",
      "[357,    65] loss: 19989.740\n",
      "[357,    70] loss: 29301.801\n",
      "[357,    75] loss: 29313.113\n",
      "[357,    80] loss: 20576.872\n",
      "[357,    85] loss: 20705.336\n",
      "[357,    90] loss: 25736.877\n",
      "[357,    95] loss: 25486.407\n",
      "[357,   100] loss: 24613.221\n",
      "[357,   105] loss: 20076.023\n",
      "[357,   110] loss: 30805.018\n",
      "[357,   115] loss: 21572.821\n",
      "[357,   120] loss: 31065.298\n",
      "[357,   125] loss: 23654.155\n",
      "[357,   130] loss: 21120.830\n",
      "[357,   135] loss: 35726.236\n",
      "[357,   140] loss: 36399.739\n",
      "[357,   145] loss: 17749.617\n",
      "[357,   150] loss: 20825.653\n",
      "[357,   155] loss: 26343.072\n",
      "[357,   160] loss: 22539.689\n",
      "[357,   165] loss: 37017.170\n",
      "[357,   170] loss: 18825.261\n",
      "[357,   175] loss: 25239.532\n",
      "[357,   180] loss: 24508.724\n",
      "[357,   185] loss: 23999.711\n",
      "[357,   190] loss: 24241.677\n",
      "[357,   195] loss: 27115.506\n",
      "[357,   200] loss: 29907.086\n",
      "[357,   205] loss: 19902.598\n",
      "[357,   210] loss: 26499.857\n",
      "[357,   215] loss: 17333.108\n",
      "[357,   220] loss: 30693.371\n",
      "[357,   225] loss: 21983.388\n",
      "[357,   230] loss: 21033.379\n",
      "[358,     5] loss: 27657.977\n",
      "[358,    10] loss: 11707.976\n",
      "[358,    15] loss: 23002.058\n",
      "[358,    20] loss: 24392.145\n",
      "[358,    25] loss: 20530.503\n",
      "[358,    30] loss: 25447.832\n",
      "[358,    35] loss: 42297.163\n",
      "[358,    40] loss: 27752.844\n",
      "[358,    45] loss: 23541.365\n",
      "[358,    50] loss: 23602.971\n",
      "[358,    55] loss: 27176.738\n",
      "[358,    60] loss: 35205.329\n",
      "[358,    65] loss: 17790.277\n",
      "[358,    70] loss: 22265.766\n",
      "[358,    75] loss: 23475.259\n",
      "[358,    80] loss: 22552.523\n",
      "[358,    85] loss: 29642.639\n",
      "[358,    90] loss: 15932.912\n",
      "[358,    95] loss: 20126.113\n",
      "[358,   100] loss: 29380.725\n",
      "[358,   105] loss: 20914.333\n",
      "[358,   110] loss: 21224.921\n",
      "[358,   115] loss: 21257.924\n",
      "[358,   120] loss: 24585.267\n",
      "[358,   125] loss: 23746.805\n",
      "[358,   130] loss: 22932.605\n",
      "[358,   135] loss: 31902.782\n",
      "[358,   140] loss: 21209.510\n",
      "[358,   145] loss: 26930.888\n",
      "[358,   150] loss: 40828.443\n",
      "[358,   155] loss: 33917.905\n",
      "[358,   160] loss: 21410.103\n",
      "[358,   165] loss: 25314.296\n",
      "[358,   170] loss: 27398.380\n",
      "[358,   175] loss: 21256.730\n",
      "[358,   180] loss: 26602.168\n",
      "[358,   185] loss: 41561.939\n",
      "[358,   190] loss: 27021.862\n",
      "[358,   195] loss: 30410.287\n",
      "[358,   200] loss: 19012.561\n",
      "[358,   205] loss: 22717.602\n",
      "[358,   210] loss: 46008.939\n",
      "[358,   215] loss: 21305.836\n",
      "[358,   220] loss: 24695.678\n",
      "[358,   225] loss: 16730.893\n",
      "[358,   230] loss: 24744.989\n",
      "[359,     5] loss: 28601.757\n",
      "[359,    10] loss: 26572.248\n",
      "[359,    15] loss: 16111.327\n",
      "[359,    20] loss: 30981.123\n",
      "[359,    25] loss: 28361.518\n",
      "[359,    30] loss: 20071.663\n",
      "[359,    35] loss: 23738.755\n",
      "[359,    40] loss: 19717.367\n",
      "[359,    45] loss: 43950.884\n",
      "[359,    50] loss: 29138.222\n",
      "[359,    55] loss: 19695.328\n",
      "[359,    60] loss: 26090.935\n",
      "[359,    65] loss: 19062.544\n",
      "[359,    70] loss: 20483.339\n",
      "[359,    75] loss: 23200.909\n",
      "[359,    80] loss: 21959.410\n",
      "[359,    85] loss: 18327.919\n",
      "[359,    90] loss: 26479.024\n",
      "[359,    95] loss: 20961.007\n",
      "[359,   100] loss: 33072.015\n",
      "[359,   105] loss: 18470.728\n",
      "[359,   110] loss: 33490.673\n",
      "[359,   115] loss: 24380.541\n",
      "[359,   120] loss: 25874.954\n",
      "[359,   125] loss: 29150.294\n",
      "[359,   130] loss: 20259.541\n",
      "[359,   135] loss: 23503.638\n",
      "[359,   140] loss: 23255.172\n",
      "[359,   145] loss: 41921.988\n",
      "[359,   150] loss: 29110.917\n",
      "[359,   155] loss: 27386.904\n",
      "[359,   160] loss: 31303.062\n",
      "[359,   165] loss: 23151.274\n",
      "[359,   170] loss: 21746.404\n",
      "[359,   175] loss: 22290.932\n",
      "[359,   180] loss: 28603.584\n",
      "[359,   185] loss: 25910.917\n",
      "[359,   190] loss: 24417.368\n",
      "[359,   195] loss: 37446.200\n",
      "[359,   200] loss: 32662.241\n",
      "[359,   205] loss: 30078.167\n",
      "[359,   210] loss: 24744.182\n",
      "[359,   215] loss: 20600.616\n",
      "[359,   220] loss: 16025.731\n",
      "[359,   225] loss: 20308.616\n",
      "[359,   230] loss: 28422.820\n",
      "[360,     5] loss: 24699.072\n",
      "[360,    10] loss: 19908.775\n",
      "[360,    15] loss: 23826.477\n",
      "[360,    20] loss: 25385.366\n",
      "[360,    25] loss: 16255.412\n",
      "[360,    30] loss: 23219.668\n",
      "[360,    35] loss: 28989.961\n",
      "[360,    40] loss: 25816.633\n",
      "[360,    45] loss: 20307.089\n",
      "[360,    50] loss: 41353.081\n",
      "[360,    55] loss: 26239.836\n",
      "[360,    60] loss: 23209.805\n",
      "[360,    65] loss: 19558.195\n",
      "[360,    70] loss: 21960.083\n",
      "[360,    75] loss: 23299.047\n",
      "[360,    80] loss: 38113.066\n",
      "[360,    85] loss: 34305.490\n",
      "[360,    90] loss: 25363.046\n",
      "[360,    95] loss: 29031.685\n",
      "[360,   100] loss: 24249.465\n",
      "[360,   105] loss: 25683.822\n",
      "[360,   110] loss: 23093.120\n",
      "[360,   115] loss: 27163.087\n",
      "[360,   120] loss: 25414.349\n",
      "[360,   125] loss: 17439.314\n",
      "[360,   130] loss: 27879.945\n",
      "[360,   135] loss: 29353.356\n",
      "[360,   140] loss: 31639.446\n",
      "[360,   145] loss: 29256.719\n",
      "[360,   150] loss: 24424.853\n",
      "[360,   155] loss: 18661.099\n",
      "[360,   160] loss: 24415.137\n",
      "[360,   165] loss: 25250.154\n",
      "[360,   170] loss: 21738.591\n",
      "[360,   175] loss: 27531.908\n",
      "[360,   180] loss: 20542.730\n",
      "[360,   185] loss: 18937.715\n",
      "[360,   190] loss: 20411.543\n",
      "[360,   195] loss: 28772.715\n",
      "[360,   200] loss: 24555.248\n",
      "[360,   205] loss: 17460.963\n",
      "[360,   210] loss: 24473.054\n",
      "[360,   215] loss: 19646.179\n",
      "[360,   220] loss: 51024.901\n",
      "[360,   225] loss: 40114.622\n",
      "[360,   230] loss: 18134.146\n",
      "[361,     5] loss: 36501.629\n",
      "[361,    10] loss: 31153.381\n",
      "[361,    15] loss: 45872.447\n",
      "[361,    20] loss: 20817.253\n",
      "[361,    25] loss: 21345.867\n",
      "[361,    30] loss: 20977.900\n",
      "[361,    35] loss: 31751.736\n",
      "[361,    40] loss: 23718.732\n",
      "[361,    45] loss: 30394.420\n",
      "[361,    50] loss: 31172.580\n",
      "[361,    55] loss: 26284.090\n",
      "[361,    60] loss: 20242.379\n",
      "[361,    65] loss: 22400.926\n",
      "[361,    70] loss: 22163.966\n",
      "[361,    75] loss: 21647.516\n",
      "[361,    80] loss: 39681.220\n",
      "[361,    85] loss: 26097.930\n",
      "[361,    90] loss: 21257.671\n",
      "[361,    95] loss: 30196.034\n",
      "[361,   100] loss: 20795.934\n",
      "[361,   105] loss: 21423.168\n",
      "[361,   110] loss: 20466.982\n",
      "[361,   115] loss: 25268.798\n",
      "[361,   120] loss: 32882.134\n",
      "[361,   125] loss: 17224.455\n",
      "[361,   130] loss: 27890.947\n",
      "[361,   135] loss: 20971.423\n",
      "[361,   140] loss: 25796.349\n",
      "[361,   145] loss: 17931.746\n",
      "[361,   150] loss: 19769.312\n",
      "[361,   155] loss: 15530.854\n",
      "[361,   160] loss: 24444.971\n",
      "[361,   165] loss: 31007.571\n",
      "[361,   170] loss: 30480.364\n",
      "[361,   175] loss: 30096.100\n",
      "[361,   180] loss: 26054.166\n",
      "[361,   185] loss: 23311.204\n",
      "[361,   190] loss: 28141.195\n",
      "[361,   195] loss: 16418.855\n",
      "[361,   200] loss: 20921.731\n",
      "[361,   205] loss: 21765.775\n",
      "[361,   210] loss: 24802.666\n",
      "[361,   215] loss: 46159.281\n",
      "[361,   220] loss: 27143.659\n",
      "[361,   225] loss: 20715.977\n",
      "[361,   230] loss: 21184.063\n",
      "[362,     5] loss: 20303.068\n",
      "[362,    10] loss: 27584.349\n",
      "[362,    15] loss: 15394.455\n",
      "[362,    20] loss: 39500.546\n",
      "[362,    25] loss: 20171.454\n",
      "[362,    30] loss: 22707.643\n",
      "[362,    35] loss: 22421.528\n",
      "[362,    40] loss: 31074.079\n",
      "[362,    45] loss: 18137.442\n",
      "[362,    50] loss: 21810.988\n",
      "[362,    55] loss: 15826.687\n",
      "[362,    60] loss: 22397.988\n",
      "[362,    65] loss: 27180.993\n",
      "[362,    70] loss: 25950.010\n",
      "[362,    75] loss: 29892.886\n",
      "[362,    80] loss: 27663.493\n",
      "[362,    85] loss: 21908.870\n",
      "[362,    90] loss: 17061.062\n",
      "[362,    95] loss: 53024.634\n",
      "[362,   100] loss: 19900.114\n",
      "[362,   105] loss: 26418.084\n",
      "[362,   110] loss: 25540.291\n",
      "[362,   115] loss: 18894.360\n",
      "[362,   120] loss: 34940.753\n",
      "[362,   125] loss: 22872.148\n",
      "[362,   130] loss: 31752.342\n",
      "[362,   135] loss: 27959.431\n",
      "[362,   140] loss: 34430.518\n",
      "[362,   145] loss: 27709.907\n",
      "[362,   150] loss: 27381.711\n",
      "[362,   155] loss: 34087.739\n",
      "[362,   160] loss: 19771.596\n",
      "[362,   165] loss: 38620.756\n",
      "[362,   170] loss: 47586.145\n",
      "[362,   175] loss: 19600.585\n",
      "[362,   180] loss: 20262.942\n",
      "[362,   185] loss: 25371.738\n",
      "[362,   190] loss: 25837.931\n",
      "[362,   195] loss: 21773.616\n",
      "[362,   200] loss: 28021.717\n",
      "[362,   205] loss: 21631.973\n",
      "[362,   210] loss: 15942.938\n",
      "[362,   215] loss: 13163.480\n",
      "[362,   220] loss: 30769.376\n",
      "[362,   225] loss: 22518.519\n",
      "[362,   230] loss: 28178.976\n",
      "[363,     5] loss: 19101.124\n",
      "[363,    10] loss: 41614.857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[363,    15] loss: 26745.979\n",
      "[363,    20] loss: 29640.536\n",
      "[363,    25] loss: 30438.146\n",
      "[363,    30] loss: 21138.706\n",
      "[363,    35] loss: 27610.993\n",
      "[363,    40] loss: 30986.421\n",
      "[363,    45] loss: 29491.459\n",
      "[363,    50] loss: 24815.431\n",
      "[363,    55] loss: 22732.345\n",
      "[363,    60] loss: 21033.559\n",
      "[363,    65] loss: 24163.094\n",
      "[363,    70] loss: 23090.596\n",
      "[363,    75] loss: 33955.930\n",
      "[363,    80] loss: 22251.005\n",
      "[363,    85] loss: 31118.512\n",
      "[363,    90] loss: 15489.948\n",
      "[363,    95] loss: 16160.388\n",
      "[363,   100] loss: 25827.490\n",
      "[363,   105] loss: 29331.387\n",
      "[363,   110] loss: 21982.188\n",
      "[363,   115] loss: 21688.120\n",
      "[363,   120] loss: 18883.256\n",
      "[363,   125] loss: 19304.251\n",
      "[363,   130] loss: 35943.230\n",
      "[363,   135] loss: 33321.211\n",
      "[363,   140] loss: 21042.552\n",
      "[363,   145] loss: 20997.835\n",
      "[363,   150] loss: 22581.329\n",
      "[363,   155] loss: 34099.965\n",
      "[363,   160] loss: 17608.252\n",
      "[363,   165] loss: 23279.285\n",
      "[363,   170] loss: 28091.215\n",
      "[363,   175] loss: 21013.942\n",
      "[363,   180] loss: 22552.764\n",
      "[363,   185] loss: 22289.358\n",
      "[363,   190] loss: 26226.588\n",
      "[363,   195] loss: 30459.914\n",
      "[363,   200] loss: 29029.646\n",
      "[363,   205] loss: 19282.643\n",
      "[363,   210] loss: 21604.481\n",
      "[363,   215] loss: 45364.702\n",
      "[363,   220] loss: 25529.530\n",
      "[363,   225] loss: 33698.658\n",
      "[363,   230] loss: 17172.589\n",
      "[364,     5] loss: 25263.899\n",
      "[364,    10] loss: 26079.085\n",
      "[364,    15] loss: 21874.420\n",
      "[364,    20] loss: 30169.670\n",
      "[364,    25] loss: 34005.824\n",
      "[364,    30] loss: 26604.773\n",
      "[364,    35] loss: 32251.880\n",
      "[364,    40] loss: 27764.801\n",
      "[364,    45] loss: 20111.077\n",
      "[364,    50] loss: 25507.614\n",
      "[364,    55] loss: 22921.064\n",
      "[364,    60] loss: 25914.805\n",
      "[364,    65] loss: 26212.041\n",
      "[364,    70] loss: 24504.540\n",
      "[364,    75] loss: 28019.130\n",
      "[364,    80] loss: 23992.021\n",
      "[364,    85] loss: 22779.608\n",
      "[364,    90] loss: 18763.520\n",
      "[364,    95] loss: 50840.268\n",
      "[364,   100] loss: 20290.560\n",
      "[364,   105] loss: 33302.548\n",
      "[364,   110] loss: 16570.971\n",
      "[364,   115] loss: 26205.193\n",
      "[364,   120] loss: 33220.608\n",
      "[364,   125] loss: 16486.474\n",
      "[364,   130] loss: 20775.517\n",
      "[364,   135] loss: 24551.698\n",
      "[364,   140] loss: 19104.143\n",
      "[364,   145] loss: 22945.544\n",
      "[364,   150] loss: 22129.329\n",
      "[364,   155] loss: 20677.521\n",
      "[364,   160] loss: 29475.577\n",
      "[364,   165] loss: 28026.504\n",
      "[364,   170] loss: 30288.386\n",
      "[364,   175] loss: 21281.629\n",
      "[364,   180] loss: 25950.779\n",
      "[364,   185] loss: 18998.157\n",
      "[364,   190] loss: 36461.487\n",
      "[364,   195] loss: 21426.931\n",
      "[364,   200] loss: 21611.597\n",
      "[364,   205] loss: 29128.968\n",
      "[364,   210] loss: 27825.181\n",
      "[364,   215] loss: 25676.589\n",
      "[364,   220] loss: 36950.020\n",
      "[364,   225] loss: 16133.174\n",
      "[364,   230] loss: 22234.357\n",
      "[365,     5] loss: 21165.727\n",
      "[365,    10] loss: 28587.794\n",
      "[365,    15] loss: 22199.611\n",
      "[365,    20] loss: 23616.489\n",
      "[365,    25] loss: 17658.079\n",
      "[365,    30] loss: 30494.461\n",
      "[365,    35] loss: 21685.832\n",
      "[365,    40] loss: 31092.378\n",
      "[365,    45] loss: 25309.374\n",
      "[365,    50] loss: 30946.639\n",
      "[365,    55] loss: 22886.326\n",
      "[365,    60] loss: 24344.338\n",
      "[365,    65] loss: 28298.966\n",
      "[365,    70] loss: 25067.722\n",
      "[365,    75] loss: 16583.591\n",
      "[365,    80] loss: 26326.451\n",
      "[365,    85] loss: 28241.107\n",
      "[365,    90] loss: 28053.244\n",
      "[365,    95] loss: 24585.835\n",
      "[365,   100] loss: 30642.070\n",
      "[365,   105] loss: 37202.723\n",
      "[365,   110] loss: 20349.862\n",
      "[365,   115] loss: 19459.319\n",
      "[365,   120] loss: 21070.002\n",
      "[365,   125] loss: 36886.537\n",
      "[365,   130] loss: 24213.993\n",
      "[365,   135] loss: 22275.715\n",
      "[365,   140] loss: 26140.035\n",
      "[365,   145] loss: 44236.315\n",
      "[365,   150] loss: 15457.105\n",
      "[365,   155] loss: 28943.359\n",
      "[365,   160] loss: 30670.086\n",
      "[365,   165] loss: 22256.588\n",
      "[365,   170] loss: 40389.937\n",
      "[365,   175] loss: 23892.151\n",
      "[365,   180] loss: 22759.874\n",
      "[365,   185] loss: 18589.830\n",
      "[365,   190] loss: 17963.893\n",
      "[365,   195] loss: 27361.072\n",
      "[365,   200] loss: 27828.916\n",
      "[365,   205] loss: 27634.536\n",
      "[365,   210] loss: 26335.732\n",
      "[365,   215] loss: 26029.075\n",
      "[365,   220] loss: 26553.712\n",
      "[365,   225] loss: 19784.170\n",
      "[365,   230] loss: 22515.769\n",
      "[366,     5] loss: 25048.559\n",
      "[366,    10] loss: 24522.495\n",
      "[366,    15] loss: 25220.253\n",
      "[366,    20] loss: 17621.959\n",
      "[366,    25] loss: 28871.360\n",
      "[366,    30] loss: 23741.738\n",
      "[366,    35] loss: 14913.173\n",
      "[366,    40] loss: 22981.589\n",
      "[366,    45] loss: 32326.963\n",
      "[366,    50] loss: 18310.222\n",
      "[366,    55] loss: 26477.478\n",
      "[366,    60] loss: 31511.409\n",
      "[366,    65] loss: 26030.008\n",
      "[366,    70] loss: 33780.825\n",
      "[366,    75] loss: 20742.699\n",
      "[366,    80] loss: 35941.052\n",
      "[366,    85] loss: 36123.728\n",
      "[366,    90] loss: 27124.743\n",
      "[366,    95] loss: 22058.053\n",
      "[366,   100] loss: 25422.974\n",
      "[366,   105] loss: 25795.832\n",
      "[366,   110] loss: 39249.064\n",
      "[366,   115] loss: 21348.380\n",
      "[366,   120] loss: 19318.691\n",
      "[366,   125] loss: 31336.938\n",
      "[366,   130] loss: 25821.516\n",
      "[366,   135] loss: 45202.835\n",
      "[366,   140] loss: 28538.422\n",
      "[366,   145] loss: 19046.251\n",
      "[366,   150] loss: 16189.356\n",
      "[366,   155] loss: 24328.716\n",
      "[366,   160] loss: 23485.122\n",
      "[366,   165] loss: 21860.410\n",
      "[366,   170] loss: 26283.762\n",
      "[366,   175] loss: 28599.489\n",
      "[366,   180] loss: 26856.650\n",
      "[366,   185] loss: 18126.167\n",
      "[366,   190] loss: 23758.957\n",
      "[366,   195] loss: 23737.951\n",
      "[366,   200] loss: 28185.027\n",
      "[366,   205] loss: 31613.648\n",
      "[366,   210] loss: 28422.708\n",
      "[366,   215] loss: 22909.945\n",
      "[366,   220] loss: 17927.690\n",
      "[366,   225] loss: 24392.791\n",
      "[366,   230] loss: 19237.578\n",
      "[367,     5] loss: 33097.007\n",
      "[367,    10] loss: 30618.196\n",
      "[367,    15] loss: 19997.205\n",
      "[367,    20] loss: 35902.623\n",
      "[367,    25] loss: 26245.833\n",
      "[367,    30] loss: 24729.971\n",
      "[367,    35] loss: 25709.851\n",
      "[367,    40] loss: 25756.418\n",
      "[367,    45] loss: 21321.699\n",
      "[367,    50] loss: 28978.418\n",
      "[367,    55] loss: 30740.156\n",
      "[367,    60] loss: 27017.674\n",
      "[367,    65] loss: 14620.525\n",
      "[367,    70] loss: 32162.345\n",
      "[367,    75] loss: 30422.220\n",
      "[367,    80] loss: 22677.693\n",
      "[367,    85] loss: 43518.221\n",
      "[367,    90] loss: 28642.100\n",
      "[367,    95] loss: 21436.295\n",
      "[367,   100] loss: 33840.360\n",
      "[367,   105] loss: 19626.796\n",
      "[367,   110] loss: 26954.650\n",
      "[367,   115] loss: 22788.805\n",
      "[367,   120] loss: 26394.281\n",
      "[367,   125] loss: 22804.957\n",
      "[367,   130] loss: 28293.125\n",
      "[367,   135] loss: 32504.976\n",
      "[367,   140] loss: 24528.539\n",
      "[367,   145] loss: 17670.438\n",
      "[367,   150] loss: 18035.752\n",
      "[367,   155] loss: 33670.699\n",
      "[367,   160] loss: 14614.347\n",
      "[367,   165] loss: 27677.184\n",
      "[367,   170] loss: 18230.175\n",
      "[367,   175] loss: 23678.449\n",
      "[367,   180] loss: 26664.085\n",
      "[367,   185] loss: 32053.241\n",
      "[367,   190] loss: 25421.637\n",
      "[367,   195] loss: 21643.088\n",
      "[367,   200] loss: 19782.883\n",
      "[367,   205] loss: 18582.202\n",
      "[367,   210] loss: 27284.270\n",
      "[367,   215] loss: 21751.682\n",
      "[367,   220] loss: 30515.236\n",
      "[367,   225] loss: 26261.534\n",
      "[367,   230] loss: 23036.436\n",
      "[368,     5] loss: 23183.180\n",
      "[368,    10] loss: 22387.443\n",
      "[368,    15] loss: 43035.906\n",
      "[368,    20] loss: 27200.888\n",
      "[368,    25] loss: 13641.028\n",
      "[368,    30] loss: 23542.975\n",
      "[368,    35] loss: 21222.396\n",
      "[368,    40] loss: 22684.127\n",
      "[368,    45] loss: 17472.637\n",
      "[368,    50] loss: 26804.248\n",
      "[368,    55] loss: 27191.321\n",
      "[368,    60] loss: 28874.407\n",
      "[368,    65] loss: 53333.949\n",
      "[368,    70] loss: 15005.837\n",
      "[368,    75] loss: 23571.312\n",
      "[368,    80] loss: 27714.977\n",
      "[368,    85] loss: 23130.624\n",
      "[368,    90] loss: 37751.099\n",
      "[368,    95] loss: 30380.727\n",
      "[368,   100] loss: 27492.645\n",
      "[368,   105] loss: 27339.706\n",
      "[368,   110] loss: 25458.465\n",
      "[368,   115] loss: 21824.602\n",
      "[368,   120] loss: 24845.895\n",
      "[368,   125] loss: 18196.695\n",
      "[368,   130] loss: 24365.051\n",
      "[368,   135] loss: 23346.334\n",
      "[368,   140] loss: 27607.996\n",
      "[368,   145] loss: 24246.135\n",
      "[368,   150] loss: 32373.529\n",
      "[368,   155] loss: 26948.268\n",
      "[368,   160] loss: 23539.628\n",
      "[368,   165] loss: 26275.933\n",
      "[368,   170] loss: 29749.606\n",
      "[368,   175] loss: 22372.211\n",
      "[368,   180] loss: 21712.861\n",
      "[368,   185] loss: 21426.832\n",
      "[368,   190] loss: 19845.814\n",
      "[368,   195] loss: 25340.201\n",
      "[368,   200] loss: 35826.461\n",
      "[368,   205] loss: 25753.126\n",
      "[368,   210] loss: 27022.661\n",
      "[368,   215] loss: 20114.616\n",
      "[368,   220] loss: 26295.120\n",
      "[368,   225] loss: 24060.216\n",
      "[368,   230] loss: 23035.550\n",
      "[369,     5] loss: 44475.365\n",
      "[369,    10] loss: 27442.219\n",
      "[369,    15] loss: 53241.364\n",
      "[369,    20] loss: 27562.946\n",
      "[369,    25] loss: 19084.064\n",
      "[369,    30] loss: 24408.844\n",
      "[369,    35] loss: 20758.369\n",
      "[369,    40] loss: 34047.673\n",
      "[369,    45] loss: 26889.966\n",
      "[369,    50] loss: 30954.823\n",
      "[369,    55] loss: 21294.833\n",
      "[369,    60] loss: 27804.746\n",
      "[369,    65] loss: 25113.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369,    70] loss: 24661.830\n",
      "[369,    75] loss: 19043.098\n",
      "[369,    80] loss: 19054.894\n",
      "[369,    85] loss: 24635.034\n",
      "[369,    90] loss: 25907.472\n",
      "[369,    95] loss: 32295.508\n",
      "[369,   100] loss: 24743.792\n",
      "[369,   105] loss: 18910.941\n",
      "[369,   110] loss: 25098.906\n",
      "[369,   115] loss: 21613.710\n",
      "[369,   120] loss: 18687.211\n",
      "[369,   125] loss: 28982.142\n",
      "[369,   130] loss: 17791.468\n",
      "[369,   135] loss: 22750.842\n",
      "[369,   140] loss: 22490.917\n",
      "[369,   145] loss: 26871.922\n",
      "[369,   150] loss: 23245.906\n",
      "[369,   155] loss: 27528.732\n",
      "[369,   160] loss: 24014.873\n",
      "[369,   165] loss: 27140.272\n",
      "[369,   170] loss: 22090.204\n",
      "[369,   175] loss: 21106.677\n",
      "[369,   180] loss: 32501.440\n",
      "[369,   185] loss: 29209.297\n",
      "[369,   190] loss: 20309.339\n",
      "[369,   195] loss: 22607.227\n",
      "[369,   200] loss: 26954.335\n",
      "[369,   205] loss: 18276.505\n",
      "[369,   210] loss: 22150.601\n",
      "[369,   215] loss: 24433.639\n",
      "[369,   220] loss: 31260.447\n",
      "[369,   225] loss: 25005.363\n",
      "[369,   230] loss: 27004.186\n",
      "[370,     5] loss: 24754.337\n",
      "[370,    10] loss: 35486.718\n",
      "[370,    15] loss: 11454.800\n",
      "[370,    20] loss: 22993.024\n",
      "[370,    25] loss: 23707.407\n",
      "[370,    30] loss: 20188.591\n",
      "[370,    35] loss: 22990.094\n",
      "[370,    40] loss: 35093.872\n",
      "[370,    45] loss: 23288.129\n",
      "[370,    50] loss: 15179.921\n",
      "[370,    55] loss: 30278.145\n",
      "[370,    60] loss: 20349.530\n",
      "[370,    65] loss: 35661.392\n",
      "[370,    70] loss: 21529.789\n",
      "[370,    75] loss: 35332.378\n",
      "[370,    80] loss: 23871.647\n",
      "[370,    85] loss: 20701.012\n",
      "[370,    90] loss: 18980.494\n",
      "[370,    95] loss: 31774.896\n",
      "[370,   100] loss: 21973.863\n",
      "[370,   105] loss: 24066.499\n",
      "[370,   110] loss: 26231.701\n",
      "[370,   115] loss: 28603.602\n",
      "[370,   120] loss: 29294.888\n",
      "[370,   125] loss: 26511.730\n",
      "[370,   130] loss: 22800.001\n",
      "[370,   135] loss: 43698.532\n",
      "[370,   140] loss: 21718.951\n",
      "[370,   145] loss: 20572.274\n",
      "[370,   150] loss: 29177.371\n",
      "[370,   155] loss: 24843.351\n",
      "[370,   160] loss: 28017.516\n",
      "[370,   165] loss: 26786.258\n",
      "[370,   170] loss: 26718.153\n",
      "[370,   175] loss: 26691.204\n",
      "[370,   180] loss: 36458.795\n",
      "[370,   185] loss: 16153.035\n",
      "[370,   190] loss: 26415.335\n",
      "[370,   195] loss: 43316.242\n",
      "[370,   200] loss: 31973.487\n",
      "[370,   205] loss: 19888.797\n",
      "[370,   210] loss: 21637.937\n",
      "[370,   215] loss: 21193.308\n",
      "[370,   220] loss: 19068.636\n",
      "[370,   225] loss: 18142.541\n",
      "[370,   230] loss: 22069.341\n",
      "[371,     5] loss: 34176.267\n",
      "[371,    10] loss: 21716.171\n",
      "[371,    15] loss: 23516.096\n",
      "[371,    20] loss: 28113.091\n",
      "[371,    25] loss: 38908.669\n",
      "[371,    30] loss: 22650.798\n",
      "[371,    35] loss: 16055.470\n",
      "[371,    40] loss: 24576.614\n",
      "[371,    45] loss: 27094.439\n",
      "[371,    50] loss: 29911.134\n",
      "[371,    55] loss: 18135.554\n",
      "[371,    60] loss: 18941.392\n",
      "[371,    65] loss: 16635.702\n",
      "[371,    70] loss: 29171.233\n",
      "[371,    75] loss: 32035.843\n",
      "[371,    80] loss: 24565.822\n",
      "[371,    85] loss: 23126.300\n",
      "[371,    90] loss: 28699.403\n",
      "[371,    95] loss: 25666.550\n",
      "[371,   100] loss: 32754.832\n",
      "[371,   105] loss: 17352.464\n",
      "[371,   110] loss: 26115.670\n",
      "[371,   115] loss: 14532.608\n",
      "[371,   120] loss: 20915.512\n",
      "[371,   125] loss: 21359.125\n",
      "[371,   130] loss: 24830.407\n",
      "[371,   135] loss: 19045.174\n",
      "[371,   140] loss: 21122.660\n",
      "[371,   145] loss: 22320.451\n",
      "[371,   150] loss: 24144.320\n",
      "[371,   155] loss: 19230.532\n",
      "[371,   160] loss: 25888.755\n",
      "[371,   165] loss: 25925.431\n",
      "[371,   170] loss: 55988.534\n",
      "[371,   175] loss: 27837.722\n",
      "[371,   180] loss: 23410.130\n",
      "[371,   185] loss: 35015.626\n",
      "[371,   190] loss: 29050.912\n",
      "[371,   195] loss: 20663.474\n",
      "[371,   200] loss: 31958.221\n",
      "[371,   205] loss: 16859.729\n",
      "[371,   210] loss: 28246.845\n",
      "[371,   215] loss: 25436.077\n",
      "[371,   220] loss: 36903.198\n",
      "[371,   225] loss: 21132.406\n",
      "[371,   230] loss: 28571.708\n",
      "[372,     5] loss: 29549.729\n",
      "[372,    10] loss: 17702.911\n",
      "[372,    15] loss: 29473.232\n",
      "[372,    20] loss: 25397.409\n",
      "[372,    25] loss: 22162.707\n",
      "[372,    30] loss: 17469.984\n",
      "[372,    35] loss: 25216.855\n",
      "[372,    40] loss: 20462.340\n",
      "[372,    45] loss: 20918.547\n",
      "[372,    50] loss: 31838.877\n",
      "[372,    55] loss: 21151.027\n",
      "[372,    60] loss: 27893.835\n",
      "[372,    65] loss: 29930.908\n",
      "[372,    70] loss: 54487.614\n",
      "[372,    75] loss: 21286.114\n",
      "[372,    80] loss: 24562.735\n",
      "[372,    85] loss: 23427.388\n",
      "[372,    90] loss: 30427.763\n",
      "[372,    95] loss: 27972.252\n",
      "[372,   100] loss: 25706.582\n",
      "[372,   105] loss: 23029.281\n",
      "[372,   110] loss: 22048.126\n",
      "[372,   115] loss: 32006.703\n",
      "[372,   120] loss: 31088.492\n",
      "[372,   125] loss: 18979.524\n",
      "[372,   130] loss: 21567.810\n",
      "[372,   135] loss: 26978.462\n",
      "[372,   140] loss: 25088.478\n",
      "[372,   145] loss: 44541.182\n",
      "[372,   150] loss: 19129.937\n",
      "[372,   155] loss: 23226.750\n",
      "[372,   160] loss: 21464.903\n",
      "[372,   165] loss: 27371.243\n",
      "[372,   170] loss: 17621.687\n",
      "[372,   175] loss: 29034.610\n",
      "[372,   180] loss: 16145.362\n",
      "[372,   185] loss: 16391.387\n",
      "[372,   190] loss: 28528.913\n",
      "[372,   195] loss: 27020.316\n",
      "[372,   200] loss: 28590.771\n",
      "[372,   205] loss: 26244.066\n",
      "[372,   210] loss: 25530.592\n",
      "[372,   215] loss: 22327.013\n",
      "[372,   220] loss: 27799.240\n",
      "[372,   225] loss: 21998.363\n",
      "[372,   230] loss: 13548.642\n",
      "[373,     5] loss: 26065.904\n",
      "[373,    10] loss: 21500.987\n",
      "[373,    15] loss: 27237.211\n",
      "[373,    20] loss: 24789.386\n",
      "[373,    25] loss: 45674.997\n",
      "[373,    30] loss: 34488.543\n",
      "[373,    35] loss: 24816.615\n",
      "[373,    40] loss: 35191.609\n",
      "[373,    45] loss: 19571.927\n",
      "[373,    50] loss: 27931.437\n",
      "[373,    55] loss: 18933.823\n",
      "[373,    60] loss: 20606.367\n",
      "[373,    65] loss: 19879.485\n",
      "[373,    70] loss: 22859.284\n",
      "[373,    75] loss: 21219.453\n",
      "[373,    80] loss: 29723.239\n",
      "[373,    85] loss: 24125.769\n",
      "[373,    90] loss: 17762.211\n",
      "[373,    95] loss: 22241.533\n",
      "[373,   100] loss: 26258.049\n",
      "[373,   105] loss: 22357.407\n",
      "[373,   110] loss: 21804.492\n",
      "[373,   115] loss: 36664.617\n",
      "[373,   120] loss: 34293.197\n",
      "[373,   125] loss: 21846.724\n",
      "[373,   130] loss: 25752.370\n",
      "[373,   135] loss: 24099.923\n",
      "[373,   140] loss: 25826.471\n",
      "[373,   145] loss: 28994.319\n",
      "[373,   150] loss: 20644.338\n",
      "[373,   155] loss: 26804.079\n",
      "[373,   160] loss: 17789.190\n",
      "[373,   165] loss: 17497.399\n",
      "[373,   170] loss: 28627.043\n",
      "[373,   175] loss: 24297.049\n",
      "[373,   180] loss: 33226.577\n",
      "[373,   185] loss: 26677.633\n",
      "[373,   190] loss: 20481.645\n",
      "[373,   195] loss: 31547.750\n",
      "[373,   200] loss: 28823.824\n",
      "[373,   205] loss: 29127.333\n",
      "[373,   210] loss: 24228.416\n",
      "[373,   215] loss: 27968.246\n",
      "[373,   220] loss: 17557.623\n",
      "[373,   225] loss: 19431.927\n",
      "[373,   230] loss: 28248.735\n",
      "[374,     5] loss: 22654.683\n",
      "[374,    10] loss: 18345.369\n",
      "[374,    15] loss: 24142.900\n",
      "[374,    20] loss: 20679.279\n",
      "[374,    25] loss: 16961.998\n",
      "[374,    30] loss: 22983.495\n",
      "[374,    35] loss: 23097.598\n",
      "[374,    40] loss: 19316.136\n",
      "[374,    45] loss: 44983.549\n",
      "[374,    50] loss: 51923.488\n",
      "[374,    55] loss: 23124.717\n",
      "[374,    60] loss: 24984.420\n",
      "[374,    65] loss: 38701.604\n",
      "[374,    70] loss: 27682.944\n",
      "[374,    75] loss: 20123.333\n",
      "[374,    80] loss: 25917.559\n",
      "[374,    85] loss: 20181.730\n",
      "[374,    90] loss: 26079.731\n",
      "[374,    95] loss: 22268.261\n",
      "[374,   100] loss: 21061.311\n",
      "[374,   105] loss: 19807.572\n",
      "[374,   110] loss: 18749.250\n",
      "[374,   115] loss: 27933.212\n",
      "[374,   120] loss: 25209.901\n",
      "[374,   125] loss: 23919.099\n",
      "[374,   130] loss: 21315.023\n",
      "[374,   135] loss: 22767.007\n",
      "[374,   140] loss: 19007.681\n",
      "[374,   145] loss: 34338.108\n",
      "[374,   150] loss: 21367.720\n",
      "[374,   155] loss: 22457.513\n",
      "[374,   160] loss: 22994.635\n",
      "[374,   165] loss: 15851.753\n",
      "[374,   170] loss: 23460.271\n",
      "[374,   175] loss: 32762.604\n",
      "[374,   180] loss: 24798.979\n",
      "[374,   185] loss: 23864.063\n",
      "[374,   190] loss: 26616.183\n",
      "[374,   195] loss: 32265.953\n",
      "[374,   200] loss: 21237.404\n",
      "[374,   205] loss: 28743.541\n",
      "[374,   210] loss: 24933.491\n",
      "[374,   215] loss: 27264.225\n",
      "[374,   220] loss: 38395.558\n",
      "[374,   225] loss: 33397.493\n",
      "[374,   230] loss: 32429.878\n",
      "[375,     5] loss: 26603.005\n",
      "[375,    10] loss: 38168.522\n",
      "[375,    15] loss: 21807.798\n",
      "[375,    20] loss: 23548.255\n",
      "[375,    25] loss: 36241.283\n",
      "[375,    30] loss: 28247.763\n",
      "[375,    35] loss: 44379.055\n",
      "[375,    40] loss: 30359.907\n",
      "[375,    45] loss: 20997.501\n",
      "[375,    50] loss: 26791.284\n",
      "[375,    55] loss: 19091.459\n",
      "[375,    60] loss: 21734.330\n",
      "[375,    65] loss: 45691.083\n",
      "[375,    70] loss: 14529.434\n",
      "[375,    75] loss: 29494.371\n",
      "[375,    80] loss: 24075.358\n",
      "[375,    85] loss: 26922.016\n",
      "[375,    90] loss: 25298.681\n",
      "[375,    95] loss: 26937.425\n",
      "[375,   100] loss: 26013.550\n",
      "[375,   105] loss: 19068.900\n",
      "[375,   110] loss: 18755.394\n",
      "[375,   115] loss: 18915.814\n",
      "[375,   120] loss: 24907.435\n",
      "[375,   125] loss: 20837.263\n",
      "[375,   130] loss: 29211.342\n",
      "[375,   135] loss: 21129.592\n",
      "[375,   140] loss: 18917.141\n",
      "[375,   145] loss: 23482.003\n",
      "[375,   150] loss: 19168.224\n",
      "[375,   155] loss: 21396.514\n",
      "[375,   160] loss: 26429.831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[375,   165] loss: 25707.722\n",
      "[375,   170] loss: 35749.562\n",
      "[375,   175] loss: 22815.030\n",
      "[375,   180] loss: 24483.101\n",
      "[375,   185] loss: 23055.209\n",
      "[375,   190] loss: 24668.129\n",
      "[375,   195] loss: 22180.605\n",
      "[375,   200] loss: 20954.905\n",
      "[375,   205] loss: 30329.701\n",
      "[375,   210] loss: 25372.371\n",
      "[375,   215] loss: 32360.007\n",
      "[375,   220] loss: 33341.435\n",
      "[375,   225] loss: 19289.137\n",
      "[375,   230] loss: 21402.025\n",
      "[376,     5] loss: 20553.154\n",
      "[376,    10] loss: 25751.939\n",
      "[376,    15] loss: 35434.978\n",
      "[376,    20] loss: 21889.648\n",
      "[376,    25] loss: 29884.182\n",
      "[376,    30] loss: 23784.086\n",
      "[376,    35] loss: 17381.724\n",
      "[376,    40] loss: 33131.438\n",
      "[376,    45] loss: 18302.666\n",
      "[376,    50] loss: 26441.696\n",
      "[376,    55] loss: 42667.264\n",
      "[376,    60] loss: 29236.853\n",
      "[376,    65] loss: 38770.033\n",
      "[376,    70] loss: 32618.309\n",
      "[376,    75] loss: 32320.212\n",
      "[376,    80] loss: 18904.685\n",
      "[376,    85] loss: 30892.392\n",
      "[376,    90] loss: 21116.779\n",
      "[376,    95] loss: 30292.263\n",
      "[376,   100] loss: 23329.062\n",
      "[376,   105] loss: 20815.530\n",
      "[376,   110] loss: 23661.285\n",
      "[376,   115] loss: 21917.184\n",
      "[376,   120] loss: 30750.809\n",
      "[376,   125] loss: 27133.056\n",
      "[376,   130] loss: 21409.768\n",
      "[376,   135] loss: 28190.863\n",
      "[376,   140] loss: 23466.269\n",
      "[376,   145] loss: 52275.583\n",
      "[376,   150] loss: 14084.476\n",
      "[376,   155] loss: 17458.661\n",
      "[376,   160] loss: 19100.947\n",
      "[376,   165] loss: 25368.766\n",
      "[376,   170] loss: 20613.468\n",
      "[376,   175] loss: 22852.282\n",
      "[376,   180] loss: 34468.525\n",
      "[376,   185] loss: 21154.573\n",
      "[376,   190] loss: 23398.281\n",
      "[376,   195] loss: 26966.749\n",
      "[376,   200] loss: 31006.670\n",
      "[376,   205] loss: 24195.541\n",
      "[376,   210] loss: 20172.314\n",
      "[376,   215] loss: 25702.497\n",
      "[376,   220] loss: 14489.857\n",
      "[376,   225] loss: 21227.220\n",
      "[376,   230] loss: 15564.913\n",
      "[377,     5] loss: 24582.679\n",
      "[377,    10] loss: 30175.646\n",
      "[377,    15] loss: 30101.444\n",
      "[377,    20] loss: 25440.623\n",
      "[377,    25] loss: 14089.776\n",
      "[377,    30] loss: 16875.315\n",
      "[377,    35] loss: 21499.919\n",
      "[377,    40] loss: 17029.840\n",
      "[377,    45] loss: 16112.269\n",
      "[377,    50] loss: 30086.209\n",
      "[377,    55] loss: 20779.876\n",
      "[377,    60] loss: 20497.902\n",
      "[377,    65] loss: 27278.885\n",
      "[377,    70] loss: 23322.073\n",
      "[377,    75] loss: 15419.791\n",
      "[377,    80] loss: 21171.608\n",
      "[377,    85] loss: 15942.990\n",
      "[377,    90] loss: 26157.902\n",
      "[377,    95] loss: 50935.082\n",
      "[377,   100] loss: 28991.284\n",
      "[377,   105] loss: 27024.346\n",
      "[377,   110] loss: 26210.395\n",
      "[377,   115] loss: 22562.286\n",
      "[377,   120] loss: 25478.256\n",
      "[377,   125] loss: 20965.824\n",
      "[377,   130] loss: 36695.644\n",
      "[377,   135] loss: 37440.434\n",
      "[377,   140] loss: 27581.284\n",
      "[377,   145] loss: 30361.654\n",
      "[377,   150] loss: 18734.135\n",
      "[377,   155] loss: 23195.565\n",
      "[377,   160] loss: 26803.416\n",
      "[377,   165] loss: 20423.477\n",
      "[377,   170] loss: 34222.043\n",
      "[377,   175] loss: 20834.300\n",
      "[377,   180] loss: 30696.776\n",
      "[377,   185] loss: 23537.610\n",
      "[377,   190] loss: 33190.805\n",
      "[377,   195] loss: 23379.839\n",
      "[377,   200] loss: 33973.368\n",
      "[377,   205] loss: 19477.365\n",
      "[377,   210] loss: 22575.279\n",
      "[377,   215] loss: 30858.866\n",
      "[377,   220] loss: 31559.898\n",
      "[377,   225] loss: 21288.523\n",
      "[377,   230] loss: 24669.987\n",
      "[378,     5] loss: 22079.316\n",
      "[378,    10] loss: 28585.011\n",
      "[378,    15] loss: 22733.254\n",
      "[378,    20] loss: 21596.212\n",
      "[378,    25] loss: 18824.796\n",
      "[378,    30] loss: 38466.260\n",
      "[378,    35] loss: 28593.621\n",
      "[378,    40] loss: 23909.444\n",
      "[378,    45] loss: 24888.895\n",
      "[378,    50] loss: 24814.346\n",
      "[378,    55] loss: 16012.988\n",
      "[378,    60] loss: 23282.951\n",
      "[378,    65] loss: 40388.128\n",
      "[378,    70] loss: 19789.995\n",
      "[378,    75] loss: 23390.825\n",
      "[378,    80] loss: 32453.584\n",
      "[378,    85] loss: 28842.905\n",
      "[378,    90] loss: 26270.670\n",
      "[378,    95] loss: 37724.072\n",
      "[378,   100] loss: 22405.925\n",
      "[378,   105] loss: 31601.701\n",
      "[378,   110] loss: 30723.953\n",
      "[378,   115] loss: 27814.964\n",
      "[378,   120] loss: 33790.620\n",
      "[378,   125] loss: 22392.642\n",
      "[378,   130] loss: 37918.994\n",
      "[378,   135] loss: 27130.420\n",
      "[378,   140] loss: 15015.575\n",
      "[378,   145] loss: 20479.900\n",
      "[378,   150] loss: 20849.183\n",
      "[378,   155] loss: 27939.679\n",
      "[378,   160] loss: 26616.526\n",
      "[378,   165] loss: 29079.111\n",
      "[378,   170] loss: 22656.311\n",
      "[378,   175] loss: 29285.537\n",
      "[378,   180] loss: 17145.222\n",
      "[378,   185] loss: 25732.136\n",
      "[378,   190] loss: 24422.073\n",
      "[378,   195] loss: 21101.957\n",
      "[378,   200] loss: 17669.966\n",
      "[378,   205] loss: 20490.175\n",
      "[378,   210] loss: 21308.205\n",
      "[378,   215] loss: 19264.419\n",
      "[378,   220] loss: 23472.644\n",
      "[378,   225] loss: 47424.039\n",
      "[378,   230] loss: 22464.202\n",
      "[379,     5] loss: 24478.216\n",
      "[379,    10] loss: 20968.843\n",
      "[379,    15] loss: 24349.719\n",
      "[379,    20] loss: 18348.991\n",
      "[379,    25] loss: 14919.691\n",
      "[379,    30] loss: 52983.949\n",
      "[379,    35] loss: 24416.734\n",
      "[379,    40] loss: 19720.902\n",
      "[379,    45] loss: 16591.015\n",
      "[379,    50] loss: 25770.439\n",
      "[379,    55] loss: 18345.844\n",
      "[379,    60] loss: 31943.673\n",
      "[379,    65] loss: 42513.384\n",
      "[379,    70] loss: 36859.598\n",
      "[379,    75] loss: 20888.559\n",
      "[379,    80] loss: 23126.292\n",
      "[379,    85] loss: 28981.925\n",
      "[379,    90] loss: 20410.227\n",
      "[379,    95] loss: 15757.585\n",
      "[379,   100] loss: 25417.636\n",
      "[379,   105] loss: 22557.620\n",
      "[379,   110] loss: 19875.322\n",
      "[379,   115] loss: 28524.198\n",
      "[379,   120] loss: 26012.360\n",
      "[379,   125] loss: 22294.576\n",
      "[379,   130] loss: 22969.857\n",
      "[379,   135] loss: 24813.706\n",
      "[379,   140] loss: 23237.079\n",
      "[379,   145] loss: 33879.750\n",
      "[379,   150] loss: 29966.952\n",
      "[379,   155] loss: 21957.566\n",
      "[379,   160] loss: 22876.818\n",
      "[379,   165] loss: 21258.730\n",
      "[379,   170] loss: 33286.120\n",
      "[379,   175] loss: 22209.887\n",
      "[379,   180] loss: 24615.219\n",
      "[379,   185] loss: 25060.793\n",
      "[379,   190] loss: 26867.506\n",
      "[379,   195] loss: 46398.625\n",
      "[379,   200] loss: 27021.648\n",
      "[379,   205] loss: 23173.685\n",
      "[379,   210] loss: 27749.919\n",
      "[379,   215] loss: 24151.334\n",
      "[379,   220] loss: 12595.178\n",
      "[379,   225] loss: 21913.329\n",
      "[379,   230] loss: 33637.203\n",
      "[380,     5] loss: 22094.197\n",
      "[380,    10] loss: 22279.934\n",
      "[380,    15] loss: 33365.406\n",
      "[380,    20] loss: 25859.883\n",
      "[380,    25] loss: 25159.826\n",
      "[380,    30] loss: 22893.393\n",
      "[380,    35] loss: 21108.724\n",
      "[380,    40] loss: 28298.171\n",
      "[380,    45] loss: 20979.571\n",
      "[380,    50] loss: 26162.389\n",
      "[380,    55] loss: 31649.477\n",
      "[380,    60] loss: 23868.823\n",
      "[380,    65] loss: 21749.327\n",
      "[380,    70] loss: 31476.661\n",
      "[380,    75] loss: 28428.779\n",
      "[380,    80] loss: 18479.260\n",
      "[380,    85] loss: 27488.994\n",
      "[380,    90] loss: 24363.921\n",
      "[380,    95] loss: 30701.929\n",
      "[380,   100] loss: 22565.012\n",
      "[380,   105] loss: 16912.703\n",
      "[380,   110] loss: 37622.010\n",
      "[380,   115] loss: 21957.335\n",
      "[380,   120] loss: 23255.614\n",
      "[380,   125] loss: 23734.075\n",
      "[380,   130] loss: 27308.823\n",
      "[380,   135] loss: 20153.793\n",
      "[380,   140] loss: 24555.934\n",
      "[380,   145] loss: 23720.494\n",
      "[380,   150] loss: 23057.937\n",
      "[380,   155] loss: 20074.778\n",
      "[380,   160] loss: 14040.326\n",
      "[380,   165] loss: 14026.380\n",
      "[380,   170] loss: 36615.430\n",
      "[380,   175] loss: 18109.020\n",
      "[380,   180] loss: 15034.261\n",
      "[380,   185] loss: 22997.191\n",
      "[380,   190] loss: 26444.116\n",
      "[380,   195] loss: 21465.102\n",
      "[380,   200] loss: 24370.356\n",
      "[380,   205] loss: 42241.050\n",
      "[380,   210] loss: 29320.184\n",
      "[380,   215] loss: 27538.418\n",
      "[380,   220] loss: 56660.868\n",
      "[380,   225] loss: 33922.259\n",
      "[380,   230] loss: 24979.779\n",
      "[381,     5] loss: 23447.644\n",
      "[381,    10] loss: 28720.114\n",
      "[381,    15] loss: 27665.983\n",
      "[381,    20] loss: 22327.587\n",
      "[381,    25] loss: 25473.540\n",
      "[381,    30] loss: 22099.455\n",
      "[381,    35] loss: 17666.806\n",
      "[381,    40] loss: 29693.861\n",
      "[381,    45] loss: 20566.089\n",
      "[381,    50] loss: 31008.941\n",
      "[381,    55] loss: 27021.156\n",
      "[381,    60] loss: 23150.064\n",
      "[381,    65] loss: 20672.036\n",
      "[381,    70] loss: 27232.652\n",
      "[381,    75] loss: 21867.879\n",
      "[381,    80] loss: 23955.654\n",
      "[381,    85] loss: 44531.002\n",
      "[381,    90] loss: 23118.587\n",
      "[381,    95] loss: 16290.723\n",
      "[381,   100] loss: 20589.950\n",
      "[381,   105] loss: 25751.183\n",
      "[381,   110] loss: 23670.270\n",
      "[381,   115] loss: 24030.804\n",
      "[381,   120] loss: 30826.247\n",
      "[381,   125] loss: 32034.201\n",
      "[381,   130] loss: 24042.541\n",
      "[381,   135] loss: 41050.039\n",
      "[381,   140] loss: 18141.288\n",
      "[381,   145] loss: 23355.481\n",
      "[381,   150] loss: 25963.787\n",
      "[381,   155] loss: 30156.545\n",
      "[381,   160] loss: 27503.537\n",
      "[381,   165] loss: 40680.320\n",
      "[381,   170] loss: 20712.027\n",
      "[381,   175] loss: 36811.450\n",
      "[381,   180] loss: 20759.228\n",
      "[381,   185] loss: 23703.094\n",
      "[381,   190] loss: 22861.891\n",
      "[381,   195] loss: 24350.438\n",
      "[381,   200] loss: 25096.123\n",
      "[381,   205] loss: 18080.302\n",
      "[381,   210] loss: 16974.327\n",
      "[381,   215] loss: 25291.192\n",
      "[381,   220] loss: 29139.041\n",
      "[381,   225] loss: 15973.604\n",
      "[381,   230] loss: 35890.461\n",
      "[382,     5] loss: 30098.433\n",
      "[382,    10] loss: 19451.776\n",
      "[382,    15] loss: 22512.842\n",
      "[382,    20] loss: 28724.985\n",
      "[382,    25] loss: 20673.198\n",
      "[382,    30] loss: 33222.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[382,    35] loss: 28371.284\n",
      "[382,    40] loss: 33991.964\n",
      "[382,    45] loss: 22661.902\n",
      "[382,    50] loss: 28612.795\n",
      "[382,    55] loss: 28175.363\n",
      "[382,    60] loss: 26511.236\n",
      "[382,    65] loss: 23694.345\n",
      "[382,    70] loss: 21737.054\n",
      "[382,    75] loss: 18885.763\n",
      "[382,    80] loss: 25923.384\n",
      "[382,    85] loss: 23966.349\n",
      "[382,    90] loss: 25600.987\n",
      "[382,    95] loss: 26373.875\n",
      "[382,   100] loss: 19628.422\n",
      "[382,   105] loss: 19281.191\n",
      "[382,   110] loss: 24127.200\n",
      "[382,   115] loss: 28862.050\n",
      "[382,   120] loss: 29851.857\n",
      "[382,   125] loss: 25797.670\n",
      "[382,   130] loss: 24349.672\n",
      "[382,   135] loss: 13689.229\n",
      "[382,   140] loss: 25128.809\n",
      "[382,   145] loss: 29394.607\n",
      "[382,   150] loss: 27697.593\n",
      "[382,   155] loss: 51056.068\n",
      "[382,   160] loss: 22903.681\n",
      "[382,   165] loss: 21276.367\n",
      "[382,   170] loss: 30101.214\n",
      "[382,   175] loss: 23823.950\n",
      "[382,   180] loss: 22949.954\n",
      "[382,   185] loss: 27360.396\n",
      "[382,   190] loss: 16588.594\n",
      "[382,   195] loss: 18169.432\n",
      "[382,   200] loss: 15146.881\n",
      "[382,   205] loss: 18740.846\n",
      "[382,   210] loss: 32965.885\n",
      "[382,   215] loss: 23694.016\n",
      "[382,   220] loss: 31205.695\n",
      "[382,   225] loss: 30533.883\n",
      "[382,   230] loss: 41612.431\n",
      "[383,     5] loss: 21799.855\n",
      "[383,    10] loss: 21267.738\n",
      "[383,    15] loss: 55610.771\n",
      "[383,    20] loss: 36339.087\n",
      "[383,    25] loss: 26171.273\n",
      "[383,    30] loss: 23104.098\n",
      "[383,    35] loss: 20084.291\n",
      "[383,    40] loss: 23748.623\n",
      "[383,    45] loss: 19599.026\n",
      "[383,    50] loss: 27388.609\n",
      "[383,    55] loss: 19231.686\n",
      "[383,    60] loss: 23449.152\n",
      "[383,    65] loss: 21250.094\n",
      "[383,    70] loss: 29988.716\n",
      "[383,    75] loss: 19847.246\n",
      "[383,    80] loss: 24505.180\n",
      "[383,    85] loss: 16773.428\n",
      "[383,    90] loss: 26315.820\n",
      "[383,    95] loss: 27811.245\n",
      "[383,   100] loss: 42239.467\n",
      "[383,   105] loss: 24771.993\n",
      "[383,   110] loss: 37205.512\n",
      "[383,   115] loss: 31199.575\n",
      "[383,   120] loss: 17792.747\n",
      "[383,   125] loss: 21821.707\n",
      "[383,   130] loss: 35243.809\n",
      "[383,   135] loss: 27062.513\n",
      "[383,   140] loss: 20317.853\n",
      "[383,   145] loss: 20563.931\n",
      "[383,   150] loss: 31770.219\n",
      "[383,   155] loss: 29911.389\n",
      "[383,   160] loss: 29916.663\n",
      "[383,   165] loss: 19991.619\n",
      "[383,   170] loss: 18859.720\n",
      "[383,   175] loss: 18217.306\n",
      "[383,   180] loss: 21762.141\n",
      "[383,   185] loss: 29650.018\n",
      "[383,   190] loss: 20073.488\n",
      "[383,   195] loss: 28957.678\n",
      "[383,   200] loss: 19760.955\n",
      "[383,   205] loss: 25773.682\n",
      "[383,   210] loss: 20886.112\n",
      "[383,   215] loss: 32183.777\n",
      "[383,   220] loss: 25984.316\n",
      "[383,   225] loss: 20756.641\n",
      "[383,   230] loss: 26263.521\n",
      "[384,     5] loss: 19870.478\n",
      "[384,    10] loss: 38276.138\n",
      "[384,    15] loss: 25385.326\n",
      "[384,    20] loss: 24373.724\n",
      "[384,    25] loss: 27244.004\n",
      "[384,    30] loss: 21526.629\n",
      "[384,    35] loss: 29042.891\n",
      "[384,    40] loss: 44151.325\n",
      "[384,    45] loss: 20526.275\n",
      "[384,    50] loss: 24995.947\n",
      "[384,    55] loss: 23248.365\n",
      "[384,    60] loss: 25434.685\n",
      "[384,    65] loss: 25191.511\n",
      "[384,    70] loss: 30232.541\n",
      "[384,    75] loss: 13453.016\n",
      "[384,    80] loss: 27543.698\n",
      "[384,    85] loss: 21142.957\n",
      "[384,    90] loss: 21251.906\n",
      "[384,    95] loss: 20215.986\n",
      "[384,   100] loss: 29484.847\n",
      "[384,   105] loss: 24492.130\n",
      "[384,   110] loss: 19153.259\n",
      "[384,   115] loss: 25199.420\n",
      "[384,   120] loss: 19959.482\n",
      "[384,   125] loss: 23827.008\n",
      "[384,   130] loss: 28557.684\n",
      "[384,   135] loss: 18270.954\n",
      "[384,   140] loss: 30010.438\n",
      "[384,   145] loss: 14101.628\n",
      "[384,   150] loss: 26956.581\n",
      "[384,   155] loss: 20098.013\n",
      "[384,   160] loss: 13492.844\n",
      "[384,   165] loss: 35285.879\n",
      "[384,   170] loss: 34974.668\n",
      "[384,   175] loss: 32033.508\n",
      "[384,   180] loss: 28577.460\n",
      "[384,   185] loss: 19986.525\n",
      "[384,   190] loss: 33215.947\n",
      "[384,   195] loss: 23736.371\n",
      "[384,   200] loss: 19628.761\n",
      "[384,   205] loss: 25743.688\n",
      "[384,   210] loss: 53277.247\n",
      "[384,   215] loss: 26082.438\n",
      "[384,   220] loss: 29243.692\n",
      "[384,   225] loss: 17883.976\n",
      "[384,   230] loss: 27643.418\n",
      "[385,     5] loss: 28329.393\n",
      "[385,    10] loss: 23869.592\n",
      "[385,    15] loss: 26274.916\n",
      "[385,    20] loss: 25547.664\n",
      "[385,    25] loss: 23452.450\n",
      "[385,    30] loss: 42307.235\n",
      "[385,    35] loss: 23004.975\n",
      "[385,    40] loss: 32279.047\n",
      "[385,    45] loss: 31712.959\n",
      "[385,    50] loss: 30020.912\n",
      "[385,    55] loss: 25126.263\n",
      "[385,    60] loss: 13716.077\n",
      "[385,    65] loss: 41334.389\n",
      "[385,    70] loss: 18342.849\n",
      "[385,    75] loss: 21671.806\n",
      "[385,    80] loss: 23517.001\n",
      "[385,    85] loss: 21689.333\n",
      "[385,    90] loss: 45862.829\n",
      "[385,    95] loss: 27603.720\n",
      "[385,   100] loss: 22841.274\n",
      "[385,   105] loss: 28850.440\n",
      "[385,   110] loss: 30775.292\n",
      "[385,   115] loss: 17687.759\n",
      "[385,   120] loss: 25695.946\n",
      "[385,   125] loss: 33234.809\n",
      "[385,   130] loss: 22940.626\n",
      "[385,   135] loss: 19192.795\n",
      "[385,   140] loss: 17323.492\n",
      "[385,   145] loss: 16847.930\n",
      "[385,   150] loss: 16622.943\n",
      "[385,   155] loss: 20325.448\n",
      "[385,   160] loss: 15913.690\n",
      "[385,   165] loss: 32271.590\n",
      "[385,   170] loss: 16080.994\n",
      "[385,   175] loss: 25000.372\n",
      "[385,   180] loss: 28369.660\n",
      "[385,   185] loss: 24455.425\n",
      "[385,   190] loss: 21290.397\n",
      "[385,   195] loss: 20809.558\n",
      "[385,   200] loss: 30437.210\n",
      "[385,   205] loss: 27605.934\n",
      "[385,   210] loss: 34452.106\n",
      "[385,   215] loss: 37010.175\n",
      "[385,   220] loss: 20641.223\n",
      "[385,   225] loss: 19367.766\n",
      "[385,   230] loss: 24436.260\n",
      "[386,     5] loss: 18893.561\n",
      "[386,    10] loss: 27747.029\n",
      "[386,    15] loss: 24780.071\n",
      "[386,    20] loss: 29838.233\n",
      "[386,    25] loss: 17269.398\n",
      "[386,    30] loss: 27121.138\n",
      "[386,    35] loss: 16401.737\n",
      "[386,    40] loss: 20069.910\n",
      "[386,    45] loss: 20840.669\n",
      "[386,    50] loss: 22623.138\n",
      "[386,    55] loss: 51170.014\n",
      "[386,    60] loss: 21925.613\n",
      "[386,    65] loss: 27882.922\n",
      "[386,    70] loss: 23601.296\n",
      "[386,    75] loss: 21493.683\n",
      "[386,    80] loss: 38701.869\n",
      "[386,    85] loss: 32075.895\n",
      "[386,    90] loss: 22399.088\n",
      "[386,    95] loss: 27919.188\n",
      "[386,   100] loss: 31866.164\n",
      "[386,   105] loss: 28241.938\n",
      "[386,   110] loss: 15112.778\n",
      "[386,   115] loss: 25639.122\n",
      "[386,   120] loss: 23835.692\n",
      "[386,   125] loss: 19152.214\n",
      "[386,   130] loss: 32631.820\n",
      "[386,   135] loss: 21645.233\n",
      "[386,   140] loss: 27169.589\n",
      "[386,   145] loss: 19536.870\n",
      "[386,   150] loss: 20219.937\n",
      "[386,   155] loss: 19948.998\n",
      "[386,   160] loss: 23099.777\n",
      "[386,   165] loss: 32900.164\n",
      "[386,   170] loss: 31051.880\n",
      "[386,   175] loss: 43380.697\n",
      "[386,   180] loss: 27892.164\n",
      "[386,   185] loss: 21824.975\n",
      "[386,   190] loss: 28333.423\n",
      "[386,   195] loss: 16955.475\n",
      "[386,   200] loss: 24039.603\n",
      "[386,   205] loss: 33462.558\n",
      "[386,   210] loss: 21838.534\n",
      "[386,   215] loss: 17425.821\n",
      "[386,   220] loss: 26624.531\n",
      "[386,   225] loss: 17190.930\n",
      "[386,   230] loss: 15823.451\n",
      "[387,     5] loss: 17480.895\n",
      "[387,    10] loss: 23392.224\n",
      "[387,    15] loss: 19323.856\n",
      "[387,    20] loss: 20184.269\n",
      "[387,    25] loss: 38235.878\n",
      "[387,    30] loss: 28015.365\n",
      "[387,    35] loss: 14934.333\n",
      "[387,    40] loss: 37159.384\n",
      "[387,    45] loss: 25614.039\n",
      "[387,    50] loss: 26702.748\n",
      "[387,    55] loss: 27680.492\n",
      "[387,    60] loss: 18776.219\n",
      "[387,    65] loss: 26952.297\n",
      "[387,    70] loss: 16924.640\n",
      "[387,    75] loss: 30070.526\n",
      "[387,    80] loss: 22327.008\n",
      "[387,    85] loss: 21780.027\n",
      "[387,    90] loss: 19460.424\n",
      "[387,    95] loss: 29713.681\n",
      "[387,   100] loss: 33540.363\n",
      "[387,   105] loss: 29840.201\n",
      "[387,   110] loss: 24265.115\n",
      "[387,   115] loss: 19911.356\n",
      "[387,   120] loss: 23511.345\n",
      "[387,   125] loss: 19806.952\n",
      "[387,   130] loss: 28724.473\n",
      "[387,   135] loss: 31942.389\n",
      "[387,   140] loss: 27417.258\n",
      "[387,   145] loss: 14221.140\n",
      "[387,   150] loss: 46721.101\n",
      "[387,   155] loss: 23227.001\n",
      "[387,   160] loss: 21190.510\n",
      "[387,   165] loss: 19597.849\n",
      "[387,   170] loss: 33855.607\n",
      "[387,   175] loss: 23687.040\n",
      "[387,   180] loss: 16034.724\n",
      "[387,   185] loss: 22718.271\n",
      "[387,   190] loss: 28694.285\n",
      "[387,   195] loss: 18654.577\n",
      "[387,   200] loss: 29321.346\n",
      "[387,   205] loss: 30112.938\n",
      "[387,   210] loss: 46622.487\n",
      "[387,   215] loss: 19696.520\n",
      "[387,   220] loss: 27193.775\n",
      "[387,   225] loss: 22770.612\n",
      "[387,   230] loss: 31776.942\n",
      "[388,     5] loss: 15814.685\n",
      "[388,    10] loss: 30433.325\n",
      "[388,    15] loss: 22558.013\n",
      "[388,    20] loss: 28444.682\n",
      "[388,    25] loss: 22479.937\n",
      "[388,    30] loss: 14759.006\n",
      "[388,    35] loss: 24636.531\n",
      "[388,    40] loss: 35908.618\n",
      "[388,    45] loss: 28840.605\n",
      "[388,    50] loss: 18125.655\n",
      "[388,    55] loss: 17333.199\n",
      "[388,    60] loss: 20806.900\n",
      "[388,    65] loss: 26947.849\n",
      "[388,    70] loss: 22586.584\n",
      "[388,    75] loss: 20733.605\n",
      "[388,    80] loss: 26556.828\n",
      "[388,    85] loss: 23673.696\n",
      "[388,    90] loss: 21017.818\n",
      "[388,    95] loss: 15581.238\n",
      "[388,   100] loss: 41765.544\n",
      "[388,   105] loss: 24638.679\n",
      "[388,   110] loss: 19417.338\n",
      "[388,   115] loss: 23688.113\n",
      "[388,   120] loss: 38055.713\n",
      "[388,   125] loss: 27150.400\n",
      "[388,   130] loss: 25419.293\n",
      "[388,   135] loss: 22867.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[388,   140] loss: 22467.217\n",
      "[388,   145] loss: 36067.203\n",
      "[388,   150] loss: 34317.292\n",
      "[388,   155] loss: 27913.846\n",
      "[388,   160] loss: 20823.696\n",
      "[388,   165] loss: 37718.852\n",
      "[388,   170] loss: 20703.202\n",
      "[388,   175] loss: 21003.890\n",
      "[388,   180] loss: 27599.370\n",
      "[388,   185] loss: 18738.860\n",
      "[388,   190] loss: 19707.029\n",
      "[388,   195] loss: 27804.295\n",
      "[388,   200] loss: 28888.966\n",
      "[388,   205] loss: 27695.735\n",
      "[388,   210] loss: 30208.185\n",
      "[388,   215] loss: 26010.746\n",
      "[388,   220] loss: 36454.069\n",
      "[388,   225] loss: 24990.685\n",
      "[388,   230] loss: 22929.348\n",
      "[389,     5] loss: 28613.162\n",
      "[389,    10] loss: 24423.131\n",
      "[389,    15] loss: 14785.405\n",
      "[389,    20] loss: 34261.175\n",
      "[389,    25] loss: 17876.476\n",
      "[389,    30] loss: 23901.104\n",
      "[389,    35] loss: 28999.672\n",
      "[389,    40] loss: 26060.661\n",
      "[389,    45] loss: 22499.612\n",
      "[389,    50] loss: 27916.653\n",
      "[389,    55] loss: 15901.635\n",
      "[389,    60] loss: 27147.755\n",
      "[389,    65] loss: 29807.682\n",
      "[389,    70] loss: 30516.381\n",
      "[389,    75] loss: 23114.153\n",
      "[389,    80] loss: 34995.160\n",
      "[389,    85] loss: 23711.081\n",
      "[389,    90] loss: 22666.198\n",
      "[389,    95] loss: 17857.794\n",
      "[389,   100] loss: 26508.765\n",
      "[389,   105] loss: 16466.147\n",
      "[389,   110] loss: 16470.183\n",
      "[389,   115] loss: 16653.765\n",
      "[389,   120] loss: 41100.285\n",
      "[389,   125] loss: 29277.126\n",
      "[389,   130] loss: 20964.732\n",
      "[389,   135] loss: 12201.024\n",
      "[389,   140] loss: 29844.491\n",
      "[389,   145] loss: 21814.801\n",
      "[389,   150] loss: 16466.299\n",
      "[389,   155] loss: 35620.851\n",
      "[389,   160] loss: 17758.154\n",
      "[389,   165] loss: 25505.493\n",
      "[389,   170] loss: 33615.202\n",
      "[389,   175] loss: 21986.156\n",
      "[389,   180] loss: 18977.070\n",
      "[389,   185] loss: 51613.057\n",
      "[389,   190] loss: 38181.345\n",
      "[389,   195] loss: 20753.911\n",
      "[389,   200] loss: 23873.846\n",
      "[389,   205] loss: 45335.117\n",
      "[389,   210] loss: 24126.101\n",
      "[389,   215] loss: 20100.056\n",
      "[389,   220] loss: 32557.627\n",
      "[389,   225] loss: 25753.571\n",
      "[389,   230] loss: 15646.987\n",
      "[390,     5] loss: 34276.122\n",
      "[390,    10] loss: 28923.105\n",
      "[390,    15] loss: 24077.216\n",
      "[390,    20] loss: 32666.343\n",
      "[390,    25] loss: 16733.813\n",
      "[390,    30] loss: 25658.296\n",
      "[390,    35] loss: 21577.375\n",
      "[390,    40] loss: 24012.725\n",
      "[390,    45] loss: 26363.884\n",
      "[390,    50] loss: 22518.497\n",
      "[390,    55] loss: 33669.243\n",
      "[390,    60] loss: 17739.860\n",
      "[390,    65] loss: 20714.064\n",
      "[390,    70] loss: 38822.479\n",
      "[390,    75] loss: 22274.030\n",
      "[390,    80] loss: 18940.751\n",
      "[390,    85] loss: 23047.157\n",
      "[390,    90] loss: 23135.629\n",
      "[390,    95] loss: 17266.611\n",
      "[390,   100] loss: 29272.773\n",
      "[390,   105] loss: 24322.717\n",
      "[390,   110] loss: 22191.298\n",
      "[390,   115] loss: 19970.151\n",
      "[390,   120] loss: 21397.847\n",
      "[390,   125] loss: 22083.276\n",
      "[390,   130] loss: 34980.276\n",
      "[390,   135] loss: 47417.677\n",
      "[390,   140] loss: 30404.144\n",
      "[390,   145] loss: 29287.574\n",
      "[390,   150] loss: 22175.986\n",
      "[390,   155] loss: 20424.471\n",
      "[390,   160] loss: 27789.173\n",
      "[390,   165] loss: 22172.945\n",
      "[390,   170] loss: 28355.302\n",
      "[390,   175] loss: 17097.614\n",
      "[390,   180] loss: 31249.624\n",
      "[390,   185] loss: 21284.313\n",
      "[390,   190] loss: 26685.197\n",
      "[390,   195] loss: 23118.261\n",
      "[390,   200] loss: 25431.284\n",
      "[390,   205] loss: 33120.726\n",
      "[390,   210] loss: 18513.707\n",
      "[390,   215] loss: 20377.597\n",
      "[390,   220] loss: 29277.369\n",
      "[390,   225] loss: 25051.357\n",
      "[390,   230] loss: 26335.398\n",
      "[391,     5] loss: 23896.673\n",
      "[391,    10] loss: 27459.934\n",
      "[391,    15] loss: 22637.890\n",
      "[391,    20] loss: 21583.665\n",
      "[391,    25] loss: 25473.508\n",
      "[391,    30] loss: 20957.253\n",
      "[391,    35] loss: 22148.408\n",
      "[391,    40] loss: 21053.348\n",
      "[391,    45] loss: 25584.880\n",
      "[391,    50] loss: 18753.926\n",
      "[391,    55] loss: 28728.596\n",
      "[391,    60] loss: 34669.392\n",
      "[391,    65] loss: 42663.671\n",
      "[391,    70] loss: 20398.842\n",
      "[391,    75] loss: 23951.394\n",
      "[391,    80] loss: 31827.243\n",
      "[391,    85] loss: 25911.974\n",
      "[391,    90] loss: 21355.540\n",
      "[391,    95] loss: 25284.489\n",
      "[391,   100] loss: 33134.086\n",
      "[391,   105] loss: 24696.839\n",
      "[391,   110] loss: 19230.437\n",
      "[391,   115] loss: 38623.680\n",
      "[391,   120] loss: 21307.582\n",
      "[391,   125] loss: 23701.883\n",
      "[391,   130] loss: 22948.363\n",
      "[391,   135] loss: 31274.147\n",
      "[391,   140] loss: 48016.949\n",
      "[391,   145] loss: 21447.804\n",
      "[391,   150] loss: 25913.973\n",
      "[391,   155] loss: 23053.232\n",
      "[391,   160] loss: 22407.831\n",
      "[391,   165] loss: 21538.614\n",
      "[391,   170] loss: 19928.989\n",
      "[391,   175] loss: 23548.436\n",
      "[391,   180] loss: 17594.359\n",
      "[391,   185] loss: 21185.769\n",
      "[391,   190] loss: 27056.024\n",
      "[391,   195] loss: 20861.194\n",
      "[391,   200] loss: 31409.419\n",
      "[391,   205] loss: 29464.186\n",
      "[391,   210] loss: 21660.959\n",
      "[391,   215] loss: 19175.547\n",
      "[391,   220] loss: 29992.846\n",
      "[391,   225] loss: 27069.770\n",
      "[391,   230] loss: 26017.434\n",
      "[392,     5] loss: 17417.935\n",
      "[392,    10] loss: 26158.441\n",
      "[392,    15] loss: 19646.811\n",
      "[392,    20] loss: 21103.008\n",
      "[392,    25] loss: 23209.285\n",
      "[392,    30] loss: 34926.408\n",
      "[392,    35] loss: 20343.122\n",
      "[392,    40] loss: 32103.532\n",
      "[392,    45] loss: 21790.421\n",
      "[392,    50] loss: 24235.613\n",
      "[392,    55] loss: 29013.428\n",
      "[392,    60] loss: 32954.853\n",
      "[392,    65] loss: 16979.422\n",
      "[392,    70] loss: 19426.788\n",
      "[392,    75] loss: 18312.199\n",
      "[392,    80] loss: 22190.939\n",
      "[392,    85] loss: 31094.773\n",
      "[392,    90] loss: 45232.767\n",
      "[392,    95] loss: 41723.029\n",
      "[392,   100] loss: 26382.268\n",
      "[392,   105] loss: 22709.646\n",
      "[392,   110] loss: 15872.841\n",
      "[392,   115] loss: 20573.123\n",
      "[392,   120] loss: 27354.933\n",
      "[392,   125] loss: 37317.869\n",
      "[392,   130] loss: 21889.106\n",
      "[392,   135] loss: 27350.616\n",
      "[392,   140] loss: 25762.479\n",
      "[392,   145] loss: 28922.945\n",
      "[392,   150] loss: 31444.121\n",
      "[392,   155] loss: 21495.010\n",
      "[392,   160] loss: 17137.823\n",
      "[392,   165] loss: 25673.522\n",
      "[392,   170] loss: 23086.377\n",
      "[392,   175] loss: 27043.824\n",
      "[392,   180] loss: 15271.372\n",
      "[392,   185] loss: 17447.450\n",
      "[392,   190] loss: 22803.343\n",
      "[392,   195] loss: 33996.621\n",
      "[392,   200] loss: 24787.768\n",
      "[392,   205] loss: 29429.685\n",
      "[392,   210] loss: 24011.615\n",
      "[392,   215] loss: 34002.535\n",
      "[392,   220] loss: 27486.714\n",
      "[392,   225] loss: 23466.082\n",
      "[392,   230] loss: 30774.203\n",
      "[393,     5] loss: 27399.464\n",
      "[393,    10] loss: 25599.004\n",
      "[393,    15] loss: 24368.719\n",
      "[393,    20] loss: 22227.174\n",
      "[393,    25] loss: 23994.945\n",
      "[393,    30] loss: 17523.226\n",
      "[393,    35] loss: 23589.052\n",
      "[393,    40] loss: 42068.548\n",
      "[393,    45] loss: 18492.127\n",
      "[393,    50] loss: 26448.311\n",
      "[393,    55] loss: 28780.603\n",
      "[393,    60] loss: 29880.684\n",
      "[393,    65] loss: 26370.412\n",
      "[393,    70] loss: 43088.842\n",
      "[393,    75] loss: 22176.060\n",
      "[393,    80] loss: 13531.575\n",
      "[393,    85] loss: 22646.208\n",
      "[393,    90] loss: 22189.787\n",
      "[393,    95] loss: 35383.084\n",
      "[393,   100] loss: 17342.256\n",
      "[393,   105] loss: 17397.111\n",
      "[393,   110] loss: 20267.647\n",
      "[393,   115] loss: 18951.157\n",
      "[393,   120] loss: 21322.722\n",
      "[393,   125] loss: 41510.910\n",
      "[393,   130] loss: 16592.301\n",
      "[393,   135] loss: 23298.614\n",
      "[393,   140] loss: 22390.151\n",
      "[393,   145] loss: 23445.827\n",
      "[393,   150] loss: 26174.363\n",
      "[393,   155] loss: 22048.893\n",
      "[393,   160] loss: 32923.909\n",
      "[393,   165] loss: 28463.007\n",
      "[393,   170] loss: 21102.502\n",
      "[393,   175] loss: 26442.797\n",
      "[393,   180] loss: 30303.170\n",
      "[393,   185] loss: 27930.591\n",
      "[393,   190] loss: 21431.964\n",
      "[393,   195] loss: 50448.839\n",
      "[393,   200] loss: 25838.835\n",
      "[393,   205] loss: 19012.263\n",
      "[393,   210] loss: 26460.881\n",
      "[393,   215] loss: 24639.910\n",
      "[393,   220] loss: 29884.702\n",
      "[393,   225] loss: 15883.054\n",
      "[393,   230] loss: 27092.091\n",
      "[394,     5] loss: 45848.615\n",
      "[394,    10] loss: 22713.396\n",
      "[394,    15] loss: 26475.413\n",
      "[394,    20] loss: 35445.259\n",
      "[394,    25] loss: 25045.644\n",
      "[394,    30] loss: 25612.378\n",
      "[394,    35] loss: 27243.369\n",
      "[394,    40] loss: 22081.962\n",
      "[394,    45] loss: 28416.913\n",
      "[394,    50] loss: 19635.287\n",
      "[394,    55] loss: 32899.305\n",
      "[394,    60] loss: 27267.884\n",
      "[394,    65] loss: 26124.496\n",
      "[394,    70] loss: 27484.569\n",
      "[394,    75] loss: 28964.797\n",
      "[394,    80] loss: 20457.142\n",
      "[394,    85] loss: 23981.603\n",
      "[394,    90] loss: 27603.531\n",
      "[394,    95] loss: 35029.981\n",
      "[394,   100] loss: 22310.739\n",
      "[394,   105] loss: 20017.480\n",
      "[394,   110] loss: 16854.946\n",
      "[394,   115] loss: 23731.978\n",
      "[394,   120] loss: 28659.311\n",
      "[394,   125] loss: 26934.963\n",
      "[394,   130] loss: 24949.874\n",
      "[394,   135] loss: 23446.782\n",
      "[394,   140] loss: 24598.865\n",
      "[394,   145] loss: 37424.842\n",
      "[394,   150] loss: 17275.695\n",
      "[394,   155] loss: 25497.431\n",
      "[394,   160] loss: 23940.542\n",
      "[394,   165] loss: 18178.944\n",
      "[394,   170] loss: 32689.385\n",
      "[394,   175] loss: 27904.218\n",
      "[394,   180] loss: 27085.848\n",
      "[394,   185] loss: 20315.947\n",
      "[394,   190] loss: 25883.815\n",
      "[394,   195] loss: 23035.110\n",
      "[394,   200] loss: 18114.815\n",
      "[394,   205] loss: 23363.277\n",
      "[394,   210] loss: 21633.036\n",
      "[394,   215] loss: 31880.939\n",
      "[394,   220] loss: 23928.665\n",
      "[394,   225] loss: 24079.027\n",
      "[394,   230] loss: 15359.590\n",
      "[395,     5] loss: 17513.235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[395,    10] loss: 21399.842\n",
      "[395,    15] loss: 28067.029\n",
      "[395,    20] loss: 29551.152\n",
      "[395,    25] loss: 20766.869\n",
      "[395,    30] loss: 27241.332\n",
      "[395,    35] loss: 28809.021\n",
      "[395,    40] loss: 22943.895\n",
      "[395,    45] loss: 29506.221\n",
      "[395,    50] loss: 28100.081\n",
      "[395,    55] loss: 23393.407\n",
      "[395,    60] loss: 20262.782\n",
      "[395,    65] loss: 23158.333\n",
      "[395,    70] loss: 27671.020\n",
      "[395,    75] loss: 29689.497\n",
      "[395,    80] loss: 47698.275\n",
      "[395,    85] loss: 26899.459\n",
      "[395,    90] loss: 21189.022\n",
      "[395,    95] loss: 22410.962\n",
      "[395,   100] loss: 24328.273\n",
      "[395,   105] loss: 31001.864\n",
      "[395,   110] loss: 23829.712\n",
      "[395,   115] loss: 21256.386\n",
      "[395,   120] loss: 30533.517\n",
      "[395,   125] loss: 27386.112\n",
      "[395,   130] loss: 34002.410\n",
      "[395,   135] loss: 23942.207\n",
      "[395,   140] loss: 21835.754\n",
      "[395,   145] loss: 18348.092\n",
      "[395,   150] loss: 21834.015\n",
      "[395,   155] loss: 21906.760\n",
      "[395,   160] loss: 16385.279\n",
      "[395,   165] loss: 26676.345\n",
      "[395,   170] loss: 28146.941\n",
      "[395,   175] loss: 27814.601\n",
      "[395,   180] loss: 25979.124\n",
      "[395,   185] loss: 23548.601\n",
      "[395,   190] loss: 22517.378\n",
      "[395,   195] loss: 22363.841\n",
      "[395,   200] loss: 36347.970\n",
      "[395,   205] loss: 28168.427\n",
      "[395,   210] loss: 21120.844\n",
      "[395,   215] loss: 36393.430\n",
      "[395,   220] loss: 24590.214\n",
      "[395,   225] loss: 22622.036\n",
      "[395,   230] loss: 18811.717\n",
      "[396,     5] loss: 31023.872\n",
      "[396,    10] loss: 22032.617\n",
      "[396,    15] loss: 21025.612\n",
      "[396,    20] loss: 21143.059\n",
      "[396,    25] loss: 28022.664\n",
      "[396,    30] loss: 16003.074\n",
      "[396,    35] loss: 31706.106\n",
      "[396,    40] loss: 21568.851\n",
      "[396,    45] loss: 36927.980\n",
      "[396,    50] loss: 23052.560\n",
      "[396,    55] loss: 52046.560\n",
      "[396,    60] loss: 15346.392\n",
      "[396,    65] loss: 34471.702\n",
      "[396,    70] loss: 35476.748\n",
      "[396,    75] loss: 27910.686\n",
      "[396,    80] loss: 18802.320\n",
      "[396,    85] loss: 16479.946\n",
      "[396,    90] loss: 28914.949\n",
      "[396,    95] loss: 22333.504\n",
      "[396,   100] loss: 21975.140\n",
      "[396,   105] loss: 28004.861\n",
      "[396,   110] loss: 23002.455\n",
      "[396,   115] loss: 20460.672\n",
      "[396,   120] loss: 19116.525\n",
      "[396,   125] loss: 22304.357\n",
      "[396,   130] loss: 28971.328\n",
      "[396,   135] loss: 21997.057\n",
      "[396,   140] loss: 19639.466\n",
      "[396,   145] loss: 28252.454\n",
      "[396,   150] loss: 37055.018\n",
      "[396,   155] loss: 25274.040\n",
      "[396,   160] loss: 22672.981\n",
      "[396,   165] loss: 24196.545\n",
      "[396,   170] loss: 27462.758\n",
      "[396,   175] loss: 24848.242\n",
      "[396,   180] loss: 30300.921\n",
      "[396,   185] loss: 34294.106\n",
      "[396,   190] loss: 20476.446\n",
      "[396,   195] loss: 22386.734\n",
      "[396,   200] loss: 21286.515\n",
      "[396,   205] loss: 15418.555\n",
      "[396,   210] loss: 15858.963\n",
      "[396,   215] loss: 24730.326\n",
      "[396,   220] loss: 17689.327\n",
      "[396,   225] loss: 29188.591\n",
      "[396,   230] loss: 37769.936\n",
      "[397,     5] loss: 27753.410\n",
      "[397,    10] loss: 30971.133\n",
      "[397,    15] loss: 19322.059\n",
      "[397,    20] loss: 17267.821\n",
      "[397,    25] loss: 21196.730\n",
      "[397,    30] loss: 15947.250\n",
      "[397,    35] loss: 34919.805\n",
      "[397,    40] loss: 26251.571\n",
      "[397,    45] loss: 22512.145\n",
      "[397,    50] loss: 16087.286\n",
      "[397,    55] loss: 26446.191\n",
      "[397,    60] loss: 36830.777\n",
      "[397,    65] loss: 19376.500\n",
      "[397,    70] loss: 30021.970\n",
      "[397,    75] loss: 40491.331\n",
      "[397,    80] loss: 23468.211\n",
      "[397,    85] loss: 29454.826\n",
      "[397,    90] loss: 28568.440\n",
      "[397,    95] loss: 19889.129\n",
      "[397,   100] loss: 18906.828\n",
      "[397,   105] loss: 28302.621\n",
      "[397,   110] loss: 21510.231\n",
      "[397,   115] loss: 26380.258\n",
      "[397,   120] loss: 24837.023\n",
      "[397,   125] loss: 24684.400\n",
      "[397,   130] loss: 18419.789\n",
      "[397,   135] loss: 28158.273\n",
      "[397,   140] loss: 50409.367\n",
      "[397,   145] loss: 38793.815\n",
      "[397,   150] loss: 18493.970\n",
      "[397,   155] loss: 23475.424\n",
      "[397,   160] loss: 20317.188\n",
      "[397,   165] loss: 26109.003\n",
      "[397,   170] loss: 28864.838\n",
      "[397,   175] loss: 16655.139\n",
      "[397,   180] loss: 22116.865\n",
      "[397,   185] loss: 22611.376\n",
      "[397,   190] loss: 30684.730\n",
      "[397,   195] loss: 28038.177\n",
      "[397,   200] loss: 25305.008\n",
      "[397,   205] loss: 17565.165\n",
      "[397,   210] loss: 30886.205\n",
      "[397,   215] loss: 20626.185\n",
      "[397,   220] loss: 21477.712\n",
      "[397,   225] loss: 34136.762\n",
      "[397,   230] loss: 20750.166\n",
      "[398,     5] loss: 22516.911\n",
      "[398,    10] loss: 21085.082\n",
      "[398,    15] loss: 27435.310\n",
      "[398,    20] loss: 38851.425\n",
      "[398,    25] loss: 19903.218\n",
      "[398,    30] loss: 22613.929\n",
      "[398,    35] loss: 27954.193\n",
      "[398,    40] loss: 20450.220\n",
      "[398,    45] loss: 24881.531\n",
      "[398,    50] loss: 26963.461\n",
      "[398,    55] loss: 15870.619\n",
      "[398,    60] loss: 22809.070\n",
      "[398,    65] loss: 23455.614\n",
      "[398,    70] loss: 25270.142\n",
      "[398,    75] loss: 30328.718\n",
      "[398,    80] loss: 23838.290\n",
      "[398,    85] loss: 30148.596\n",
      "[398,    90] loss: 36077.163\n",
      "[398,    95] loss: 42854.990\n",
      "[398,   100] loss: 20428.339\n",
      "[398,   105] loss: 22030.058\n",
      "[398,   110] loss: 21352.635\n",
      "[398,   115] loss: 28368.190\n",
      "[398,   120] loss: 19426.399\n",
      "[398,   125] loss: 27819.834\n",
      "[398,   130] loss: 21655.791\n",
      "[398,   135] loss: 22624.021\n",
      "[398,   140] loss: 21353.678\n",
      "[398,   145] loss: 38276.118\n",
      "[398,   150] loss: 23443.895\n",
      "[398,   155] loss: 32551.577\n",
      "[398,   160] loss: 29278.013\n",
      "[398,   165] loss: 20816.968\n",
      "[398,   170] loss: 22571.097\n",
      "[398,   175] loss: 28198.583\n",
      "[398,   180] loss: 26276.037\n",
      "[398,   185] loss: 25947.633\n",
      "[398,   190] loss: 37900.302\n",
      "[398,   195] loss: 20517.745\n",
      "[398,   200] loss: 20557.566\n",
      "[398,   205] loss: 24561.448\n",
      "[398,   210] loss: 24035.670\n",
      "[398,   215] loss: 20128.811\n",
      "[398,   220] loss: 20763.729\n",
      "[398,   225] loss: 22264.326\n",
      "[398,   230] loss: 28839.113\n",
      "[399,     5] loss: 30400.354\n",
      "[399,    10] loss: 18659.636\n",
      "[399,    15] loss: 28639.789\n",
      "[399,    20] loss: 33854.514\n",
      "[399,    25] loss: 24197.762\n",
      "[399,    30] loss: 25673.321\n",
      "[399,    35] loss: 18993.683\n",
      "[399,    40] loss: 23199.801\n",
      "[399,    45] loss: 22709.256\n",
      "[399,    50] loss: 23324.146\n",
      "[399,    55] loss: 36150.039\n",
      "[399,    60] loss: 29769.890\n",
      "[399,    65] loss: 29086.991\n",
      "[399,    70] loss: 19382.958\n",
      "[399,    75] loss: 24492.816\n",
      "[399,    80] loss: 13730.673\n",
      "[399,    85] loss: 25203.298\n",
      "[399,    90] loss: 21036.971\n",
      "[399,    95] loss: 32077.712\n",
      "[399,   100] loss: 30350.950\n",
      "[399,   105] loss: 18768.352\n",
      "[399,   110] loss: 24546.128\n",
      "[399,   115] loss: 24520.944\n",
      "[399,   120] loss: 22370.166\n",
      "[399,   125] loss: 27128.801\n",
      "[399,   130] loss: 24537.048\n",
      "[399,   135] loss: 23987.598\n",
      "[399,   140] loss: 28181.192\n",
      "[399,   145] loss: 22148.080\n",
      "[399,   150] loss: 27046.385\n",
      "[399,   155] loss: 26800.375\n",
      "[399,   160] loss: 47277.857\n",
      "[399,   165] loss: 14670.797\n",
      "[399,   170] loss: 17805.696\n",
      "[399,   175] loss: 21691.874\n",
      "[399,   180] loss: 21199.089\n",
      "[399,   185] loss: 35623.432\n",
      "[399,   190] loss: 34563.442\n",
      "[399,   195] loss: 21866.136\n",
      "[399,   200] loss: 17917.725\n",
      "[399,   205] loss: 25964.483\n",
      "[399,   210] loss: 45923.302\n",
      "[399,   215] loss: 31641.470\n",
      "[399,   220] loss: 26434.396\n",
      "[399,   225] loss: 20048.070\n",
      "[399,   230] loss: 21260.610\n",
      "[400,     5] loss: 27368.835\n",
      "[400,    10] loss: 28046.512\n",
      "[400,    15] loss: 31288.755\n",
      "[400,    20] loss: 37214.752\n",
      "[400,    25] loss: 22847.121\n",
      "[400,    30] loss: 28200.216\n",
      "[400,    35] loss: 24633.765\n",
      "[400,    40] loss: 50097.800\n",
      "[400,    45] loss: 19377.774\n",
      "[400,    50] loss: 25229.413\n",
      "[400,    55] loss: 12814.068\n",
      "[400,    60] loss: 29125.417\n",
      "[400,    65] loss: 12552.882\n",
      "[400,    70] loss: 20431.729\n",
      "[400,    75] loss: 24920.562\n",
      "[400,    80] loss: 27742.979\n",
      "[400,    85] loss: 31268.956\n",
      "[400,    90] loss: 30568.251\n",
      "[400,    95] loss: 27206.209\n",
      "[400,   100] loss: 33260.850\n",
      "[400,   105] loss: 28200.167\n",
      "[400,   110] loss: 28004.310\n",
      "[400,   115] loss: 18871.744\n",
      "[400,   120] loss: 18804.254\n",
      "[400,   125] loss: 28574.257\n",
      "[400,   130] loss: 23152.533\n",
      "[400,   135] loss: 27125.959\n",
      "[400,   140] loss: 13807.199\n",
      "[400,   145] loss: 28491.419\n",
      "[400,   150] loss: 27699.759\n",
      "[400,   155] loss: 18672.750\n",
      "[400,   160] loss: 26572.821\n",
      "[400,   165] loss: 20775.962\n",
      "[400,   170] loss: 18388.944\n",
      "[400,   175] loss: 23127.574\n",
      "[400,   180] loss: 27714.797\n",
      "[400,   185] loss: 24830.408\n",
      "[400,   190] loss: 19297.929\n",
      "[400,   195] loss: 22990.897\n",
      "[400,   200] loss: 23785.744\n",
      "[400,   205] loss: 24616.644\n",
      "[400,   210] loss: 23085.473\n",
      "[400,   215] loss: 34757.506\n",
      "[400,   220] loss: 25061.778\n",
      "[400,   225] loss: 20348.975\n",
      "[400,   230] loss: 26508.741\n",
      "[401,     5] loss: 16007.498\n",
      "[401,    10] loss: 23654.000\n",
      "[401,    15] loss: 44357.993\n",
      "[401,    20] loss: 28325.409\n",
      "[401,    25] loss: 23532.262\n",
      "[401,    30] loss: 27754.993\n",
      "[401,    35] loss: 21801.416\n",
      "[401,    40] loss: 24985.145\n",
      "[401,    45] loss: 18644.012\n",
      "[401,    50] loss: 35671.570\n",
      "[401,    55] loss: 26433.454\n",
      "[401,    60] loss: 21068.464\n",
      "[401,    65] loss: 18638.010\n",
      "[401,    70] loss: 22579.009\n",
      "[401,    75] loss: 29317.622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[401,    80] loss: 41392.800\n",
      "[401,    85] loss: 12734.434\n",
      "[401,    90] loss: 41315.121\n",
      "[401,    95] loss: 30878.909\n",
      "[401,   100] loss: 38347.398\n",
      "[401,   105] loss: 24780.696\n",
      "[401,   110] loss: 27165.350\n",
      "[401,   115] loss: 36165.535\n",
      "[401,   120] loss: 19164.821\n",
      "[401,   125] loss: 23378.353\n",
      "[401,   130] loss: 16888.591\n",
      "[401,   135] loss: 23516.667\n",
      "[401,   140] loss: 28551.762\n",
      "[401,   145] loss: 19642.966\n",
      "[401,   150] loss: 35048.721\n",
      "[401,   155] loss: 29387.824\n",
      "[401,   160] loss: 19252.606\n",
      "[401,   165] loss: 28285.616\n",
      "[401,   170] loss: 22881.643\n",
      "[401,   175] loss: 18801.192\n",
      "[401,   180] loss: 18655.406\n",
      "[401,   185] loss: 31157.723\n",
      "[401,   190] loss: 20587.125\n",
      "[401,   195] loss: 15567.488\n",
      "[401,   200] loss: 20888.191\n",
      "[401,   205] loss: 30522.762\n",
      "[401,   210] loss: 20852.837\n",
      "[401,   215] loss: 18707.875\n",
      "[401,   220] loss: 26472.848\n",
      "[401,   225] loss: 28908.149\n",
      "[401,   230] loss: 24964.628\n",
      "[402,     5] loss: 24164.790\n",
      "[402,    10] loss: 19633.356\n",
      "[402,    15] loss: 24797.784\n",
      "[402,    20] loss: 26530.917\n",
      "[402,    25] loss: 25017.977\n",
      "[402,    30] loss: 41370.046\n",
      "[402,    35] loss: 13059.998\n",
      "[402,    40] loss: 25989.467\n",
      "[402,    45] loss: 21230.917\n",
      "[402,    50] loss: 19392.892\n",
      "[402,    55] loss: 28922.356\n",
      "[402,    60] loss: 47466.549\n",
      "[402,    65] loss: 18247.711\n",
      "[402,    70] loss: 18650.171\n",
      "[402,    75] loss: 24595.270\n",
      "[402,    80] loss: 24850.615\n",
      "[402,    85] loss: 30726.060\n",
      "[402,    90] loss: 21681.088\n",
      "[402,    95] loss: 25598.198\n",
      "[402,   100] loss: 36787.207\n",
      "[402,   105] loss: 20905.751\n",
      "[402,   110] loss: 26755.042\n",
      "[402,   115] loss: 27028.138\n",
      "[402,   120] loss: 24174.000\n",
      "[402,   125] loss: 33052.929\n",
      "[402,   130] loss: 18466.908\n",
      "[402,   135] loss: 27607.186\n",
      "[402,   140] loss: 20664.152\n",
      "[402,   145] loss: 33134.473\n",
      "[402,   150] loss: 30721.981\n",
      "[402,   155] loss: 27056.364\n",
      "[402,   160] loss: 32648.866\n",
      "[402,   165] loss: 26451.371\n",
      "[402,   170] loss: 27049.519\n",
      "[402,   175] loss: 25203.954\n",
      "[402,   180] loss: 16024.474\n",
      "[402,   185] loss: 22663.259\n",
      "[402,   190] loss: 25999.863\n",
      "[402,   195] loss: 28374.682\n",
      "[402,   200] loss: 26157.637\n",
      "[402,   205] loss: 23259.886\n",
      "[402,   210] loss: 21328.741\n",
      "[402,   215] loss: 22863.868\n",
      "[402,   220] loss: 22517.264\n",
      "[402,   225] loss: 30518.914\n",
      "[402,   230] loss: 18523.020\n",
      "[403,     5] loss: 25384.086\n",
      "[403,    10] loss: 25510.820\n",
      "[403,    15] loss: 30042.543\n",
      "[403,    20] loss: 19743.772\n",
      "[403,    25] loss: 22569.172\n",
      "[403,    30] loss: 22292.576\n",
      "[403,    35] loss: 21588.143\n",
      "[403,    40] loss: 25824.364\n",
      "[403,    45] loss: 26781.447\n",
      "[403,    50] loss: 24356.272\n",
      "[403,    55] loss: 17242.176\n",
      "[403,    60] loss: 22093.108\n",
      "[403,    65] loss: 18971.023\n",
      "[403,    70] loss: 32460.566\n",
      "[403,    75] loss: 15557.732\n",
      "[403,    80] loss: 21457.914\n",
      "[403,    85] loss: 23346.241\n",
      "[403,    90] loss: 37997.053\n",
      "[403,    95] loss: 33060.436\n",
      "[403,   100] loss: 22551.786\n",
      "[403,   105] loss: 17776.488\n",
      "[403,   110] loss: 25845.882\n",
      "[403,   115] loss: 29249.269\n",
      "[403,   120] loss: 27797.042\n",
      "[403,   125] loss: 20179.682\n",
      "[403,   130] loss: 26330.884\n",
      "[403,   135] loss: 22977.480\n",
      "[403,   140] loss: 29944.450\n",
      "[403,   145] loss: 24799.961\n",
      "[403,   150] loss: 27445.772\n",
      "[403,   155] loss: 24883.448\n",
      "[403,   160] loss: 22830.778\n",
      "[403,   165] loss: 42402.877\n",
      "[403,   170] loss: 27844.684\n",
      "[403,   175] loss: 25673.333\n",
      "[403,   180] loss: 36589.259\n",
      "[403,   185] loss: 22460.370\n",
      "[403,   190] loss: 21030.764\n",
      "[403,   195] loss: 29429.724\n",
      "[403,   200] loss: 41143.983\n",
      "[403,   205] loss: 30295.898\n",
      "[403,   210] loss: 20928.712\n",
      "[403,   215] loss: 17680.215\n",
      "[403,   220] loss: 32404.507\n",
      "[403,   225] loss: 23887.393\n",
      "[403,   230] loss: 19341.369\n",
      "[404,     5] loss: 18791.311\n",
      "[404,    10] loss: 25971.731\n",
      "[404,    15] loss: 27876.334\n",
      "[404,    20] loss: 26164.213\n",
      "[404,    25] loss: 25941.195\n",
      "[404,    30] loss: 15342.870\n",
      "[404,    35] loss: 18156.406\n",
      "[404,    40] loss: 24391.228\n",
      "[404,    45] loss: 24534.408\n",
      "[404,    50] loss: 18895.508\n",
      "[404,    55] loss: 12415.292\n",
      "[404,    60] loss: 34631.577\n",
      "[404,    65] loss: 24535.929\n",
      "[404,    70] loss: 29316.501\n",
      "[404,    75] loss: 23883.453\n",
      "[404,    80] loss: 17651.613\n",
      "[404,    85] loss: 31342.501\n",
      "[404,    90] loss: 20890.881\n",
      "[404,    95] loss: 32539.434\n",
      "[404,   100] loss: 29531.296\n",
      "[404,   105] loss: 24131.860\n",
      "[404,   110] loss: 19592.989\n",
      "[404,   115] loss: 20963.091\n",
      "[404,   120] loss: 29822.933\n",
      "[404,   125] loss: 24938.454\n",
      "[404,   130] loss: 22229.023\n",
      "[404,   135] loss: 30247.875\n",
      "[404,   140] loss: 36634.115\n",
      "[404,   145] loss: 25635.722\n",
      "[404,   150] loss: 29709.906\n",
      "[404,   155] loss: 24603.822\n",
      "[404,   160] loss: 42672.091\n",
      "[404,   165] loss: 23733.692\n",
      "[404,   170] loss: 34948.112\n",
      "[404,   175] loss: 22303.464\n",
      "[404,   180] loss: 29056.897\n",
      "[404,   185] loss: 17330.375\n",
      "[404,   190] loss: 23376.807\n",
      "[404,   195] loss: 33797.326\n",
      "[404,   200] loss: 22507.344\n",
      "[404,   205] loss: 26853.176\n",
      "[404,   210] loss: 22034.163\n",
      "[404,   215] loss: 33379.667\n",
      "[404,   220] loss: 28285.394\n",
      "[404,   225] loss: 22802.025\n",
      "[404,   230] loss: 23544.738\n",
      "[405,     5] loss: 21540.296\n",
      "[405,    10] loss: 49560.279\n",
      "[405,    15] loss: 29482.339\n",
      "[405,    20] loss: 21493.822\n",
      "[405,    25] loss: 29875.218\n",
      "[405,    30] loss: 15025.718\n",
      "[405,    35] loss: 23988.546\n",
      "[405,    40] loss: 22592.661\n",
      "[405,    45] loss: 23738.700\n",
      "[405,    50] loss: 24142.282\n",
      "[405,    55] loss: 29714.266\n",
      "[405,    60] loss: 24483.737\n",
      "[405,    65] loss: 28290.745\n",
      "[405,    70] loss: 40924.188\n",
      "[405,    75] loss: 16463.010\n",
      "[405,    80] loss: 26370.777\n",
      "[405,    85] loss: 37289.833\n",
      "[405,    90] loss: 28190.556\n",
      "[405,    95] loss: 18191.924\n",
      "[405,   100] loss: 31430.520\n",
      "[405,   105] loss: 28720.746\n",
      "[405,   110] loss: 27148.373\n",
      "[405,   115] loss: 27732.706\n",
      "[405,   120] loss: 23959.833\n",
      "[405,   125] loss: 19648.245\n",
      "[405,   130] loss: 22399.508\n",
      "[405,   135] loss: 21556.652\n",
      "[405,   140] loss: 29659.304\n",
      "[405,   145] loss: 24189.223\n",
      "[405,   150] loss: 16429.313\n",
      "[405,   155] loss: 21493.567\n",
      "[405,   160] loss: 18470.319\n",
      "[405,   165] loss: 15487.515\n",
      "[405,   170] loss: 24284.018\n",
      "[405,   175] loss: 34557.172\n",
      "[405,   180] loss: 19820.096\n",
      "[405,   185] loss: 24140.613\n",
      "[405,   190] loss: 21971.874\n",
      "[405,   195] loss: 25285.659\n",
      "[405,   200] loss: 27544.328\n",
      "[405,   205] loss: 29016.844\n",
      "[405,   210] loss: 29569.008\n",
      "[405,   215] loss: 24994.125\n",
      "[405,   220] loss: 23693.699\n",
      "[405,   225] loss: 26436.164\n",
      "[405,   230] loss: 22055.589\n",
      "[406,     5] loss: 28566.802\n",
      "[406,    10] loss: 21055.352\n",
      "[406,    15] loss: 22855.995\n",
      "[406,    20] loss: 19872.077\n",
      "[406,    25] loss: 25963.398\n",
      "[406,    30] loss: 24547.813\n",
      "[406,    35] loss: 21915.809\n",
      "[406,    40] loss: 27332.830\n",
      "[406,    45] loss: 25974.539\n",
      "[406,    50] loss: 24028.090\n",
      "[406,    55] loss: 22978.364\n",
      "[406,    60] loss: 23283.112\n",
      "[406,    65] loss: 17790.004\n",
      "[406,    70] loss: 24398.500\n",
      "[406,    75] loss: 30032.004\n",
      "[406,    80] loss: 32385.069\n",
      "[406,    85] loss: 42699.407\n",
      "[406,    90] loss: 18337.440\n",
      "[406,    95] loss: 20944.157\n",
      "[406,   100] loss: 33776.387\n",
      "[406,   105] loss: 31142.042\n",
      "[406,   110] loss: 21572.498\n",
      "[406,   115] loss: 31176.567\n",
      "[406,   120] loss: 26604.714\n",
      "[406,   125] loss: 23075.496\n",
      "[406,   130] loss: 29998.707\n",
      "[406,   135] loss: 23818.449\n",
      "[406,   140] loss: 24407.135\n",
      "[406,   145] loss: 30932.819\n",
      "[406,   150] loss: 30563.456\n",
      "[406,   155] loss: 26786.166\n",
      "[406,   160] loss: 27846.120\n",
      "[406,   165] loss: 26634.480\n",
      "[406,   170] loss: 17894.516\n",
      "[406,   175] loss: 16377.624\n",
      "[406,   180] loss: 33853.766\n",
      "[406,   185] loss: 29262.543\n",
      "[406,   190] loss: 25692.073\n",
      "[406,   195] loss: 19662.903\n",
      "[406,   200] loss: 20569.871\n",
      "[406,   205] loss: 17971.849\n",
      "[406,   210] loss: 32801.392\n",
      "[406,   215] loss: 23195.282\n",
      "[406,   220] loss: 11241.238\n",
      "[406,   225] loss: 31700.258\n",
      "[406,   230] loss: 27065.466\n",
      "[407,     5] loss: 27855.566\n",
      "[407,    10] loss: 33092.236\n",
      "[407,    15] loss: 31457.555\n",
      "[407,    20] loss: 19096.172\n",
      "[407,    25] loss: 21992.888\n",
      "[407,    30] loss: 28048.832\n",
      "[407,    35] loss: 23454.430\n",
      "[407,    40] loss: 26708.521\n",
      "[407,    45] loss: 36534.069\n",
      "[407,    50] loss: 24564.993\n",
      "[407,    55] loss: 26817.887\n",
      "[407,    60] loss: 17819.672\n",
      "[407,    65] loss: 18509.662\n",
      "[407,    70] loss: 19487.062\n",
      "[407,    75] loss: 30345.816\n",
      "[407,    80] loss: 33735.785\n",
      "[407,    85] loss: 27239.024\n",
      "[407,    90] loss: 29126.092\n",
      "[407,    95] loss: 20620.494\n",
      "[407,   100] loss: 18003.501\n",
      "[407,   105] loss: 31324.277\n",
      "[407,   110] loss: 12466.755\n",
      "[407,   115] loss: 22042.948\n",
      "[407,   120] loss: 31377.526\n",
      "[407,   125] loss: 50182.618\n",
      "[407,   130] loss: 22446.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[407,   135] loss: 22144.845\n",
      "[407,   140] loss: 20141.484\n",
      "[407,   145] loss: 26105.558\n",
      "[407,   150] loss: 18369.010\n",
      "[407,   155] loss: 19512.912\n",
      "[407,   160] loss: 31022.057\n",
      "[407,   165] loss: 27100.332\n",
      "[407,   170] loss: 28943.603\n",
      "[407,   175] loss: 18504.490\n",
      "[407,   180] loss: 23351.839\n",
      "[407,   185] loss: 29812.777\n",
      "[407,   190] loss: 18424.430\n",
      "[407,   195] loss: 26550.365\n",
      "[407,   200] loss: 35349.912\n",
      "[407,   205] loss: 22472.055\n",
      "[407,   210] loss: 23862.019\n",
      "[407,   215] loss: 28350.877\n",
      "[407,   220] loss: 18985.000\n",
      "[407,   225] loss: 21178.186\n",
      "[407,   230] loss: 33196.661\n",
      "[408,     5] loss: 26423.793\n",
      "[408,    10] loss: 25456.994\n",
      "[408,    15] loss: 29045.771\n",
      "[408,    20] loss: 30949.227\n",
      "[408,    25] loss: 25704.514\n",
      "[408,    30] loss: 29374.021\n",
      "[408,    35] loss: 50297.493\n",
      "[408,    40] loss: 31515.526\n",
      "[408,    45] loss: 24628.504\n",
      "[408,    50] loss: 27944.873\n",
      "[408,    55] loss: 22820.732\n",
      "[408,    60] loss: 22174.469\n",
      "[408,    65] loss: 23049.031\n",
      "[408,    70] loss: 36454.686\n",
      "[408,    75] loss: 23682.717\n",
      "[408,    80] loss: 21582.981\n",
      "[408,    85] loss: 26000.851\n",
      "[408,    90] loss: 29521.672\n",
      "[408,    95] loss: 44952.229\n",
      "[408,   100] loss: 26398.182\n",
      "[408,   105] loss: 19720.556\n",
      "[408,   110] loss: 12239.860\n",
      "[408,   115] loss: 20736.080\n",
      "[408,   120] loss: 30770.463\n",
      "[408,   125] loss: 19420.271\n",
      "[408,   130] loss: 28533.977\n",
      "[408,   135] loss: 19637.705\n",
      "[408,   140] loss: 19257.258\n",
      "[408,   145] loss: 14466.262\n",
      "[408,   150] loss: 21141.530\n",
      "[408,   155] loss: 22225.390\n",
      "[408,   160] loss: 20869.268\n",
      "[408,   165] loss: 27149.827\n",
      "[408,   170] loss: 20873.694\n",
      "[408,   175] loss: 19047.635\n",
      "[408,   180] loss: 25396.595\n",
      "[408,   185] loss: 13168.173\n",
      "[408,   190] loss: 40239.344\n",
      "[408,   195] loss: 21085.597\n",
      "[408,   200] loss: 30424.171\n",
      "[408,   205] loss: 29702.810\n",
      "[408,   210] loss: 18507.755\n",
      "[408,   215] loss: 25770.438\n",
      "[408,   220] loss: 32862.943\n",
      "[408,   225] loss: 22255.923\n",
      "[408,   230] loss: 25673.683\n",
      "[409,     5] loss: 27797.698\n",
      "[409,    10] loss: 29635.079\n",
      "[409,    15] loss: 28557.359\n",
      "[409,    20] loss: 24583.831\n",
      "[409,    25] loss: 30871.198\n",
      "[409,    30] loss: 28492.318\n",
      "[409,    35] loss: 22050.282\n",
      "[409,    40] loss: 23570.498\n",
      "[409,    45] loss: 26720.300\n",
      "[409,    50] loss: 24784.522\n",
      "[409,    55] loss: 18458.085\n",
      "[409,    60] loss: 23847.849\n",
      "[409,    65] loss: 23722.209\n",
      "[409,    70] loss: 17607.099\n",
      "[409,    75] loss: 23030.638\n",
      "[409,    80] loss: 18713.070\n",
      "[409,    85] loss: 26408.596\n",
      "[409,    90] loss: 25930.358\n",
      "[409,    95] loss: 32540.297\n",
      "[409,   100] loss: 19210.809\n",
      "[409,   105] loss: 23258.066\n",
      "[409,   110] loss: 15082.604\n",
      "[409,   115] loss: 21713.096\n",
      "[409,   120] loss: 19424.134\n",
      "[409,   125] loss: 40668.533\n",
      "[409,   130] loss: 19496.990\n",
      "[409,   135] loss: 25781.557\n",
      "[409,   140] loss: 24379.694\n",
      "[409,   145] loss: 26348.609\n",
      "[409,   150] loss: 34077.111\n",
      "[409,   155] loss: 20321.062\n",
      "[409,   160] loss: 31408.911\n",
      "[409,   165] loss: 27382.807\n",
      "[409,   170] loss: 28299.748\n",
      "[409,   175] loss: 30873.560\n",
      "[409,   180] loss: 28880.209\n",
      "[409,   185] loss: 18732.972\n",
      "[409,   190] loss: 36668.567\n",
      "[409,   195] loss: 18468.912\n",
      "[409,   200] loss: 23200.071\n",
      "[409,   205] loss: 35919.995\n",
      "[409,   210] loss: 30912.259\n",
      "[409,   215] loss: 30291.933\n",
      "[409,   220] loss: 22128.519\n",
      "[409,   225] loss: 26364.233\n",
      "[409,   230] loss: 18068.523\n",
      "[410,     5] loss: 27821.192\n",
      "[410,    10] loss: 13937.273\n",
      "[410,    15] loss: 25092.055\n",
      "[410,    20] loss: 51440.862\n",
      "[410,    25] loss: 28413.301\n",
      "[410,    30] loss: 25559.465\n",
      "[410,    35] loss: 19296.136\n",
      "[410,    40] loss: 26788.420\n",
      "[410,    45] loss: 20748.748\n",
      "[410,    50] loss: 25281.259\n",
      "[410,    55] loss: 21539.748\n",
      "[410,    60] loss: 27236.322\n",
      "[410,    65] loss: 18772.993\n",
      "[410,    70] loss: 21853.015\n",
      "[410,    75] loss: 30210.721\n",
      "[410,    80] loss: 31343.943\n",
      "[410,    85] loss: 27104.790\n",
      "[410,    90] loss: 35019.431\n",
      "[410,    95] loss: 16309.981\n",
      "[410,   100] loss: 19358.372\n",
      "[410,   105] loss: 20207.376\n",
      "[410,   110] loss: 37150.031\n",
      "[410,   115] loss: 23082.481\n",
      "[410,   120] loss: 28383.239\n",
      "[410,   125] loss: 26600.839\n",
      "[410,   130] loss: 23864.682\n",
      "[410,   135] loss: 24988.711\n",
      "[410,   140] loss: 17248.296\n",
      "[410,   145] loss: 17351.251\n",
      "[410,   150] loss: 26668.241\n",
      "[410,   155] loss: 22798.001\n",
      "[410,   160] loss: 22019.613\n",
      "[410,   165] loss: 30279.050\n",
      "[410,   170] loss: 22734.559\n",
      "[410,   175] loss: 19543.554\n",
      "[410,   180] loss: 18412.156\n",
      "[410,   185] loss: 27184.429\n",
      "[410,   190] loss: 25235.211\n",
      "[410,   195] loss: 31277.837\n",
      "[410,   200] loss: 25977.717\n",
      "[410,   205] loss: 25034.798\n",
      "[410,   210] loss: 44739.327\n",
      "[410,   215] loss: 26483.613\n",
      "[410,   220] loss: 17046.685\n",
      "[410,   225] loss: 42203.958\n",
      "[410,   230] loss: 21818.208\n",
      "[411,     5] loss: 57435.773\n",
      "[411,    10] loss: 23737.698\n",
      "[411,    15] loss: 39675.607\n",
      "[411,    20] loss: 22471.766\n",
      "[411,    25] loss: 24401.302\n",
      "[411,    30] loss: 22664.129\n",
      "[411,    35] loss: 31624.664\n",
      "[411,    40] loss: 31704.714\n",
      "[411,    45] loss: 20545.845\n",
      "[411,    50] loss: 33187.848\n",
      "[411,    55] loss: 17889.024\n",
      "[411,    60] loss: 35814.840\n",
      "[411,    65] loss: 28378.612\n",
      "[411,    70] loss: 14318.251\n",
      "[411,    75] loss: 30371.668\n",
      "[411,    80] loss: 19607.228\n",
      "[411,    85] loss: 20697.262\n",
      "[411,    90] loss: 17123.752\n",
      "[411,    95] loss: 15751.829\n",
      "[411,   100] loss: 27084.716\n",
      "[411,   105] loss: 22830.807\n",
      "[411,   110] loss: 34491.610\n",
      "[411,   115] loss: 21877.181\n",
      "[411,   120] loss: 20119.560\n",
      "[411,   125] loss: 26558.880\n",
      "[411,   130] loss: 33069.126\n",
      "[411,   135] loss: 21264.863\n",
      "[411,   140] loss: 33255.248\n",
      "[411,   145] loss: 21431.269\n",
      "[411,   150] loss: 23164.608\n",
      "[411,   155] loss: 33490.278\n",
      "[411,   160] loss: 33268.975\n",
      "[411,   165] loss: 22228.333\n",
      "[411,   170] loss: 23537.470\n",
      "[411,   175] loss: 28423.710\n",
      "[411,   180] loss: 21705.285\n",
      "[411,   185] loss: 17624.160\n",
      "[411,   190] loss: 21088.457\n",
      "[411,   195] loss: 16153.847\n",
      "[411,   200] loss: 24681.160\n",
      "[411,   205] loss: 31064.804\n",
      "[411,   210] loss: 29556.275\n",
      "[411,   215] loss: 20379.658\n",
      "[411,   220] loss: 16963.720\n",
      "[411,   225] loss: 28224.087\n",
      "[411,   230] loss: 16741.497\n",
      "[412,     5] loss: 24425.489\n",
      "[412,    10] loss: 17315.283\n",
      "[412,    15] loss: 32748.047\n",
      "[412,    20] loss: 18220.755\n",
      "[412,    25] loss: 19393.203\n",
      "[412,    30] loss: 35374.619\n",
      "[412,    35] loss: 11571.990\n",
      "[412,    40] loss: 19923.563\n",
      "[412,    45] loss: 20789.139\n",
      "[412,    50] loss: 29174.924\n",
      "[412,    55] loss: 18558.771\n",
      "[412,    60] loss: 24231.917\n",
      "[412,    65] loss: 23441.379\n",
      "[412,    70] loss: 25123.663\n",
      "[412,    75] loss: 47537.500\n",
      "[412,    80] loss: 28219.592\n",
      "[412,    85] loss: 24075.200\n",
      "[412,    90] loss: 22764.816\n",
      "[412,    95] loss: 32061.419\n",
      "[412,   100] loss: 16057.646\n",
      "[412,   105] loss: 24668.830\n",
      "[412,   110] loss: 31308.255\n",
      "[412,   115] loss: 26675.281\n",
      "[412,   120] loss: 23854.793\n",
      "[412,   125] loss: 19029.833\n",
      "[412,   130] loss: 17273.947\n",
      "[412,   135] loss: 30515.988\n",
      "[412,   140] loss: 39862.684\n",
      "[412,   145] loss: 30259.289\n",
      "[412,   150] loss: 24371.671\n",
      "[412,   155] loss: 25618.136\n",
      "[412,   160] loss: 28941.294\n",
      "[412,   165] loss: 29308.588\n",
      "[412,   170] loss: 20507.173\n",
      "[412,   175] loss: 24930.669\n",
      "[412,   180] loss: 30101.728\n",
      "[412,   185] loss: 22343.260\n",
      "[412,   190] loss: 35142.541\n",
      "[412,   195] loss: 19155.518\n",
      "[412,   200] loss: 26141.636\n",
      "[412,   205] loss: 18923.389\n",
      "[412,   210] loss: 20966.539\n",
      "[412,   215] loss: 22625.131\n",
      "[412,   220] loss: 26482.050\n",
      "[412,   225] loss: 39021.801\n",
      "[412,   230] loss: 20522.550\n",
      "[413,     5] loss: 27383.411\n",
      "[413,    10] loss: 22714.371\n",
      "[413,    15] loss: 27763.040\n",
      "[413,    20] loss: 32118.998\n",
      "[413,    25] loss: 28506.951\n",
      "[413,    30] loss: 34307.038\n",
      "[413,    35] loss: 22883.389\n",
      "[413,    40] loss: 24868.736\n",
      "[413,    45] loss: 20758.902\n",
      "[413,    50] loss: 31687.807\n",
      "[413,    55] loss: 19115.013\n",
      "[413,    60] loss: 25953.068\n",
      "[413,    65] loss: 25834.715\n",
      "[413,    70] loss: 27553.077\n",
      "[413,    75] loss: 29168.976\n",
      "[413,    80] loss: 22755.941\n",
      "[413,    85] loss: 21496.886\n",
      "[413,    90] loss: 25471.406\n",
      "[413,    95] loss: 26233.810\n",
      "[413,   100] loss: 16685.437\n",
      "[413,   105] loss: 15896.611\n",
      "[413,   110] loss: 30259.283\n",
      "[413,   115] loss: 25848.307\n",
      "[413,   120] loss: 25721.487\n",
      "[413,   125] loss: 23287.414\n",
      "[413,   130] loss: 17762.665\n",
      "[413,   135] loss: 39307.978\n",
      "[413,   140] loss: 25807.464\n",
      "[413,   145] loss: 23891.676\n",
      "[413,   150] loss: 22934.836\n",
      "[413,   155] loss: 28488.673\n",
      "[413,   160] loss: 22396.849\n",
      "[413,   165] loss: 24456.491\n",
      "[413,   170] loss: 20410.036\n",
      "[413,   175] loss: 20559.717\n",
      "[413,   180] loss: 43706.943\n",
      "[413,   185] loss: 26242.025\n",
      "[413,   190] loss: 26743.507\n",
      "[413,   195] loss: 24240.295\n",
      "[413,   200] loss: 19426.786\n",
      "[413,   205] loss: 21482.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[413,   210] loss: 20332.856\n",
      "[413,   215] loss: 24038.472\n",
      "[413,   220] loss: 31974.254\n",
      "[413,   225] loss: 34209.513\n",
      "[413,   230] loss: 25525.490\n",
      "[414,     5] loss: 17111.704\n",
      "[414,    10] loss: 17402.617\n",
      "[414,    15] loss: 21760.283\n",
      "[414,    20] loss: 17948.047\n",
      "[414,    25] loss: 28412.405\n",
      "[414,    30] loss: 28324.529\n",
      "[414,    35] loss: 23266.220\n",
      "[414,    40] loss: 26876.144\n",
      "[414,    45] loss: 29517.121\n",
      "[414,    50] loss: 26019.725\n",
      "[414,    55] loss: 23458.049\n",
      "[414,    60] loss: 20311.676\n",
      "[414,    65] loss: 17260.359\n",
      "[414,    70] loss: 31709.714\n",
      "[414,    75] loss: 20394.043\n",
      "[414,    80] loss: 28718.161\n",
      "[414,    85] loss: 28290.545\n",
      "[414,    90] loss: 28591.640\n",
      "[414,    95] loss: 27158.868\n",
      "[414,   100] loss: 21529.758\n",
      "[414,   105] loss: 28906.638\n",
      "[414,   110] loss: 34932.396\n",
      "[414,   115] loss: 19393.049\n",
      "[414,   120] loss: 19921.528\n",
      "[414,   125] loss: 24333.380\n",
      "[414,   130] loss: 31360.273\n",
      "[414,   135] loss: 20936.336\n",
      "[414,   140] loss: 30192.574\n",
      "[414,   145] loss: 28317.419\n",
      "[414,   150] loss: 25634.323\n",
      "[414,   155] loss: 25197.703\n",
      "[414,   160] loss: 39288.686\n",
      "[414,   165] loss: 24271.761\n",
      "[414,   170] loss: 25908.401\n",
      "[414,   175] loss: 24073.800\n",
      "[414,   180] loss: 29361.821\n",
      "[414,   185] loss: 37194.534\n",
      "[414,   190] loss: 31340.999\n",
      "[414,   195] loss: 22002.266\n",
      "[414,   200] loss: 27583.326\n",
      "[414,   205] loss: 27249.983\n",
      "[414,   210] loss: 25428.006\n",
      "[414,   215] loss: 19353.355\n",
      "[414,   220] loss: 19523.362\n",
      "[414,   225] loss: 22133.486\n",
      "[414,   230] loss: 27251.785\n",
      "[415,     5] loss: 29671.274\n",
      "[415,    10] loss: 25523.650\n",
      "[415,    15] loss: 26116.917\n",
      "[415,    20] loss: 31222.888\n",
      "[415,    25] loss: 29743.543\n",
      "[415,    30] loss: 32399.365\n",
      "[415,    35] loss: 25480.324\n",
      "[415,    40] loss: 34622.853\n",
      "[415,    45] loss: 37836.419\n",
      "[415,    50] loss: 23959.733\n",
      "[415,    55] loss: 11837.313\n",
      "[415,    60] loss: 19942.612\n",
      "[415,    65] loss: 24243.539\n",
      "[415,    70] loss: 14527.754\n",
      "[415,    75] loss: 21365.684\n",
      "[415,    80] loss: 15685.489\n",
      "[415,    85] loss: 29484.193\n",
      "[415,    90] loss: 42977.543\n",
      "[415,    95] loss: 28073.848\n",
      "[415,   100] loss: 46241.414\n",
      "[415,   105] loss: 26937.906\n",
      "[415,   110] loss: 24322.540\n",
      "[415,   115] loss: 30563.381\n",
      "[415,   120] loss: 29327.909\n",
      "[415,   125] loss: 24227.878\n",
      "[415,   130] loss: 21324.130\n",
      "[415,   135] loss: 19611.365\n",
      "[415,   140] loss: 20517.893\n",
      "[415,   145] loss: 31907.275\n",
      "[415,   150] loss: 24753.992\n",
      "[415,   155] loss: 15825.946\n",
      "[415,   160] loss: 30276.287\n",
      "[415,   165] loss: 20957.870\n",
      "[415,   170] loss: 19537.564\n",
      "[415,   175] loss: 17432.445\n",
      "[415,   180] loss: 15373.445\n",
      "[415,   185] loss: 24527.336\n",
      "[415,   190] loss: 19201.598\n",
      "[415,   195] loss: 18419.034\n",
      "[415,   200] loss: 39633.587\n",
      "[415,   205] loss: 30004.050\n",
      "[415,   210] loss: 24449.867\n",
      "[415,   215] loss: 28048.471\n",
      "[415,   220] loss: 30860.025\n",
      "[415,   225] loss: 18189.935\n",
      "[415,   230] loss: 17910.013\n",
      "[416,     5] loss: 28326.141\n",
      "[416,    10] loss: 22058.248\n",
      "[416,    15] loss: 24213.533\n",
      "[416,    20] loss: 27113.806\n",
      "[416,    25] loss: 25976.429\n",
      "[416,    30] loss: 28125.530\n",
      "[416,    35] loss: 20932.685\n",
      "[416,    40] loss: 27568.914\n",
      "[416,    45] loss: 24077.500\n",
      "[416,    50] loss: 36732.433\n",
      "[416,    55] loss: 27951.818\n",
      "[416,    60] loss: 14706.346\n",
      "[416,    65] loss: 18009.573\n",
      "[416,    70] loss: 27448.226\n",
      "[416,    75] loss: 21859.160\n",
      "[416,    80] loss: 23118.279\n",
      "[416,    85] loss: 25386.431\n",
      "[416,    90] loss: 19451.218\n",
      "[416,    95] loss: 23550.226\n",
      "[416,   100] loss: 19175.741\n",
      "[416,   105] loss: 16760.679\n",
      "[416,   110] loss: 17585.549\n",
      "[416,   115] loss: 42184.760\n",
      "[416,   120] loss: 26404.951\n",
      "[416,   125] loss: 23176.985\n",
      "[416,   130] loss: 19854.748\n",
      "[416,   135] loss: 29600.846\n",
      "[416,   140] loss: 32895.211\n",
      "[416,   145] loss: 24785.589\n",
      "[416,   150] loss: 31384.949\n",
      "[416,   155] loss: 23679.820\n",
      "[416,   160] loss: 32484.173\n",
      "[416,   165] loss: 17633.836\n",
      "[416,   170] loss: 21398.713\n",
      "[416,   175] loss: 29302.671\n",
      "[416,   180] loss: 29223.083\n",
      "[416,   185] loss: 32996.591\n",
      "[416,   190] loss: 21909.666\n",
      "[416,   195] loss: 43958.367\n",
      "[416,   200] loss: 17417.957\n",
      "[416,   205] loss: 22782.282\n",
      "[416,   210] loss: 33360.947\n",
      "[416,   215] loss: 21383.659\n",
      "[416,   220] loss: 21490.560\n",
      "[416,   225] loss: 30177.195\n",
      "[416,   230] loss: 20728.588\n",
      "[417,     5] loss: 14092.414\n",
      "[417,    10] loss: 27292.477\n",
      "[417,    15] loss: 26466.961\n",
      "[417,    20] loss: 17090.564\n",
      "[417,    25] loss: 25886.337\n",
      "[417,    30] loss: 22644.741\n",
      "[417,    35] loss: 16361.498\n",
      "[417,    40] loss: 26211.672\n",
      "[417,    45] loss: 25418.930\n",
      "[417,    50] loss: 19607.123\n",
      "[417,    55] loss: 35921.994\n",
      "[417,    60] loss: 18823.516\n",
      "[417,    65] loss: 20784.991\n",
      "[417,    70] loss: 26739.351\n",
      "[417,    75] loss: 20704.738\n",
      "[417,    80] loss: 31421.848\n",
      "[417,    85] loss: 25948.901\n",
      "[417,    90] loss: 29903.472\n",
      "[417,    95] loss: 29990.770\n",
      "[417,   100] loss: 26460.242\n",
      "[417,   105] loss: 24839.239\n",
      "[417,   110] loss: 19117.450\n",
      "[417,   115] loss: 35336.270\n",
      "[417,   120] loss: 18038.868\n",
      "[417,   125] loss: 24404.175\n",
      "[417,   130] loss: 25390.625\n",
      "[417,   135] loss: 39229.086\n",
      "[417,   140] loss: 13632.261\n",
      "[417,   145] loss: 36584.069\n",
      "[417,   150] loss: 21884.392\n",
      "[417,   155] loss: 39519.083\n",
      "[417,   160] loss: 32147.239\n",
      "[417,   165] loss: 24239.923\n",
      "[417,   170] loss: 19498.659\n",
      "[417,   175] loss: 29272.786\n",
      "[417,   180] loss: 23437.107\n",
      "[417,   185] loss: 30703.345\n",
      "[417,   190] loss: 21707.031\n",
      "[417,   195] loss: 20769.080\n",
      "[417,   200] loss: 19028.670\n",
      "[417,   205] loss: 21063.843\n",
      "[417,   210] loss: 48615.882\n",
      "[417,   215] loss: 23672.208\n",
      "[417,   220] loss: 25548.109\n",
      "[417,   225] loss: 24054.727\n",
      "[417,   230] loss: 27228.836\n",
      "[418,     5] loss: 24631.541\n",
      "[418,    10] loss: 24482.293\n",
      "[418,    15] loss: 22287.919\n",
      "[418,    20] loss: 27077.093\n",
      "[418,    25] loss: 22590.463\n",
      "[418,    30] loss: 22422.385\n",
      "[418,    35] loss: 26311.771\n",
      "[418,    40] loss: 29026.518\n",
      "[418,    45] loss: 17407.255\n",
      "[418,    50] loss: 31905.816\n",
      "[418,    55] loss: 24852.289\n",
      "[418,    60] loss: 23538.358\n",
      "[418,    65] loss: 22772.251\n",
      "[418,    70] loss: 27345.749\n",
      "[418,    75] loss: 24479.344\n",
      "[418,    80] loss: 28593.382\n",
      "[418,    85] loss: 27761.117\n",
      "[418,    90] loss: 26409.744\n",
      "[418,    95] loss: 25719.294\n",
      "[418,   100] loss: 32141.872\n",
      "[418,   105] loss: 25579.296\n",
      "[418,   110] loss: 34316.346\n",
      "[418,   115] loss: 28204.528\n",
      "[418,   120] loss: 22935.078\n",
      "[418,   125] loss: 24356.725\n",
      "[418,   130] loss: 20308.115\n",
      "[418,   135] loss: 29101.349\n",
      "[418,   140] loss: 29477.224\n",
      "[418,   145] loss: 28137.591\n",
      "[418,   150] loss: 20077.049\n",
      "[418,   155] loss: 18534.056\n",
      "[418,   160] loss: 53398.232\n",
      "[418,   165] loss: 27488.875\n",
      "[418,   170] loss: 21524.247\n",
      "[418,   175] loss: 19060.544\n",
      "[418,   180] loss: 21520.052\n",
      "[418,   185] loss: 26784.772\n",
      "[418,   190] loss: 22210.020\n",
      "[418,   195] loss: 21446.434\n",
      "[418,   200] loss: 14163.082\n",
      "[418,   205] loss: 39539.752\n",
      "[418,   210] loss: 21747.736\n",
      "[418,   215] loss: 20824.819\n",
      "[418,   220] loss: 23708.246\n",
      "[418,   225] loss: 25899.984\n",
      "[418,   230] loss: 21375.511\n",
      "[419,     5] loss: 36004.616\n",
      "[419,    10] loss: 30038.065\n",
      "[419,    15] loss: 21462.750\n",
      "[419,    20] loss: 43981.223\n",
      "[419,    25] loss: 26653.501\n",
      "[419,    30] loss: 18582.426\n",
      "[419,    35] loss: 17471.194\n",
      "[419,    40] loss: 28842.652\n",
      "[419,    45] loss: 34743.130\n",
      "[419,    50] loss: 25308.184\n",
      "[419,    55] loss: 22504.835\n",
      "[419,    60] loss: 20730.372\n",
      "[419,    65] loss: 16797.112\n",
      "[419,    70] loss: 29073.938\n",
      "[419,    75] loss: 24838.727\n",
      "[419,    80] loss: 25807.432\n",
      "[419,    85] loss: 33562.943\n",
      "[419,    90] loss: 29386.026\n",
      "[419,    95] loss: 13275.135\n",
      "[419,   100] loss: 26697.234\n",
      "[419,   105] loss: 34236.009\n",
      "[419,   110] loss: 25800.935\n",
      "[419,   115] loss: 26390.307\n",
      "[419,   120] loss: 23681.549\n",
      "[419,   125] loss: 22167.662\n",
      "[419,   130] loss: 28022.833\n",
      "[419,   135] loss: 21296.838\n",
      "[419,   140] loss: 26445.926\n",
      "[419,   145] loss: 16935.000\n",
      "[419,   150] loss: 18626.030\n",
      "[419,   155] loss: 27105.204\n",
      "[419,   160] loss: 15177.326\n",
      "[419,   165] loss: 18558.003\n",
      "[419,   170] loss: 34331.168\n",
      "[419,   175] loss: 47980.009\n",
      "[419,   180] loss: 22278.311\n",
      "[419,   185] loss: 18959.761\n",
      "[419,   190] loss: 26037.427\n",
      "[419,   195] loss: 24021.389\n",
      "[419,   200] loss: 31904.944\n",
      "[419,   205] loss: 31078.805\n",
      "[419,   210] loss: 16761.414\n",
      "[419,   215] loss: 17775.515\n",
      "[419,   220] loss: 23186.604\n",
      "[419,   225] loss: 30481.502\n",
      "[419,   230] loss: 23313.069\n",
      "[420,     5] loss: 30919.929\n",
      "[420,    10] loss: 24395.228\n",
      "[420,    15] loss: 32385.511\n",
      "[420,    20] loss: 17673.707\n",
      "[420,    25] loss: 27398.008\n",
      "[420,    30] loss: 25834.666\n",
      "[420,    35] loss: 27181.427\n",
      "[420,    40] loss: 26051.341\n",
      "[420,    45] loss: 23328.905\n",
      "[420,    50] loss: 20169.750\n",
      "[420,    55] loss: 21755.242\n",
      "[420,    60] loss: 24767.911\n",
      "[420,    65] loss: 41913.015\n",
      "[420,    70] loss: 27185.949\n",
      "[420,    75] loss: 24451.121\n",
      "[420,    80] loss: 47188.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420,    85] loss: 24703.694\n",
      "[420,    90] loss: 21073.070\n",
      "[420,    95] loss: 18531.593\n",
      "[420,   100] loss: 25318.457\n",
      "[420,   105] loss: 23000.574\n",
      "[420,   110] loss: 23773.162\n",
      "[420,   115] loss: 21291.513\n",
      "[420,   120] loss: 21944.810\n",
      "[420,   125] loss: 20694.188\n",
      "[420,   130] loss: 20965.465\n",
      "[420,   135] loss: 24976.675\n",
      "[420,   140] loss: 23805.919\n",
      "[420,   145] loss: 27937.726\n",
      "[420,   150] loss: 25755.157\n",
      "[420,   155] loss: 24910.639\n",
      "[420,   160] loss: 23968.464\n",
      "[420,   165] loss: 26193.069\n",
      "[420,   170] loss: 34147.859\n",
      "[420,   175] loss: 29358.433\n",
      "[420,   180] loss: 27036.673\n",
      "[420,   185] loss: 26808.280\n",
      "[420,   190] loss: 25434.881\n",
      "[420,   195] loss: 18659.240\n",
      "[420,   200] loss: 22357.530\n",
      "[420,   205] loss: 21518.034\n",
      "[420,   210] loss: 18676.230\n",
      "[420,   215] loss: 28125.689\n",
      "[420,   220] loss: 22046.986\n",
      "[420,   225] loss: 24218.038\n",
      "[420,   230] loss: 37172.812\n",
      "[421,     5] loss: 24004.562\n",
      "[421,    10] loss: 19121.406\n",
      "[421,    15] loss: 24002.498\n",
      "[421,    20] loss: 36136.888\n",
      "[421,    25] loss: 23651.677\n",
      "[421,    30] loss: 23129.791\n",
      "[421,    35] loss: 29263.075\n",
      "[421,    40] loss: 29238.757\n",
      "[421,    45] loss: 46906.904\n",
      "[421,    50] loss: 22127.077\n",
      "[421,    55] loss: 30813.089\n",
      "[421,    60] loss: 29076.963\n",
      "[421,    65] loss: 21495.316\n",
      "[421,    70] loss: 23232.057\n",
      "[421,    75] loss: 28230.041\n",
      "[421,    80] loss: 19842.655\n",
      "[421,    85] loss: 33076.795\n",
      "[421,    90] loss: 17539.761\n",
      "[421,    95] loss: 16415.429\n",
      "[421,   100] loss: 16471.220\n",
      "[421,   105] loss: 22618.751\n",
      "[421,   110] loss: 17625.260\n",
      "[421,   115] loss: 22094.374\n",
      "[421,   120] loss: 25254.099\n",
      "[421,   125] loss: 25807.861\n",
      "[421,   130] loss: 14877.664\n",
      "[421,   135] loss: 20834.030\n",
      "[421,   140] loss: 24780.035\n",
      "[421,   145] loss: 21533.731\n",
      "[421,   150] loss: 30256.902\n",
      "[421,   155] loss: 48613.549\n",
      "[421,   160] loss: 29153.385\n",
      "[421,   165] loss: 32307.528\n",
      "[421,   170] loss: 20756.093\n",
      "[421,   175] loss: 29672.194\n",
      "[421,   180] loss: 24713.179\n",
      "[421,   185] loss: 22009.917\n",
      "[421,   190] loss: 20179.714\n",
      "[421,   195] loss: 31228.812\n",
      "[421,   200] loss: 18689.603\n",
      "[421,   205] loss: 21790.287\n",
      "[421,   210] loss: 27432.377\n",
      "[421,   215] loss: 17846.314\n",
      "[421,   220] loss: 31608.449\n",
      "[421,   225] loss: 19478.968\n",
      "[421,   230] loss: 33521.359\n",
      "[422,     5] loss: 19460.475\n",
      "[422,    10] loss: 26010.932\n",
      "[422,    15] loss: 26352.889\n",
      "[422,    20] loss: 22636.404\n",
      "[422,    25] loss: 24215.123\n",
      "[422,    30] loss: 24064.171\n",
      "[422,    35] loss: 27661.912\n",
      "[422,    40] loss: 17596.258\n",
      "[422,    45] loss: 20280.644\n",
      "[422,    50] loss: 18965.803\n",
      "[422,    55] loss: 23213.458\n",
      "[422,    60] loss: 23617.466\n",
      "[422,    65] loss: 18617.917\n",
      "[422,    70] loss: 28437.610\n",
      "[422,    75] loss: 26105.814\n",
      "[422,    80] loss: 21571.779\n",
      "[422,    85] loss: 19590.565\n",
      "[422,    90] loss: 54299.260\n",
      "[422,    95] loss: 21014.948\n",
      "[422,   100] loss: 21085.997\n",
      "[422,   105] loss: 44465.544\n",
      "[422,   110] loss: 24191.228\n",
      "[422,   115] loss: 29273.601\n",
      "[422,   120] loss: 29324.143\n",
      "[422,   125] loss: 17874.936\n",
      "[422,   130] loss: 37571.921\n",
      "[422,   135] loss: 23127.096\n",
      "[422,   140] loss: 21860.112\n",
      "[422,   145] loss: 27182.663\n",
      "[422,   150] loss: 20228.791\n",
      "[422,   155] loss: 30701.509\n",
      "[422,   160] loss: 18537.991\n",
      "[422,   165] loss: 23404.564\n",
      "[422,   170] loss: 19289.670\n",
      "[422,   175] loss: 38962.012\n",
      "[422,   180] loss: 20300.521\n",
      "[422,   185] loss: 26020.734\n",
      "[422,   190] loss: 28372.702\n",
      "[422,   195] loss: 36415.006\n",
      "[422,   200] loss: 21675.776\n",
      "[422,   205] loss: 25246.949\n",
      "[422,   210] loss: 29870.563\n",
      "[422,   215] loss: 29373.238\n",
      "[422,   220] loss: 24101.262\n",
      "[422,   225] loss: 25169.901\n",
      "[422,   230] loss: 17155.642\n",
      "[423,     5] loss: 38076.093\n",
      "[423,    10] loss: 30481.086\n",
      "[423,    15] loss: 21415.510\n",
      "[423,    20] loss: 17499.497\n",
      "[423,    25] loss: 20662.400\n",
      "[423,    30] loss: 23794.339\n",
      "[423,    35] loss: 23684.461\n",
      "[423,    40] loss: 21696.067\n",
      "[423,    45] loss: 20802.211\n",
      "[423,    50] loss: 28877.667\n",
      "[423,    55] loss: 27546.167\n",
      "[423,    60] loss: 20799.363\n",
      "[423,    65] loss: 21129.200\n",
      "[423,    70] loss: 32006.116\n",
      "[423,    75] loss: 23472.652\n",
      "[423,    80] loss: 21331.649\n",
      "[423,    85] loss: 19722.063\n",
      "[423,    90] loss: 20097.638\n",
      "[423,    95] loss: 33639.861\n",
      "[423,   100] loss: 31666.870\n",
      "[423,   105] loss: 29184.495\n",
      "[423,   110] loss: 20958.271\n",
      "[423,   115] loss: 22111.668\n",
      "[423,   120] loss: 19750.002\n",
      "[423,   125] loss: 19523.150\n",
      "[423,   130] loss: 49521.774\n",
      "[423,   135] loss: 27371.900\n",
      "[423,   140] loss: 17132.043\n",
      "[423,   145] loss: 31427.191\n",
      "[423,   150] loss: 19137.942\n",
      "[423,   155] loss: 35229.554\n",
      "[423,   160] loss: 23565.721\n",
      "[423,   165] loss: 26995.955\n",
      "[423,   170] loss: 25115.323\n",
      "[423,   175] loss: 25587.624\n",
      "[423,   180] loss: 32636.530\n",
      "[423,   185] loss: 27440.608\n",
      "[423,   190] loss: 32326.157\n",
      "[423,   195] loss: 22097.591\n",
      "[423,   200] loss: 28897.185\n",
      "[423,   205] loss: 22435.382\n",
      "[423,   210] loss: 29821.287\n",
      "[423,   215] loss: 27036.394\n",
      "[423,   220] loss: 24376.151\n",
      "[423,   225] loss: 21802.210\n",
      "[423,   230] loss: 18645.514\n",
      "[424,     5] loss: 28194.135\n",
      "[424,    10] loss: 31140.840\n",
      "[424,    15] loss: 49504.824\n",
      "[424,    20] loss: 23605.615\n",
      "[424,    25] loss: 22964.494\n",
      "[424,    30] loss: 31223.602\n",
      "[424,    35] loss: 24247.177\n",
      "[424,    40] loss: 25068.646\n",
      "[424,    45] loss: 17924.945\n",
      "[424,    50] loss: 26653.024\n",
      "[424,    55] loss: 25999.784\n",
      "[424,    60] loss: 19691.821\n",
      "[424,    65] loss: 24226.445\n",
      "[424,    70] loss: 31284.712\n",
      "[424,    75] loss: 27491.329\n",
      "[424,    80] loss: 26466.191\n",
      "[424,    85] loss: 18988.703\n",
      "[424,    90] loss: 21047.735\n",
      "[424,    95] loss: 25045.235\n",
      "[424,   100] loss: 21766.633\n",
      "[424,   105] loss: 18813.427\n",
      "[424,   110] loss: 27764.730\n",
      "[424,   115] loss: 36299.429\n",
      "[424,   120] loss: 25018.487\n",
      "[424,   125] loss: 29225.600\n",
      "[424,   130] loss: 21109.838\n",
      "[424,   135] loss: 16419.639\n",
      "[424,   140] loss: 23150.284\n",
      "[424,   145] loss: 18983.476\n",
      "[424,   150] loss: 24367.771\n",
      "[424,   155] loss: 26165.795\n",
      "[424,   160] loss: 35836.129\n",
      "[424,   165] loss: 29515.841\n",
      "[424,   170] loss: 20066.275\n",
      "[424,   175] loss: 31315.022\n",
      "[424,   180] loss: 22531.996\n",
      "[424,   185] loss: 34963.352\n",
      "[424,   190] loss: 34449.708\n",
      "[424,   195] loss: 29673.031\n",
      "[424,   200] loss: 22012.898\n",
      "[424,   205] loss: 22758.205\n",
      "[424,   210] loss: 23318.762\n",
      "[424,   215] loss: 17257.015\n",
      "[424,   220] loss: 22288.612\n",
      "[424,   225] loss: 17890.803\n",
      "[424,   230] loss: 23322.541\n",
      "[425,     5] loss: 24809.012\n",
      "[425,    10] loss: 16171.651\n",
      "[425,    15] loss: 28888.166\n",
      "[425,    20] loss: 27765.250\n",
      "[425,    25] loss: 18268.288\n",
      "[425,    30] loss: 35854.782\n",
      "[425,    35] loss: 26699.972\n",
      "[425,    40] loss: 21473.991\n",
      "[425,    45] loss: 16598.888\n",
      "[425,    50] loss: 20963.924\n",
      "[425,    55] loss: 43484.091\n",
      "[425,    60] loss: 31960.331\n",
      "[425,    65] loss: 27805.084\n",
      "[425,    70] loss: 17638.553\n",
      "[425,    75] loss: 24279.260\n",
      "[425,    80] loss: 25286.175\n",
      "[425,    85] loss: 24241.980\n",
      "[425,    90] loss: 20048.528\n",
      "[425,    95] loss: 20434.940\n",
      "[425,   100] loss: 23108.960\n",
      "[425,   105] loss: 23544.679\n",
      "[425,   110] loss: 32550.503\n",
      "[425,   115] loss: 32038.321\n",
      "[425,   120] loss: 47521.772\n",
      "[425,   125] loss: 21820.725\n",
      "[425,   130] loss: 31443.020\n",
      "[425,   135] loss: 31163.967\n",
      "[425,   140] loss: 16770.816\n",
      "[425,   145] loss: 27635.664\n",
      "[425,   150] loss: 20194.259\n",
      "[425,   155] loss: 24235.880\n",
      "[425,   160] loss: 18559.433\n",
      "[425,   165] loss: 24966.297\n",
      "[425,   170] loss: 39219.742\n",
      "[425,   175] loss: 28929.095\n",
      "[425,   180] loss: 21994.508\n",
      "[425,   185] loss: 24197.878\n",
      "[425,   190] loss: 22708.495\n",
      "[425,   195] loss: 26650.465\n",
      "[425,   200] loss: 20002.508\n",
      "[425,   205] loss: 29223.897\n",
      "[425,   210] loss: 26339.426\n",
      "[425,   215] loss: 24493.314\n",
      "[425,   220] loss: 22050.043\n",
      "[425,   225] loss: 22311.146\n",
      "[425,   230] loss: 17435.891\n",
      "[426,     5] loss: 24555.143\n",
      "[426,    10] loss: 22570.507\n",
      "[426,    15] loss: 15790.662\n",
      "[426,    20] loss: 28979.470\n",
      "[426,    25] loss: 29146.285\n",
      "[426,    30] loss: 22423.876\n",
      "[426,    35] loss: 30432.196\n",
      "[426,    40] loss: 22378.942\n",
      "[426,    45] loss: 41191.671\n",
      "[426,    50] loss: 29049.969\n",
      "[426,    55] loss: 20177.756\n",
      "[426,    60] loss: 21475.063\n",
      "[426,    65] loss: 24949.074\n",
      "[426,    70] loss: 19297.984\n",
      "[426,    75] loss: 41467.696\n",
      "[426,    80] loss: 25205.385\n",
      "[426,    85] loss: 28263.701\n",
      "[426,    90] loss: 22351.077\n",
      "[426,    95] loss: 28247.929\n",
      "[426,   100] loss: 25866.161\n",
      "[426,   105] loss: 15592.182\n",
      "[426,   110] loss: 21552.381\n",
      "[426,   115] loss: 23254.296\n",
      "[426,   120] loss: 26041.795\n",
      "[426,   125] loss: 20797.420\n",
      "[426,   130] loss: 30617.243\n",
      "[426,   135] loss: 20231.814\n",
      "[426,   140] loss: 24005.886\n",
      "[426,   145] loss: 31751.362\n",
      "[426,   150] loss: 33350.082\n",
      "[426,   155] loss: 33275.137\n",
      "[426,   160] loss: 17036.391\n",
      "[426,   165] loss: 25675.756\n",
      "[426,   170] loss: 27850.386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426,   175] loss: 19289.863\n",
      "[426,   180] loss: 17685.745\n",
      "[426,   185] loss: 22121.325\n",
      "[426,   190] loss: 27808.022\n",
      "[426,   195] loss: 24430.417\n",
      "[426,   200] loss: 30197.661\n",
      "[426,   205] loss: 14143.638\n",
      "[426,   210] loss: 32741.022\n",
      "[426,   215] loss: 20159.073\n",
      "[426,   220] loss: 27270.811\n",
      "[426,   225] loss: 23153.459\n",
      "[426,   230] loss: 25315.999\n",
      "[427,     5] loss: 24676.481\n",
      "[427,    10] loss: 25444.144\n",
      "[427,    15] loss: 26035.648\n",
      "[427,    20] loss: 28480.897\n",
      "[427,    25] loss: 33731.869\n",
      "[427,    30] loss: 19210.837\n",
      "[427,    35] loss: 26846.672\n",
      "[427,    40] loss: 28259.574\n",
      "[427,    45] loss: 32849.992\n",
      "[427,    50] loss: 24380.454\n",
      "[427,    55] loss: 20631.471\n",
      "[427,    60] loss: 23597.839\n",
      "[427,    65] loss: 24255.507\n",
      "[427,    70] loss: 24745.330\n",
      "[427,    75] loss: 26543.514\n",
      "[427,    80] loss: 18235.424\n",
      "[427,    85] loss: 19290.504\n",
      "[427,    90] loss: 21893.559\n",
      "[427,    95] loss: 24582.362\n",
      "[427,   100] loss: 29152.670\n",
      "[427,   105] loss: 22670.425\n",
      "[427,   110] loss: 38592.998\n",
      "[427,   115] loss: 43692.640\n",
      "[427,   120] loss: 18298.612\n",
      "[427,   125] loss: 31133.523\n",
      "[427,   130] loss: 15634.350\n",
      "[427,   135] loss: 22120.609\n",
      "[427,   140] loss: 36613.117\n",
      "[427,   145] loss: 18464.767\n",
      "[427,   150] loss: 22264.464\n",
      "[427,   155] loss: 24947.995\n",
      "[427,   160] loss: 20079.766\n",
      "[427,   165] loss: 23472.205\n",
      "[427,   170] loss: 28614.851\n",
      "[427,   175] loss: 28466.606\n",
      "[427,   180] loss: 47740.168\n",
      "[427,   185] loss: 25430.607\n",
      "[427,   190] loss: 23629.214\n",
      "[427,   195] loss: 30539.993\n",
      "[427,   200] loss: 17301.291\n",
      "[427,   205] loss: 28115.402\n",
      "[427,   210] loss: 16857.179\n",
      "[427,   215] loss: 17454.139\n",
      "[427,   220] loss: 25476.041\n",
      "[427,   225] loss: 24614.212\n",
      "[427,   230] loss: 13136.617\n",
      "[428,     5] loss: 29127.547\n",
      "[428,    10] loss: 24998.812\n",
      "[428,    15] loss: 26301.980\n",
      "[428,    20] loss: 18172.497\n",
      "[428,    25] loss: 33605.789\n",
      "[428,    30] loss: 26012.532\n",
      "[428,    35] loss: 27296.505\n",
      "[428,    40] loss: 27477.702\n",
      "[428,    45] loss: 25183.469\n",
      "[428,    50] loss: 22396.012\n",
      "[428,    55] loss: 22685.065\n",
      "[428,    60] loss: 25092.469\n",
      "[428,    65] loss: 15796.444\n",
      "[428,    70] loss: 25773.862\n",
      "[428,    75] loss: 20577.486\n",
      "[428,    80] loss: 29682.622\n",
      "[428,    85] loss: 26542.972\n",
      "[428,    90] loss: 52810.012\n",
      "[428,    95] loss: 29138.269\n",
      "[428,   100] loss: 28743.990\n",
      "[428,   105] loss: 23825.317\n",
      "[428,   110] loss: 18459.839\n",
      "[428,   115] loss: 17127.476\n",
      "[428,   120] loss: 19236.484\n",
      "[428,   125] loss: 15141.110\n",
      "[428,   130] loss: 36960.780\n",
      "[428,   135] loss: 28567.516\n",
      "[428,   140] loss: 23074.486\n",
      "[428,   145] loss: 20945.096\n",
      "[428,   150] loss: 26065.830\n",
      "[428,   155] loss: 28311.092\n",
      "[428,   160] loss: 19184.376\n",
      "[428,   165] loss: 23302.618\n",
      "[428,   170] loss: 22446.389\n",
      "[428,   175] loss: 19718.507\n",
      "[428,   180] loss: 48491.528\n",
      "[428,   185] loss: 31813.827\n",
      "[428,   190] loss: 16868.713\n",
      "[428,   195] loss: 32369.693\n",
      "[428,   200] loss: 21709.139\n",
      "[428,   205] loss: 23944.484\n",
      "[428,   210] loss: 25590.991\n",
      "[428,   215] loss: 26109.164\n",
      "[428,   220] loss: 23810.437\n",
      "[428,   225] loss: 21871.865\n",
      "[428,   230] loss: 25348.338\n",
      "[429,     5] loss: 33165.277\n",
      "[429,    10] loss: 31516.118\n",
      "[429,    15] loss: 28029.939\n",
      "[429,    20] loss: 25099.775\n",
      "[429,    25] loss: 18566.459\n",
      "[429,    30] loss: 28123.225\n",
      "[429,    35] loss: 24006.777\n",
      "[429,    40] loss: 29386.178\n",
      "[429,    45] loss: 24103.111\n",
      "[429,    50] loss: 32873.345\n",
      "[429,    55] loss: 21894.198\n",
      "[429,    60] loss: 24523.977\n",
      "[429,    65] loss: 21604.265\n",
      "[429,    70] loss: 31438.030\n",
      "[429,    75] loss: 27293.869\n",
      "[429,    80] loss: 23375.923\n",
      "[429,    85] loss: 23105.665\n",
      "[429,    90] loss: 20752.292\n",
      "[429,    95] loss: 24337.606\n",
      "[429,   100] loss: 24559.544\n",
      "[429,   105] loss: 22002.135\n",
      "[429,   110] loss: 24167.250\n",
      "[429,   115] loss: 44733.847\n",
      "[429,   120] loss: 22863.062\n",
      "[429,   125] loss: 24204.586\n",
      "[429,   130] loss: 26501.663\n",
      "[429,   135] loss: 43287.457\n",
      "[429,   140] loss: 29181.136\n",
      "[429,   145] loss: 36381.499\n",
      "[429,   150] loss: 20318.858\n",
      "[429,   155] loss: 26750.152\n",
      "[429,   160] loss: 24082.145\n",
      "[429,   165] loss: 27683.818\n",
      "[429,   170] loss: 24802.686\n",
      "[429,   175] loss: 15389.291\n",
      "[429,   180] loss: 21502.878\n",
      "[429,   185] loss: 25713.063\n",
      "[429,   190] loss: 28727.432\n",
      "[429,   195] loss: 27051.629\n",
      "[429,   200] loss: 20338.475\n",
      "[429,   205] loss: 33108.834\n",
      "[429,   210] loss: 19387.892\n",
      "[429,   215] loss: 21185.588\n",
      "[429,   220] loss: 13039.853\n",
      "[429,   225] loss: 27054.693\n",
      "[429,   230] loss: 13087.780\n",
      "[430,     5] loss: 22460.935\n",
      "[430,    10] loss: 21256.452\n",
      "[430,    15] loss: 19290.340\n",
      "[430,    20] loss: 21063.186\n",
      "[430,    25] loss: 25953.780\n",
      "[430,    30] loss: 23557.565\n",
      "[430,    35] loss: 27294.638\n",
      "[430,    40] loss: 32116.136\n",
      "[430,    45] loss: 22515.338\n",
      "[430,    50] loss: 25369.086\n",
      "[430,    55] loss: 25392.991\n",
      "[430,    60] loss: 16991.078\n",
      "[430,    65] loss: 17252.051\n",
      "[430,    70] loss: 23500.711\n",
      "[430,    75] loss: 24497.147\n",
      "[430,    80] loss: 20678.319\n",
      "[430,    85] loss: 31467.707\n",
      "[430,    90] loss: 21823.542\n",
      "[430,    95] loss: 29454.657\n",
      "[430,   100] loss: 32376.333\n",
      "[430,   105] loss: 23568.886\n",
      "[430,   110] loss: 23987.730\n",
      "[430,   115] loss: 26987.925\n",
      "[430,   120] loss: 32788.192\n",
      "[430,   125] loss: 18124.082\n",
      "[430,   130] loss: 28657.415\n",
      "[430,   135] loss: 31205.115\n",
      "[430,   140] loss: 19540.545\n",
      "[430,   145] loss: 34404.453\n",
      "[430,   150] loss: 21859.356\n",
      "[430,   155] loss: 30727.117\n",
      "[430,   160] loss: 28751.418\n",
      "[430,   165] loss: 22560.503\n",
      "[430,   170] loss: 23786.420\n",
      "[430,   175] loss: 41074.485\n",
      "[430,   180] loss: 26507.822\n",
      "[430,   185] loss: 29675.702\n",
      "[430,   190] loss: 19252.628\n",
      "[430,   195] loss: 20676.021\n",
      "[430,   200] loss: 28084.574\n",
      "[430,   205] loss: 24156.959\n",
      "[430,   210] loss: 26984.289\n",
      "[430,   215] loss: 17179.466\n",
      "[430,   220] loss: 31070.186\n",
      "[430,   225] loss: 23630.110\n",
      "[430,   230] loss: 30414.559\n",
      "[431,     5] loss: 21139.395\n",
      "[431,    10] loss: 25441.120\n",
      "[431,    15] loss: 25493.125\n",
      "[431,    20] loss: 23857.719\n",
      "[431,    25] loss: 29895.569\n",
      "[431,    30] loss: 24472.384\n",
      "[431,    35] loss: 24186.707\n",
      "[431,    40] loss: 29786.623\n",
      "[431,    45] loss: 51506.130\n",
      "[431,    50] loss: 29805.102\n",
      "[431,    55] loss: 17852.378\n",
      "[431,    60] loss: 32099.391\n",
      "[431,    65] loss: 28671.888\n",
      "[431,    70] loss: 18476.380\n",
      "[431,    75] loss: 21002.133\n",
      "[431,    80] loss: 27041.072\n",
      "[431,    85] loss: 17284.538\n",
      "[431,    90] loss: 23235.229\n",
      "[431,    95] loss: 27207.009\n",
      "[431,   100] loss: 27933.215\n",
      "[431,   105] loss: 33230.577\n",
      "[431,   110] loss: 25231.078\n",
      "[431,   115] loss: 22921.701\n",
      "[431,   120] loss: 16240.101\n",
      "[431,   125] loss: 18056.496\n",
      "[431,   130] loss: 28705.313\n",
      "[431,   135] loss: 26908.489\n",
      "[431,   140] loss: 22971.571\n",
      "[431,   145] loss: 32022.934\n",
      "[431,   150] loss: 27201.925\n",
      "[431,   155] loss: 20709.393\n",
      "[431,   160] loss: 20960.519\n",
      "[431,   165] loss: 19178.310\n",
      "[431,   170] loss: 20705.726\n",
      "[431,   175] loss: 22201.590\n",
      "[431,   180] loss: 18980.067\n",
      "[431,   185] loss: 30146.664\n",
      "[431,   190] loss: 23590.755\n",
      "[431,   195] loss: 17479.279\n",
      "[431,   200] loss: 15349.411\n",
      "[431,   205] loss: 20020.360\n",
      "[431,   210] loss: 29265.237\n",
      "[431,   215] loss: 15449.493\n",
      "[431,   220] loss: 30834.550\n",
      "[431,   225] loss: 27212.624\n",
      "[431,   230] loss: 30030.172\n",
      "[432,     5] loss: 17875.129\n",
      "[432,    10] loss: 25995.191\n",
      "[432,    15] loss: 30633.622\n",
      "[432,    20] loss: 23813.808\n",
      "[432,    25] loss: 24760.170\n",
      "[432,    30] loss: 24123.548\n",
      "[432,    35] loss: 19666.142\n",
      "[432,    40] loss: 18344.404\n",
      "[432,    45] loss: 31691.564\n",
      "[432,    50] loss: 24633.988\n",
      "[432,    55] loss: 29111.552\n",
      "[432,    60] loss: 23536.785\n",
      "[432,    65] loss: 24458.925\n",
      "[432,    70] loss: 30964.356\n",
      "[432,    75] loss: 32614.998\n",
      "[432,    80] loss: 27492.362\n",
      "[432,    85] loss: 25961.967\n",
      "[432,    90] loss: 27532.452\n",
      "[432,    95] loss: 21165.171\n",
      "[432,   100] loss: 44704.027\n",
      "[432,   105] loss: 26985.146\n",
      "[432,   110] loss: 22302.905\n",
      "[432,   115] loss: 21539.326\n",
      "[432,   120] loss: 44822.729\n",
      "[432,   125] loss: 17837.145\n",
      "[432,   130] loss: 20434.589\n",
      "[432,   135] loss: 22539.768\n",
      "[432,   140] loss: 16588.315\n",
      "[432,   145] loss: 25006.881\n",
      "[432,   150] loss: 24940.511\n",
      "[432,   155] loss: 25910.971\n",
      "[432,   160] loss: 19209.969\n",
      "[432,   165] loss: 18620.081\n",
      "[432,   170] loss: 29726.624\n",
      "[432,   175] loss: 20057.248\n",
      "[432,   180] loss: 20554.093\n",
      "[432,   185] loss: 23896.145\n",
      "[432,   190] loss: 28964.796\n",
      "[432,   195] loss: 25052.938\n",
      "[432,   200] loss: 31337.059\n",
      "[432,   205] loss: 22694.787\n",
      "[432,   210] loss: 28910.485\n",
      "[432,   215] loss: 24336.303\n",
      "[432,   220] loss: 29966.266\n",
      "[432,   225] loss: 29989.268\n",
      "[432,   230] loss: 23920.045\n",
      "[433,     5] loss: 27735.193\n",
      "[433,    10] loss: 35774.448\n",
      "[433,    15] loss: 21710.666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[433,    20] loss: 20682.232\n",
      "[433,    25] loss: 15585.637\n",
      "[433,    30] loss: 22558.382\n",
      "[433,    35] loss: 44573.324\n",
      "[433,    40] loss: 22850.678\n",
      "[433,    45] loss: 30890.902\n",
      "[433,    50] loss: 28992.237\n",
      "[433,    55] loss: 25813.667\n",
      "[433,    60] loss: 22208.584\n",
      "[433,    65] loss: 24205.028\n",
      "[433,    70] loss: 12548.033\n",
      "[433,    75] loss: 26248.718\n",
      "[433,    80] loss: 21158.562\n",
      "[433,    85] loss: 24529.422\n",
      "[433,    90] loss: 28267.595\n",
      "[433,    95] loss: 18494.325\n",
      "[433,   100] loss: 22877.645\n",
      "[433,   105] loss: 27434.637\n",
      "[433,   110] loss: 38296.009\n",
      "[433,   115] loss: 27156.966\n",
      "[433,   120] loss: 26413.269\n",
      "[433,   125] loss: 17581.063\n",
      "[433,   130] loss: 20296.450\n",
      "[433,   135] loss: 29060.068\n",
      "[433,   140] loss: 21206.623\n",
      "[433,   145] loss: 24857.209\n",
      "[433,   150] loss: 41073.000\n",
      "[433,   155] loss: 16521.367\n",
      "[433,   160] loss: 36716.930\n",
      "[433,   165] loss: 27535.172\n",
      "[433,   170] loss: 27827.563\n",
      "[433,   175] loss: 32713.671\n",
      "[433,   180] loss: 24200.211\n",
      "[433,   185] loss: 17978.094\n",
      "[433,   190] loss: 25671.215\n",
      "[433,   195] loss: 20420.926\n",
      "[433,   200] loss: 18253.368\n",
      "[433,   205] loss: 23335.271\n",
      "[433,   210] loss: 20940.085\n",
      "[433,   215] loss: 24210.735\n",
      "[433,   220] loss: 38434.246\n",
      "[433,   225] loss: 24492.272\n",
      "[433,   230] loss: 25781.918\n",
      "[434,     5] loss: 25196.353\n",
      "[434,    10] loss: 37976.692\n",
      "[434,    15] loss: 25848.066\n",
      "[434,    20] loss: 33866.750\n",
      "[434,    25] loss: 27553.064\n",
      "[434,    30] loss: 24820.019\n",
      "[434,    35] loss: 30917.572\n",
      "[434,    40] loss: 15325.834\n",
      "[434,    45] loss: 21762.569\n",
      "[434,    50] loss: 24842.463\n",
      "[434,    55] loss: 24490.786\n",
      "[434,    60] loss: 31427.255\n",
      "[434,    65] loss: 22290.858\n",
      "[434,    70] loss: 21160.394\n",
      "[434,    75] loss: 33633.147\n",
      "[434,    80] loss: 25050.409\n",
      "[434,    85] loss: 20591.108\n",
      "[434,    90] loss: 19498.755\n",
      "[434,    95] loss: 21536.577\n",
      "[434,   100] loss: 25377.661\n",
      "[434,   105] loss: 17050.704\n",
      "[434,   110] loss: 47229.939\n",
      "[434,   115] loss: 27971.220\n",
      "[434,   120] loss: 24891.094\n",
      "[434,   125] loss: 17705.843\n",
      "[434,   130] loss: 23978.829\n",
      "[434,   135] loss: 30045.212\n",
      "[434,   140] loss: 26413.578\n",
      "[434,   145] loss: 27617.089\n",
      "[434,   150] loss: 21211.209\n",
      "[434,   155] loss: 17685.141\n",
      "[434,   160] loss: 23477.611\n",
      "[434,   165] loss: 15533.976\n",
      "[434,   170] loss: 21954.786\n",
      "[434,   175] loss: 18330.649\n",
      "[434,   180] loss: 23352.527\n",
      "[434,   185] loss: 27112.831\n",
      "[434,   190] loss: 23676.679\n",
      "[434,   195] loss: 25946.980\n",
      "[434,   200] loss: 21563.315\n",
      "[434,   205] loss: 31910.452\n",
      "[434,   210] loss: 22078.024\n",
      "[434,   215] loss: 32745.014\n",
      "[434,   220] loss: 21456.604\n",
      "[434,   225] loss: 36575.201\n",
      "[434,   230] loss: 29022.678\n",
      "[435,     5] loss: 40852.720\n",
      "[435,    10] loss: 18901.229\n",
      "[435,    15] loss: 28970.606\n",
      "[435,    20] loss: 23501.894\n",
      "[435,    25] loss: 20116.845\n",
      "[435,    30] loss: 36623.312\n",
      "[435,    35] loss: 14792.119\n",
      "[435,    40] loss: 21981.252\n",
      "[435,    45] loss: 19969.232\n",
      "[435,    50] loss: 17940.650\n",
      "[435,    55] loss: 25248.873\n",
      "[435,    60] loss: 23631.391\n",
      "[435,    65] loss: 23003.600\n",
      "[435,    70] loss: 32905.962\n",
      "[435,    75] loss: 26288.687\n",
      "[435,    80] loss: 16900.928\n",
      "[435,    85] loss: 22741.077\n",
      "[435,    90] loss: 21281.056\n",
      "[435,    95] loss: 27703.216\n",
      "[435,   100] loss: 24516.249\n",
      "[435,   105] loss: 29308.529\n",
      "[435,   110] loss: 32386.359\n",
      "[435,   115] loss: 31568.702\n",
      "[435,   120] loss: 22970.982\n",
      "[435,   125] loss: 26527.371\n",
      "[435,   130] loss: 15564.515\n",
      "[435,   135] loss: 27518.079\n",
      "[435,   140] loss: 31458.961\n",
      "[435,   145] loss: 25568.550\n",
      "[435,   150] loss: 18973.915\n",
      "[435,   155] loss: 34330.301\n",
      "[435,   160] loss: 19635.752\n",
      "[435,   165] loss: 30209.822\n",
      "[435,   170] loss: 36993.627\n",
      "[435,   175] loss: 25715.503\n",
      "[435,   180] loss: 28558.345\n",
      "[435,   185] loss: 20606.015\n",
      "[435,   190] loss: 25305.962\n",
      "[435,   195] loss: 29909.363\n",
      "[435,   200] loss: 24444.398\n",
      "[435,   205] loss: 28796.012\n",
      "[435,   210] loss: 28104.736\n",
      "[435,   215] loss: 23230.673\n",
      "[435,   220] loss: 27906.874\n",
      "[435,   225] loss: 24703.947\n",
      "[435,   230] loss: 19515.095\n",
      "[436,     5] loss: 30765.184\n",
      "[436,    10] loss: 32853.192\n",
      "[436,    15] loss: 21532.971\n",
      "[436,    20] loss: 19770.759\n",
      "[436,    25] loss: 26711.050\n",
      "[436,    30] loss: 22296.734\n",
      "[436,    35] loss: 21300.435\n",
      "[436,    40] loss: 21490.161\n",
      "[436,    45] loss: 20167.781\n",
      "[436,    50] loss: 51272.461\n",
      "[436,    55] loss: 33147.511\n",
      "[436,    60] loss: 25091.905\n",
      "[436,    65] loss: 29613.294\n",
      "[436,    70] loss: 21021.395\n",
      "[436,    75] loss: 27185.555\n",
      "[436,    80] loss: 24745.711\n",
      "[436,    85] loss: 33391.189\n",
      "[436,    90] loss: 18857.906\n",
      "[436,    95] loss: 26771.698\n",
      "[436,   100] loss: 27876.577\n",
      "[436,   105] loss: 13741.270\n",
      "[436,   110] loss: 24656.374\n",
      "[436,   115] loss: 17371.242\n",
      "[436,   120] loss: 37413.589\n",
      "[436,   125] loss: 26431.559\n",
      "[436,   130] loss: 25606.509\n",
      "[436,   135] loss: 14699.820\n",
      "[436,   140] loss: 47845.855\n",
      "[436,   145] loss: 18148.287\n",
      "[436,   150] loss: 23589.654\n",
      "[436,   155] loss: 24067.363\n",
      "[436,   160] loss: 24640.843\n",
      "[436,   165] loss: 15636.053\n",
      "[436,   170] loss: 24261.088\n",
      "[436,   175] loss: 17663.494\n",
      "[436,   180] loss: 27288.048\n",
      "[436,   185] loss: 23750.199\n",
      "[436,   190] loss: 30703.622\n",
      "[436,   195] loss: 20041.778\n",
      "[436,   200] loss: 22933.078\n",
      "[436,   205] loss: 31504.107\n",
      "[436,   210] loss: 23701.882\n",
      "[436,   215] loss: 23941.303\n",
      "[436,   220] loss: 15322.690\n",
      "[436,   225] loss: 26754.914\n",
      "[436,   230] loss: 32270.450\n",
      "[437,     5] loss: 19425.296\n",
      "[437,    10] loss: 15333.909\n",
      "[437,    15] loss: 21546.958\n",
      "[437,    20] loss: 16971.700\n",
      "[437,    25] loss: 17184.716\n",
      "[437,    30] loss: 25225.148\n",
      "[437,    35] loss: 25608.874\n",
      "[437,    40] loss: 23933.452\n",
      "[437,    45] loss: 18007.009\n",
      "[437,    50] loss: 35414.167\n",
      "[437,    55] loss: 25597.223\n",
      "[437,    60] loss: 21046.198\n",
      "[437,    65] loss: 19718.999\n",
      "[437,    70] loss: 16128.432\n",
      "[437,    75] loss: 26215.106\n",
      "[437,    80] loss: 21412.358\n",
      "[437,    85] loss: 25477.717\n",
      "[437,    90] loss: 24098.045\n",
      "[437,    95] loss: 27515.733\n",
      "[437,   100] loss: 26184.443\n",
      "[437,   105] loss: 29939.268\n",
      "[437,   110] loss: 23923.151\n",
      "[437,   115] loss: 43494.640\n",
      "[437,   120] loss: 28109.932\n",
      "[437,   125] loss: 14722.761\n",
      "[437,   130] loss: 19888.740\n",
      "[437,   135] loss: 32948.897\n",
      "[437,   140] loss: 23428.257\n",
      "[437,   145] loss: 27408.181\n",
      "[437,   150] loss: 22697.624\n",
      "[437,   155] loss: 53022.091\n",
      "[437,   160] loss: 21537.888\n",
      "[437,   165] loss: 30358.759\n",
      "[437,   170] loss: 24869.662\n",
      "[437,   175] loss: 39490.200\n",
      "[437,   180] loss: 32074.307\n",
      "[437,   185] loss: 27484.263\n",
      "[437,   190] loss: 23191.618\n",
      "[437,   195] loss: 20571.796\n",
      "[437,   200] loss: 34260.828\n",
      "[437,   205] loss: 18375.848\n",
      "[437,   210] loss: 16847.025\n",
      "[437,   215] loss: 26337.760\n",
      "[437,   220] loss: 31005.204\n",
      "[437,   225] loss: 33583.310\n",
      "[437,   230] loss: 22988.664\n",
      "[438,     5] loss: 22503.758\n",
      "[438,    10] loss: 22968.939\n",
      "[438,    15] loss: 19111.676\n",
      "[438,    20] loss: 24538.994\n",
      "[438,    25] loss: 16220.137\n",
      "[438,    30] loss: 20902.654\n",
      "[438,    35] loss: 26600.480\n",
      "[438,    40] loss: 29214.729\n",
      "[438,    45] loss: 41630.852\n",
      "[438,    50] loss: 25569.022\n",
      "[438,    55] loss: 26913.971\n",
      "[438,    60] loss: 27355.281\n",
      "[438,    65] loss: 19327.274\n",
      "[438,    70] loss: 33740.816\n",
      "[438,    75] loss: 24392.549\n",
      "[438,    80] loss: 34900.404\n",
      "[438,    85] loss: 25323.643\n",
      "[438,    90] loss: 33163.023\n",
      "[438,    95] loss: 27631.212\n",
      "[438,   100] loss: 49832.004\n",
      "[438,   105] loss: 25561.510\n",
      "[438,   110] loss: 24308.449\n",
      "[438,   115] loss: 21239.374\n",
      "[438,   120] loss: 25820.423\n",
      "[438,   125] loss: 28348.099\n",
      "[438,   130] loss: 18531.296\n",
      "[438,   135] loss: 24060.468\n",
      "[438,   140] loss: 24854.853\n",
      "[438,   145] loss: 24727.589\n",
      "[438,   150] loss: 11945.264\n",
      "[438,   155] loss: 19526.265\n",
      "[438,   160] loss: 22338.320\n",
      "[438,   165] loss: 17713.497\n",
      "[438,   170] loss: 24622.654\n",
      "[438,   175] loss: 21549.965\n",
      "[438,   180] loss: 24607.124\n",
      "[438,   185] loss: 26911.630\n",
      "[438,   190] loss: 23456.775\n",
      "[438,   195] loss: 22795.207\n",
      "[438,   200] loss: 23609.660\n",
      "[438,   205] loss: 25491.481\n",
      "[438,   210] loss: 36786.182\n",
      "[438,   215] loss: 27003.518\n",
      "[438,   220] loss: 29378.720\n",
      "[438,   225] loss: 24411.922\n",
      "[438,   230] loss: 19527.562\n",
      "[439,     5] loss: 26559.599\n",
      "[439,    10] loss: 29566.630\n",
      "[439,    15] loss: 24025.143\n",
      "[439,    20] loss: 28355.230\n",
      "[439,    25] loss: 28474.411\n",
      "[439,    30] loss: 18383.697\n",
      "[439,    35] loss: 24482.772\n",
      "[439,    40] loss: 31451.320\n",
      "[439,    45] loss: 36370.489\n",
      "[439,    50] loss: 17223.405\n",
      "[439,    55] loss: 43583.607\n",
      "[439,    60] loss: 28747.715\n",
      "[439,    65] loss: 24211.089\n",
      "[439,    70] loss: 29721.368\n",
      "[439,    75] loss: 14669.469\n",
      "[439,    80] loss: 15157.349\n",
      "[439,    85] loss: 25239.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[439,    90] loss: 32905.511\n",
      "[439,    95] loss: 20697.604\n",
      "[439,   100] loss: 16772.983\n",
      "[439,   105] loss: 50124.435\n",
      "[439,   110] loss: 15826.211\n",
      "[439,   115] loss: 18444.507\n",
      "[439,   120] loss: 18348.628\n",
      "[439,   125] loss: 23847.318\n",
      "[439,   130] loss: 23384.602\n",
      "[439,   135] loss: 23484.484\n",
      "[439,   140] loss: 24262.073\n",
      "[439,   145] loss: 23621.055\n",
      "[439,   150] loss: 23561.440\n",
      "[439,   155] loss: 16449.078\n",
      "[439,   160] loss: 29661.453\n",
      "[439,   165] loss: 39712.848\n",
      "[439,   170] loss: 21792.398\n",
      "[439,   175] loss: 27300.491\n",
      "[439,   180] loss: 28322.328\n",
      "[439,   185] loss: 17849.408\n",
      "[439,   190] loss: 34760.655\n",
      "[439,   195] loss: 21150.258\n",
      "[439,   200] loss: 23561.976\n",
      "[439,   205] loss: 32260.013\n",
      "[439,   210] loss: 18093.409\n",
      "[439,   215] loss: 22070.236\n",
      "[439,   220] loss: 23445.960\n",
      "[439,   225] loss: 26320.460\n",
      "[439,   230] loss: 21643.444\n",
      "[440,     5] loss: 16135.691\n",
      "[440,    10] loss: 26214.594\n",
      "[440,    15] loss: 23656.301\n",
      "[440,    20] loss: 24826.514\n",
      "[440,    25] loss: 17570.735\n",
      "[440,    30] loss: 20990.650\n",
      "[440,    35] loss: 18741.971\n",
      "[440,    40] loss: 16275.652\n",
      "[440,    45] loss: 27709.675\n",
      "[440,    50] loss: 25931.718\n",
      "[440,    55] loss: 43643.132\n",
      "[440,    60] loss: 18224.383\n",
      "[440,    65] loss: 29018.589\n",
      "[440,    70] loss: 24860.639\n",
      "[440,    75] loss: 22623.154\n",
      "[440,    80] loss: 27647.530\n",
      "[440,    85] loss: 31705.361\n",
      "[440,    90] loss: 32121.647\n",
      "[440,    95] loss: 28998.470\n",
      "[440,   100] loss: 30922.246\n",
      "[440,   105] loss: 29590.177\n",
      "[440,   110] loss: 29508.323\n",
      "[440,   115] loss: 31359.158\n",
      "[440,   120] loss: 18602.736\n",
      "[440,   125] loss: 27228.343\n",
      "[440,   130] loss: 29473.054\n",
      "[440,   135] loss: 27075.899\n",
      "[440,   140] loss: 33669.026\n",
      "[440,   145] loss: 32068.841\n",
      "[440,   150] loss: 14729.237\n",
      "[440,   155] loss: 23544.311\n",
      "[440,   160] loss: 20814.824\n",
      "[440,   165] loss: 32660.159\n",
      "[440,   170] loss: 16969.357\n",
      "[440,   175] loss: 20290.721\n",
      "[440,   180] loss: 21891.897\n",
      "[440,   185] loss: 35902.058\n",
      "[440,   190] loss: 39487.057\n",
      "[440,   195] loss: 23604.248\n",
      "[440,   200] loss: 32190.895\n",
      "[440,   205] loss: 16725.866\n",
      "[440,   210] loss: 23272.846\n",
      "[440,   215] loss: 19817.476\n",
      "[440,   220] loss: 16623.300\n",
      "[440,   225] loss: 24386.253\n",
      "[440,   230] loss: 23789.354\n",
      "[441,     5] loss: 22069.338\n",
      "[441,    10] loss: 19984.709\n",
      "[441,    15] loss: 23881.446\n",
      "[441,    20] loss: 29108.819\n",
      "[441,    25] loss: 17593.820\n",
      "[441,    30] loss: 21906.144\n",
      "[441,    35] loss: 23945.833\n",
      "[441,    40] loss: 23624.561\n",
      "[441,    45] loss: 24030.570\n",
      "[441,    50] loss: 17306.031\n",
      "[441,    55] loss: 26618.965\n",
      "[441,    60] loss: 26210.844\n",
      "[441,    65] loss: 33885.465\n",
      "[441,    70] loss: 21762.548\n",
      "[441,    75] loss: 50936.241\n",
      "[441,    80] loss: 24268.705\n",
      "[441,    85] loss: 22371.446\n",
      "[441,    90] loss: 17171.909\n",
      "[441,    95] loss: 29410.675\n",
      "[441,   100] loss: 16615.790\n",
      "[441,   105] loss: 29628.176\n",
      "[441,   110] loss: 27419.492\n",
      "[441,   115] loss: 23784.330\n",
      "[441,   120] loss: 33163.509\n",
      "[441,   125] loss: 13511.748\n",
      "[441,   130] loss: 18753.184\n",
      "[441,   135] loss: 25722.842\n",
      "[441,   140] loss: 22347.969\n",
      "[441,   145] loss: 32255.201\n",
      "[441,   150] loss: 22195.304\n",
      "[441,   155] loss: 26974.493\n",
      "[441,   160] loss: 26439.966\n",
      "[441,   165] loss: 23325.599\n",
      "[441,   170] loss: 16992.186\n",
      "[441,   175] loss: 26704.296\n",
      "[441,   180] loss: 27274.630\n",
      "[441,   185] loss: 34667.276\n",
      "[441,   190] loss: 23359.486\n",
      "[441,   195] loss: 30997.681\n",
      "[441,   200] loss: 26996.038\n",
      "[441,   205] loss: 22046.489\n",
      "[441,   210] loss: 41338.309\n",
      "[441,   215] loss: 26160.573\n",
      "[441,   220] loss: 27351.086\n",
      "[441,   225] loss: 17067.470\n",
      "[441,   230] loss: 24498.827\n",
      "[442,     5] loss: 22486.343\n",
      "[442,    10] loss: 25006.738\n",
      "[442,    15] loss: 25729.943\n",
      "[442,    20] loss: 34377.128\n",
      "[442,    25] loss: 16713.703\n",
      "[442,    30] loss: 37939.291\n",
      "[442,    35] loss: 28689.620\n",
      "[442,    40] loss: 25438.569\n",
      "[442,    45] loss: 24234.877\n",
      "[442,    50] loss: 23097.845\n",
      "[442,    55] loss: 27123.994\n",
      "[442,    60] loss: 22706.636\n",
      "[442,    65] loss: 28647.197\n",
      "[442,    70] loss: 25616.443\n",
      "[442,    75] loss: 25020.727\n",
      "[442,    80] loss: 17747.043\n",
      "[442,    85] loss: 19909.990\n",
      "[442,    90] loss: 21737.822\n",
      "[442,    95] loss: 25869.796\n",
      "[442,   100] loss: 28287.863\n",
      "[442,   105] loss: 28909.956\n",
      "[442,   110] loss: 24377.860\n",
      "[442,   115] loss: 16441.929\n",
      "[442,   120] loss: 28898.548\n",
      "[442,   125] loss: 16606.130\n",
      "[442,   130] loss: 49130.651\n",
      "[442,   135] loss: 25557.498\n",
      "[442,   140] loss: 26660.417\n",
      "[442,   145] loss: 21628.112\n",
      "[442,   150] loss: 22571.955\n",
      "[442,   155] loss: 25351.350\n",
      "[442,   160] loss: 23940.722\n",
      "[442,   165] loss: 29018.666\n",
      "[442,   170] loss: 26221.188\n",
      "[442,   175] loss: 26058.009\n",
      "[442,   180] loss: 21820.396\n",
      "[442,   185] loss: 43620.508\n",
      "[442,   190] loss: 20885.362\n",
      "[442,   195] loss: 21624.658\n",
      "[442,   200] loss: 20564.857\n",
      "[442,   205] loss: 19752.851\n",
      "[442,   210] loss: 21514.575\n",
      "[442,   215] loss: 26265.310\n",
      "[442,   220] loss: 32684.315\n",
      "[442,   225] loss: 14963.305\n",
      "[442,   230] loss: 27327.568\n",
      "[443,     5] loss: 23376.785\n",
      "[443,    10] loss: 31674.954\n",
      "[443,    15] loss: 27992.273\n",
      "[443,    20] loss: 20778.744\n",
      "[443,    25] loss: 22855.797\n",
      "[443,    30] loss: 38202.158\n",
      "[443,    35] loss: 18380.680\n",
      "[443,    40] loss: 27913.319\n",
      "[443,    45] loss: 23726.413\n",
      "[443,    50] loss: 41657.311\n",
      "[443,    55] loss: 30918.025\n",
      "[443,    60] loss: 19152.070\n",
      "[443,    65] loss: 25688.946\n",
      "[443,    70] loss: 12354.229\n",
      "[443,    75] loss: 33673.422\n",
      "[443,    80] loss: 19307.449\n",
      "[443,    85] loss: 24756.318\n",
      "[443,    90] loss: 26202.349\n",
      "[443,    95] loss: 20956.479\n",
      "[443,   100] loss: 32913.169\n",
      "[443,   105] loss: 23826.119\n",
      "[443,   110] loss: 20582.266\n",
      "[443,   115] loss: 37030.494\n",
      "[443,   120] loss: 27251.993\n",
      "[443,   125] loss: 22008.774\n",
      "[443,   130] loss: 19434.837\n",
      "[443,   135] loss: 26179.960\n",
      "[443,   140] loss: 20640.198\n",
      "[443,   145] loss: 26315.023\n",
      "[443,   150] loss: 23078.448\n",
      "[443,   155] loss: 17166.400\n",
      "[443,   160] loss: 25775.067\n",
      "[443,   165] loss: 30168.101\n",
      "[443,   170] loss: 22728.477\n",
      "[443,   175] loss: 29046.737\n",
      "[443,   180] loss: 23000.502\n",
      "[443,   185] loss: 25803.189\n",
      "[443,   190] loss: 20204.369\n",
      "[443,   195] loss: 21513.484\n",
      "[443,   200] loss: 26977.994\n",
      "[443,   205] loss: 47571.862\n",
      "[443,   210] loss: 23457.941\n",
      "[443,   215] loss: 15260.058\n",
      "[443,   220] loss: 27447.037\n",
      "[443,   225] loss: 26487.328\n",
      "[443,   230] loss: 19909.528\n",
      "[444,     5] loss: 31050.626\n",
      "[444,    10] loss: 33843.605\n",
      "[444,    15] loss: 27855.300\n",
      "[444,    20] loss: 19998.438\n",
      "[444,    25] loss: 34061.360\n",
      "[444,    30] loss: 21173.978\n",
      "[444,    35] loss: 24385.332\n",
      "[444,    40] loss: 23443.960\n",
      "[444,    45] loss: 23478.852\n",
      "[444,    50] loss: 30980.688\n",
      "[444,    55] loss: 21029.074\n",
      "[444,    60] loss: 30803.283\n",
      "[444,    65] loss: 26914.404\n",
      "[444,    70] loss: 23227.678\n",
      "[444,    75] loss: 18820.283\n",
      "[444,    80] loss: 24534.649\n",
      "[444,    85] loss: 40907.421\n",
      "[444,    90] loss: 24483.379\n",
      "[444,    95] loss: 17589.258\n",
      "[444,   100] loss: 19956.547\n",
      "[444,   105] loss: 32073.793\n",
      "[444,   110] loss: 22069.439\n",
      "[444,   115] loss: 17457.525\n",
      "[444,   120] loss: 20181.447\n",
      "[444,   125] loss: 35153.543\n",
      "[444,   130] loss: 40145.224\n",
      "[444,   135] loss: 25382.342\n",
      "[444,   140] loss: 21431.951\n",
      "[444,   145] loss: 18247.328\n",
      "[444,   150] loss: 22150.766\n",
      "[444,   155] loss: 22677.861\n",
      "[444,   160] loss: 16305.866\n",
      "[444,   165] loss: 18411.485\n",
      "[444,   170] loss: 27016.736\n",
      "[444,   175] loss: 27966.183\n",
      "[444,   180] loss: 25093.255\n",
      "[444,   185] loss: 18262.793\n",
      "[444,   190] loss: 28497.394\n",
      "[444,   195] loss: 23916.736\n",
      "[444,   200] loss: 30419.083\n",
      "[444,   205] loss: 40152.414\n",
      "[444,   210] loss: 22285.944\n",
      "[444,   215] loss: 31443.201\n",
      "[444,   220] loss: 20876.355\n",
      "[444,   225] loss: 29976.960\n",
      "[444,   230] loss: 18138.500\n",
      "[445,     5] loss: 23033.272\n",
      "[445,    10] loss: 24089.973\n",
      "[445,    15] loss: 16276.326\n",
      "[445,    20] loss: 23112.990\n",
      "[445,    25] loss: 24356.600\n",
      "[445,    30] loss: 13508.336\n",
      "[445,    35] loss: 20099.134\n",
      "[445,    40] loss: 26998.945\n",
      "[445,    45] loss: 23870.618\n",
      "[445,    50] loss: 30556.310\n",
      "[445,    55] loss: 18886.262\n",
      "[445,    60] loss: 40545.637\n",
      "[445,    65] loss: 18922.647\n",
      "[445,    70] loss: 26758.193\n",
      "[445,    75] loss: 25649.414\n",
      "[445,    80] loss: 40493.025\n",
      "[445,    85] loss: 28183.684\n",
      "[445,    90] loss: 27021.739\n",
      "[445,    95] loss: 32370.050\n",
      "[445,   100] loss: 22951.996\n",
      "[445,   105] loss: 23819.606\n",
      "[445,   110] loss: 22842.425\n",
      "[445,   115] loss: 20461.938\n",
      "[445,   120] loss: 29402.379\n",
      "[445,   125] loss: 26537.160\n",
      "[445,   130] loss: 26947.890\n",
      "[445,   135] loss: 27231.975\n",
      "[445,   140] loss: 20591.521\n",
      "[445,   145] loss: 20846.614\n",
      "[445,   150] loss: 23331.859\n",
      "[445,   155] loss: 19388.534\n",
      "[445,   160] loss: 35666.402\n",
      "[445,   165] loss: 42586.859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[445,   170] loss: 17729.023\n",
      "[445,   175] loss: 23234.359\n",
      "[445,   180] loss: 27674.256\n",
      "[445,   185] loss: 26100.381\n",
      "[445,   190] loss: 22885.678\n",
      "[445,   195] loss: 18855.609\n",
      "[445,   200] loss: 52056.919\n",
      "[445,   205] loss: 16713.911\n",
      "[445,   210] loss: 29137.172\n",
      "[445,   215] loss: 30627.488\n",
      "[445,   220] loss: 19627.375\n",
      "[445,   225] loss: 23716.519\n",
      "[445,   230] loss: 23664.512\n",
      "[446,     5] loss: 42842.511\n",
      "[446,    10] loss: 22222.655\n",
      "[446,    15] loss: 19515.206\n",
      "[446,    20] loss: 38017.080\n",
      "[446,    25] loss: 49379.687\n",
      "[446,    30] loss: 28456.439\n",
      "[446,    35] loss: 17483.105\n",
      "[446,    40] loss: 22026.503\n",
      "[446,    45] loss: 23042.476\n",
      "[446,    50] loss: 28851.543\n",
      "[446,    55] loss: 20638.305\n",
      "[446,    60] loss: 18724.476\n",
      "[446,    65] loss: 16642.432\n",
      "[446,    70] loss: 22122.504\n",
      "[446,    75] loss: 19006.086\n",
      "[446,    80] loss: 21040.072\n",
      "[446,    85] loss: 24126.010\n",
      "[446,    90] loss: 20534.831\n",
      "[446,    95] loss: 16734.173\n",
      "[446,   100] loss: 21190.392\n",
      "[446,   105] loss: 18316.393\n",
      "[446,   110] loss: 30220.123\n",
      "[446,   115] loss: 23950.624\n",
      "[446,   120] loss: 33214.086\n",
      "[446,   125] loss: 18333.373\n",
      "[446,   130] loss: 25700.167\n",
      "[446,   135] loss: 33130.026\n",
      "[446,   140] loss: 23536.822\n",
      "[446,   145] loss: 26596.032\n",
      "[446,   150] loss: 18194.862\n",
      "[446,   155] loss: 16798.125\n",
      "[446,   160] loss: 24489.732\n",
      "[446,   165] loss: 19549.897\n",
      "[446,   170] loss: 24587.906\n",
      "[446,   175] loss: 25561.977\n",
      "[446,   180] loss: 32117.731\n",
      "[446,   185] loss: 21024.621\n",
      "[446,   190] loss: 24416.884\n",
      "[446,   195] loss: 23230.026\n",
      "[446,   200] loss: 22664.150\n",
      "[446,   205] loss: 34962.070\n",
      "[446,   210] loss: 24155.828\n",
      "[446,   215] loss: 32738.710\n",
      "[446,   220] loss: 25764.368\n",
      "[446,   225] loss: 28901.659\n",
      "[446,   230] loss: 49284.330\n",
      "[447,     5] loss: 17781.582\n",
      "[447,    10] loss: 37356.663\n",
      "[447,    15] loss: 40423.681\n",
      "[447,    20] loss: 23473.265\n",
      "[447,    25] loss: 11779.393\n",
      "[447,    30] loss: 20329.509\n",
      "[447,    35] loss: 21389.961\n",
      "[447,    40] loss: 32284.028\n",
      "[447,    45] loss: 20441.260\n",
      "[447,    50] loss: 19577.440\n",
      "[447,    55] loss: 26796.366\n",
      "[447,    60] loss: 23443.468\n",
      "[447,    65] loss: 25989.559\n",
      "[447,    70] loss: 27755.712\n",
      "[447,    75] loss: 16127.057\n",
      "[447,    80] loss: 23445.111\n",
      "[447,    85] loss: 23535.705\n",
      "[447,    90] loss: 19305.120\n",
      "[447,    95] loss: 28207.911\n",
      "[447,   100] loss: 26238.068\n",
      "[447,   105] loss: 30268.488\n",
      "[447,   110] loss: 22441.835\n",
      "[447,   115] loss: 20807.223\n",
      "[447,   120] loss: 29201.292\n",
      "[447,   125] loss: 24114.080\n",
      "[447,   130] loss: 19355.692\n",
      "[447,   135] loss: 34031.540\n",
      "[447,   140] loss: 26542.514\n",
      "[447,   145] loss: 20606.790\n",
      "[447,   150] loss: 27293.440\n",
      "[447,   155] loss: 21984.510\n",
      "[447,   160] loss: 31869.047\n",
      "[447,   165] loss: 26214.119\n",
      "[447,   170] loss: 16837.354\n",
      "[447,   175] loss: 24515.803\n",
      "[447,   180] loss: 24208.211\n",
      "[447,   185] loss: 24875.284\n",
      "[447,   190] loss: 31654.805\n",
      "[447,   195] loss: 27278.666\n",
      "[447,   200] loss: 22683.197\n",
      "[447,   205] loss: 29553.239\n",
      "[447,   210] loss: 24908.136\n",
      "[447,   215] loss: 34252.014\n",
      "[447,   220] loss: 25594.561\n",
      "[447,   225] loss: 28602.778\n",
      "[447,   230] loss: 30583.660\n",
      "[448,     5] loss: 17376.436\n",
      "[448,    10] loss: 29715.438\n",
      "[448,    15] loss: 19344.233\n",
      "[448,    20] loss: 22459.076\n",
      "[448,    25] loss: 20651.619\n",
      "[448,    30] loss: 30025.295\n",
      "[448,    35] loss: 31624.084\n",
      "[448,    40] loss: 21450.001\n",
      "[448,    45] loss: 31621.737\n",
      "[448,    50] loss: 18261.036\n",
      "[448,    55] loss: 24543.162\n",
      "[448,    60] loss: 20661.432\n",
      "[448,    65] loss: 25092.003\n",
      "[448,    70] loss: 29423.436\n",
      "[448,    75] loss: 18651.279\n",
      "[448,    80] loss: 19384.188\n",
      "[448,    85] loss: 35445.946\n",
      "[448,    90] loss: 34758.945\n",
      "[448,    95] loss: 39439.694\n",
      "[448,   100] loss: 23621.822\n",
      "[448,   105] loss: 18539.829\n",
      "[448,   110] loss: 31241.812\n",
      "[448,   115] loss: 26690.805\n",
      "[448,   120] loss: 29862.872\n",
      "[448,   125] loss: 15445.821\n",
      "[448,   130] loss: 35804.688\n",
      "[448,   135] loss: 20055.472\n",
      "[448,   140] loss: 34758.156\n",
      "[448,   145] loss: 27449.050\n",
      "[448,   150] loss: 21254.258\n",
      "[448,   155] loss: 21127.954\n",
      "[448,   160] loss: 35288.644\n",
      "[448,   165] loss: 19471.096\n",
      "[448,   170] loss: 27979.452\n",
      "[448,   175] loss: 27959.764\n",
      "[448,   180] loss: 24797.224\n",
      "[448,   185] loss: 15278.655\n",
      "[448,   190] loss: 20827.676\n",
      "[448,   195] loss: 23923.616\n",
      "[448,   200] loss: 20127.961\n",
      "[448,   205] loss: 32408.141\n",
      "[448,   210] loss: 24115.166\n",
      "[448,   215] loss: 23290.711\n",
      "[448,   220] loss: 24852.787\n",
      "[448,   225] loss: 34376.487\n",
      "[448,   230] loss: 17725.491\n",
      "[449,     5] loss: 25199.046\n",
      "[449,    10] loss: 18401.850\n",
      "[449,    15] loss: 25778.169\n",
      "[449,    20] loss: 25281.958\n",
      "[449,    25] loss: 22390.461\n",
      "[449,    30] loss: 31228.991\n",
      "[449,    35] loss: 29484.425\n",
      "[449,    40] loss: 32959.872\n",
      "[449,    45] loss: 29652.929\n",
      "[449,    50] loss: 31712.896\n",
      "[449,    55] loss: 21702.620\n",
      "[449,    60] loss: 30261.785\n",
      "[449,    65] loss: 23469.937\n",
      "[449,    70] loss: 26048.057\n",
      "[449,    75] loss: 33358.392\n",
      "[449,    80] loss: 20295.567\n",
      "[449,    85] loss: 21675.652\n",
      "[449,    90] loss: 20413.327\n",
      "[449,    95] loss: 18596.183\n",
      "[449,   100] loss: 21183.718\n",
      "[449,   105] loss: 22855.404\n",
      "[449,   110] loss: 14597.167\n",
      "[449,   115] loss: 28121.239\n",
      "[449,   120] loss: 18453.247\n",
      "[449,   125] loss: 22416.437\n",
      "[449,   130] loss: 26784.474\n",
      "[449,   135] loss: 32299.122\n",
      "[449,   140] loss: 27522.943\n",
      "[449,   145] loss: 27091.259\n",
      "[449,   150] loss: 33221.230\n",
      "[449,   155] loss: 22575.365\n",
      "[449,   160] loss: 24279.860\n",
      "[449,   165] loss: 14058.139\n",
      "[449,   170] loss: 21712.359\n",
      "[449,   175] loss: 48567.160\n",
      "[449,   180] loss: 21900.893\n",
      "[449,   185] loss: 28826.014\n",
      "[449,   190] loss: 27559.518\n",
      "[449,   195] loss: 27442.870\n",
      "[449,   200] loss: 24465.539\n",
      "[449,   205] loss: 22082.052\n",
      "[449,   210] loss: 35262.892\n",
      "[449,   215] loss: 24554.271\n",
      "[449,   220] loss: 23934.087\n",
      "[449,   225] loss: 23633.160\n",
      "[449,   230] loss: 19001.339\n",
      "[450,     5] loss: 28369.066\n",
      "[450,    10] loss: 33519.484\n",
      "[450,    15] loss: 32053.421\n",
      "[450,    20] loss: 16173.796\n",
      "[450,    25] loss: 23067.433\n",
      "[450,    30] loss: 26166.473\n",
      "[450,    35] loss: 26594.763\n",
      "[450,    40] loss: 22309.276\n",
      "[450,    45] loss: 27984.866\n",
      "[450,    50] loss: 33221.259\n",
      "[450,    55] loss: 26872.388\n",
      "[450,    60] loss: 24593.961\n",
      "[450,    65] loss: 22473.474\n",
      "[450,    70] loss: 25954.678\n",
      "[450,    75] loss: 19314.026\n",
      "[450,    80] loss: 27306.976\n",
      "[450,    85] loss: 21337.378\n",
      "[450,    90] loss: 29730.665\n",
      "[450,    95] loss: 22256.802\n",
      "[450,   100] loss: 19140.041\n",
      "[450,   105] loss: 23480.509\n",
      "[450,   110] loss: 20753.827\n",
      "[450,   115] loss: 19002.731\n",
      "[450,   120] loss: 19267.786\n",
      "[450,   125] loss: 31821.067\n",
      "[450,   130] loss: 13527.780\n",
      "[450,   135] loss: 20675.026\n",
      "[450,   140] loss: 24341.542\n",
      "[450,   145] loss: 18731.430\n",
      "[450,   150] loss: 32458.585\n",
      "[450,   155] loss: 23378.146\n",
      "[450,   160] loss: 19273.217\n",
      "[450,   165] loss: 19414.561\n",
      "[450,   170] loss: 29210.314\n",
      "[450,   175] loss: 24594.299\n",
      "[450,   180] loss: 30740.787\n",
      "[450,   185] loss: 20507.033\n",
      "[450,   190] loss: 18843.157\n",
      "[450,   195] loss: 24379.162\n",
      "[450,   200] loss: 29777.875\n",
      "[450,   205] loss: 30499.128\n",
      "[450,   210] loss: 32116.565\n",
      "[450,   215] loss: 37560.012\n",
      "[450,   220] loss: 18182.143\n",
      "[450,   225] loss: 53738.017\n",
      "[450,   230] loss: 24260.596\n",
      "[451,     5] loss: 25470.231\n",
      "[451,    10] loss: 39455.446\n",
      "[451,    15] loss: 33003.175\n",
      "[451,    20] loss: 19464.556\n",
      "[451,    25] loss: 30288.825\n",
      "[451,    30] loss: 17443.147\n",
      "[451,    35] loss: 22160.729\n",
      "[451,    40] loss: 34502.790\n",
      "[451,    45] loss: 19080.580\n",
      "[451,    50] loss: 26287.537\n",
      "[451,    55] loss: 15820.032\n",
      "[451,    60] loss: 21100.513\n",
      "[451,    65] loss: 28288.929\n",
      "[451,    70] loss: 29792.035\n",
      "[451,    75] loss: 26601.145\n",
      "[451,    80] loss: 18448.637\n",
      "[451,    85] loss: 29700.509\n",
      "[451,    90] loss: 23855.882\n",
      "[451,    95] loss: 28497.970\n",
      "[451,   100] loss: 26381.994\n",
      "[451,   105] loss: 38801.632\n",
      "[451,   110] loss: 15020.598\n",
      "[451,   115] loss: 24033.896\n",
      "[451,   120] loss: 33692.457\n",
      "[451,   125] loss: 29477.270\n",
      "[451,   130] loss: 21438.772\n",
      "[451,   135] loss: 24542.921\n",
      "[451,   140] loss: 22836.763\n",
      "[451,   145] loss: 20316.987\n",
      "[451,   150] loss: 20369.874\n",
      "[451,   155] loss: 30113.949\n",
      "[451,   160] loss: 26898.804\n",
      "[451,   165] loss: 19311.293\n",
      "[451,   170] loss: 25305.692\n",
      "[451,   175] loss: 20794.232\n",
      "[451,   180] loss: 26777.712\n",
      "[451,   185] loss: 25661.220\n",
      "[451,   190] loss: 17534.332\n",
      "[451,   195] loss: 27521.374\n",
      "[451,   200] loss: 23047.578\n",
      "[451,   205] loss: 17603.348\n",
      "[451,   210] loss: 32823.363\n",
      "[451,   215] loss: 23965.912\n",
      "[451,   220] loss: 23592.800\n",
      "[451,   225] loss: 37413.880\n",
      "[451,   230] loss: 35081.123\n",
      "[452,     5] loss: 30436.631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452,    10] loss: 19806.122\n",
      "[452,    15] loss: 34721.707\n",
      "[452,    20] loss: 18513.137\n",
      "[452,    25] loss: 23429.297\n",
      "[452,    30] loss: 25807.162\n",
      "[452,    35] loss: 25575.019\n",
      "[452,    40] loss: 23574.327\n",
      "[452,    45] loss: 21786.682\n",
      "[452,    50] loss: 17409.013\n",
      "[452,    55] loss: 26002.641\n",
      "[452,    60] loss: 39103.044\n",
      "[452,    65] loss: 38076.794\n",
      "[452,    70] loss: 23686.583\n",
      "[452,    75] loss: 30092.346\n",
      "[452,    80] loss: 15591.607\n",
      "[452,    85] loss: 27830.187\n",
      "[452,    90] loss: 30523.477\n",
      "[452,    95] loss: 16595.646\n",
      "[452,   100] loss: 20387.012\n",
      "[452,   105] loss: 19854.951\n",
      "[452,   110] loss: 25441.447\n",
      "[452,   115] loss: 27799.779\n",
      "[452,   120] loss: 21309.407\n",
      "[452,   125] loss: 33943.252\n",
      "[452,   130] loss: 25891.002\n",
      "[452,   135] loss: 37617.147\n",
      "[452,   140] loss: 27869.129\n",
      "[452,   145] loss: 53741.967\n",
      "[452,   150] loss: 17251.799\n",
      "[452,   155] loss: 23809.972\n",
      "[452,   160] loss: 15267.771\n",
      "[452,   165] loss: 28319.658\n",
      "[452,   170] loss: 24110.025\n",
      "[452,   175] loss: 21921.032\n",
      "[452,   180] loss: 26253.510\n",
      "[452,   185] loss: 23975.947\n",
      "[452,   190] loss: 24404.568\n",
      "[452,   195] loss: 33275.869\n",
      "[452,   200] loss: 28272.793\n",
      "[452,   205] loss: 28188.094\n",
      "[452,   210] loss: 16652.522\n",
      "[452,   215] loss: 17980.198\n",
      "[452,   220] loss: 13928.456\n",
      "[452,   225] loss: 26149.720\n",
      "[452,   230] loss: 22673.916\n",
      "[453,     5] loss: 23234.961\n",
      "[453,    10] loss: 23540.256\n",
      "[453,    15] loss: 22075.366\n",
      "[453,    20] loss: 30088.818\n",
      "[453,    25] loss: 42166.654\n",
      "[453,    30] loss: 20299.768\n",
      "[453,    35] loss: 24867.267\n",
      "[453,    40] loss: 24426.522\n",
      "[453,    45] loss: 35184.958\n",
      "[453,    50] loss: 24989.540\n",
      "[453,    55] loss: 29070.416\n",
      "[453,    60] loss: 21411.866\n",
      "[453,    65] loss: 22234.828\n",
      "[453,    70] loss: 26679.655\n",
      "[453,    75] loss: 22629.756\n",
      "[453,    80] loss: 26389.619\n",
      "[453,    85] loss: 13982.505\n",
      "[453,    90] loss: 19067.125\n",
      "[453,    95] loss: 19612.849\n",
      "[453,   100] loss: 24186.561\n",
      "[453,   105] loss: 20025.755\n",
      "[453,   110] loss: 22508.859\n",
      "[453,   115] loss: 39499.172\n",
      "[453,   120] loss: 22142.986\n",
      "[453,   125] loss: 16877.308\n",
      "[453,   130] loss: 19857.479\n",
      "[453,   135] loss: 28279.317\n",
      "[453,   140] loss: 20086.609\n",
      "[453,   145] loss: 27633.936\n",
      "[453,   150] loss: 17206.283\n",
      "[453,   155] loss: 25315.545\n",
      "[453,   160] loss: 31338.984\n",
      "[453,   165] loss: 14452.214\n",
      "[453,   170] loss: 26058.937\n",
      "[453,   175] loss: 37502.665\n",
      "[453,   180] loss: 27049.754\n",
      "[453,   185] loss: 19711.341\n",
      "[453,   190] loss: 21808.963\n",
      "[453,   195] loss: 41641.068\n",
      "[453,   200] loss: 17942.242\n",
      "[453,   205] loss: 26225.297\n",
      "[453,   210] loss: 29605.578\n",
      "[453,   215] loss: 34285.891\n",
      "[453,   220] loss: 32914.978\n",
      "[453,   225] loss: 36239.191\n",
      "[453,   230] loss: 24728.647\n",
      "[454,     5] loss: 34122.475\n",
      "[454,    10] loss: 18372.281\n",
      "[454,    15] loss: 20065.304\n",
      "[454,    20] loss: 54019.612\n",
      "[454,    25] loss: 25449.835\n",
      "[454,    30] loss: 22419.512\n",
      "[454,    35] loss: 20544.243\n",
      "[454,    40] loss: 24559.385\n",
      "[454,    45] loss: 23398.709\n",
      "[454,    50] loss: 32610.805\n",
      "[454,    55] loss: 28783.415\n",
      "[454,    60] loss: 21493.706\n",
      "[454,    65] loss: 22942.391\n",
      "[454,    70] loss: 27928.900\n",
      "[454,    75] loss: 48778.663\n",
      "[454,    80] loss: 24238.873\n",
      "[454,    85] loss: 22755.058\n",
      "[454,    90] loss: 20429.048\n",
      "[454,    95] loss: 17209.856\n",
      "[454,   100] loss: 17129.104\n",
      "[454,   105] loss: 21502.082\n",
      "[454,   110] loss: 25080.466\n",
      "[454,   115] loss: 17273.243\n",
      "[454,   120] loss: 28249.905\n",
      "[454,   125] loss: 35986.477\n",
      "[454,   130] loss: 19798.921\n",
      "[454,   135] loss: 20984.930\n",
      "[454,   140] loss: 27362.824\n",
      "[454,   145] loss: 17474.580\n",
      "[454,   150] loss: 19877.884\n",
      "[454,   155] loss: 37112.056\n",
      "[454,   160] loss: 19404.858\n",
      "[454,   165] loss: 18524.451\n",
      "[454,   170] loss: 21857.502\n",
      "[454,   175] loss: 20386.131\n",
      "[454,   180] loss: 21890.489\n",
      "[454,   185] loss: 35659.855\n",
      "[454,   190] loss: 34862.707\n",
      "[454,   195] loss: 19895.347\n",
      "[454,   200] loss: 28002.755\n",
      "[454,   205] loss: 26096.470\n",
      "[454,   210] loss: 24867.694\n",
      "[454,   215] loss: 22440.338\n",
      "[454,   220] loss: 21746.598\n",
      "[454,   225] loss: 23226.606\n",
      "[454,   230] loss: 34413.240\n",
      "[455,     5] loss: 22474.914\n",
      "[455,    10] loss: 29029.020\n",
      "[455,    15] loss: 25173.041\n",
      "[455,    20] loss: 15138.447\n",
      "[455,    25] loss: 23654.929\n",
      "[455,    30] loss: 25764.842\n",
      "[455,    35] loss: 25164.760\n",
      "[455,    40] loss: 24683.229\n",
      "[455,    45] loss: 29735.150\n",
      "[455,    50] loss: 21191.495\n",
      "[455,    55] loss: 25068.498\n",
      "[455,    60] loss: 24177.368\n",
      "[455,    65] loss: 18381.594\n",
      "[455,    70] loss: 20022.429\n",
      "[455,    75] loss: 21366.969\n",
      "[455,    80] loss: 22119.156\n",
      "[455,    85] loss: 29054.707\n",
      "[455,    90] loss: 26913.647\n",
      "[455,    95] loss: 18524.992\n",
      "[455,   100] loss: 31667.210\n",
      "[455,   105] loss: 28798.099\n",
      "[455,   110] loss: 29269.693\n",
      "[455,   115] loss: 30788.250\n",
      "[455,   120] loss: 20250.734\n",
      "[455,   125] loss: 51570.462\n",
      "[455,   130] loss: 24455.459\n",
      "[455,   135] loss: 34547.007\n",
      "[455,   140] loss: 20926.634\n",
      "[455,   145] loss: 23522.386\n",
      "[455,   150] loss: 27105.381\n",
      "[455,   155] loss: 23603.879\n",
      "[455,   160] loss: 26703.618\n",
      "[455,   165] loss: 28949.017\n",
      "[455,   170] loss: 21982.092\n",
      "[455,   175] loss: 30068.321\n",
      "[455,   180] loss: 31465.153\n",
      "[455,   185] loss: 21087.093\n",
      "[455,   190] loss: 16285.649\n",
      "[455,   195] loss: 19821.221\n",
      "[455,   200] loss: 20997.913\n",
      "[455,   205] loss: 28374.753\n",
      "[455,   210] loss: 20404.816\n",
      "[455,   215] loss: 16896.229\n",
      "[455,   220] loss: 27679.120\n",
      "[455,   225] loss: 21222.306\n",
      "[455,   230] loss: 50186.927\n",
      "[456,     5] loss: 20479.722\n",
      "[456,    10] loss: 25020.174\n",
      "[456,    15] loss: 18566.564\n",
      "[456,    20] loss: 37980.957\n",
      "[456,    25] loss: 27233.144\n",
      "[456,    30] loss: 25619.886\n",
      "[456,    35] loss: 21571.549\n",
      "[456,    40] loss: 26179.470\n",
      "[456,    45] loss: 25818.418\n",
      "[456,    50] loss: 24391.838\n",
      "[456,    55] loss: 22279.969\n",
      "[456,    60] loss: 25067.075\n",
      "[456,    65] loss: 28665.850\n",
      "[456,    70] loss: 31837.565\n",
      "[456,    75] loss: 27622.179\n",
      "[456,    80] loss: 18810.205\n",
      "[456,    85] loss: 25046.892\n",
      "[456,    90] loss: 25446.313\n",
      "[456,    95] loss: 25746.181\n",
      "[456,   100] loss: 20322.711\n",
      "[456,   105] loss: 31681.440\n",
      "[456,   110] loss: 24991.691\n",
      "[456,   115] loss: 23935.821\n",
      "[456,   120] loss: 17817.133\n",
      "[456,   125] loss: 27979.314\n",
      "[456,   130] loss: 14883.184\n",
      "[456,   135] loss: 20608.187\n",
      "[456,   140] loss: 34696.086\n",
      "[456,   145] loss: 25934.411\n",
      "[456,   150] loss: 17646.127\n",
      "[456,   155] loss: 19634.112\n",
      "[456,   160] loss: 27324.759\n",
      "[456,   165] loss: 17770.517\n",
      "[456,   170] loss: 38094.688\n",
      "[456,   175] loss: 27880.343\n",
      "[456,   180] loss: 19469.934\n",
      "[456,   185] loss: 20425.872\n",
      "[456,   190] loss: 27287.344\n",
      "[456,   195] loss: 30761.391\n",
      "[456,   200] loss: 26303.571\n",
      "[456,   205] loss: 18309.004\n",
      "[456,   210] loss: 29822.354\n",
      "[456,   215] loss: 17956.929\n",
      "[456,   220] loss: 24942.653\n",
      "[456,   225] loss: 24672.779\n",
      "[456,   230] loss: 34633.552\n",
      "[457,     5] loss: 25647.932\n",
      "[457,    10] loss: 38767.164\n",
      "[457,    15] loss: 21980.692\n",
      "[457,    20] loss: 29939.406\n",
      "[457,    25] loss: 32372.915\n",
      "[457,    30] loss: 29892.068\n",
      "[457,    35] loss: 28394.214\n",
      "[457,    40] loss: 23611.681\n",
      "[457,    45] loss: 24851.267\n",
      "[457,    50] loss: 16920.392\n",
      "[457,    55] loss: 21223.100\n",
      "[457,    60] loss: 27055.433\n",
      "[457,    65] loss: 17205.138\n",
      "[457,    70] loss: 20696.938\n",
      "[457,    75] loss: 32076.995\n",
      "[457,    80] loss: 22578.032\n",
      "[457,    85] loss: 20775.010\n",
      "[457,    90] loss: 35880.064\n",
      "[457,    95] loss: 33585.822\n",
      "[457,   100] loss: 24332.354\n",
      "[457,   105] loss: 22418.738\n",
      "[457,   110] loss: 20383.122\n",
      "[457,   115] loss: 25388.195\n",
      "[457,   120] loss: 23300.674\n",
      "[457,   125] loss: 24322.180\n",
      "[457,   130] loss: 21714.046\n",
      "[457,   135] loss: 25622.228\n",
      "[457,   140] loss: 26995.110\n",
      "[457,   145] loss: 22373.234\n",
      "[457,   150] loss: 41786.779\n",
      "[457,   155] loss: 17783.376\n",
      "[457,   160] loss: 14684.614\n",
      "[457,   165] loss: 22770.757\n",
      "[457,   170] loss: 26026.786\n",
      "[457,   175] loss: 21817.119\n",
      "[457,   180] loss: 23342.925\n",
      "[457,   185] loss: 39936.729\n",
      "[457,   190] loss: 21579.503\n",
      "[457,   195] loss: 27152.009\n",
      "[457,   200] loss: 27317.920\n",
      "[457,   205] loss: 14412.479\n",
      "[457,   210] loss: 23445.711\n",
      "[457,   215] loss: 27252.882\n",
      "[457,   220] loss: 25455.937\n",
      "[457,   225] loss: 29517.877\n",
      "[457,   230] loss: 20030.063\n",
      "[458,     5] loss: 30987.923\n",
      "[458,    10] loss: 17363.221\n",
      "[458,    15] loss: 22469.280\n",
      "[458,    20] loss: 21943.077\n",
      "[458,    25] loss: 21424.535\n",
      "[458,    30] loss: 23883.353\n",
      "[458,    35] loss: 25107.447\n",
      "[458,    40] loss: 29484.110\n",
      "[458,    45] loss: 24718.148\n",
      "[458,    50] loss: 29250.340\n",
      "[458,    55] loss: 32346.213\n",
      "[458,    60] loss: 20541.668\n",
      "[458,    65] loss: 24180.791\n",
      "[458,    70] loss: 32697.575\n",
      "[458,    75] loss: 20176.459\n",
      "[458,    80] loss: 24475.404\n",
      "[458,    85] loss: 20048.472\n",
      "[458,    90] loss: 21742.288\n",
      "[458,    95] loss: 21286.991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[458,   100] loss: 22086.582\n",
      "[458,   105] loss: 23843.659\n",
      "[458,   110] loss: 21772.258\n",
      "[458,   115] loss: 27371.360\n",
      "[458,   120] loss: 23214.380\n",
      "[458,   125] loss: 46178.408\n",
      "[458,   130] loss: 32004.232\n",
      "[458,   135] loss: 21602.048\n",
      "[458,   140] loss: 22742.846\n",
      "[458,   145] loss: 27912.168\n",
      "[458,   150] loss: 19511.441\n",
      "[458,   155] loss: 20938.981\n",
      "[458,   160] loss: 32143.883\n",
      "[458,   165] loss: 26275.008\n",
      "[458,   170] loss: 33468.783\n",
      "[458,   175] loss: 25866.873\n",
      "[458,   180] loss: 33572.451\n",
      "[458,   185] loss: 16076.525\n",
      "[458,   190] loss: 19317.901\n",
      "[458,   195] loss: 33142.305\n",
      "[458,   200] loss: 24041.976\n",
      "[458,   205] loss: 24418.465\n",
      "[458,   210] loss: 22665.169\n",
      "[458,   215] loss: 30712.429\n",
      "[458,   220] loss: 33815.864\n",
      "[458,   225] loss: 21243.868\n",
      "[458,   230] loss: 21580.204\n",
      "[459,     5] loss: 19801.243\n",
      "[459,    10] loss: 20697.954\n",
      "[459,    15] loss: 23991.276\n",
      "[459,    20] loss: 27199.918\n",
      "[459,    25] loss: 20277.502\n",
      "[459,    30] loss: 33744.191\n",
      "[459,    35] loss: 22247.144\n",
      "[459,    40] loss: 23461.921\n",
      "[459,    45] loss: 26480.554\n",
      "[459,    50] loss: 21332.253\n",
      "[459,    55] loss: 22313.872\n",
      "[459,    60] loss: 27760.409\n",
      "[459,    65] loss: 21044.878\n",
      "[459,    70] loss: 23603.670\n",
      "[459,    75] loss: 23679.249\n",
      "[459,    80] loss: 26209.333\n",
      "[459,    85] loss: 39276.024\n",
      "[459,    90] loss: 26116.410\n",
      "[459,    95] loss: 29100.862\n",
      "[459,   100] loss: 32501.559\n",
      "[459,   105] loss: 23052.582\n",
      "[459,   110] loss: 29146.181\n",
      "[459,   115] loss: 28076.595\n",
      "[459,   120] loss: 24364.013\n",
      "[459,   125] loss: 26802.019\n",
      "[459,   130] loss: 23672.356\n",
      "[459,   135] loss: 31654.396\n",
      "[459,   140] loss: 16620.370\n",
      "[459,   145] loss: 16734.760\n",
      "[459,   150] loss: 23120.143\n",
      "[459,   155] loss: 18290.006\n",
      "[459,   160] loss: 21231.185\n",
      "[459,   165] loss: 38775.704\n",
      "[459,   170] loss: 25674.067\n",
      "[459,   175] loss: 19531.666\n",
      "[459,   180] loss: 24967.735\n",
      "[459,   185] loss: 31917.801\n",
      "[459,   190] loss: 41807.346\n",
      "[459,   195] loss: 13548.933\n",
      "[459,   200] loss: 27922.815\n",
      "[459,   205] loss: 40127.241\n",
      "[459,   210] loss: 19288.871\n",
      "[459,   215] loss: 21260.687\n",
      "[459,   220] loss: 21788.777\n",
      "[459,   225] loss: 23283.752\n",
      "[459,   230] loss: 23858.339\n",
      "[460,     5] loss: 20624.645\n",
      "[460,    10] loss: 26685.052\n",
      "[460,    15] loss: 23037.861\n",
      "[460,    20] loss: 15405.226\n",
      "[460,    25] loss: 30504.252\n",
      "[460,    30] loss: 17796.535\n",
      "[460,    35] loss: 29147.370\n",
      "[460,    40] loss: 33672.332\n",
      "[460,    45] loss: 18247.358\n",
      "[460,    50] loss: 20452.903\n",
      "[460,    55] loss: 21781.450\n",
      "[460,    60] loss: 26539.997\n",
      "[460,    65] loss: 25527.764\n",
      "[460,    70] loss: 38879.029\n",
      "[460,    75] loss: 21727.691\n",
      "[460,    80] loss: 14730.594\n",
      "[460,    85] loss: 26301.445\n",
      "[460,    90] loss: 24970.005\n",
      "[460,    95] loss: 24030.621\n",
      "[460,   100] loss: 17027.008\n",
      "[460,   105] loss: 22513.223\n",
      "[460,   110] loss: 18934.072\n",
      "[460,   115] loss: 36538.279\n",
      "[460,   120] loss: 28977.902\n",
      "[460,   125] loss: 21722.476\n",
      "[460,   130] loss: 29530.377\n",
      "[460,   135] loss: 28915.196\n",
      "[460,   140] loss: 13453.812\n",
      "[460,   145] loss: 38438.869\n",
      "[460,   150] loss: 38393.446\n",
      "[460,   155] loss: 16400.180\n",
      "[460,   160] loss: 41393.959\n",
      "[460,   165] loss: 19049.450\n",
      "[460,   170] loss: 31861.895\n",
      "[460,   175] loss: 31358.984\n",
      "[460,   180] loss: 26686.603\n",
      "[460,   185] loss: 22480.563\n",
      "[460,   190] loss: 23387.643\n",
      "[460,   195] loss: 34651.227\n",
      "[460,   200] loss: 29806.603\n",
      "[460,   205] loss: 25360.064\n",
      "[460,   210] loss: 22951.117\n",
      "[460,   215] loss: 19270.930\n",
      "[460,   220] loss: 28557.896\n",
      "[460,   225] loss: 26048.004\n",
      "[460,   230] loss: 20573.383\n",
      "[461,     5] loss: 25500.929\n",
      "[461,    10] loss: 25403.312\n",
      "[461,    15] loss: 18610.023\n",
      "[461,    20] loss: 25548.731\n",
      "[461,    25] loss: 30382.364\n",
      "[461,    30] loss: 31313.862\n",
      "[461,    35] loss: 32788.530\n",
      "[461,    40] loss: 25641.777\n",
      "[461,    45] loss: 21907.122\n",
      "[461,    50] loss: 24155.926\n",
      "[461,    55] loss: 22903.101\n",
      "[461,    60] loss: 37146.383\n",
      "[461,    65] loss: 19742.021\n",
      "[461,    70] loss: 23472.036\n",
      "[461,    75] loss: 35738.547\n",
      "[461,    80] loss: 29270.803\n",
      "[461,    85] loss: 29793.254\n",
      "[461,    90] loss: 26644.369\n",
      "[461,    95] loss: 38594.528\n",
      "[461,   100] loss: 22496.476\n",
      "[461,   105] loss: 27279.169\n",
      "[461,   110] loss: 20208.950\n",
      "[461,   115] loss: 26733.241\n",
      "[461,   120] loss: 20070.965\n",
      "[461,   125] loss: 19821.159\n",
      "[461,   130] loss: 21364.254\n",
      "[461,   135] loss: 19207.877\n",
      "[461,   140] loss: 14839.516\n",
      "[461,   145] loss: 17202.065\n",
      "[461,   150] loss: 23903.257\n",
      "[461,   155] loss: 28180.044\n",
      "[461,   160] loss: 21153.730\n",
      "[461,   165] loss: 24588.943\n",
      "[461,   170] loss: 20621.941\n",
      "[461,   175] loss: 17782.893\n",
      "[461,   180] loss: 23220.134\n",
      "[461,   185] loss: 32752.023\n",
      "[461,   190] loss: 29155.584\n",
      "[461,   195] loss: 20928.490\n",
      "[461,   200] loss: 44481.553\n",
      "[461,   205] loss: 30504.146\n",
      "[461,   210] loss: 25461.040\n",
      "[461,   215] loss: 24513.943\n",
      "[461,   220] loss: 25576.707\n",
      "[461,   225] loss: 23179.624\n",
      "[461,   230] loss: 21753.756\n",
      "[462,     5] loss: 36458.437\n",
      "[462,    10] loss: 12095.124\n",
      "[462,    15] loss: 27066.955\n",
      "[462,    20] loss: 30294.679\n",
      "[462,    25] loss: 20766.083\n",
      "[462,    30] loss: 24599.104\n",
      "[462,    35] loss: 24286.752\n",
      "[462,    40] loss: 23093.788\n",
      "[462,    45] loss: 23008.530\n",
      "[462,    50] loss: 16519.777\n",
      "[462,    55] loss: 20995.220\n",
      "[462,    60] loss: 17090.274\n",
      "[462,    65] loss: 21883.554\n",
      "[462,    70] loss: 19768.956\n",
      "[462,    75] loss: 20223.290\n",
      "[462,    80] loss: 28643.046\n",
      "[462,    85] loss: 17232.967\n",
      "[462,    90] loss: 33376.891\n",
      "[462,    95] loss: 23241.635\n",
      "[462,   100] loss: 20278.103\n",
      "[462,   105] loss: 53861.474\n",
      "[462,   110] loss: 27721.583\n",
      "[462,   115] loss: 27855.145\n",
      "[462,   120] loss: 25073.027\n",
      "[462,   125] loss: 25887.072\n",
      "[462,   130] loss: 26610.959\n",
      "[462,   135] loss: 31049.848\n",
      "[462,   140] loss: 26619.127\n",
      "[462,   145] loss: 17446.858\n",
      "[462,   150] loss: 19065.819\n",
      "[462,   155] loss: 24534.009\n",
      "[462,   160] loss: 21586.206\n",
      "[462,   165] loss: 35845.596\n",
      "[462,   170] loss: 22677.574\n",
      "[462,   175] loss: 19624.273\n",
      "[462,   180] loss: 20966.199\n",
      "[462,   185] loss: 26552.548\n",
      "[462,   190] loss: 20293.591\n",
      "[462,   195] loss: 16813.873\n",
      "[462,   200] loss: 22634.569\n",
      "[462,   205] loss: 31230.477\n",
      "[462,   210] loss: 27778.494\n",
      "[462,   215] loss: 29451.673\n",
      "[462,   220] loss: 22545.934\n",
      "[462,   225] loss: 43357.290\n",
      "[462,   230] loss: 33395.944\n",
      "[463,     5] loss: 24682.362\n",
      "[463,    10] loss: 17007.633\n",
      "[463,    15] loss: 22636.883\n",
      "[463,    20] loss: 25400.679\n",
      "[463,    25] loss: 20193.749\n",
      "[463,    30] loss: 27484.633\n",
      "[463,    35] loss: 29004.395\n",
      "[463,    40] loss: 47664.780\n",
      "[463,    45] loss: 37434.466\n",
      "[463,    50] loss: 22516.488\n",
      "[463,    55] loss: 16706.746\n",
      "[463,    60] loss: 23967.840\n",
      "[463,    65] loss: 22097.436\n",
      "[463,    70] loss: 24959.763\n",
      "[463,    75] loss: 21126.106\n",
      "[463,    80] loss: 19020.624\n",
      "[463,    85] loss: 24605.613\n",
      "[463,    90] loss: 18407.126\n",
      "[463,    95] loss: 29319.908\n",
      "[463,   100] loss: 20836.349\n",
      "[463,   105] loss: 23759.844\n",
      "[463,   110] loss: 22125.425\n",
      "[463,   115] loss: 25921.146\n",
      "[463,   120] loss: 26415.095\n",
      "[463,   125] loss: 23176.791\n",
      "[463,   130] loss: 29710.438\n",
      "[463,   135] loss: 15650.506\n",
      "[463,   140] loss: 19489.098\n",
      "[463,   145] loss: 18090.239\n",
      "[463,   150] loss: 28165.284\n",
      "[463,   155] loss: 27869.956\n",
      "[463,   160] loss: 26838.523\n",
      "[463,   165] loss: 41074.458\n",
      "[463,   170] loss: 29029.264\n",
      "[463,   175] loss: 33695.174\n",
      "[463,   180] loss: 35047.791\n",
      "[463,   185] loss: 24189.213\n",
      "[463,   190] loss: 22801.427\n",
      "[463,   195] loss: 30818.883\n",
      "[463,   200] loss: 32035.889\n",
      "[463,   205] loss: 19642.165\n",
      "[463,   210] loss: 23165.254\n",
      "[463,   215] loss: 27734.652\n",
      "[463,   220] loss: 19678.723\n",
      "[463,   225] loss: 21049.220\n",
      "[463,   230] loss: 17681.799\n",
      "[464,     5] loss: 22192.489\n",
      "[464,    10] loss: 26120.906\n",
      "[464,    15] loss: 28471.935\n",
      "[464,    20] loss: 15975.242\n",
      "[464,    25] loss: 20258.039\n",
      "[464,    30] loss: 19323.574\n",
      "[464,    35] loss: 20380.407\n",
      "[464,    40] loss: 20233.119\n",
      "[464,    45] loss: 20283.221\n",
      "[464,    50] loss: 20858.639\n",
      "[464,    55] loss: 25085.982\n",
      "[464,    60] loss: 35528.709\n",
      "[464,    65] loss: 25034.678\n",
      "[464,    70] loss: 36143.331\n",
      "[464,    75] loss: 38374.319\n",
      "[464,    80] loss: 24159.526\n",
      "[464,    85] loss: 19507.781\n",
      "[464,    90] loss: 20277.526\n",
      "[464,    95] loss: 30334.384\n",
      "[464,   100] loss: 17301.761\n",
      "[464,   105] loss: 30330.888\n",
      "[464,   110] loss: 18368.635\n",
      "[464,   115] loss: 31494.937\n",
      "[464,   120] loss: 24362.347\n",
      "[464,   125] loss: 46667.371\n",
      "[464,   130] loss: 25729.324\n",
      "[464,   135] loss: 18931.859\n",
      "[464,   140] loss: 34429.725\n",
      "[464,   145] loss: 32116.223\n",
      "[464,   150] loss: 19147.140\n",
      "[464,   155] loss: 22622.389\n",
      "[464,   160] loss: 18560.180\n",
      "[464,   165] loss: 17628.408\n",
      "[464,   170] loss: 40845.486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464,   175] loss: 28041.811\n",
      "[464,   180] loss: 22717.220\n",
      "[464,   185] loss: 18161.250\n",
      "[464,   190] loss: 20737.718\n",
      "[464,   195] loss: 30593.489\n",
      "[464,   200] loss: 29453.380\n",
      "[464,   205] loss: 36852.036\n",
      "[464,   210] loss: 25045.727\n",
      "[464,   215] loss: 27885.029\n",
      "[464,   220] loss: 22401.185\n",
      "[464,   225] loss: 21233.984\n",
      "[464,   230] loss: 17461.970\n",
      "[465,     5] loss: 21145.501\n",
      "[465,    10] loss: 16202.566\n",
      "[465,    15] loss: 22059.648\n",
      "[465,    20] loss: 29080.151\n",
      "[465,    25] loss: 31966.150\n",
      "[465,    30] loss: 29527.953\n",
      "[465,    35] loss: 48557.264\n",
      "[465,    40] loss: 20957.162\n",
      "[465,    45] loss: 29240.404\n",
      "[465,    50] loss: 19573.901\n",
      "[465,    55] loss: 25234.194\n",
      "[465,    60] loss: 19309.800\n",
      "[465,    65] loss: 29531.769\n",
      "[465,    70] loss: 13943.945\n",
      "[465,    75] loss: 30311.284\n",
      "[465,    80] loss: 25190.946\n",
      "[465,    85] loss: 30326.316\n",
      "[465,    90] loss: 22505.607\n",
      "[465,    95] loss: 18066.104\n",
      "[465,   100] loss: 30702.747\n",
      "[465,   105] loss: 32668.022\n",
      "[465,   110] loss: 22677.417\n",
      "[465,   115] loss: 25214.933\n",
      "[465,   120] loss: 22894.198\n",
      "[465,   125] loss: 29826.335\n",
      "[465,   130] loss: 21407.314\n",
      "[465,   135] loss: 23389.125\n",
      "[465,   140] loss: 34236.933\n",
      "[465,   145] loss: 35354.981\n",
      "[465,   150] loss: 18240.585\n",
      "[465,   155] loss: 21351.537\n",
      "[465,   160] loss: 18961.620\n",
      "[465,   165] loss: 27946.671\n",
      "[465,   170] loss: 25386.096\n",
      "[465,   175] loss: 26679.643\n",
      "[465,   180] loss: 27161.963\n",
      "[465,   185] loss: 23142.664\n",
      "[465,   190] loss: 14051.176\n",
      "[465,   195] loss: 28194.901\n",
      "[465,   200] loss: 29101.857\n",
      "[465,   205] loss: 43607.934\n",
      "[465,   210] loss: 20939.490\n",
      "[465,   215] loss: 23054.440\n",
      "[465,   220] loss: 18180.498\n",
      "[465,   225] loss: 22572.269\n",
      "[465,   230] loss: 24107.621\n",
      "[466,     5] loss: 30217.589\n",
      "[466,    10] loss: 17814.474\n",
      "[466,    15] loss: 19336.716\n",
      "[466,    20] loss: 20759.295\n",
      "[466,    25] loss: 34201.225\n",
      "[466,    30] loss: 15253.644\n",
      "[466,    35] loss: 25043.848\n",
      "[466,    40] loss: 33652.510\n",
      "[466,    45] loss: 21107.304\n",
      "[466,    50] loss: 41175.364\n",
      "[466,    55] loss: 27328.423\n",
      "[466,    60] loss: 19190.039\n",
      "[466,    65] loss: 20583.381\n",
      "[466,    70] loss: 21904.992\n",
      "[466,    75] loss: 14488.718\n",
      "[466,    80] loss: 14722.740\n",
      "[466,    85] loss: 27174.314\n",
      "[466,    90] loss: 14385.923\n",
      "[466,    95] loss: 29914.518\n",
      "[466,   100] loss: 20244.135\n",
      "[466,   105] loss: 29529.376\n",
      "[466,   110] loss: 26631.203\n",
      "[466,   115] loss: 18829.382\n",
      "[466,   120] loss: 24140.478\n",
      "[466,   125] loss: 19064.738\n",
      "[466,   130] loss: 32997.948\n",
      "[466,   135] loss: 28901.877\n",
      "[466,   140] loss: 19732.940\n",
      "[466,   145] loss: 21430.752\n",
      "[466,   150] loss: 31948.500\n",
      "[466,   155] loss: 38286.851\n",
      "[466,   160] loss: 26455.028\n",
      "[466,   165] loss: 36941.930\n",
      "[466,   170] loss: 30217.183\n",
      "[466,   175] loss: 26673.289\n",
      "[466,   180] loss: 25555.834\n",
      "[466,   185] loss: 26104.414\n",
      "[466,   190] loss: 25649.002\n",
      "[466,   195] loss: 18631.278\n",
      "[466,   200] loss: 25912.145\n",
      "[466,   205] loss: 22172.456\n",
      "[466,   210] loss: 35272.768\n",
      "[466,   215] loss: 26411.052\n",
      "[466,   220] loss: 47026.245\n",
      "[466,   225] loss: 20350.073\n",
      "[466,   230] loss: 19832.631\n",
      "[467,     5] loss: 26801.499\n",
      "[467,    10] loss: 22029.324\n",
      "[467,    15] loss: 27892.144\n",
      "[467,    20] loss: 27698.737\n",
      "[467,    25] loss: 21516.354\n",
      "[467,    30] loss: 23637.615\n",
      "[467,    35] loss: 15889.439\n",
      "[467,    40] loss: 24797.872\n",
      "[467,    45] loss: 22748.735\n",
      "[467,    50] loss: 21230.462\n",
      "[467,    55] loss: 21284.574\n",
      "[467,    60] loss: 21519.659\n",
      "[467,    65] loss: 29648.350\n",
      "[467,    70] loss: 24447.533\n",
      "[467,    75] loss: 16692.282\n",
      "[467,    80] loss: 26667.623\n",
      "[467,    85] loss: 26234.485\n",
      "[467,    90] loss: 24455.978\n",
      "[467,    95] loss: 26258.934\n",
      "[467,   100] loss: 26427.223\n",
      "[467,   105] loss: 32694.761\n",
      "[467,   110] loss: 28310.699\n",
      "[467,   115] loss: 22697.110\n",
      "[467,   120] loss: 25376.174\n",
      "[467,   125] loss: 17984.977\n",
      "[467,   130] loss: 22412.144\n",
      "[467,   135] loss: 17941.318\n",
      "[467,   140] loss: 42776.741\n",
      "[467,   145] loss: 16900.699\n",
      "[467,   150] loss: 27792.629\n",
      "[467,   155] loss: 33455.287\n",
      "[467,   160] loss: 47933.384\n",
      "[467,   165] loss: 26295.043\n",
      "[467,   170] loss: 16032.369\n",
      "[467,   175] loss: 24415.420\n",
      "[467,   180] loss: 15747.209\n",
      "[467,   185] loss: 26842.371\n",
      "[467,   190] loss: 22157.377\n",
      "[467,   195] loss: 28558.503\n",
      "[467,   200] loss: 24171.937\n",
      "[467,   205] loss: 26652.662\n",
      "[467,   210] loss: 15812.256\n",
      "[467,   215] loss: 36028.685\n",
      "[467,   220] loss: 35368.270\n",
      "[467,   225] loss: 23441.859\n",
      "[467,   230] loss: 28528.743\n",
      "[468,     5] loss: 20342.176\n",
      "[468,    10] loss: 28577.088\n",
      "[468,    15] loss: 20072.774\n",
      "[468,    20] loss: 17219.306\n",
      "[468,    25] loss: 27906.395\n",
      "[468,    30] loss: 31479.409\n",
      "[468,    35] loss: 41380.307\n",
      "[468,    40] loss: 31943.886\n",
      "[468,    45] loss: 34471.217\n",
      "[468,    50] loss: 18839.886\n",
      "[468,    55] loss: 19894.005\n",
      "[468,    60] loss: 28929.893\n",
      "[468,    65] loss: 18597.094\n",
      "[468,    70] loss: 16707.961\n",
      "[468,    75] loss: 25556.325\n",
      "[468,    80] loss: 23626.376\n",
      "[468,    85] loss: 28123.947\n",
      "[468,    90] loss: 19073.347\n",
      "[468,    95] loss: 20939.564\n",
      "[468,   100] loss: 38422.539\n",
      "[468,   105] loss: 24420.164\n",
      "[468,   110] loss: 18211.212\n",
      "[468,   115] loss: 25944.282\n",
      "[468,   120] loss: 16996.418\n",
      "[468,   125] loss: 18802.745\n",
      "[468,   130] loss: 32977.269\n",
      "[468,   135] loss: 19606.807\n",
      "[468,   140] loss: 19134.028\n",
      "[468,   145] loss: 17416.776\n",
      "[468,   150] loss: 30569.657\n",
      "[468,   155] loss: 19863.647\n",
      "[468,   160] loss: 26284.836\n",
      "[468,   165] loss: 24379.584\n",
      "[468,   170] loss: 25172.575\n",
      "[468,   175] loss: 21974.778\n",
      "[468,   180] loss: 22957.191\n",
      "[468,   185] loss: 24932.014\n",
      "[468,   190] loss: 18780.232\n",
      "[468,   195] loss: 46603.287\n",
      "[468,   200] loss: 36116.373\n",
      "[468,   205] loss: 30851.328\n",
      "[468,   210] loss: 32852.330\n",
      "[468,   215] loss: 18516.919\n",
      "[468,   220] loss: 20304.177\n",
      "[468,   225] loss: 24530.137\n",
      "[468,   230] loss: 38256.206\n",
      "[469,     5] loss: 23526.990\n",
      "[469,    10] loss: 15246.061\n",
      "[469,    15] loss: 17318.554\n",
      "[469,    20] loss: 24843.374\n",
      "[469,    25] loss: 28510.102\n",
      "[469,    30] loss: 24377.085\n",
      "[469,    35] loss: 27653.752\n",
      "[469,    40] loss: 14261.672\n",
      "[469,    45] loss: 16848.235\n",
      "[469,    50] loss: 28415.061\n",
      "[469,    55] loss: 27372.799\n",
      "[469,    60] loss: 25444.066\n",
      "[469,    65] loss: 21538.489\n",
      "[469,    70] loss: 28323.999\n",
      "[469,    75] loss: 20989.438\n",
      "[469,    80] loss: 20406.711\n",
      "[469,    85] loss: 21869.858\n",
      "[469,    90] loss: 48614.389\n",
      "[469,    95] loss: 18712.481\n",
      "[469,   100] loss: 33038.427\n",
      "[469,   105] loss: 29324.397\n",
      "[469,   110] loss: 20188.620\n",
      "[469,   115] loss: 30255.000\n",
      "[469,   120] loss: 22172.661\n",
      "[469,   125] loss: 12704.620\n",
      "[469,   130] loss: 34680.834\n",
      "[469,   135] loss: 19052.970\n",
      "[469,   140] loss: 18266.963\n",
      "[469,   145] loss: 19076.591\n",
      "[469,   150] loss: 29698.796\n",
      "[469,   155] loss: 28113.822\n",
      "[469,   160] loss: 28913.695\n",
      "[469,   165] loss: 33515.515\n",
      "[469,   170] loss: 21557.101\n",
      "[469,   175] loss: 30113.424\n",
      "[469,   180] loss: 41668.682\n",
      "[469,   185] loss: 31293.988\n",
      "[469,   190] loss: 18895.581\n",
      "[469,   195] loss: 23921.269\n",
      "[469,   200] loss: 24459.927\n",
      "[469,   205] loss: 19949.013\n",
      "[469,   210] loss: 39085.324\n",
      "[469,   215] loss: 23822.366\n",
      "[469,   220] loss: 30744.617\n",
      "[469,   225] loss: 19381.951\n",
      "[469,   230] loss: 25430.471\n",
      "[470,     5] loss: 27705.287\n",
      "[470,    10] loss: 25037.288\n",
      "[470,    15] loss: 31537.231\n",
      "[470,    20] loss: 24873.164\n",
      "[470,    25] loss: 27414.067\n",
      "[470,    30] loss: 20979.998\n",
      "[470,    35] loss: 17672.723\n",
      "[470,    40] loss: 27436.164\n",
      "[470,    45] loss: 25318.144\n",
      "[470,    50] loss: 23655.942\n",
      "[470,    55] loss: 30749.904\n",
      "[470,    60] loss: 21970.415\n",
      "[470,    65] loss: 21340.575\n",
      "[470,    70] loss: 23223.891\n",
      "[470,    75] loss: 25460.250\n",
      "[470,    80] loss: 45318.877\n",
      "[470,    85] loss: 28075.047\n",
      "[470,    90] loss: 32437.754\n",
      "[470,    95] loss: 28052.127\n",
      "[470,   100] loss: 21949.880\n",
      "[470,   105] loss: 27215.036\n",
      "[470,   110] loss: 22775.840\n",
      "[470,   115] loss: 24625.987\n",
      "[470,   120] loss: 32910.708\n",
      "[470,   125] loss: 35707.024\n",
      "[470,   130] loss: 27494.865\n",
      "[470,   135] loss: 28502.490\n",
      "[470,   140] loss: 21260.127\n",
      "[470,   145] loss: 23100.823\n",
      "[470,   150] loss: 17443.597\n",
      "[470,   155] loss: 22396.571\n",
      "[470,   160] loss: 27149.323\n",
      "[470,   165] loss: 27391.603\n",
      "[470,   170] loss: 16469.859\n",
      "[470,   175] loss: 20967.776\n",
      "[470,   180] loss: 18945.656\n",
      "[470,   185] loss: 19723.571\n",
      "[470,   190] loss: 21875.892\n",
      "[470,   195] loss: 27996.713\n",
      "[470,   200] loss: 39295.848\n",
      "[470,   205] loss: 23882.069\n",
      "[470,   210] loss: 19437.614\n",
      "[470,   215] loss: 18714.013\n",
      "[470,   220] loss: 25251.493\n",
      "[470,   225] loss: 28060.038\n",
      "[470,   230] loss: 22123.904\n",
      "[471,     5] loss: 22783.637\n",
      "[471,    10] loss: 27592.249\n",
      "[471,    15] loss: 36506.147\n",
      "[471,    20] loss: 37017.327\n",
      "[471,    25] loss: 25827.212\n",
      "[471,    30] loss: 20364.473\n",
      "[471,    35] loss: 27094.985\n",
      "[471,    40] loss: 24949.285\n",
      "[471,    45] loss: 21993.319\n",
      "[471,    50] loss: 27418.890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[471,    55] loss: 25727.401\n",
      "[471,    60] loss: 22901.924\n",
      "[471,    65] loss: 31343.560\n",
      "[471,    70] loss: 29879.113\n",
      "[471,    75] loss: 26790.139\n",
      "[471,    80] loss: 29540.923\n",
      "[471,    85] loss: 22231.862\n",
      "[471,    90] loss: 35013.877\n",
      "[471,    95] loss: 22694.238\n",
      "[471,   100] loss: 25786.654\n",
      "[471,   105] loss: 42962.612\n",
      "[471,   110] loss: 29543.659\n",
      "[471,   115] loss: 28521.993\n",
      "[471,   120] loss: 22703.592\n",
      "[471,   125] loss: 21902.199\n",
      "[471,   130] loss: 20147.185\n",
      "[471,   135] loss: 18106.905\n",
      "[471,   140] loss: 21192.854\n",
      "[471,   145] loss: 18465.229\n",
      "[471,   150] loss: 14870.580\n",
      "[471,   155] loss: 13923.504\n",
      "[471,   160] loss: 28129.850\n",
      "[471,   165] loss: 31063.669\n",
      "[471,   170] loss: 28976.078\n",
      "[471,   175] loss: 19170.661\n",
      "[471,   180] loss: 30115.797\n",
      "[471,   185] loss: 30903.938\n",
      "[471,   190] loss: 17329.124\n",
      "[471,   195] loss: 20667.695\n",
      "[471,   200] loss: 20995.697\n",
      "[471,   205] loss: 21516.156\n",
      "[471,   210] loss: 30487.812\n",
      "[471,   215] loss: 27051.886\n",
      "[471,   220] loss: 25802.631\n",
      "[471,   225] loss: 17836.010\n",
      "[471,   230] loss: 27757.925\n",
      "[472,     5] loss: 23999.014\n",
      "[472,    10] loss: 20675.071\n",
      "[472,    15] loss: 27069.702\n",
      "[472,    20] loss: 25755.672\n",
      "[472,    25] loss: 28021.709\n",
      "[472,    30] loss: 21701.419\n",
      "[472,    35] loss: 18906.038\n",
      "[472,    40] loss: 18993.951\n",
      "[472,    45] loss: 28043.602\n",
      "[472,    50] loss: 24522.224\n",
      "[472,    55] loss: 24742.502\n",
      "[472,    60] loss: 21576.654\n",
      "[472,    65] loss: 20565.484\n",
      "[472,    70] loss: 27273.359\n",
      "[472,    75] loss: 35211.137\n",
      "[472,    80] loss: 30464.502\n",
      "[472,    85] loss: 38692.737\n",
      "[472,    90] loss: 24436.465\n",
      "[472,    95] loss: 19942.943\n",
      "[472,   100] loss: 22006.149\n",
      "[472,   105] loss: 19838.553\n",
      "[472,   110] loss: 39858.483\n",
      "[472,   115] loss: 17255.478\n",
      "[472,   120] loss: 19310.267\n",
      "[472,   125] loss: 28090.867\n",
      "[472,   130] loss: 20789.261\n",
      "[472,   135] loss: 26210.175\n",
      "[472,   140] loss: 29514.383\n",
      "[472,   145] loss: 23642.527\n",
      "[472,   150] loss: 16828.938\n",
      "[472,   155] loss: 38640.657\n",
      "[472,   160] loss: 26125.366\n",
      "[472,   165] loss: 23542.354\n",
      "[472,   170] loss: 24666.310\n",
      "[472,   175] loss: 28241.254\n",
      "[472,   180] loss: 22463.454\n",
      "[472,   185] loss: 32363.520\n",
      "[472,   190] loss: 21448.647\n",
      "[472,   195] loss: 38263.179\n",
      "[472,   200] loss: 23029.513\n",
      "[472,   205] loss: 21207.629\n",
      "[472,   210] loss: 22219.816\n",
      "[472,   215] loss: 17054.550\n",
      "[472,   220] loss: 28905.236\n",
      "[472,   225] loss: 28696.341\n",
      "[472,   230] loss: 32648.424\n",
      "[473,     5] loss: 34488.064\n",
      "[473,    10] loss: 22265.399\n",
      "[473,    15] loss: 21799.360\n",
      "[473,    20] loss: 33722.223\n",
      "[473,    25] loss: 25230.538\n",
      "[473,    30] loss: 20590.141\n",
      "[473,    35] loss: 22697.469\n",
      "[473,    40] loss: 28450.049\n",
      "[473,    45] loss: 22975.445\n",
      "[473,    50] loss: 21391.277\n",
      "[473,    55] loss: 31177.286\n",
      "[473,    60] loss: 26078.765\n",
      "[473,    65] loss: 20318.714\n",
      "[473,    70] loss: 22393.333\n",
      "[473,    75] loss: 30196.657\n",
      "[473,    80] loss: 17248.679\n",
      "[473,    85] loss: 21281.140\n",
      "[473,    90] loss: 19923.198\n",
      "[473,    95] loss: 31225.798\n",
      "[473,   100] loss: 46855.894\n",
      "[473,   105] loss: 20064.094\n",
      "[473,   110] loss: 21568.679\n",
      "[473,   115] loss: 28066.163\n",
      "[473,   120] loss: 19213.274\n",
      "[473,   125] loss: 24386.463\n",
      "[473,   130] loss: 21589.160\n",
      "[473,   135] loss: 37427.116\n",
      "[473,   140] loss: 26046.567\n",
      "[473,   145] loss: 20401.121\n",
      "[473,   150] loss: 23890.799\n",
      "[473,   155] loss: 23459.353\n",
      "[473,   160] loss: 29777.255\n",
      "[473,   165] loss: 32734.186\n",
      "[473,   170] loss: 20145.690\n",
      "[473,   175] loss: 30688.362\n",
      "[473,   180] loss: 24309.125\n",
      "[473,   185] loss: 21472.304\n",
      "[473,   190] loss: 38533.755\n",
      "[473,   195] loss: 24012.690\n",
      "[473,   200] loss: 24782.694\n",
      "[473,   205] loss: 20991.774\n",
      "[473,   210] loss: 18816.097\n",
      "[473,   215] loss: 21812.718\n",
      "[473,   220] loss: 28241.968\n",
      "[473,   225] loss: 24935.110\n",
      "[473,   230] loss: 24329.911\n",
      "[474,     5] loss: 22687.938\n",
      "[474,    10] loss: 20119.913\n",
      "[474,    15] loss: 24246.978\n",
      "[474,    20] loss: 19886.636\n",
      "[474,    25] loss: 17301.627\n",
      "[474,    30] loss: 29224.078\n",
      "[474,    35] loss: 45832.209\n",
      "[474,    40] loss: 26650.299\n",
      "[474,    45] loss: 18414.867\n",
      "[474,    50] loss: 32860.634\n",
      "[474,    55] loss: 24861.916\n",
      "[474,    60] loss: 22865.209\n",
      "[474,    65] loss: 27748.536\n",
      "[474,    70] loss: 25790.985\n",
      "[474,    75] loss: 32861.028\n",
      "[474,    80] loss: 25535.897\n",
      "[474,    85] loss: 26323.133\n",
      "[474,    90] loss: 27630.220\n",
      "[474,    95] loss: 28927.448\n",
      "[474,   100] loss: 28533.848\n",
      "[474,   105] loss: 20972.712\n",
      "[474,   110] loss: 28006.570\n",
      "[474,   115] loss: 20286.398\n",
      "[474,   120] loss: 36595.931\n",
      "[474,   125] loss: 33651.945\n",
      "[474,   130] loss: 22006.100\n",
      "[474,   135] loss: 23423.144\n",
      "[474,   140] loss: 21170.125\n",
      "[474,   145] loss: 24711.435\n",
      "[474,   150] loss: 28520.144\n",
      "[474,   155] loss: 20085.741\n",
      "[474,   160] loss: 26475.390\n",
      "[474,   165] loss: 26241.126\n",
      "[474,   170] loss: 27133.131\n",
      "[474,   175] loss: 19741.195\n",
      "[474,   180] loss: 17362.132\n",
      "[474,   185] loss: 29203.325\n",
      "[474,   190] loss: 16402.368\n",
      "[474,   195] loss: 25185.020\n",
      "[474,   200] loss: 21865.888\n",
      "[474,   205] loss: 23660.674\n",
      "[474,   210] loss: 19470.064\n",
      "[474,   215] loss: 23853.231\n",
      "[474,   220] loss: 30910.956\n",
      "[474,   225] loss: 14021.059\n",
      "[474,   230] loss: 36165.984\n",
      "[475,     5] loss: 22868.225\n",
      "[475,    10] loss: 27689.625\n",
      "[475,    15] loss: 27254.518\n",
      "[475,    20] loss: 20300.209\n",
      "[475,    25] loss: 35756.249\n",
      "[475,    30] loss: 23949.627\n",
      "[475,    35] loss: 20990.580\n",
      "[475,    40] loss: 40922.914\n",
      "[475,    45] loss: 18902.517\n",
      "[475,    50] loss: 23535.247\n",
      "[475,    55] loss: 17432.284\n",
      "[475,    60] loss: 29029.320\n",
      "[475,    65] loss: 23230.735\n",
      "[475,    70] loss: 24381.158\n",
      "[475,    75] loss: 24056.140\n",
      "[475,    80] loss: 22391.735\n",
      "[475,    85] loss: 27317.360\n",
      "[475,    90] loss: 15902.525\n",
      "[475,    95] loss: 27240.188\n",
      "[475,   100] loss: 21866.425\n",
      "[475,   105] loss: 23916.928\n",
      "[475,   110] loss: 29653.718\n",
      "[475,   115] loss: 27565.150\n",
      "[475,   120] loss: 28010.417\n",
      "[475,   125] loss: 24832.180\n",
      "[475,   130] loss: 33223.973\n",
      "[475,   135] loss: 34834.618\n",
      "[475,   140] loss: 16987.036\n",
      "[475,   145] loss: 20683.238\n",
      "[475,   150] loss: 22776.715\n",
      "[475,   155] loss: 23232.032\n",
      "[475,   160] loss: 22425.529\n",
      "[475,   165] loss: 19649.388\n",
      "[475,   170] loss: 26515.660\n",
      "[475,   175] loss: 40755.547\n",
      "[475,   180] loss: 17157.777\n",
      "[475,   185] loss: 22385.631\n",
      "[475,   190] loss: 30272.350\n",
      "[475,   195] loss: 25665.962\n",
      "[475,   200] loss: 23672.655\n",
      "[475,   205] loss: 18992.998\n",
      "[475,   210] loss: 26381.586\n",
      "[475,   215] loss: 30601.894\n",
      "[475,   220] loss: 22208.803\n",
      "[475,   225] loss: 42054.578\n",
      "[475,   230] loss: 18764.105\n",
      "[476,     5] loss: 20024.245\n",
      "[476,    10] loss: 37488.690\n",
      "[476,    15] loss: 21194.871\n",
      "[476,    20] loss: 24898.599\n",
      "[476,    25] loss: 21991.646\n",
      "[476,    30] loss: 25600.936\n",
      "[476,    35] loss: 22025.140\n",
      "[476,    40] loss: 21651.943\n",
      "[476,    45] loss: 18663.953\n",
      "[476,    50] loss: 22395.918\n",
      "[476,    55] loss: 25426.338\n",
      "[476,    60] loss: 16506.177\n",
      "[476,    65] loss: 23865.446\n",
      "[476,    70] loss: 28156.405\n",
      "[476,    75] loss: 26122.566\n",
      "[476,    80] loss: 24668.001\n",
      "[476,    85] loss: 27765.723\n",
      "[476,    90] loss: 27395.870\n",
      "[476,    95] loss: 31376.137\n",
      "[476,   100] loss: 22249.660\n",
      "[476,   105] loss: 17936.485\n",
      "[476,   110] loss: 22486.615\n",
      "[476,   115] loss: 56459.483\n",
      "[476,   120] loss: 27024.941\n",
      "[476,   125] loss: 22640.043\n",
      "[476,   130] loss: 27299.953\n",
      "[476,   135] loss: 31358.843\n",
      "[476,   140] loss: 22657.743\n",
      "[476,   145] loss: 33304.750\n",
      "[476,   150] loss: 13177.311\n",
      "[476,   155] loss: 17156.895\n",
      "[476,   160] loss: 23021.085\n",
      "[476,   165] loss: 27930.089\n",
      "[476,   170] loss: 36818.312\n",
      "[476,   175] loss: 26921.670\n",
      "[476,   180] loss: 26287.126\n",
      "[476,   185] loss: 22008.410\n",
      "[476,   190] loss: 22893.589\n",
      "[476,   195] loss: 18719.093\n",
      "[476,   200] loss: 32911.155\n",
      "[476,   205] loss: 25102.240\n",
      "[476,   210] loss: 25322.737\n",
      "[476,   215] loss: 26729.564\n",
      "[476,   220] loss: 27985.211\n",
      "[476,   225] loss: 23090.099\n",
      "[476,   230] loss: 24849.139\n",
      "[477,     5] loss: 24405.721\n",
      "[477,    10] loss: 34551.548\n",
      "[477,    15] loss: 16050.495\n",
      "[477,    20] loss: 28725.968\n",
      "[477,    25] loss: 23348.191\n",
      "[477,    30] loss: 23650.362\n",
      "[477,    35] loss: 29177.849\n",
      "[477,    40] loss: 25991.186\n",
      "[477,    45] loss: 22835.328\n",
      "[477,    50] loss: 24447.561\n",
      "[477,    55] loss: 24292.109\n",
      "[477,    60] loss: 24353.165\n",
      "[477,    65] loss: 23855.099\n",
      "[477,    70] loss: 26474.420\n",
      "[477,    75] loss: 21517.034\n",
      "[477,    80] loss: 24768.722\n",
      "[477,    85] loss: 23959.881\n",
      "[477,    90] loss: 33576.047\n",
      "[477,    95] loss: 25266.265\n",
      "[477,   100] loss: 19387.876\n",
      "[477,   105] loss: 28612.461\n",
      "[477,   110] loss: 21016.598\n",
      "[477,   115] loss: 25945.766\n",
      "[477,   120] loss: 18665.273\n",
      "[477,   125] loss: 32434.494\n",
      "[477,   130] loss: 18918.036\n",
      "[477,   135] loss: 50779.482\n",
      "[477,   140] loss: 23484.339\n",
      "[477,   145] loss: 38053.473\n",
      "[477,   150] loss: 24224.116\n",
      "[477,   155] loss: 27720.567\n",
      "[477,   160] loss: 18999.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[477,   165] loss: 22865.750\n",
      "[477,   170] loss: 25678.643\n",
      "[477,   175] loss: 20619.672\n",
      "[477,   180] loss: 23026.250\n",
      "[477,   185] loss: 15020.199\n",
      "[477,   190] loss: 27012.384\n",
      "[477,   195] loss: 19946.618\n",
      "[477,   200] loss: 26644.518\n",
      "[477,   205] loss: 25273.835\n",
      "[477,   210] loss: 25396.025\n",
      "[477,   215] loss: 36816.494\n",
      "[477,   220] loss: 25313.082\n",
      "[477,   225] loss: 22892.439\n",
      "[477,   230] loss: 24704.420\n",
      "[478,     5] loss: 23097.828\n",
      "[478,    10] loss: 26459.834\n",
      "[478,    15] loss: 24324.338\n",
      "[478,    20] loss: 23277.876\n",
      "[478,    25] loss: 25897.212\n",
      "[478,    30] loss: 29182.354\n",
      "[478,    35] loss: 30504.594\n",
      "[478,    40] loss: 24232.059\n",
      "[478,    45] loss: 22286.185\n",
      "[478,    50] loss: 28307.119\n",
      "[478,    55] loss: 22817.094\n",
      "[478,    60] loss: 36759.731\n",
      "[478,    65] loss: 22225.547\n",
      "[478,    70] loss: 20124.562\n",
      "[478,    75] loss: 31245.663\n",
      "[478,    80] loss: 20157.114\n",
      "[478,    85] loss: 24857.303\n",
      "[478,    90] loss: 26748.089\n",
      "[478,    95] loss: 25838.315\n",
      "[478,   100] loss: 38066.087\n",
      "[478,   105] loss: 26292.254\n",
      "[478,   110] loss: 22143.218\n",
      "[478,   115] loss: 25618.638\n",
      "[478,   120] loss: 44285.870\n",
      "[478,   125] loss: 26519.115\n",
      "[478,   130] loss: 25733.173\n",
      "[478,   135] loss: 24753.823\n",
      "[478,   140] loss: 19513.380\n",
      "[478,   145] loss: 16544.292\n",
      "[478,   150] loss: 24478.397\n",
      "[478,   155] loss: 18526.056\n",
      "[478,   160] loss: 25539.400\n",
      "[478,   165] loss: 20844.113\n",
      "[478,   170] loss: 21951.609\n",
      "[478,   175] loss: 22585.829\n",
      "[478,   180] loss: 27544.608\n",
      "[478,   185] loss: 17818.248\n",
      "[478,   190] loss: 25670.234\n",
      "[478,   195] loss: 29518.051\n",
      "[478,   200] loss: 18992.182\n",
      "[478,   205] loss: 27911.783\n",
      "[478,   210] loss: 26601.870\n",
      "[478,   215] loss: 22370.202\n",
      "[478,   220] loss: 30280.893\n",
      "[478,   225] loss: 23285.228\n",
      "[478,   230] loss: 27624.227\n",
      "[479,     5] loss: 21340.908\n",
      "[479,    10] loss: 24747.873\n",
      "[479,    15] loss: 28178.082\n",
      "[479,    20] loss: 26125.621\n",
      "[479,    25] loss: 23345.765\n",
      "[479,    30] loss: 22089.578\n",
      "[479,    35] loss: 27118.925\n",
      "[479,    40] loss: 17928.981\n",
      "[479,    45] loss: 32298.306\n",
      "[479,    50] loss: 51790.882\n",
      "[479,    55] loss: 25094.984\n",
      "[479,    60] loss: 20767.485\n",
      "[479,    65] loss: 18778.592\n",
      "[479,    70] loss: 26188.247\n",
      "[479,    75] loss: 29936.973\n",
      "[479,    80] loss: 34398.662\n",
      "[479,    85] loss: 21476.259\n",
      "[479,    90] loss: 29191.275\n",
      "[479,    95] loss: 18768.773\n",
      "[479,   100] loss: 30901.020\n",
      "[479,   105] loss: 27609.394\n",
      "[479,   110] loss: 19135.223\n",
      "[479,   115] loss: 23078.442\n",
      "[479,   120] loss: 21543.627\n",
      "[479,   125] loss: 24511.112\n",
      "[479,   130] loss: 18278.713\n",
      "[479,   135] loss: 22397.894\n",
      "[479,   140] loss: 22621.209\n",
      "[479,   145] loss: 30075.670\n",
      "[479,   150] loss: 21119.050\n",
      "[479,   155] loss: 20647.860\n",
      "[479,   160] loss: 34663.954\n",
      "[479,   165] loss: 26668.376\n",
      "[479,   170] loss: 31371.016\n",
      "[479,   175] loss: 28353.944\n",
      "[479,   180] loss: 27272.141\n",
      "[479,   185] loss: 30493.758\n",
      "[479,   190] loss: 18462.041\n",
      "[479,   195] loss: 25971.956\n",
      "[479,   200] loss: 31593.143\n",
      "[479,   205] loss: 20979.931\n",
      "[479,   210] loss: 27249.962\n",
      "[479,   215] loss: 20696.017\n",
      "[479,   220] loss: 18853.234\n",
      "[479,   225] loss: 22398.578\n",
      "[479,   230] loss: 23667.956\n",
      "[480,     5] loss: 21522.378\n",
      "[480,    10] loss: 26255.899\n",
      "[480,    15] loss: 23671.442\n",
      "[480,    20] loss: 22268.346\n",
      "[480,    25] loss: 22230.658\n",
      "[480,    30] loss: 21423.433\n",
      "[480,    35] loss: 22929.809\n",
      "[480,    40] loss: 20593.669\n",
      "[480,    45] loss: 21087.452\n",
      "[480,    50] loss: 28924.510\n",
      "[480,    55] loss: 29965.880\n",
      "[480,    60] loss: 21714.606\n",
      "[480,    65] loss: 22862.153\n",
      "[480,    70] loss: 30005.129\n",
      "[480,    75] loss: 31435.749\n",
      "[480,    80] loss: 32601.159\n",
      "[480,    85] loss: 19093.532\n",
      "[480,    90] loss: 18682.678\n",
      "[480,    95] loss: 30981.315\n",
      "[480,   100] loss: 24749.594\n",
      "[480,   105] loss: 24534.214\n",
      "[480,   110] loss: 19512.133\n",
      "[480,   115] loss: 17506.269\n",
      "[480,   120] loss: 23837.743\n",
      "[480,   125] loss: 15913.045\n",
      "[480,   130] loss: 26173.962\n",
      "[480,   135] loss: 33094.385\n",
      "[480,   140] loss: 25142.075\n",
      "[480,   145] loss: 34010.817\n",
      "[480,   150] loss: 24146.240\n",
      "[480,   155] loss: 23901.162\n",
      "[480,   160] loss: 24785.449\n",
      "[480,   165] loss: 18223.138\n",
      "[480,   170] loss: 16932.232\n",
      "[480,   175] loss: 26268.763\n",
      "[480,   180] loss: 47881.098\n",
      "[480,   185] loss: 47757.100\n",
      "[480,   190] loss: 22058.578\n",
      "[480,   195] loss: 26892.931\n",
      "[480,   200] loss: 22924.033\n",
      "[480,   205] loss: 23994.228\n",
      "[480,   210] loss: 20770.943\n",
      "[480,   215] loss: 26971.936\n",
      "[480,   220] loss: 22554.836\n",
      "[480,   225] loss: 29639.785\n",
      "[480,   230] loss: 33389.838\n",
      "[481,     5] loss: 21876.977\n",
      "[481,    10] loss: 22890.276\n",
      "[481,    15] loss: 23248.870\n",
      "[481,    20] loss: 29925.632\n",
      "[481,    25] loss: 28926.719\n",
      "[481,    30] loss: 20895.377\n",
      "[481,    35] loss: 24320.317\n",
      "[481,    40] loss: 23669.974\n",
      "[481,    45] loss: 29687.224\n",
      "[481,    50] loss: 24796.360\n",
      "[481,    55] loss: 21781.571\n",
      "[481,    60] loss: 18502.936\n",
      "[481,    65] loss: 24009.606\n",
      "[481,    70] loss: 19392.619\n",
      "[481,    75] loss: 33921.754\n",
      "[481,    80] loss: 31131.291\n",
      "[481,    85] loss: 18311.547\n",
      "[481,    90] loss: 29734.191\n",
      "[481,    95] loss: 31123.312\n",
      "[481,   100] loss: 20267.670\n",
      "[481,   105] loss: 21433.020\n",
      "[481,   110] loss: 23753.195\n",
      "[481,   115] loss: 19920.965\n",
      "[481,   120] loss: 19037.853\n",
      "[481,   125] loss: 22653.042\n",
      "[481,   130] loss: 31512.204\n",
      "[481,   135] loss: 21047.283\n",
      "[481,   140] loss: 35720.563\n",
      "[481,   145] loss: 21209.137\n",
      "[481,   150] loss: 30834.360\n",
      "[481,   155] loss: 37462.091\n",
      "[481,   160] loss: 25532.909\n",
      "[481,   165] loss: 21842.471\n",
      "[481,   170] loss: 25440.079\n",
      "[481,   175] loss: 16849.933\n",
      "[481,   180] loss: 23464.257\n",
      "[481,   185] loss: 27185.454\n",
      "[481,   190] loss: 30918.601\n",
      "[481,   195] loss: 26017.884\n",
      "[481,   200] loss: 25169.814\n",
      "[481,   205] loss: 24691.459\n",
      "[481,   210] loss: 21984.551\n",
      "[481,   215] loss: 28550.781\n",
      "[481,   220] loss: 13937.427\n",
      "[481,   225] loss: 43373.883\n",
      "[481,   230] loss: 22164.293\n",
      "[482,     5] loss: 20001.840\n",
      "[482,    10] loss: 17997.478\n",
      "[482,    15] loss: 16846.498\n",
      "[482,    20] loss: 24455.595\n",
      "[482,    25] loss: 23999.731\n",
      "[482,    30] loss: 23322.308\n",
      "[482,    35] loss: 27732.273\n",
      "[482,    40] loss: 32493.862\n",
      "[482,    45] loss: 24909.284\n",
      "[482,    50] loss: 24058.888\n",
      "[482,    55] loss: 24209.528\n",
      "[482,    60] loss: 27604.917\n",
      "[482,    65] loss: 23354.008\n",
      "[482,    70] loss: 25280.988\n",
      "[482,    75] loss: 21479.199\n",
      "[482,    80] loss: 47556.928\n",
      "[482,    85] loss: 34006.439\n",
      "[482,    90] loss: 38983.268\n",
      "[482,    95] loss: 27899.298\n",
      "[482,   100] loss: 26147.012\n",
      "[482,   105] loss: 17400.825\n",
      "[482,   110] loss: 23749.773\n",
      "[482,   115] loss: 20177.849\n",
      "[482,   120] loss: 21104.789\n",
      "[482,   125] loss: 19967.401\n",
      "[482,   130] loss: 23850.875\n",
      "[482,   135] loss: 26810.569\n",
      "[482,   140] loss: 24469.336\n",
      "[482,   145] loss: 27849.336\n",
      "[482,   150] loss: 24482.195\n",
      "[482,   155] loss: 28581.331\n",
      "[482,   160] loss: 20687.238\n",
      "[482,   165] loss: 28015.935\n",
      "[482,   170] loss: 16375.162\n",
      "[482,   175] loss: 32565.888\n",
      "[482,   180] loss: 34163.492\n",
      "[482,   185] loss: 21911.790\n",
      "[482,   190] loss: 29526.539\n",
      "[482,   195] loss: 21754.456\n",
      "[482,   200] loss: 20848.298\n",
      "[482,   205] loss: 17062.049\n",
      "[482,   210] loss: 38582.562\n",
      "[482,   215] loss: 22037.948\n",
      "[482,   220] loss: 24574.569\n",
      "[482,   225] loss: 26773.727\n",
      "[482,   230] loss: 23224.145\n",
      "[483,     5] loss: 22687.676\n",
      "[483,    10] loss: 23601.838\n",
      "[483,    15] loss: 33564.930\n",
      "[483,    20] loss: 27400.609\n",
      "[483,    25] loss: 12854.018\n",
      "[483,    30] loss: 20533.245\n",
      "[483,    35] loss: 21493.483\n",
      "[483,    40] loss: 30505.023\n",
      "[483,    45] loss: 31484.331\n",
      "[483,    50] loss: 31106.246\n",
      "[483,    55] loss: 17647.652\n",
      "[483,    60] loss: 26308.768\n",
      "[483,    65] loss: 27563.770\n",
      "[483,    70] loss: 19267.883\n",
      "[483,    75] loss: 29660.240\n",
      "[483,    80] loss: 27102.058\n",
      "[483,    85] loss: 21280.565\n",
      "[483,    90] loss: 29617.271\n",
      "[483,    95] loss: 21137.668\n",
      "[483,   100] loss: 16412.289\n",
      "[483,   105] loss: 28671.082\n",
      "[483,   110] loss: 23171.774\n",
      "[483,   115] loss: 40555.702\n",
      "[483,   120] loss: 28193.866\n",
      "[483,   125] loss: 15497.829\n",
      "[483,   130] loss: 50151.106\n",
      "[483,   135] loss: 27693.537\n",
      "[483,   140] loss: 25243.500\n",
      "[483,   145] loss: 25695.812\n",
      "[483,   150] loss: 21496.378\n",
      "[483,   155] loss: 24053.046\n",
      "[483,   160] loss: 24521.882\n",
      "[483,   165] loss: 27701.600\n",
      "[483,   170] loss: 18620.889\n",
      "[483,   175] loss: 22957.049\n",
      "[483,   180] loss: 18193.436\n",
      "[483,   185] loss: 20976.120\n",
      "[483,   190] loss: 19706.570\n",
      "[483,   195] loss: 17147.295\n",
      "[483,   200] loss: 31753.598\n",
      "[483,   205] loss: 30202.940\n",
      "[483,   210] loss: 18210.613\n",
      "[483,   215] loss: 25649.231\n",
      "[483,   220] loss: 27335.928\n",
      "[483,   225] loss: 41397.321\n",
      "[483,   230] loss: 20286.569\n",
      "[484,     5] loss: 20335.286\n",
      "[484,    10] loss: 28333.763\n",
      "[484,    15] loss: 22724.620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[484,    20] loss: 25058.983\n",
      "[484,    25] loss: 19775.329\n",
      "[484,    30] loss: 21070.214\n",
      "[484,    35] loss: 19921.348\n",
      "[484,    40] loss: 28273.125\n",
      "[484,    45] loss: 24906.139\n",
      "[484,    50] loss: 19079.925\n",
      "[484,    55] loss: 30036.655\n",
      "[484,    60] loss: 28697.342\n",
      "[484,    65] loss: 38718.919\n",
      "[484,    70] loss: 16504.105\n",
      "[484,    75] loss: 32774.936\n",
      "[484,    80] loss: 20729.147\n",
      "[484,    85] loss: 22088.086\n",
      "[484,    90] loss: 14791.543\n",
      "[484,    95] loss: 23578.924\n",
      "[484,   100] loss: 26869.830\n",
      "[484,   105] loss: 23973.546\n",
      "[484,   110] loss: 24381.559\n",
      "[484,   115] loss: 33042.671\n",
      "[484,   120] loss: 24116.749\n",
      "[484,   125] loss: 37300.756\n",
      "[484,   130] loss: 25498.675\n",
      "[484,   135] loss: 20354.943\n",
      "[484,   140] loss: 20596.745\n",
      "[484,   145] loss: 28764.670\n",
      "[484,   150] loss: 31933.288\n",
      "[484,   155] loss: 27160.412\n",
      "[484,   160] loss: 31947.051\n",
      "[484,   165] loss: 23186.808\n",
      "[484,   170] loss: 25629.906\n",
      "[484,   175] loss: 17183.376\n",
      "[484,   180] loss: 20483.467\n",
      "[484,   185] loss: 23071.408\n",
      "[484,   190] loss: 15767.122\n",
      "[484,   195] loss: 33900.772\n",
      "[484,   200] loss: 22135.138\n",
      "[484,   205] loss: 32446.828\n",
      "[484,   210] loss: 30387.239\n",
      "[484,   215] loss: 23424.158\n",
      "[484,   220] loss: 25712.928\n",
      "[484,   225] loss: 38585.989\n",
      "[484,   230] loss: 22521.222\n",
      "[485,     5] loss: 28292.080\n",
      "[485,    10] loss: 22402.053\n",
      "[485,    15] loss: 18875.064\n",
      "[485,    20] loss: 26938.034\n",
      "[485,    25] loss: 29477.509\n",
      "[485,    30] loss: 23189.298\n",
      "[485,    35] loss: 16422.247\n",
      "[485,    40] loss: 20065.474\n",
      "[485,    45] loss: 16945.382\n",
      "[485,    50] loss: 25306.870\n",
      "[485,    55] loss: 28554.048\n",
      "[485,    60] loss: 28152.727\n",
      "[485,    65] loss: 46964.668\n",
      "[485,    70] loss: 17533.587\n",
      "[485,    75] loss: 19919.267\n",
      "[485,    80] loss: 19095.195\n",
      "[485,    85] loss: 27243.556\n",
      "[485,    90] loss: 26180.969\n",
      "[485,    95] loss: 25622.620\n",
      "[485,   100] loss: 23671.606\n",
      "[485,   105] loss: 24408.580\n",
      "[485,   110] loss: 23309.068\n",
      "[485,   115] loss: 21594.378\n",
      "[485,   120] loss: 28798.993\n",
      "[485,   125] loss: 21553.169\n",
      "[485,   130] loss: 20158.161\n",
      "[485,   135] loss: 24242.784\n",
      "[485,   140] loss: 27781.702\n",
      "[485,   145] loss: 30088.557\n",
      "[485,   150] loss: 28570.045\n",
      "[485,   155] loss: 35369.507\n",
      "[485,   160] loss: 22013.721\n",
      "[485,   165] loss: 19146.365\n",
      "[485,   170] loss: 20835.223\n",
      "[485,   175] loss: 42753.003\n",
      "[485,   180] loss: 21172.000\n",
      "[485,   185] loss: 47832.965\n",
      "[485,   190] loss: 29509.856\n",
      "[485,   195] loss: 20121.758\n",
      "[485,   200] loss: 24793.291\n",
      "[485,   205] loss: 23602.375\n",
      "[485,   210] loss: 24524.144\n",
      "[485,   215] loss: 24774.188\n",
      "[485,   220] loss: 21344.029\n",
      "[485,   225] loss: 25893.285\n",
      "[485,   230] loss: 23495.464\n",
      "[486,     5] loss: 25022.184\n",
      "[486,    10] loss: 27568.664\n",
      "[486,    15] loss: 20514.626\n",
      "[486,    20] loss: 23788.101\n",
      "[486,    25] loss: 20193.599\n",
      "[486,    30] loss: 28465.316\n",
      "[486,    35] loss: 20085.749\n",
      "[486,    40] loss: 53109.054\n",
      "[486,    45] loss: 38808.846\n",
      "[486,    50] loss: 21173.040\n",
      "[486,    55] loss: 22401.097\n",
      "[486,    60] loss: 42198.364\n",
      "[486,    65] loss: 36807.841\n",
      "[486,    70] loss: 20613.791\n",
      "[486,    75] loss: 17415.875\n",
      "[486,    80] loss: 26762.172\n",
      "[486,    85] loss: 23512.808\n",
      "[486,    90] loss: 20059.323\n",
      "[486,    95] loss: 26855.269\n",
      "[486,   100] loss: 19744.838\n",
      "[486,   105] loss: 17060.476\n",
      "[486,   110] loss: 24049.579\n",
      "[486,   115] loss: 17590.146\n",
      "[486,   120] loss: 21046.320\n",
      "[486,   125] loss: 21359.040\n",
      "[486,   130] loss: 21968.778\n",
      "[486,   135] loss: 31771.451\n",
      "[486,   140] loss: 23467.058\n",
      "[486,   145] loss: 34293.162\n",
      "[486,   150] loss: 18655.688\n",
      "[486,   155] loss: 22114.556\n",
      "[486,   160] loss: 15382.776\n",
      "[486,   165] loss: 32567.781\n",
      "[486,   170] loss: 25226.422\n",
      "[486,   175] loss: 25573.195\n",
      "[486,   180] loss: 29202.735\n",
      "[486,   185] loss: 27707.276\n",
      "[486,   190] loss: 12422.363\n",
      "[486,   195] loss: 17864.932\n",
      "[486,   200] loss: 24991.222\n",
      "[486,   205] loss: 28611.415\n",
      "[486,   210] loss: 30600.489\n",
      "[486,   215] loss: 29858.521\n",
      "[486,   220] loss: 29002.064\n",
      "[486,   225] loss: 24066.091\n",
      "[486,   230] loss: 21397.286\n",
      "[487,     5] loss: 26796.172\n",
      "[487,    10] loss: 24164.010\n",
      "[487,    15] loss: 22047.117\n",
      "[487,    20] loss: 16948.866\n",
      "[487,    25] loss: 36424.704\n",
      "[487,    30] loss: 32737.701\n",
      "[487,    35] loss: 25193.691\n",
      "[487,    40] loss: 24142.362\n",
      "[487,    45] loss: 22905.565\n",
      "[487,    50] loss: 22205.666\n",
      "[487,    55] loss: 16717.166\n",
      "[487,    60] loss: 21317.576\n",
      "[487,    65] loss: 22640.315\n",
      "[487,    70] loss: 25164.373\n",
      "[487,    75] loss: 25433.821\n",
      "[487,    80] loss: 59292.015\n",
      "[487,    85] loss: 24589.499\n",
      "[487,    90] loss: 16014.106\n",
      "[487,    95] loss: 18351.358\n",
      "[487,   100] loss: 37520.511\n",
      "[487,   105] loss: 21501.183\n",
      "[487,   110] loss: 29178.775\n",
      "[487,   115] loss: 21409.143\n",
      "[487,   120] loss: 25764.898\n",
      "[487,   125] loss: 17760.901\n",
      "[487,   130] loss: 26586.837\n",
      "[487,   135] loss: 22023.033\n",
      "[487,   140] loss: 27187.046\n",
      "[487,   145] loss: 32536.062\n",
      "[487,   150] loss: 25481.212\n",
      "[487,   155] loss: 20581.611\n",
      "[487,   160] loss: 22502.830\n",
      "[487,   165] loss: 32390.271\n",
      "[487,   170] loss: 19510.826\n",
      "[487,   175] loss: 22555.105\n",
      "[487,   180] loss: 21978.846\n",
      "[487,   185] loss: 11301.507\n",
      "[487,   190] loss: 35797.386\n",
      "[487,   195] loss: 40043.964\n",
      "[487,   200] loss: 32985.886\n",
      "[487,   205] loss: 16728.782\n",
      "[487,   210] loss: 25800.321\n",
      "[487,   215] loss: 23386.361\n",
      "[487,   220] loss: 20149.423\n",
      "[487,   225] loss: 19476.234\n",
      "[487,   230] loss: 24888.515\n",
      "[488,     5] loss: 24303.185\n",
      "[488,    10] loss: 28535.490\n",
      "[488,    15] loss: 24316.896\n",
      "[488,    20] loss: 19926.619\n",
      "[488,    25] loss: 18487.641\n",
      "[488,    30] loss: 24632.330\n",
      "[488,    35] loss: 14321.142\n",
      "[488,    40] loss: 35409.032\n",
      "[488,    45] loss: 22348.121\n",
      "[488,    50] loss: 23490.932\n",
      "[488,    55] loss: 44876.356\n",
      "[488,    60] loss: 21504.524\n",
      "[488,    65] loss: 44879.084\n",
      "[488,    70] loss: 26565.964\n",
      "[488,    75] loss: 23020.630\n",
      "[488,    80] loss: 21693.123\n",
      "[488,    85] loss: 19307.049\n",
      "[488,    90] loss: 18727.558\n",
      "[488,    95] loss: 36519.572\n",
      "[488,   100] loss: 23780.229\n",
      "[488,   105] loss: 25029.893\n",
      "[488,   110] loss: 33897.852\n",
      "[488,   115] loss: 25119.653\n",
      "[488,   120] loss: 26600.654\n",
      "[488,   125] loss: 13971.425\n",
      "[488,   130] loss: 33900.005\n",
      "[488,   135] loss: 23470.422\n",
      "[488,   140] loss: 23569.192\n",
      "[488,   145] loss: 17829.043\n",
      "[488,   150] loss: 27144.738\n",
      "[488,   155] loss: 29028.548\n",
      "[488,   160] loss: 28644.063\n",
      "[488,   165] loss: 28078.025\n",
      "[488,   170] loss: 33416.332\n",
      "[488,   175] loss: 18072.464\n",
      "[488,   180] loss: 26143.575\n",
      "[488,   185] loss: 23640.492\n",
      "[488,   190] loss: 28856.068\n",
      "[488,   195] loss: 23774.901\n",
      "[488,   200] loss: 25607.612\n",
      "[488,   205] loss: 23559.137\n",
      "[488,   210] loss: 14200.708\n",
      "[488,   215] loss: 32700.257\n",
      "[488,   220] loss: 14345.213\n",
      "[488,   225] loss: 22815.097\n",
      "[488,   230] loss: 24239.730\n",
      "[489,     5] loss: 38104.307\n",
      "[489,    10] loss: 25826.327\n",
      "[489,    15] loss: 26028.542\n",
      "[489,    20] loss: 17695.122\n",
      "[489,    25] loss: 17282.706\n",
      "[489,    30] loss: 19521.567\n",
      "[489,    35] loss: 18103.496\n",
      "[489,    40] loss: 19764.928\n",
      "[489,    45] loss: 27415.759\n",
      "[489,    50] loss: 23487.840\n",
      "[489,    55] loss: 22836.775\n",
      "[489,    60] loss: 25492.925\n",
      "[489,    65] loss: 29547.304\n",
      "[489,    70] loss: 14492.121\n",
      "[489,    75] loss: 20308.152\n",
      "[489,    80] loss: 23658.924\n",
      "[489,    85] loss: 41148.444\n",
      "[489,    90] loss: 25478.059\n",
      "[489,    95] loss: 18267.977\n",
      "[489,   100] loss: 20187.853\n",
      "[489,   105] loss: 27758.727\n",
      "[489,   110] loss: 25427.578\n",
      "[489,   115] loss: 34964.869\n",
      "[489,   120] loss: 31270.062\n",
      "[489,   125] loss: 22544.925\n",
      "[489,   130] loss: 31974.241\n",
      "[489,   135] loss: 34966.136\n",
      "[489,   140] loss: 47678.205\n",
      "[489,   145] loss: 26335.970\n",
      "[489,   150] loss: 25789.887\n",
      "[489,   155] loss: 32412.924\n",
      "[489,   160] loss: 18305.878\n",
      "[489,   165] loss: 23630.884\n",
      "[489,   170] loss: 29542.630\n",
      "[489,   175] loss: 18418.218\n",
      "[489,   180] loss: 15571.406\n",
      "[489,   185] loss: 28443.390\n",
      "[489,   190] loss: 19378.654\n",
      "[489,   195] loss: 18819.217\n",
      "[489,   200] loss: 27078.671\n",
      "[489,   205] loss: 11335.477\n",
      "[489,   210] loss: 18911.119\n",
      "[489,   215] loss: 24169.301\n",
      "[489,   220] loss: 29809.060\n",
      "[489,   225] loss: 34134.889\n",
      "[489,   230] loss: 32332.265\n",
      "[490,     5] loss: 31085.173\n",
      "[490,    10] loss: 23853.492\n",
      "[490,    15] loss: 21581.969\n",
      "[490,    20] loss: 31646.541\n",
      "[490,    25] loss: 21166.145\n",
      "[490,    30] loss: 23757.670\n",
      "[490,    35] loss: 16820.756\n",
      "[490,    40] loss: 20722.565\n",
      "[490,    45] loss: 34870.588\n",
      "[490,    50] loss: 29368.689\n",
      "[490,    55] loss: 34977.859\n",
      "[490,    60] loss: 20872.408\n",
      "[490,    65] loss: 21457.596\n",
      "[490,    70] loss: 27084.156\n",
      "[490,    75] loss: 26869.534\n",
      "[490,    80] loss: 21472.330\n",
      "[490,    85] loss: 27172.728\n",
      "[490,    90] loss: 31280.801\n",
      "[490,    95] loss: 34032.570\n",
      "[490,   100] loss: 27845.196\n",
      "[490,   105] loss: 17315.818\n",
      "[490,   110] loss: 26607.332\n",
      "[490,   115] loss: 22672.169\n",
      "[490,   120] loss: 23187.111\n",
      "[490,   125] loss: 31416.229\n",
      "[490,   130] loss: 23360.146\n",
      "[490,   135] loss: 50547.601\n",
      "[490,   140] loss: 17792.823\n",
      "[490,   145] loss: 23878.318\n",
      "[490,   150] loss: 26735.645\n",
      "[490,   155] loss: 24520.722\n",
      "[490,   160] loss: 25246.664\n",
      "[490,   165] loss: 22565.423\n",
      "[490,   170] loss: 23012.140\n",
      "[490,   175] loss: 26321.029\n",
      "[490,   180] loss: 17294.984\n",
      "[490,   185] loss: 28840.824\n",
      "[490,   190] loss: 36796.928\n",
      "[490,   195] loss: 25172.856\n",
      "[490,   200] loss: 25638.083\n",
      "[490,   205] loss: 16963.841\n",
      "[490,   210] loss: 27588.920\n",
      "[490,   215] loss: 18150.730\n",
      "[490,   220] loss: 15522.153\n",
      "[490,   225] loss: 20246.686\n",
      "[490,   230] loss: 16470.368\n",
      "[491,     5] loss: 15004.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[491,    10] loss: 20841.960\n",
      "[491,    15] loss: 32548.679\n",
      "[491,    20] loss: 24191.004\n",
      "[491,    25] loss: 23469.734\n",
      "[491,    30] loss: 39227.041\n",
      "[491,    35] loss: 22929.678\n",
      "[491,    40] loss: 23743.854\n",
      "[491,    45] loss: 17580.833\n",
      "[491,    50] loss: 21503.229\n",
      "[491,    55] loss: 27145.184\n",
      "[491,    60] loss: 40547.176\n",
      "[491,    65] loss: 30582.799\n",
      "[491,    70] loss: 24418.299\n",
      "[491,    75] loss: 21090.281\n",
      "[491,    80] loss: 24571.718\n",
      "[491,    85] loss: 25247.689\n",
      "[491,    90] loss: 29887.454\n",
      "[491,    95] loss: 19154.472\n",
      "[491,   100] loss: 21779.583\n",
      "[491,   105] loss: 24612.321\n",
      "[491,   110] loss: 24298.024\n",
      "[491,   115] loss: 34587.852\n",
      "[491,   120] loss: 35350.467\n",
      "[491,   125] loss: 33766.584\n",
      "[491,   130] loss: 22857.402\n",
      "[491,   135] loss: 24482.796\n",
      "[491,   140] loss: 27811.428\n",
      "[491,   145] loss: 33240.242\n",
      "[491,   150] loss: 24479.232\n",
      "[491,   155] loss: 17937.527\n",
      "[491,   160] loss: 18805.572\n",
      "[491,   165] loss: 33403.795\n",
      "[491,   170] loss: 18596.237\n",
      "[491,   175] loss: 30512.108\n",
      "[491,   180] loss: 26305.564\n",
      "[491,   185] loss: 23611.746\n",
      "[491,   190] loss: 25659.027\n",
      "[491,   195] loss: 17477.323\n",
      "[491,   200] loss: 18385.230\n",
      "[491,   205] loss: 24708.119\n",
      "[491,   210] loss: 21883.513\n",
      "[491,   215] loss: 24706.373\n",
      "[491,   220] loss: 36109.420\n",
      "[491,   225] loss: 17061.868\n",
      "[491,   230] loss: 25102.845\n",
      "[492,     5] loss: 23226.183\n",
      "[492,    10] loss: 18392.214\n",
      "[492,    15] loss: 24125.085\n",
      "[492,    20] loss: 18999.748\n",
      "[492,    25] loss: 23905.604\n",
      "[492,    30] loss: 28386.531\n",
      "[492,    35] loss: 18859.656\n",
      "[492,    40] loss: 41778.505\n",
      "[492,    45] loss: 26436.612\n",
      "[492,    50] loss: 19770.399\n",
      "[492,    55] loss: 26716.495\n",
      "[492,    60] loss: 25677.444\n",
      "[492,    65] loss: 23915.614\n",
      "[492,    70] loss: 17405.125\n",
      "[492,    75] loss: 17033.012\n",
      "[492,    80] loss: 25966.150\n",
      "[492,    85] loss: 39331.350\n",
      "[492,    90] loss: 23923.293\n",
      "[492,    95] loss: 22034.133\n",
      "[492,   100] loss: 23254.573\n",
      "[492,   105] loss: 27276.555\n",
      "[492,   110] loss: 27950.092\n",
      "[492,   115] loss: 25566.385\n",
      "[492,   120] loss: 18634.917\n",
      "[492,   125] loss: 33945.702\n",
      "[492,   130] loss: 24388.618\n",
      "[492,   135] loss: 21277.857\n",
      "[492,   140] loss: 31428.280\n",
      "[492,   145] loss: 18961.968\n",
      "[492,   150] loss: 26795.743\n",
      "[492,   155] loss: 34707.812\n",
      "[492,   160] loss: 14567.831\n",
      "[492,   165] loss: 21168.623\n",
      "[492,   170] loss: 24609.919\n",
      "[492,   175] loss: 21535.901\n",
      "[492,   180] loss: 23962.722\n",
      "[492,   185] loss: 30198.704\n",
      "[492,   190] loss: 27165.430\n",
      "[492,   195] loss: 33518.429\n",
      "[492,   200] loss: 32946.276\n",
      "[492,   205] loss: 45793.988\n",
      "[492,   210] loss: 20803.382\n",
      "[492,   215] loss: 27275.935\n",
      "[492,   220] loss: 24717.113\n",
      "[492,   225] loss: 18341.271\n",
      "[492,   230] loss: 21548.548\n",
      "[493,     5] loss: 18797.887\n",
      "[493,    10] loss: 23157.013\n",
      "[493,    15] loss: 26795.168\n",
      "[493,    20] loss: 18273.138\n",
      "[493,    25] loss: 30197.738\n",
      "[493,    30] loss: 20236.239\n",
      "[493,    35] loss: 13931.372\n",
      "[493,    40] loss: 30335.566\n",
      "[493,    45] loss: 21754.856\n",
      "[493,    50] loss: 15117.595\n",
      "[493,    55] loss: 22439.445\n",
      "[493,    60] loss: 34836.557\n",
      "[493,    65] loss: 18640.735\n",
      "[493,    70] loss: 35893.036\n",
      "[493,    75] loss: 18814.750\n",
      "[493,    80] loss: 32694.693\n",
      "[493,    85] loss: 26870.449\n",
      "[493,    90] loss: 27360.541\n",
      "[493,    95] loss: 25013.683\n",
      "[493,   100] loss: 30938.391\n",
      "[493,   105] loss: 21130.076\n",
      "[493,   110] loss: 26855.455\n",
      "[493,   115] loss: 26133.608\n",
      "[493,   120] loss: 25849.423\n",
      "[493,   125] loss: 24107.281\n",
      "[493,   130] loss: 49320.679\n",
      "[493,   135] loss: 20738.391\n",
      "[493,   140] loss: 22515.987\n",
      "[493,   145] loss: 24276.085\n",
      "[493,   150] loss: 31347.201\n",
      "[493,   155] loss: 23382.912\n",
      "[493,   160] loss: 22281.137\n",
      "[493,   165] loss: 21670.907\n",
      "[493,   170] loss: 40119.678\n",
      "[493,   175] loss: 29078.070\n",
      "[493,   180] loss: 20984.470\n",
      "[493,   185] loss: 22887.404\n",
      "[493,   190] loss: 19077.115\n",
      "[493,   195] loss: 22781.537\n",
      "[493,   200] loss: 31867.946\n",
      "[493,   205] loss: 29227.701\n",
      "[493,   210] loss: 32624.695\n",
      "[493,   215] loss: 20673.823\n",
      "[493,   220] loss: 31520.544\n",
      "[493,   225] loss: 19527.538\n",
      "[493,   230] loss: 14342.600\n",
      "[494,     5] loss: 23029.949\n",
      "[494,    10] loss: 23659.523\n",
      "[494,    15] loss: 29251.244\n",
      "[494,    20] loss: 27111.805\n",
      "[494,    25] loss: 20808.713\n",
      "[494,    30] loss: 18373.869\n",
      "[494,    35] loss: 38815.977\n",
      "[494,    40] loss: 21111.934\n",
      "[494,    45] loss: 18854.221\n",
      "[494,    50] loss: 23005.109\n",
      "[494,    55] loss: 22026.647\n",
      "[494,    60] loss: 21624.614\n",
      "[494,    65] loss: 26689.510\n",
      "[494,    70] loss: 24839.556\n",
      "[494,    75] loss: 25285.262\n",
      "[494,    80] loss: 28060.998\n",
      "[494,    85] loss: 16344.890\n",
      "[494,    90] loss: 30424.068\n",
      "[494,    95] loss: 30774.910\n",
      "[494,   100] loss: 28550.243\n",
      "[494,   105] loss: 19971.748\n",
      "[494,   110] loss: 35912.418\n",
      "[494,   115] loss: 19586.081\n",
      "[494,   120] loss: 28163.371\n",
      "[494,   125] loss: 28660.961\n",
      "[494,   130] loss: 17131.223\n",
      "[494,   135] loss: 21164.350\n",
      "[494,   140] loss: 18274.317\n",
      "[494,   145] loss: 28238.041\n",
      "[494,   150] loss: 14255.211\n",
      "[494,   155] loss: 28799.863\n",
      "[494,   160] loss: 30756.373\n",
      "[494,   165] loss: 24564.135\n",
      "[494,   170] loss: 25015.082\n",
      "[494,   175] loss: 27362.634\n",
      "[494,   180] loss: 32099.589\n",
      "[494,   185] loss: 17120.301\n",
      "[494,   190] loss: 53076.228\n",
      "[494,   195] loss: 30182.177\n",
      "[494,   200] loss: 26989.869\n",
      "[494,   205] loss: 21965.174\n",
      "[494,   210] loss: 20834.373\n",
      "[494,   215] loss: 21220.436\n",
      "[494,   220] loss: 30085.220\n",
      "[494,   225] loss: 18463.809\n",
      "[494,   230] loss: 29352.045\n",
      "[495,     5] loss: 21534.774\n",
      "[495,    10] loss: 44640.607\n",
      "[495,    15] loss: 29814.720\n",
      "[495,    20] loss: 21688.665\n",
      "[495,    25] loss: 38925.884\n",
      "[495,    30] loss: 21326.558\n",
      "[495,    35] loss: 21907.244\n",
      "[495,    40] loss: 24375.237\n",
      "[495,    45] loss: 25567.403\n",
      "[495,    50] loss: 19529.009\n",
      "[495,    55] loss: 27936.619\n",
      "[495,    60] loss: 25846.694\n",
      "[495,    65] loss: 18926.678\n",
      "[495,    70] loss: 17187.134\n",
      "[495,    75] loss: 27367.655\n",
      "[495,    80] loss: 30354.625\n",
      "[495,    85] loss: 39556.960\n",
      "[495,    90] loss: 21525.074\n",
      "[495,    95] loss: 24616.661\n",
      "[495,   100] loss: 17438.251\n",
      "[495,   105] loss: 23474.914\n",
      "[495,   110] loss: 24183.222\n",
      "[495,   115] loss: 15067.599\n",
      "[495,   120] loss: 18042.781\n",
      "[495,   125] loss: 22300.960\n",
      "[495,   130] loss: 26708.292\n",
      "[495,   135] loss: 22799.879\n",
      "[495,   140] loss: 24742.262\n",
      "[495,   145] loss: 19210.403\n",
      "[495,   150] loss: 17618.734\n",
      "[495,   155] loss: 20505.134\n",
      "[495,   160] loss: 23980.773\n",
      "[495,   165] loss: 20743.727\n",
      "[495,   170] loss: 35190.528\n",
      "[495,   175] loss: 21230.982\n",
      "[495,   180] loss: 28349.395\n",
      "[495,   185] loss: 34159.899\n",
      "[495,   190] loss: 23403.378\n",
      "[495,   195] loss: 30481.142\n",
      "[495,   200] loss: 18772.371\n",
      "[495,   205] loss: 29034.646\n",
      "[495,   210] loss: 32512.003\n",
      "[495,   215] loss: 19931.538\n",
      "[495,   220] loss: 23758.677\n",
      "[495,   225] loss: 28932.303\n",
      "[495,   230] loss: 31231.903\n",
      "[496,     5] loss: 28006.588\n",
      "[496,    10] loss: 18138.296\n",
      "[496,    15] loss: 35011.990\n",
      "[496,    20] loss: 13552.124\n",
      "[496,    25] loss: 38870.057\n",
      "[496,    30] loss: 21908.922\n",
      "[496,    35] loss: 17366.308\n",
      "[496,    40] loss: 29044.594\n",
      "[496,    45] loss: 31812.063\n",
      "[496,    50] loss: 29486.458\n",
      "[496,    55] loss: 27072.093\n",
      "[496,    60] loss: 22750.520\n",
      "[496,    65] loss: 19779.227\n",
      "[496,    70] loss: 32303.659\n",
      "[496,    75] loss: 29539.844\n",
      "[496,    80] loss: 20538.033\n",
      "[496,    85] loss: 41737.709\n",
      "[496,    90] loss: 23738.026\n",
      "[496,    95] loss: 16322.900\n",
      "[496,   100] loss: 23098.672\n",
      "[496,   105] loss: 19111.713\n",
      "[496,   110] loss: 24902.694\n",
      "[496,   115] loss: 20425.931\n",
      "[496,   120] loss: 17750.505\n",
      "[496,   125] loss: 19768.108\n",
      "[496,   130] loss: 28910.508\n",
      "[496,   135] loss: 28943.760\n",
      "[496,   140] loss: 24117.097\n",
      "[496,   145] loss: 28823.034\n",
      "[496,   150] loss: 24808.552\n",
      "[496,   155] loss: 22834.052\n",
      "[496,   160] loss: 22490.330\n",
      "[496,   165] loss: 20679.818\n",
      "[496,   170] loss: 30658.454\n",
      "[496,   175] loss: 23602.265\n",
      "[496,   180] loss: 21240.537\n",
      "[496,   185] loss: 24183.267\n",
      "[496,   190] loss: 17622.296\n",
      "[496,   195] loss: 25556.394\n",
      "[496,   200] loss: 36865.472\n",
      "[496,   205] loss: 27628.320\n",
      "[496,   210] loss: 24248.000\n",
      "[496,   215] loss: 32562.579\n",
      "[496,   220] loss: 30196.361\n",
      "[496,   225] loss: 34412.648\n",
      "[496,   230] loss: 18646.467\n",
      "[497,     5] loss: 24468.244\n",
      "[497,    10] loss: 28585.830\n",
      "[497,    15] loss: 21103.063\n",
      "[497,    20] loss: 18827.961\n",
      "[497,    25] loss: 27657.897\n",
      "[497,    30] loss: 30316.292\n",
      "[497,    35] loss: 18456.124\n",
      "[497,    40] loss: 29930.084\n",
      "[497,    45] loss: 24859.216\n",
      "[497,    50] loss: 56043.498\n",
      "[497,    55] loss: 22100.938\n",
      "[497,    60] loss: 26906.312\n",
      "[497,    65] loss: 41077.423\n",
      "[497,    70] loss: 24868.057\n",
      "[497,    75] loss: 23071.457\n",
      "[497,    80] loss: 24802.449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[497,    85] loss: 25500.409\n",
      "[497,    90] loss: 18654.044\n",
      "[497,    95] loss: 21532.932\n",
      "[497,   100] loss: 22197.791\n",
      "[497,   105] loss: 32863.897\n",
      "[497,   110] loss: 27706.325\n",
      "[497,   115] loss: 18095.709\n",
      "[497,   120] loss: 23315.416\n",
      "[497,   125] loss: 25127.900\n",
      "[497,   130] loss: 16649.959\n",
      "[497,   135] loss: 30071.800\n",
      "[497,   140] loss: 26366.332\n",
      "[497,   145] loss: 18103.763\n",
      "[497,   150] loss: 20620.378\n",
      "[497,   155] loss: 18951.382\n",
      "[497,   160] loss: 23602.018\n",
      "[497,   165] loss: 21672.062\n",
      "[497,   170] loss: 30696.839\n",
      "[497,   175] loss: 24785.033\n",
      "[497,   180] loss: 33003.103\n",
      "[497,   185] loss: 27652.574\n",
      "[497,   190] loss: 23433.720\n",
      "[497,   195] loss: 19480.746\n",
      "[497,   200] loss: 22206.129\n",
      "[497,   205] loss: 24851.919\n",
      "[497,   210] loss: 20261.334\n",
      "[497,   215] loss: 29798.498\n",
      "[497,   220] loss: 39724.204\n",
      "[497,   225] loss: 19467.138\n",
      "[497,   230] loss: 18749.856\n",
      "[498,     5] loss: 52292.723\n",
      "[498,    10] loss: 13903.588\n",
      "[498,    15] loss: 10982.421\n",
      "[498,    20] loss: 28741.747\n",
      "[498,    25] loss: 40668.110\n",
      "[498,    30] loss: 30609.144\n",
      "[498,    35] loss: 23066.743\n",
      "[498,    40] loss: 38277.434\n",
      "[498,    45] loss: 21975.899\n",
      "[498,    50] loss: 21664.456\n",
      "[498,    55] loss: 18450.957\n",
      "[498,    60] loss: 23959.009\n",
      "[498,    65] loss: 21621.985\n",
      "[498,    70] loss: 20634.031\n",
      "[498,    75] loss: 24261.489\n",
      "[498,    80] loss: 22213.425\n",
      "[498,    85] loss: 23861.912\n",
      "[498,    90] loss: 18224.256\n",
      "[498,    95] loss: 25807.544\n",
      "[498,   100] loss: 18239.938\n",
      "[498,   105] loss: 23713.503\n",
      "[498,   110] loss: 19917.078\n",
      "[498,   115] loss: 23807.964\n",
      "[498,   120] loss: 28041.554\n",
      "[498,   125] loss: 35296.162\n",
      "[498,   130] loss: 26315.136\n",
      "[498,   135] loss: 29965.059\n",
      "[498,   140] loss: 19351.674\n",
      "[498,   145] loss: 26828.430\n",
      "[498,   150] loss: 21327.942\n",
      "[498,   155] loss: 28171.274\n",
      "[498,   160] loss: 21303.946\n",
      "[498,   165] loss: 25904.945\n",
      "[498,   170] loss: 23099.605\n",
      "[498,   175] loss: 22516.623\n",
      "[498,   180] loss: 24616.481\n",
      "[498,   185] loss: 24513.516\n",
      "[498,   190] loss: 23580.471\n",
      "[498,   195] loss: 37735.152\n",
      "[498,   200] loss: 23836.935\n",
      "[498,   205] loss: 31970.587\n",
      "[498,   210] loss: 29463.966\n",
      "[498,   215] loss: 22806.660\n",
      "[498,   220] loss: 16792.248\n",
      "[498,   225] loss: 20331.974\n",
      "[498,   230] loss: 27479.049\n",
      "[499,     5] loss: 17390.645\n",
      "[499,    10] loss: 16491.270\n",
      "[499,    15] loss: 30635.873\n",
      "[499,    20] loss: 22051.763\n",
      "[499,    25] loss: 26836.447\n",
      "[499,    30] loss: 22890.440\n",
      "[499,    35] loss: 28347.655\n",
      "[499,    40] loss: 38224.373\n",
      "[499,    45] loss: 18861.624\n",
      "[499,    50] loss: 26746.409\n",
      "[499,    55] loss: 21559.112\n",
      "[499,    60] loss: 26633.867\n",
      "[499,    65] loss: 26047.114\n",
      "[499,    70] loss: 26242.335\n",
      "[499,    75] loss: 19481.751\n",
      "[499,    80] loss: 21968.796\n",
      "[499,    85] loss: 24213.624\n",
      "[499,    90] loss: 21085.109\n",
      "[499,    95] loss: 27075.979\n",
      "[499,   100] loss: 33348.610\n",
      "[499,   105] loss: 23069.429\n",
      "[499,   110] loss: 29056.097\n",
      "[499,   115] loss: 20501.060\n",
      "[499,   120] loss: 40949.139\n",
      "[499,   125] loss: 22107.306\n",
      "[499,   130] loss: 26731.730\n",
      "[499,   135] loss: 26440.085\n",
      "[499,   140] loss: 24766.652\n",
      "[499,   145] loss: 23148.109\n",
      "[499,   150] loss: 35643.428\n",
      "[499,   155] loss: 25316.553\n",
      "[499,   160] loss: 22737.941\n",
      "[499,   165] loss: 17480.704\n",
      "[499,   170] loss: 31899.221\n",
      "[499,   175] loss: 18062.396\n",
      "[499,   180] loss: 20399.199\n",
      "[499,   185] loss: 21751.534\n",
      "[499,   190] loss: 13749.203\n",
      "[499,   195] loss: 26997.430\n",
      "[499,   200] loss: 19035.611\n",
      "[499,   205] loss: 38490.273\n",
      "[499,   210] loss: 23816.910\n",
      "[499,   215] loss: 24974.108\n",
      "[499,   220] loss: 17362.286\n",
      "[499,   225] loss: 21508.024\n",
      "[499,   230] loss: 32487.127\n",
      "[500,     5] loss: 24469.978\n",
      "[500,    10] loss: 27738.236\n",
      "[500,    15] loss: 26658.946\n",
      "[500,    20] loss: 27919.627\n",
      "[500,    25] loss: 47120.204\n",
      "[500,    30] loss: 34547.912\n",
      "[500,    35] loss: 14336.513\n",
      "[500,    40] loss: 24702.960\n",
      "[500,    45] loss: 30815.805\n",
      "[500,    50] loss: 35092.471\n",
      "[500,    55] loss: 20560.734\n",
      "[500,    60] loss: 25076.207\n",
      "[500,    65] loss: 15907.195\n",
      "[500,    70] loss: 28950.651\n",
      "[500,    75] loss: 29607.418\n",
      "[500,    80] loss: 20188.693\n",
      "[500,    85] loss: 13822.648\n",
      "[500,    90] loss: 23469.674\n",
      "[500,    95] loss: 32233.621\n",
      "[500,   100] loss: 24616.677\n",
      "[500,   105] loss: 20586.989\n",
      "[500,   110] loss: 28193.412\n",
      "[500,   115] loss: 22383.056\n",
      "[500,   120] loss: 21406.370\n",
      "[500,   125] loss: 40508.790\n",
      "[500,   130] loss: 23101.999\n",
      "[500,   135] loss: 22165.725\n",
      "[500,   140] loss: 20190.909\n",
      "[500,   145] loss: 17272.345\n",
      "[500,   150] loss: 18741.919\n",
      "[500,   155] loss: 26086.662\n",
      "[500,   160] loss: 25914.544\n",
      "[500,   165] loss: 14070.515\n",
      "[500,   170] loss: 24433.700\n",
      "[500,   175] loss: 22038.724\n",
      "[500,   180] loss: 21605.687\n",
      "[500,   185] loss: 27795.219\n",
      "[500,   190] loss: 22415.120\n",
      "[500,   195] loss: 35677.714\n",
      "[500,   200] loss: 33497.740\n",
      "[500,   205] loss: 22904.263\n",
      "[500,   210] loss: 26863.882\n",
      "[500,   215] loss: 26026.398\n",
      "[500,   220] loss: 32957.517\n",
      "[500,   225] loss: 17742.154\n",
      "[500,   230] loss: 15437.849\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "losses1 = []\n",
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(traindata, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = ntmodel(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 5 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 5))\n",
    "            running_loss = 0.0\n",
    "    losses1.append(loss.item())\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvxtest = vxtest.astype(np.float32)\n",
    "fvytest = vytest.astype(np.float32)\n",
    "tytest = torch.from_numpy(fvytest)\n",
    "txtest = torch.from_numpy(fvxtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred2 = ntmodel(txtest)\n",
    "y_pred2 = y_pred2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3)\n",
    "axs[0].set_title('Actual value')\n",
    "axs[0].plot(vytest)\n",
    "axs[1].set_title('Predicted value')\n",
    "axs[1].plot(y_pred2)\n",
    "axs[2].set_title('Compared values')\n",
    "axs[2].plot(vytest)\n",
    "axs[2].plot(y_pred2)\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 25566.637146832192\n",
      "Mean Squared Error: 1671894716.5624826\n",
      "Root Mean Squared Error: 40888.8091849406\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Absolute Error:\", mean_absolute_error(vytest, y_pred2))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(vytest, y_pred2))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(vytest, y_pred2))  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
